<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Tutorials and short posts about programming, C++, Java, Assembly, Operating Systems Development, Compilers, ...">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Blog blog("Baptiste Wicht"); (old posts, page 29) | Blog blog("Baptiste Wicht");</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="http://baptiste-wicht.com/index-29.html">
<link rel="prev" href="index-30.html" type="text/html">
<link rel="next" href="index-28.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-2175227-7']);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script><link href="favicon.ico" rel="icon" type="image/x-icon">
<link rel="publisher" href="https://plus.google.com/+BaptisteWicht">
</head>
<body>

<!-- Menubar -->

<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<div class="container-fluid">
<!-- This keeps the margins nice -->
    <div class="row">
        <div class="col-sm-3 col-lg-2">
            <nav class="navbar navbar-inverse navbar-fixed-side"><div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="http://baptiste-wicht.com/">
                        <span id="blog-title">Blog blog("Baptiste Wicht");</span>
                    </a>
                </div>
<!-- /.navbar-header -->

                <div class="collapse navbar-collapse navbar-ex1-collapse">
                    <ul class="nav navbar-nav">
<li>
<a href="stories/about.html">About</a>
                </li>
<li>
<a href="stories/publications.html">Publications</a>
                </li>
<li>
<a href="stories/projects.html">Projects</a>
                </li>
<li>
<a href="categories/index.html">Tags</a>
                </li>
<li>
<a href="archive.html">Archives</a>
                </li>
<li>
<a href="http://feeds.feedburner.com/BaptisteWicht">RSS</a>


                            </li>
<li class="navbar-content">
                                <h3>Tags</h3>
                            </li>
                            <li class="navbar-empty">
                                <div id="tag_cloud_left_container" style="line-height: 18px !important;"></div>
                            </li>
                            <li class="navbar-block">


                        </li><li class="wicht-navbar-right">
                            <a target="_blank" title="Follow @wichtounet on Twitter" href="https://twitter.com/wichtounet">
                                <img src="assets/img/twitter.png" alt="Follow @wichtounet on Twitter"></a>
                        </li>

                        <li class="wicht-navbar-right">
                            <a target="_blank" title="Follow +BaptisteWicht on Google+" href="https://plus.google.com/+BaptisteWicht">
                                <img src="assets/img/google_plus.png" alt="Follow +BaptisteWicht on Google+"></a>
                        </li>


                    </ul>
</div>
<!-- /.navbar-collapse -->
            </nav>
</div> <!-- col -->
        <div class="col-sm-9 col-lg-10">
            <div id="content"></div>
            
        <article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2016/12/pvs-studio-on-cpp-library-review.html" class="u-url">PVS-Studio on C++ Library Review</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2016-12-20T09:40:12+01:00">2016-12-20 09:40</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>PVS-Studio is a commercial static analyzer for C, C++ and C#. It works in both
Windows and Linux.</p>
<p>It has been a long time since I wanted to test it on my projects. I contacted
The PVS-Studio team and they gave me a temporary license so that I can test the
tool and make a review.</p>
<p>I tried the static analyzer on my Expression Templates Library (ETL) project.
This is a heavily-templated C++ library. I tried it on Linux of course.</p>
<div class="section" id="usage-2">
<h2>Usage</h2>
<p>The installation is very simple, simply untar an archive and put the executables
in your PATH (or use absolute paths). There are also some deb and rpm packages
for some distributions. You need strace to make the analyzer work, it should be
available on any Linux platform.</p>
<p>The usage of PVS-Studio on Linux should be straightforward. First, you can use the
analyzer directly with make and it will detect the invocations of the compiler.
For instance, here is the command I used for ETL:</p>
<pre class="code text"><a name="rest_code_752c559723d548da94c2bd4eb5ff80c3-1"></a>pvs-studio-analyzer trace -- make -j5 debug/bin/etl_test
</pre>
<p>Note that you can use several threads without issues, which is really great.
There does not seem to be any slowdown at this stage, probably only collecting
compiler arguments.</p>
<p>This first step creates a strace_out file that will be used by the next stage.</p>
<p>Once, the compilation has been analyzed, you can generate the results with the
analyze function, for which you'll need a license. Here is what I did:</p>
<pre class="code text"><a name="rest_code_f580069643b4459c9ad1752698c2a779-1"></a>pvs-studio-analyzer analyze -l ~/pvs_studio/PVS-Studio.lic -j5
</pre>
<p>Unfortunately, this didn't work for me:</p>
<pre class="code text"><a name="rest_code_60665f89921a4298b44f12553e8ec726-1"></a>No compilation units found
<a name="rest_code_60665f89921a4298b44f12553e8ec726-2"></a>Analysis finished in 0:00:00.00
</pre>
<p>Apparently, it's not able to use the strace_out it generated itself...</p>
<p>Another possibility is to use the compilation database from clang to use
PVS-Studio. So I generated my compile_commands.json file again (it was not up to
date...) with <a class="reference external" href="https://github.com/rizsotto/Bear">Bear</a>. And then, you only
need to run the analyze step:</p>
<pre class="code text"><a name="rest_code_6b00f9eef835477b94c8c676bd66966c-1"></a>pvs-studio-analyzer analyze -l ~/pvs_studio/PVS-Studio.lic -j5
</pre>
<p>Make sure you have the same compiler configured than the one used to generate
the compilation database to avoid errors with compiler arguments.</p>
<p>Unfortunately, this just printed a load of crap on my console:</p>
<pre class="code text"><a name="rest_code_321fa4266d3d4657a65379ee2704342f-1"></a>(L8Pu(]-'Lo8h&gt;uo(_uv(uo2(-&gt;'2h_u(uo2(uvU2K h&gt;'o8a=}Lkk;x[G^%cuaa8acr[VS%
<a name="rest_code_321fa4266d3d4657a65379ee2704342f-2"></a>$ckUaoc8 c'8&gt;_-o-8&gt;U2cu/kau==-8&gt;c-=cU2]Uf=c U2=u%c&amp;kU__-&gt;j}c@uvu2%cJ
<a name="rest_code_321fa4266d3d4657a65379ee2704342f-3"></a>(L8Pu(]-'Lo8h&gt;uo(_uv(uo2(-&gt;'2h_u(uo2(uvU2K h&gt;'o8a=}Lkk;JVJ^%cuaa8acr[VS%
<a name="rest_code_321fa4266d3d4657a65379ee2704342f-4"></a>$ckUaoc8 c'8&gt;_-o-8&gt;U2cu/kau==-8&gt;c-=cU2]Uf=c U2=u%c&amp;kU__-&gt;j}c@uvu2%cJ
<a name="rest_code_321fa4266d3d4657a65379ee2704342f-5"></a>(L8Pu(]-'Lo8h&gt;uo(_uv(uo2(-&gt;'2h_u(uo2(uvU2K h&gt;'o8a=}Lkk;*[G^%cuaa8acr[VS%
<a name="rest_code_321fa4266d3d4657a65379ee2704342f-6"></a>$ckUaoc8 c'8&gt;_-o-8&gt;U2cu/kau==-8&gt;c-=cU2]Uf=c U2=u%c&amp;kU__-&gt;j}c@uvu2%cJ
<a name="rest_code_321fa4266d3d4657a65379ee2704342f-7"></a>(L8Pu(]-'Lo8h&gt;uo(_uv(uo2(-&gt;'2h_u(uo2(uvU2K h&gt;'o8a=}Lkk;b[b^%cuaa8acr[VS%
<a name="rest_code_321fa4266d3d4657a65379ee2704342f-8"></a>$ckUaoc8 c'8&gt;_-o-8&gt;U2cu/kau==-8&gt;c-=cU2]Uf=c U2=u%c&amp;kU__-&gt;j}c@uvu2%cJ
<a name="rest_code_321fa4266d3d4657a65379ee2704342f-9"></a>(L8Pu(]-'Lo8h&gt;uo(_uv(uo2(-&gt;'2h_u(uo2(uvU2K h&gt;'o8a=}Lkk;[[x^%cuaa8acr[VS%
<a name="rest_code_321fa4266d3d4657a65379ee2704342f-10"></a>$ckUaoc8 c'8&gt;_-o-8&gt;U2cu/kau==-8&gt;c-=cU2]Uf=c U2=u%c&amp;kU__-&gt;j}c@uvu2%cJ
</pre>
<p>Pretty nice, isn't it ?</p>
<p>Let's try again in a file:</p>
<pre class="code text"><a name="rest_code_76c68ac3fa004470bff70317545dddc3-1"></a>pvs-studio-analyzer analyze -o results.log -l ~/pvs_studio/PVS-Studio.lic -j5
</pre>
<p>The time is quite reasonable for the analysis, it took much less time than the
compilation time. In total, it took 88 seconds to analyze all the files. It's
much faster than the clang static analyzer.</p>
<p>This time it worked, but the log file is not readable, you need to convert it
again:</p>
<pre class="code text"><a name="rest_code_7fe83609152a4d95bffb4dbf29f1a676-1"></a>plog-converter -t errorfile -o errors results.log
</pre>
<p>And finally, you can read the results of the analysis in the errors file.</p>
</div>
<div class="section" id="results-2">
<h2>Results</h2>
<p>Overall, PVS-Studio found 236 messages in the ETL library, I was expecting more.
I also wish there was an HTML report that include the source code as well as the
error message. I had to lookup at the code for each message (you could integrate
it in vim and then use the quickfix window to do that). There are some
visualization but in things like QtCreator or LibreOffice which I don't have nor
want on my computer.</p>
<p>Let's look at the results. For each message, I'll include the message from
PVS-Studio and the code if it's relevant.</p>
<p>The first is about using the comma:</p>
<pre class="code text"><a name="rest_code_d1cd556acbc948daa757274fdb086395-1"></a>include/etl/traits.hpp:674:1: error: V521 Such expressions using the ',' operator are dangerous. Make sure the expression is correct.
<a name="rest_code_d1cd556acbc948daa757274fdb086395-2"></a>include/etl/traits.hpp:674:1: error: V685 Consider inspecting the return statement. The expression contains a comma.
</pre>
<pre class="code cpp"><a name="rest_code_56f52f0c63de49dab5e4937dde5d3b38-1"></a><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">E</span><span class="o">&gt;</span>
<a name="rest_code_56f52f0c63de49dab5e4937dde5d3b38-2"></a><span class="k">constexpr</span> <span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">dimensions</span><span class="p">(</span><span class="k">const</span> <span class="n">E</span><span class="o">&amp;</span> <span class="n">expr</span><span class="p">)</span> <span class="k">noexcept</span> <span class="p">{</span>
<a name="rest_code_56f52f0c63de49dab5e4937dde5d3b38-3"></a>    <span class="k">return</span> <span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">expr</span><span class="p">,</span> <span class="n">etl_traits</span><span class="o">&lt;</span><span class="n">E</span><span class="o">&gt;::</span><span class="n">dimensions</span><span class="p">();</span>
<a name="rest_code_56f52f0c63de49dab5e4937dde5d3b38-4"></a><span class="p">}</span>
</pre>
<p>Here I'm simply using the comma operand to ignore expr to avoid a warning. To
make this compile in C++11, you need to do it in one line otherwise it's not
a constexpr function. It's probably not perfect to use this construct, but there
is no problem here.</p>
<p>There is a bunch of these, let's filter them, it remains 207 warnings. Let's
jump to the next one:</p>
<pre class="code text"><a name="rest_code_d447255158b94c23a01e76d8401783ef-1"></a>include/etl/impl/blas/fft.hpp:29:1: error: V501 There are identical sub-expressions to the left and to the right of the '==' operator: (DFTI_SINGLE) == DFTI_SINGLE
</pre>
<pre class="code cpp"><a name="rest_code_1ffa634a24a04e80be401e49c3cf8bf6-1"></a><span class="kr">inline</span> <span class="kt">void</span> <span class="nf">fft_kernel</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;*</span> <span class="n">in</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">s</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;*</span> <span class="n">out</span><span class="p">)</span> <span class="p">{</span>
<a name="rest_code_1ffa634a24a04e80be401e49c3cf8bf6-2"></a>    <span class="n">DFTI_DESCRIPTOR_HANDLE</span> <span class="n">descriptor</span><span class="p">;</span>
<a name="rest_code_1ffa634a24a04e80be401e49c3cf8bf6-3"></a>
<a name="rest_code_1ffa634a24a04e80be401e49c3cf8bf6-4"></a>    <span class="kt">void</span><span class="o">*</span> <span class="n">in_ptr</span> <span class="o">=</span> <span class="k">const_cast</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">*&gt;</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="kt">void</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">in</span><span class="p">));</span>
<a name="rest_code_1ffa634a24a04e80be401e49c3cf8bf6-5"></a>
<a name="rest_code_1ffa634a24a04e80be401e49c3cf8bf6-6"></a>    <span class="n">DftiCreateDescriptor</span><span class="p">(</span><span class="o">&amp;</span><span class="n">descriptor</span><span class="p">,</span> <span class="n">DFTI_SINGLE</span><span class="p">,</span> <span class="n">DFTI_COMPLEX</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span> <span class="c1">//Specify size and precision</span>
<a name="rest_code_1ffa634a24a04e80be401e49c3cf8bf6-7"></a>    <span class="n">DftiSetValue</span><span class="p">(</span><span class="n">descriptor</span><span class="p">,</span> <span class="n">DFTI_PLACEMENT</span><span class="p">,</span> <span class="n">DFTI_NOT_INPLACE</span><span class="p">);</span>         <span class="c1">//Out of place FFT</span>
<a name="rest_code_1ffa634a24a04e80be401e49c3cf8bf6-8"></a>    <span class="n">DftiCommitDescriptor</span><span class="p">(</span><span class="n">descriptor</span><span class="p">);</span>                                   <span class="c1">//Finalize the descriptor</span>
<a name="rest_code_1ffa634a24a04e80be401e49c3cf8bf6-9"></a>    <span class="n">DftiComputeForward</span><span class="p">(</span><span class="n">descriptor</span><span class="p">,</span> <span class="n">in_ptr</span><span class="p">,</span> <span class="n">out</span><span class="p">);</span>                        <span class="c1">//Compute the Forward FFT</span>
<a name="rest_code_1ffa634a24a04e80be401e49c3cf8bf6-10"></a>    <span class="n">DftiFreeDescriptor</span><span class="p">(</span><span class="o">&amp;</span><span class="n">descriptor</span><span class="p">);</span>                                    <span class="c1">//Free the descriptor</span>
<a name="rest_code_1ffa634a24a04e80be401e49c3cf8bf6-11"></a><span class="p">}</span>
</pre>
<p>Unfortunately, the error is inside the MKL library. Here, I really don't think
it's an issue. There is pack of them. I forgot to exclude non-ETL code from the
results. Once filter from all dependencies, 137 messages remain.</p>
<pre class="code text"><a name="rest_code_111dd50645214eea853a6856282af689-1"></a>include/etl/eval_functors.hpp:157:1: warning: V560 A part of conditional expression is always false: !padding.
</pre>
<p>This is true, but not an issue since padding is a configuration constant that
enables the use of padding in vector and matrices. There was 27 of these at
different locations and with different configuration variables.</p>
<pre class="code text"><a name="rest_code_2fa08654a2444fc3a7666d703e66359d-1"></a>include/etl/op/sub_view.hpp:161:1: note: V688 The 'i' function argument possesses the same name as one of the class members, which can result in a confusion.
</pre>
<p>This is again true, but not a bug in this particular case. It is still helpful and
I ended up changing these to avoid confusion. Again, there was a few of these.</p>
<pre class="code text"><a name="rest_code_7d8f80daa6c642db8538f20b8bcfab0d-1"></a>etl/test/src/conv_multi_multi.cpp:23:1: error: V573 Uninitialized variable 'k' was used. The variable was used to initialize itself.
</pre>
<p>This one is in the test code:</p>
<pre class="code cpp"><a name="rest_code_60d95f6168254801a31c9a2a04517769-1"></a><span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">etl</span><span class="o">::</span><span class="n">dim</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">K</span><span class="p">);</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>
<a name="rest_code_60d95f6168254801a31c9a2a04517769-2"></a>    <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">etl</span><span class="o">::</span><span class="n">dim</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">I</span><span class="p">);</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
<a name="rest_code_60d95f6168254801a31c9a2a04517769-3"></a>        <span class="n">C_ref</span><span class="p">(</span><span class="n">k</span><span class="p">)(</span><span class="n">i</span><span class="p">)</span> <span class="o">=</span> <span class="n">conv_2d_valid</span><span class="p">(</span><span class="n">I</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">K</span><span class="p">(</span><span class="n">k</span><span class="p">));</span> <span class="c1">// HERE</span>
<a name="rest_code_60d95f6168254801a31c9a2a04517769-4"></a>    <span class="p">}</span>
<a name="rest_code_60d95f6168254801a31c9a2a04517769-5"></a><span class="p">}</span>
</pre>
<p>I don't see any error, k is initialized correctly to zero in the first loop.
This is a <strong>false positive</strong> for me. There were several of these in different
places. It seems to that the use of the operator() is confusing for PVS-Studio.</p>
<pre class="code text"><a name="rest_code_b3dc5441d3e34bdeae62788ed1b8abb9-1"></a>include/etl/traits.hpp:703:1: note: V659 Declarations of functions with 'rows' name differ in the 'const' keyword only, but the bodies of these functions have different composition. This is suspicious and can possibly be an error. Check lines: 693, 703.
</pre>
<pre class="code cpp"><a name="rest_code_7304c0f2a47a47e4ac438da342420d7f-1"></a><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">E</span><span class="p">,</span> <span class="n">cpp_disable_if</span><span class="p">(</span><span class="n">decay_traits</span><span class="o">&lt;</span><span class="n">E</span><span class="o">&gt;::</span><span class="n">is_fast</span><span class="p">)</span><span class="o">&gt;</span>
<a name="rest_code_7304c0f2a47a47e4ac438da342420d7f-2"></a><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">rows</span><span class="p">(</span><span class="k">const</span> <span class="n">E</span><span class="o">&amp;</span> <span class="n">expr</span><span class="p">)</span> <span class="p">{</span> <span class="c1">//693</span>
<a name="rest_code_7304c0f2a47a47e4ac438da342420d7f-3"></a>    <span class="k">return</span> <span class="n">etl_traits</span><span class="o">&lt;</span><span class="n">E</span><span class="o">&gt;::</span><span class="n">dim</span><span class="p">(</span><span class="n">expr</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<a name="rest_code_7304c0f2a47a47e4ac438da342420d7f-4"></a><span class="p">}</span>
<a name="rest_code_7304c0f2a47a47e4ac438da342420d7f-5"></a>
<a name="rest_code_7304c0f2a47a47e4ac438da342420d7f-6"></a><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">E</span><span class="p">,</span> <span class="n">cpp_enable_if</span><span class="p">(</span><span class="n">decay_traits</span><span class="o">&lt;</span><span class="n">E</span><span class="o">&gt;::</span><span class="n">is_fast</span><span class="p">)</span><span class="o">&gt;</span>
<a name="rest_code_7304c0f2a47a47e4ac438da342420d7f-7"></a><span class="k">constexpr</span> <span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">rows</span><span class="p">(</span><span class="k">const</span> <span class="n">E</span><span class="o">&amp;</span> <span class="n">expr</span><span class="p">)</span> <span class="k">noexcept</span> <span class="p">{</span> <span class="c1">//703</span>
<a name="rest_code_7304c0f2a47a47e4ac438da342420d7f-8"></a>    <span class="k">return</span> <span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">expr</span><span class="p">,</span> <span class="n">etl_traits</span><span class="o">&lt;</span><span class="n">E</span><span class="o">&gt;::</span><span class="k">template</span> <span class="n">dim</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">();</span>
<a name="rest_code_7304c0f2a47a47e4ac438da342420d7f-9"></a><span class="p">}</span>
</pre>
<p>Unfortunately, this is again a <strong>false positive</strong> because PVS-Studio failed to
recognized SFINAE and therefore the warning is wrong.</p>
<pre class="code text"><a name="rest_code_eb53724f5d234dfa905dafc7af1d94cc-1"></a>include/etl/builder/expression_builder.hpp:345:1: note: V524 It is odd that the body of '&gt;&gt;=' function is fully equivalent to the body of '*=' function.
</pre>
<p>This one is interesting indeed. It is true that they are exactly because in ETL
&gt;&gt; is used for scalar element-wise multiplication. This is quite interesting that
PVS-Studio points that out. There was a few of these oddities but all were
normal in the library.</p>
<pre class="code text"><a name="rest_code_a1311dfae763417186e8671f1800b60f-1"></a>etl/test/src/compare.cpp:23:1: error: V501 There are identical sub-expressions to the left and to the right of the '!=' operator: a != a
</pre>
<p>Again, it is nice that PVS-Studio finds that, but this is done on purpose on the
tests to compare an object to itself. If I remove all the oddities in the test
cases, there are only 17 left in the headers. None of the warnings on the test
case was serious, but there was no more false positives either, so that's great.</p>
<pre class="code text"><a name="rest_code_095a52ec6f7f49d0b15f39d99c079ea4-1"></a>include/etl/impl/vec/sum.hpp:92:1: error: V591 Non-void function should return a value.
</pre>
<pre class="code cpp"><a name="rest_code_a0b062010d854e0fba1faada4751e938-1"></a><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="n">L</span><span class="p">,</span> <span class="n">cpp_disable_if</span><span class="p">((</span><span class="n">vec_enabled</span> <span class="o">&amp;&amp;</span> <span class="n">all_vectorizable</span><span class="o">&lt;</span><span class="n">vector_mode</span><span class="p">,</span> <span class="n">L</span><span class="o">&gt;::</span><span class="n">value</span><span class="p">))</span><span class="o">&gt;</span>
<a name="rest_code_a0b062010d854e0fba1faada4751e938-2"></a><span class="n">value_t</span><span class="o">&lt;</span><span class="n">L</span><span class="o">&gt;</span> <span class="n">sum</span><span class="p">(</span><span class="k">const</span> <span class="n">L</span><span class="o">&amp;</span> <span class="n">lhs</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">first</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">last</span><span class="p">)</span> <span class="p">{</span>
<a name="rest_code_a0b062010d854e0fba1faada4751e938-3"></a>    <span class="n">cpp_unused</span><span class="p">(</span><span class="n">lhs</span><span class="p">);</span>
<a name="rest_code_a0b062010d854e0fba1faada4751e938-4"></a>    <span class="n">cpp_unused</span><span class="p">(</span><span class="n">first</span><span class="p">);</span>
<a name="rest_code_a0b062010d854e0fba1faada4751e938-5"></a>    <span class="n">cpp_unused</span><span class="p">(</span><span class="n">last</span><span class="p">);</span>
<a name="rest_code_a0b062010d854e0fba1faada4751e938-6"></a>    <span class="n">cpp_unreachable</span><span class="p">(</span><span class="s">"vec::sum called with invalid parameters"</span><span class="p">);</span>
<a name="rest_code_a0b062010d854e0fba1faada4751e938-7"></a><span class="p">}</span>
</pre>
<p>This one is interesting. It's not a false positive since indeed the function
does not return a value, but there is a __builtin_unreachable() inside the
function and it cannot be called. In my opinion, the static analyzer should be
able to handle that, but this is really a corner case.</p>
<pre class="code text"><a name="rest_code_b219d74dbaed49e98ecdd5fe7500fe03-1"></a>include/etl/sparse.hpp:148:1: note: V550 An odd precise comparison: a == 0.0. It's probably better to use a comparison with defined precision: fabs(A - B) &lt; Epsilon.
</pre>
<pre class="code cpp"><a name="rest_code_59bd3ebbaf2c46708a11e8a0c16cd6e2-1"></a><span class="kr">inline</span> <span class="kt">bool</span> <span class="nf">is_zero</span><span class="p">(</span><span class="kt">double</span> <span class="n">a</span><span class="p">)</span> <span class="p">{</span>
<a name="rest_code_59bd3ebbaf2c46708a11e8a0c16cd6e2-2"></a>    <span class="k">return</span> <span class="n">a</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">;</span>
<a name="rest_code_59bd3ebbaf2c46708a11e8a0c16cd6e2-3"></a><span class="p">}</span>
</pre>
<p>This is not false, but again this is intended because of the comparison to zero
for a sparse matrix. There were 10 of these in the same class.</p>
<pre class="code text"><a name="rest_code_c73e32153c654273bcf8e44990c0bd04-1"></a>include/etl/impl/blas/fft.hpp:562:1: note: V656 Variables 'a_padded', 'b_padded' are initialized through the call to the same function. It's probably an error or un-optimized code. Consider inspecting the 'etl::size(c)' expression. Check lines: 561, 562.
</pre>
<pre class="code cpp"><a name="rest_code_04f673f4a3704d26bbcd988b927a6c9e-1"></a><span class="n">dyn_vector</span><span class="o">&lt;</span><span class="n">etl</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="n">type</span><span class="o">&gt;&gt;</span> <span class="n">a_padded</span><span class="p">(</span><span class="n">etl</span><span class="o">::</span><span class="n">size</span><span class="p">(</span><span class="n">c</span><span class="p">));</span>
<a name="rest_code_04f673f4a3704d26bbcd988b927a6c9e-2"></a><span class="n">dyn_vector</span><span class="o">&lt;</span><span class="n">etl</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="n">type</span><span class="o">&gt;&gt;</span> <span class="n">b_padded</span><span class="p">(</span><span class="n">etl</span><span class="o">::</span><span class="n">size</span><span class="p">(</span><span class="n">c</span><span class="p">));</span>
</pre>
<p>It's indeed constructed with the same size, but for me I don't think it's an
odd pattern. I would not consider that as a warning, especially since it's
a constructor and not a assignment.</p>
<pre class="code text"><a name="rest_code_552292314a2a423b8527c0a46569b066-1"></a>include/etl/dyn_base.hpp:312:1: warning: V690 The 'dense_dyn_base' class implements a copy constructor, but lacks the '=' operator. It is dangerous to use such a class.
</pre>
<p>This is again a kind of corner case in the library because it's a base class
and the assignment is different between the sub classes and not a real
assignment in the C++ sense.</p>
<pre class="code text"><a name="rest_code_30fc09ec90d6491d88e8d60d9b36579b-1"></a>include/etl/impl/reduc/conv_multi.hpp:657:1: warning: V711 It is dangerous to create a local variable within a loop with a same name as a variable controlling this loop.
</pre>
<pre class="code cpp"><a name="rest_code_6b882fcce86f443e9696906ac056a024-1"></a><span class="k">for</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">C</span><span class="p">;</span> <span class="o">++</span><span class="n">c</span><span class="p">)</span> <span class="p">{</span>
<a name="rest_code_6b882fcce86f443e9696906ac056a024-2"></a>    <span class="k">for</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>
<a name="rest_code_6b882fcce86f443e9696906ac056a024-3"></a>        <span class="n">conv</span><span class="p">(</span><span class="n">k</span><span class="p">)(</span><span class="n">c</span><span class="p">)</span> <span class="o">=</span> <span class="n">conv_temp</span><span class="p">(</span><span class="n">c</span><span class="p">)(</span><span class="n">k</span><span class="p">);</span>
<a name="rest_code_6b882fcce86f443e9696906ac056a024-4"></a>    <span class="p">}</span>
<a name="rest_code_6b882fcce86f443e9696906ac056a024-5"></a><span class="p">}</span>
</pre>
<p>This is again a false positive... It really seems that PVS-Studio is not able to
handle the operator().</p>
<pre class="code text"><a name="rest_code_6c7ed72cd5a74f9ebe874565aca49a35-1"></a>include/etl/impl/pooling.hpp:396:1: error: V501 There are identical sub-expressions to the left and to the right of the '||' operator: P1 || P2 || P1
</pre>
<pre class="code cpp"><a name="rest_code_167a26b701f8409b9fcd72ea284ad765-1"></a><span class="k">template</span> <span class="o">&lt;</span><span class="kt">size_t</span> <span class="n">C1</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">C2</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">C3</span><span class="p">,</span><span class="kt">size_t</span> <span class="n">S1</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">S2</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">S3</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">P1</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">P2</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">P3</span><span class="p">,</span> <span class="k">typename</span> <span class="n">A</span><span class="p">,</span> <span class="k">typename</span> <span class="n">M</span><span class="o">&gt;</span>
<a name="rest_code_167a26b701f8409b9fcd72ea284ad765-2"></a><span class="k">static</span> <span class="kt">void</span> <span class="n">apply</span><span class="p">(</span><span class="k">const</span> <span class="n">A</span><span class="o">&amp;</span> <span class="n">sub</span><span class="p">,</span> <span class="n">M</span><span class="o">&amp;&amp;</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
<a name="rest_code_167a26b701f8409b9fcd72ea284ad765-3"></a>    <span class="k">const</span> <span class="kt">size_t</span> <span class="n">o1</span> <span class="o">=</span> <span class="p">(</span><span class="n">etl</span><span class="o">::</span><span class="n">dim</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">sub</span><span class="p">)</span> <span class="o">-</span> <span class="n">C1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">P1</span><span class="p">)</span> <span class="o">/</span> <span class="n">S1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
<a name="rest_code_167a26b701f8409b9fcd72ea284ad765-4"></a>    <span class="k">const</span> <span class="kt">size_t</span> <span class="n">o2</span> <span class="o">=</span> <span class="p">(</span><span class="n">etl</span><span class="o">::</span><span class="n">dim</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">sub</span><span class="p">)</span> <span class="o">-</span> <span class="n">C2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">P2</span><span class="p">)</span> <span class="o">/</span> <span class="n">S2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
<a name="rest_code_167a26b701f8409b9fcd72ea284ad765-5"></a>    <span class="k">const</span> <span class="kt">size_t</span> <span class="n">o3</span> <span class="o">=</span> <span class="p">(</span><span class="n">etl</span><span class="o">::</span><span class="n">dim</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span><span class="n">sub</span><span class="p">)</span> <span class="o">-</span> <span class="n">C3</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">P3</span><span class="p">)</span> <span class="o">/</span> <span class="n">S3</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
<a name="rest_code_167a26b701f8409b9fcd72ea284ad765-6"></a>
<a name="rest_code_167a26b701f8409b9fcd72ea284ad765-7"></a>    <span class="k">if</span><span class="p">(</span><span class="n">P1</span> <span class="o">||</span> <span class="n">P2</span> <span class="o">||</span> <span class="n">P1</span><span class="p">){</span>
</pre>
<p>Last but not least, this time, it's entirely true and it's in fact a bug in my
code! The condition should be written like this:</p>
<pre class="code cpp"><a name="rest_code_be83421ee5314baaaa1c96d426e6e9c0-1"></a><span class="k">if</span><span class="p">(</span><span class="n">P1</span> <span class="o">||</span> <span class="n">P2</span> <span class="o">||</span> <span class="n">P3</span><span class="p">){</span>
</pre>
<p>This is now fixed in the master of ETL.</p>
</div>
<div class="section" id="conclusion-6">
<h2>Conclusion</h2>
<p>The installation was pretty easy, but the usage was not as easy as it could
because the first method by analyzing the build system did not work.
Fortunately, the system supports using the Clang compilation database directly
and therefore it was possible to use.</p>
<p>Overall, it found 236 warnings on my code base (heavily templated library).
Around 50 of them were in some of the extend libraries, but I forgot to filter
them out. The quality of the results is pretty good in my opinion. It was able
to <strong>find a bug</strong> in my implementation of pooling with padding. Unfortunately,
there was quite a few false positives, due to SFINAE, bad handling of the
operator() and no handling of __builtin_unreachable. The remaining were all
correct, but were not bug considering their usages.</p>
<p>To conclude, I think it's a great static analyzer that is really fast compared
to other one in the market. There are a few false positives, but it's really not
bad compared to other tools and some of the messages are really great. An HTML
report including the source code would be great as well.</p>
<p>If you want more information, you can consult
<a class="reference external" href="http://www.viva64.com/en/pvs-studio/">the official site</a>. There is even a way
to use it on open-source code for free, but you have to add comments on top of
each of your files.</p>
<p>I hope it was helpful ;)</p>
</div>
</div>
        </div>
            
        
    <a href="posts/2016/12/pvs-studio-on-cpp-library-review.html#disqus_thread" data-disqus-identifier="cache/posts/2016/12/pvs-studio-on-cpp-library-review.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2016/12/cpp-compiler-benchmark-on-expression-templates-library-etl.html" class="u-url">C++ Compiler benchmark on Expression Templates Library (ETL)</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2016-12-11T14:17:30+01:00">2016-12-11 14:17</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<script src="https://code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script><script src="https://code.highcharts.com/highcharts.js"></script><script src="https://code.highcharts.com/modules/exporting.js"></script><p>In my Expression Templates Library (ETL) project, I have a lot of template heavy
code that needs to run as fast as possible and that is quite intensive to
compile. In this post, I'm going to compare the performance of a few of the
kernels produced by different compilers. I've got GCC 5.4, GCC 6.20 and clang
3.9. I also included zapcc which is based on clang 4.0.</p>
<p>These tests have been run on an Haswell processor. The automatic parallelization
of ETL has been turned off for these tests.</p>
<p>Keep in mind that some of the diagrams are presented in logarithmic form.</p>
<div class="section" id="vector-multiplication">
<h2>Vector multiplication</h2>
<p>The first kernel is a very simple one, simple element-wise multiplication of two
vectors. Nothing fancy here.</p>
<div id="mul_container" style="min-width: 310px; height:400px; margin: 0 auto; "></div>
<script>
$(function () {
    Highcharts.chart('mul_container', {
        chart: { type: 'column' },
        title: { text: 'Element-wise Vector Multiplication' },
        xAxis: {
            categories: ['10', '100', '1000', '10000', '100000', '1000000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [0.021, 0.040, 0.215, 2.07, 32.1, 403]
        },
        {
            name: 'g++-6.2', data: [0.021, 0.037, 0.208, 2.17, 32.1, 376]
        },
        {
            name: 'clang-3.9', data: [0.027, 0.045, 0.243, 2.43, 32.7, 389]
        },
        {
            name: 'zapcc-4.0', data: [0.026, 0.047, 0.321, 2.5, 32.8, 411]
        }
        ]
    });
});
</script><p>For small vectors, clang is significantly slower than gcc-5.4 and gcc6.2. On
vectors from 100'000 elements, the speed is comparable for each compiler,
depending on the memory bandwidth. Overall, gcc-6.2 produces the fastest code
here. clang-4.0 is slightly slower than clang-3.9, but nothing dramatic.</p>
</div>
<div class="section" id="vector-exponentiation">
<h2>Vector exponentiation</h2>
<p>The second kernel is computing the exponentials of each elements of a vector and
storing them in another vector.</p>
<div id="exp_container" style="min-width: 310px; height:400px; margin: 0 auto; "></div>
<script>
$(function () {
    Highcharts.chart('exp_container', {
        chart: { type: 'column' },
        title: { text: 'Element-wise Vector Exponentiation' },
        xAxis: {
            categories: ['10', '100', '1000', '10000', '100000', '1000000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [0.0478, 0.137, 1.12, 9.79, 97.5, 959]
        },
        {
            name: 'g++-6.2', data: [0.0474, 0.132, 1.11, 9.71, 97, 1000]
        },
        {
            name: 'clang-3.9', data: [0.0492, 0.136, 0.959, 9.24, 92.9, 914]
        },
        {
            name: 'zapcc-4.0', data: [0.0488, 0.142, 0.952, 9.25, 91.9, 915]
        }
        ]
    });
});
</script><p>Interestingly, this time, clang versions are significantly faster for medium to
large vectors, from 1000 elements and higher, by about 5%. There is no
significant differences between the different versions of each compiler.</p>
</div>
<div class="section" id="matrix-matrix-multiplication">
<h2>Matrix-Matrix Multiplication</h2>
<p>The next kernel I did benchmark with the matrix-matrix multiplication operation.
In that case, the kernel is hand-unrolled and vectorized.</p>
<div id="gemm_container_small" style="min-width: 310px; height:400px; margin: 0 auto; "></div>
<div id="gemm_container_large" style="min-width: 310px; height:400px; margin: 0 auto; "></div>
<script>
$(function () {
    Highcharts.chart('gemm_container_small', {
        chart: { type: 'column' },
        title: { text: 'Matrix Matrix Multiplication (small)', },
        xAxis: {
            categories: ['10x10', '20x20', '40x40', '60x60', '80x80', '100x100']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [0.159, 0.815, 2.637, 13.849, 17.281, 78.903]
        },
        {
            name: 'g++-6.2', data: [0.162, 0.802, 2.431, 13.531, 17.274, 74.02]
        },
        {
            name: 'clang-3.9', data: [0.179, 1.218, 2.391, 14.981, 15.142, 61.548]
        },
        {
            name: 'zapcc-4.0', data: [0.159, 0.836, 2.712, 13.426, 15.114, 62.241]
        }
        ]
    });
    Highcharts.chart('gemm_container_large', {
        chart: { type: 'column' },
        title: { text: 'Matrix Matrix Multiplication (large)', },
        xAxis: {
            categories: ['200x200', '300x300', '400x400', '500x500', '600x600', '700x700', '800x800', '900x900', '1000x1000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [275.219, 1371, 1837, 5177, 6667, 14981, 17037, 31492, 32813]
        },
        {
            name: 'g++-6.2', data: [267.776, 1362, 1808, 5297, 6859, 15166, 15664, 30666, 33067]
        },
        {
            name: 'clang-3.9', data: [266.033, 1230, 1789, 4825, 6969, 14488, 15916, 30872, 33186]
        },
        {
            name: 'zapcc-4.0', data: [267.806, 1237, 1820, 4909, 7035, 15191, 18193, 33127, 37346]
        }
        ]
    });
});
</script><p>There are few differences between the compilers. The first thing is that for
some sizes such as 80x80 and 100x100, clang is significantly faster than GCC, by
more than 10%. The other interesting fact is that for large matrices
zapcc-clang-4.0 is always slower than clang-3.9 which is itself on par with the
two GCC versions. In my opinion, it comes from a regression in clang trunk but
it could also come from zapcc itself.</p>
<div id="std_gemm_container_large" style="min-width: 310px; height:400px; margin: 0 auto; "></div>
<script>
$(function () {
    Highcharts.chart('std_gemm_container_large', {
        chart: { type: 'column' },
        title: { text: 'Matrix Matrix Multiplication (naive)', },
        xAxis: {
            categories: ['200x200', '300x300', '400x400', '500x500', '600x600', '700x700', '800x800', '900x900', '1000x1000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [1.195, 4.891, 10.467, 22.400, 33.399,
            58.401, 77.150, 121.392, 148.469]
        },
        {
            name: 'g++-6.2', data: [1.109, 4.540, 9.964, 21.359, 31.904,
            55.282, 72.690, 113.52, 143.27]
        },
        {
            name: 'clang-3.9', data: [0.893, 3.710, 7.287, 16.244, 23.920,
            43.342, 56.771, 91.870, 112.309]
        },
        {
            name: 'zapcc-4.0', data: [5.088, 16.909, 39.632, 77.194, 133.15,
            214.539, 316.01, 447.715, 612.255]
        }
        ]
    });
});
</script><p>The results are much more interesting here! First, there is a huge regression in
clang-4.0 (or in zapcc for that matter). Indeed, it is up to 6 times slower than
clang-3.9. Moreover, the clang-3.9 is always significantly faster than gcc-6.2.
Finally, there is a small improvement in gcc-6.2 compared to gcc 5.4.</p>
</div>
<div class="section" id="fast-fourrier-transform">
<h2>Fast-Fourrier Transform</h2>
<p>The following kernel is the performance of a hand-crafted Fast-Fourrier
transform implementation.</p>
<div id="fft_container" style="min-width: 310px; height:400px; margin: 0 auto; "></div>
<script>
$(function () {
    Highcharts.chart('fft_container', {
        chart: { type: 'column' },
        title: { text: 'Fast Fourrier Transform', },
        xAxis: {
            categories: ['100', '1000', '10000', '100000', '1000000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [2.640, 27.515, 308.239, 3427.4, 41695.9]
        },
        {
            name: 'g++-6.2', data: [2.578, 26.194, 298.97, 3348.82, 40783.8]
        },
        {
            name: 'clang-3.9', data: [3.047, 30.514, 333.403, 3569.36,43860.6]
        },
        {
            name: 'zapcc-4.0', data: [3.199,33.304,317.135,4025.18,48445.3]
        }
        ]
    });
});
</script><p>On this benchmark, gcc-6.2 is the clear winner. It is significantly faster
than clang-3.9 and clang-4.0. Moreover, gcc-6.2 is also faster than gcc-5.4.
On the contrary, clang-4.0 is significantly slower than clang-3.9 except on one
configuration (10000 elements).</p>
</div>
<div class="section" id="d-convolution">
<h2>1D Convolution</h2>
<p>This kernel is about computing the 1D valid convolution of two vectors.</p>
<div id="conv1_container" style="min-width: 310px; height:400px; margin: 0 auto; "></div>
<script>
$(function () {
    Highcharts.chart('conv1_container', {
        chart: { type: 'column' },
        title: { text: '1D convolution (optimized)', },
        xAxis: {
            categories: ['1000x500', '2000x1000', '3000x1500', '4000x2000',
            '5000x2500', '6000x3000', '7000x3500', '8000x4000', '9000x4500',
            '10000x5000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [11.710, 41.002, 91.201, 158.178,
            248.985, 353.695, 486.676, 634.53, 867.101, 1082.62]
        },
        {
            name: 'g++-6.2', data: [9.307, 40.921, 90.327, 158.734, 248.892,
            354.582, 488.38, 636.899, 869.637, 1084.86]
        },
        {
            name: 'clang-3.9', data: [13.404, 41.409, 95.094, 162.339,
            256.143, 362.34, 498.66, 651.352, 886.465, 1092.24]
        },
        {
            name: 'zapcc-4.0', data: [13.528, 40.886, 94.473, 159.917,
            252.992, 356.63, 493.653, 640.348, 872.282, 1091.36]
        }
        ]
    });
});
</script><p>While clang-4.0 is faster than clang-3.9, it is still slightly slower than both
gcc versions. On the GCC side, there is not a lot of difference except on the
1000x500 on which gcc-6.2 is 25% faster.</p>
<p>And here are the results with the naive implementation:</p>
<div id="std_conv1_container" style="min-width: 310px; height:400px; margin: 0 auto; "></div>
<script>
$(function () {
    Highcharts.chart('std_conv1_container', {
        chart: { type: 'column' },
        title: { text: '1D convolution (naive)', },
        xAxis: {
            categories: ['1000x500', '2000x1000', '3000x1500', '4000x2000',
            '5000x2500', '6000x3000', '7000x3500', '8000x4000', '9000x4500',
            '10000x5000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [0.350, 1.452, 3.260, 5.823, 9.116,
            13.155, 17.922, 23.438, 29.705, 36.683]
        },
        {
            name: 'g++-6.2', data: [0.350, 1.457, 3.262, 5.823, 9.120,
            13.152, 17.922, 23.436, 29.687, 36.665]
        },
        {
            name: 'clang-3.9', data: [0.216, 0.873, 1.974, 3.517, 5.501,
            7.921, 10.793, 14.11, 17.867, 22.068]
        },
        {
            name: 'zapcc-4.0', data: [0.215, 0.873, 1.972, 3.514, 5.501,
            7.928, 10.799, 14.11, 17.879, 22.065]
        }
        ]
    });
});
</script><p>Again, on the naive version, clang is much faster than GCC on the naive, by
about 65%. This is a really large speedup.</p>
</div>
<div class="section" id="id1">
<h2>2D Convolution</h2>
<p>This next kernel is computing the 2D valid convolution of two matrices</p>
<div id="conv2_container" style="min-width: 310px; height:400px; margin: 0 auto; "></div>
<script>
$(function () {
    Highcharts.chart('conv2_container', {
        chart: { type: 'column' },
        title: { text: '2D Convolution (optimized)', },
        xAxis: {
            categories: ['100x50', '105x50', '110x55', '115x55', '120x60',
            '125x60', '130x65', '135x65', '140x70']
        },
        yAxis: {
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [327.399, 367.389, 441.457, 576.021,
            762.268, 794, 994.06, 1261.71, 1360.57]
        },
        {
            name: 'g++-6.2', data: [327.764, 367.379, 441.993, 572.241,
            761.741, 784.605, 991.717, 1266.55, 1361.59]
        },
        {
            name: 'clang-3.9', data: [330.199, 364.253, 443.483, 580.676,
            763.772, 777.39, 1000.53, 1267.75, 1375.51]
        },
        {
            name: 'zapcc-4.0', data: [339.358, 364.756, 443.807, 575.917,
            761.248, 784.695, 992.29, 1265.04, 1367.33]
        }
        ]
    });
});
</script><p>There is no clear difference between the compilers in this code. Every compiler
here has up and down.</p>
<p>Let's look at the naive implementation of the 2D convolution (units are
milliseconds here not microseconds):</p>
<div id="std_conv2_container" style="min-width: 310px; height:400px; margin: 0 auto; "></div>
<script>
$(function () {
    Highcharts.chart('std_conv2_container', {
        chart: { type: 'column' },
        title: { text: '2D Convolution (naive)', },
        xAxis: {
            categories: ['100x50', '105x50', '110x55', '115x55', '120x60',
            '125x60', '130x65', '135x65', '140x70']
        },
        yAxis: {
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [9.501,11.458,13.888, 16.489, 19.634,
            22.898, 27.012, 31.246, 36.269]
        },
        {
            name: 'g++-6.2', data: [9.502, 11.464, 13.903, 16.484, 19.642,
            22.994, 27.004, 31.248, 36.26]
        },
        {
            name: 'clang-3.9', data: [5.880, 7.136, 8.610, 10.226, 12.164,
            14.247, 17.024, 19.577, 22.510]
        },
        {
            name: 'zapcc-4.0', data: [5.875, 7.091, 8.661, 10.241, 12.218,
            14.302, 16.777, 19.424, 22.472]
        }
        ]
    });
});
</script><p>This time the difference is very large! Indeed, clang versions are about 60%
faster than the GCC versions! This is really impressive. Even though this does
not comes close to the optimized. It seems the vectorizer of clang is much more
efficient than the one from GCC.</p>
</div>
<div class="section" id="id2">
<h2>4D Convolution</h2>
<p>The final kernel that I'm testing is the batched 4D convolutions that is used a
lot in Deep Learning. This is not really a 4D convolution, but a large number
of 2D convolutions applied on 4D tensors.</p>
<div id="conv4_container" style="min-width: 310px; height:400px; margin: 0 auto; "></div>
<script>
$(function () {
    Highcharts.chart('conv4_container', {
        chart: { type: 'column' },
        title: { text: '4D Convolution', },
        xAxis: {
            categories: ['2x6x3x28x16', '2x6x3x28x16', '2x6x3x28x16',
            '2x6x3x28x16', '2x6x3x28x16', '2x6x3x28x16', '2x6x3x28x16',
            '2x6x3x28x16', '2x6x3x28x16']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [0.095, 0.402, 1.083, 2.237, 3.988,
            6.474, 9.985, 14.132, 19.539]
        },
        {
            name: 'g++-6.2', data: [0.089, 0.413, 1.081, 2.224, 3.990,
            6.462, 9.815, 14.118, 19.612]
        },
        {
            name: 'clang-3.9', data: [0.090, 0.416, 1.108, 2.277, 4.077,
            6.587, 10.024, 14.359, 20.006]
        },
        {
            name: 'zapcc-4.0', data: [0.088, 0.406, 1.080, 2.237, 3.987,
            6.484, 9.827, 14.130, 19.569]
        }
        ]
    });
});
</script><p>Again, there are very small differences between each version. The best versions
are the most recent versions of the compiler gcc-6.2 and clang-4.0 on a tie.</p>
</div>
<div class="section" id="conclusion-5">
<h2>Conclusion</h2>
<p>Overall, we can see two trends in these results. First, when working with
highly-optimized code, the choice of compiler will not make a huge difference.
On these kind of kernels, gcc-6.2 tend to perform faster than the other
compilers, but only by a very slight margin, except in some cases. On the other
hand, when working with naive implementations, clang versions really did perform
much better than GCC. The clang compiled versions of the 1D and 2D convolutions
are more than 60% faster than their GCC counter parts. This is really
impressive. Overall, clang-4.0 seems to have several performance regressions,
but since it's not still a work in progress, I would not be suprised if these
regressions are not present in the final version. Since the clang-4.0 version is
in fact the clang version used by zapcc, it's also possible that zapcc is
introducing new performance regressions.</p>
<p>Overall, my advice would be to use GCC-6.2 (or 5.4) on hand-optimized kernels
and clang when you have mostly naive implementations. However, keep in mind that
at least for the example shown here, the naive version optimized by the compiler
never comes close to the highly-optimized version.</p>
<p>As ever, takes this with a grain of salt, it's only been tested on one project
and one machine, you may obtain very different results on other projects and on
other processors.</p>
</div>
</div>
        </div>
            
        
    <a href="posts/2016/12/cpp-compiler-benchmark-on-expression-templates-library-etl.html#disqus_thread" data-disqus-identifier="cache/posts/2016/12/cpp-compiler-benchmark-on-expression-templates-library-etl.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html" class="u-url">zapcc C++ compilation speed against gcc 5.4 and clang 3.9</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2016-12-05T18:46:09+01:00">2016-12-05 18:46</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>A week ago, I compared the <a class="reference external" href="http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html">compilation time performance of zapcc against gcc-4.9.3 and clang-3.7</a>. On debug builds, zapcc was about 2 times faster than gcc and 3 times faster than clang. In this post, I'm going to try some more recent compilers, namely gcc 5.4 and clang 3.9 on the same project. If you want more information on zapcc, read the previous posts, this post will concentrate on results.</p>
<p>Again, I use my Expression Template Library
(<a class="reference external" href="https://github.com/wichtounet/etl/">ETL</a>). This is a purely header-only
library with lots of templates. I'm going to compile the full test cases.</p>
<p>The results of the two articles are not directly comparable, since they were
obtained on two different computers. The one on which the present results are
done has a less powerful and only 16Go of RAM compared to the 32Go of RAM of my
build machine. Also take into account that that the present results were
obtained on a Desktop machine, there can be some perturbations from background
tasks.</p>
<p>Just like on the previous results, it does not help using more threads than
physical cores, therefore, the results were only computed on up to 4 cores on
this machine.</p>
<p>The link time is not taken into account on the results.</p>
<div class="section" id="debug-build-2">
<h2>Debug build</h2>
<p>Let's start with the result of the debug build.</p>
<table border="1" class="docutils">
<colgroup>
<col width="55%">
<col width="15%">
<col width="15%">
<col width="15%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Compiler</th>
<th class="head">-j1</th>
<th class="head">-j2</th>
<th class="head">-j4</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>g++-5.4.0</td>
<td>469s</td>
<td>230s</td>
<td>130s</td>
</tr>
<tr>
<td>clang++-3.9</td>
<td>710s</td>
<td>371s</td>
<td>218s</td>
</tr>
<tr>
<td>zapcc++</td>
<td>214s</td>
<td>112s</td>
<td>66s</td>
</tr>
<tr>
<td>Speedup VS Clang</td>
<td>3.31</td>
<td>3.31</td>
<td>3.3</td>
</tr>
<tr>
<td>Speedup VS GCC</td>
<td>2.19</td>
<td>2.05</td>
<td>1.96</td>
</tr>
</tbody>
</table>
<p>The results are almost the same as the previous test. zapcc is 3.3 times faster
to compile than Clang and around 2 times faster than GCC. It seems that GCC 5.4
is a bit faster than GCC 4.9.3 while clang 3.9 is a bit slower than clang 3.7,
but nothing terribly significant.</p>
<p>Overall, for debug builds, zapcc can bring a very significant improvement to
your compile times.</p>
</div>
<div class="section" id="release-build-2">
<h2>Release build</h2>
<p>Let's see what is the status of Release builds. Since the results are comparable
between the numbers of threads, the results here are just for one thread.</p>
<p>This is more time consuming since a lot of optimizations are enabled and more
features from ETL are enabled as well.</p>
<table border="1" class="docutils">
<colgroup>
<col width="79%">
<col width="21%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Compiler</th>
<th class="head">-j1</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>g++-5.4.0</td>
<td>782s</td>
</tr>
<tr>
<td>clang++-3.9</td>
<td>960s</td>
</tr>
<tr>
<td>zapcc++</td>
<td>640s</td>
</tr>
<tr>
<td>Speedup VS Clang</td>
<td>1.5</td>
</tr>
<tr>
<td>Speedup VS GCC</td>
<td>1.22</td>
</tr>
</tbody>
</table>
<p>On a release build, the speedups are much less interesting. Nevertheless, they
are still significant. zapcc is still 1.2 times faster than gcc and 1.5 times
faster than clang. Then speedup against clang 3.9 is significantly higher than
it was on my experiment with clang 3.7, it's possible that clang 3.9 is slower
or simply has new optimization passes.</p>
</div>
<div class="section" id="conclusion-4">
<h2>Conclusion</h2>
<p>The previous conclusion still holds with modern version of compilers: zapcc is
much faster than other compilers on Debug builds of template heavy code. More
than 3 times faster than clang-3.9 and about 2 times faster than gcc-5.4. Since
it's based on clang, there should not be any issue compiling projects that
already compile with a recent clang. Even though the speedups are less
interesting on a release build, it is still significantly, especially compared
against clang.</p>
<p>I'm really interested in finding out what will be the pricing for zapcc once
out of the beta or if they will be able to get even faster!</p>
<p>For the comparison with gcc 4.9.3 and clang 3.7, you can have a look at
<a class="reference external" href="http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html">this article</a>.</p>
<p>If you want more information about zapcc, you can go to the
<a class="reference external" href="https://www.zapcc.com/">official website of zapcc</a></p>
</div>
</div>
        </div>
            
        
    <a href="posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html#disqus_thread" data-disqus-identifier="cache/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2016/11/new-design-faster-and-mobile-compatible.html" class="u-url">New design: Faster and mobile compatible</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2016-11-28T07:55:48+01:00">2016-11-28 07:55</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>I've finally taken the time to improve the design of the website!</p>
<p>The site was becoming slower and slower, the design was not responsive at all
and was an horror on mobile.</p>
<p>I've changed the design to focus more on content and removed superfluous things
such as the Google profile or slow things such as the 3D tag cloud. Moreover,
the design is now responsive again. It was a matter of removing a lot of bad
things I did in the CSS. Instead of having a vertical and an horizontal bars,
I now have only one vertical bar with both the navigation and a bit more
information. With these changes, the design is now also working on mobile phone!
It's about time.</p>
<p>Moreover, I've also spent quite some time working on the speed of the website.
For this, I've bundled most of the JS and CSS files together and reduced them.
Moreover, the static files are now hosted and cached by CloudFlare. I've also
removed the 3D tag cloud which was quite slow. The Google API usage for the
Google profile badge were also quite slow. Overall, the index page is now really
fast. The article pages are also much faster but it's not perfect, especially
because of Disqus that does tons of requests and redirects everywhere. I've also
got rid of the Disqus ads which were really insignificant in the end. It may
take a while for the ads to disappear according to Disqus.</p>
<p>I know that it's still not perfect, but I hope that user experience on the blog
is now improved for all readers and now article can be read on mobile normally.
I'll try to continue monitoring the speed and usability of the website to see if
I can improve it further in the coming days.</p>
<p>If you have any issue on the updated website, don't hesitate to let me know
either by commenting on this post or sending me an email (check the Contact
page).</p>
</div>
        </div>
            
        
    <a href="posts/2016/11/new-design-faster-and-mobile-compatible.html#disqus_thread" data-disqus-identifier="cache/posts/2016/11/new-design-faster-and-mobile-compatible.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2016/11/zapcc-a-faster-cpp-compiler.html" class="u-url">zapcc - a faster C++ compiler</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2016-11-26T13:17:50+01:00">2016-11-26 13:17</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>Update: For a comparison against more modern compiler versions, you can read: <a class="reference external" href="http://baptiste-wicht.com/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html">zapcc C++ compilation speed against gcc 5.4 and clang 3.9</a></p>
<p>I just joined the private beta program of zapcc. Zapcc is a c++ compiler, based
on Clang which aims at being much faster than other C++ compilers. How they are
doing this is using a caching server that saves some of the compiler structures,
which should speed up compilation a lot. The private beta is free, but once the
compiler is ready, it will be a commercial compiler.</p>
<p>Every C++ developer knows that compilation time can quickly be an issue when
programs are getting very big and especially when working with template-heavy
code.</p>
<p>To benchmark this new compiler, I use my Expression Template Library
(<a class="reference external" href="https://github.com/wichtounet/etl/">ETL</a>). This is a purely header-only
library with lots of templates. There are lots of test cases which is what I'm
going to compile. I'm going to compare against Clang-3.7 and gcc-4.9.3.</p>
<p>I have configured zapcc to let is use 2Go RAM per caching server, which is the
maximum allowed. Moreover, I killed the servers before each tests.</p>
<div class="section" id="debug-build">
<h2>Debug build</h2>
<p>Let's start with a debug build. In that configuration, there is no optimization
going on and several of the features of the library (GPU, BLAS, ...) are
disabled. This is the fastest way to compile ETL. I gathered this result on
a 4 core, 8 threads, Intel processor, with an SSD.</p>
<p>The following table presents the results with different number of threads and
the difference of zapcc compared to the other compilers:</p>
<table border="1" class="docutils">
<colgroup>
<col width="42%">
<col width="11%">
<col width="13%">
<col width="11%">
<col width="11%">
<col width="11%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Compiler</th>
<th class="head">-j1</th>
<th class="head">-j2</th>
<th class="head">-j4</th>
<th class="head">-j6</th>
<th class="head">-j8</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>g++-4.9.3</td>
<td>350s</td>
<td>185s</td>
<td>104s</td>
<td>94s</td>
<td>91s</td>
</tr>
<tr>
<td>clang++-3.7</td>
<td>513s</td>
<td>271s</td>
<td>153s</td>
<td>145s</td>
<td>138s</td>
</tr>
<tr>
<td>zapcc++</td>
<td>158s</td>
<td>87s</td>
<td>47s</td>
<td>44s</td>
<td>42s</td>
</tr>
<tr>
<td>Speedup VS Clang</td>
<td>3.24</td>
<td>3.103</td>
<td>3.25</td>
<td>3.29</td>
<td>3.28</td>
</tr>
<tr>
<td>Speedup VS GCC</td>
<td>2.21</td>
<td>2.12</td>
<td>2.21</td>
<td>2.13</td>
<td>2.16</td>
</tr>
</tbody>
</table>
<p>The result is pretty clear! zapcc is around <strong>three times faster than Clang</strong> and around
<strong>two times faster than GCC</strong>. This is pretty impressive!</p>
<p>For those that think than Clang is always faster than GCC, keep in mind that
this is not the case for template-heavy code such as this library. In all my
tests, Clang has always been slower and much memory hungrier than GCC on
template-heavy C++ code. And sometimes the difference is very significant.</p>
<p>Interestingly, we can also see that going past the physical cores is not really
interesting on this computer. On some computer, the speedups are interesting,
but not on this one. Always benchmark!</p>
</div>
<div class="section" id="release-build">
<h2>Release build</h2>
<p>We have seen the results on a debug build, let's now compare on something a bit
more timely, a release build with all options of ETL enabled (GPU, BLAS, ...),
which should make it significantly longer to compile.</p>
<p>Again, the table:</p>
<table border="1" class="docutils">
<colgroup>
<col width="40%">
<col width="12%">
<col width="12%">
<col width="12%">
<col width="12%">
<col width="12%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Compiler</th>
<th class="head">-j1</th>
<th class="head">-j2</th>
<th class="head">-j4</th>
<th class="head">-j6</th>
<th class="head">-j8</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>g++-4.9.3</td>
<td>628s</td>
<td>336s</td>
<td>197s</td>
<td>189s</td>
<td>184s</td>
</tr>
<tr>
<td>clang++-3.7</td>
<td>663s</td>
<td>388s</td>
<td>215s</td>
<td>212s</td>
<td>205s</td>
</tr>
<tr>
<td>zapcc++</td>
<td>515s</td>
<td>281s</td>
<td>173s</td>
<td>168s</td>
<td>158s</td>
</tr>
<tr>
<td>Speedup VS Clang</td>
<td>1.28</td>
<td>1.38</td>
<td>1.24</td>
<td>1.26</td>
<td>1.29</td>
</tr>
<tr>
<td>Speedup VS GCC</td>
<td>1.21</td>
<td>1.30</td>
<td>1.13</td>
<td>1.12</td>
<td>1.16</td>
</tr>
</tbody>
</table>
<p>This time, we can see that the difference is much lower. Zapcc is <strong>between 1.2
and 1.4 times faster than Clang</strong> and <strong>between 1.1 and 1.3 times faster than
GCC</strong>. This shows that most of the speedups from zapcc are in the front end of
the compiler. This is not a lot but still significant over long builds,
especially if you have few threads where the absolute difference would be
higher.</p>
<p>We can also observe that Clang is now almost on par with GCC which shows that
optimization is faster in Clang while front and backend is faster in gcc.</p>
<p>You also have to keep in mind that zapcc memory usage is higher than Clang
because of all the caching. Moreover, the server are still up in between
compilations, so this memory usage stays between builds, which may not be what
you want.</p>
<p>As for runtime, I have not seen any significant difference in performance
between the clang version and the zapcc. According to the official benchmarks
and documentation, there should not be any difference in that between zapcc and
the version of clang on which zapcc is based.</p>
</div>
<div class="section" id="incremental-build">
<h2>Incremental build</h2>
<p>Normally, zapcc should shine at incremental building, but I was unable to show
any speedup when changing a single without killing the zapcc servers. Maybe
I did something wrong in my usage of zapcc.</p>
</div>
<div class="section" id="conclusion-3">
<h2>Conclusion</h2>
<p>In conclusion, we can see that zapcc is always faster than both GCC and Clang,
on my template-heavy library. Moreover, on debug builds, it is much faster than
any of the two compilers, being more than 2 times faster than GCC and more than
3 times faster than clang. This is really great. Moreover, I have not seen any
issue with the tool so far, it can seamlessly replace Clang without problem.</p>
<p>It's a bit weird that you cannot allocate more than 2Go to the zapcc servers.</p>
<p>For a program, that's really impressive. I hope that they are continuing the
good work and especially that this motivates other compilers to improve the
speed of compilation (especially of templates).</p>
<p>If you want more information, you can go to the
<a class="reference external" href="https://www.zapcc.com/">official website of zapcc</a></p>
</div>
</div>
        </div>
            
        
    <a href="posts/2016/11/zapcc-a-faster-cpp-compiler.html#disqus_thread" data-disqus-identifier="cache/posts/2016/11/zapcc-a-faster-c++-compiler.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2016/09/blazing-fast-unit-test-compilation-with-doctest-11.html" class="u-url">Blazing fast unit test compilation with doctest 1.1</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2016-09-21T21:45:13+02:00">2016-09-21 21:45</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>You may remember <a class="reference external" href="http://baptiste-wicht.com/posts/2016/06/reduce-compilation-time-by-another-16-with-catch.html">my quest for faster compilation times</a>. I had made several changes to the Catch test framework macros in order to save some compilation at the expense of my test code looking a bit less nice:</p>
<pre class="code cpp"><a name="rest_code_9399a9d3683d452a97b6626a15a07b67-1"></a><span class="n">REQUIRE</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="mi">9</span><span class="p">);</span> <span class="c1">//Before</span>
<a name="rest_code_9399a9d3683d452a97b6626a15a07b67-2"></a><span class="n">REQUIRE_EQUALS</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">9</span><span class="p">);</span> <span class="c1">//After</span>
</pre>
<p>The first line is a little bit better, but using several optimizations, I was
able to dramatically change the compilation time of the test cases of ETL. In
the end, I don't think that the difference between the two lines justifies the
high overhead in compilation times.</p>
<div class="section" id="doctest">
<h2>doctest</h2>
<p><a class="reference external" href="https://github.com/onqtam/doctest">doctest</a> is a framework quite similar to
Catch but that claims to be much lighter. I tested doctest 1.0 early on, but at
this point it was actually slower than Catch and especially slower than my
versions of the macro.</p>
<p>Today, doctest 1.1 was released with promises of being even lighter than before
and providing several new ways of speeding up compilation. If you want the
results directly, you can take a look at the next section.</p>
<p>First of all, this new version improved the basic macros to make expression
decomposition faster. When you use the standard REQUIRE macro, the expression is
composed by using several template techniques and operator overloading. This is
really slow to compile. By removing the need for this decomposition, the fast
Catch macros are much faster to compile.</p>
<p>Moreover, doctest 1.1 also introduces CHECK_EQ that does not any expression
decomposition. This is close to what I did in my macros expect that it is
directly integrated into the framework and preserves all its features. It is
also possible to bypass the expression checking code by using FAST_CHECK_EQ
macro. In that case, the exceptions are not captured. Finally, a new
configuration option is introduced (DOCTEST_CONFIG_SUPER_FAST_ASSERTS) that
removes some features related to automatic debugger breaks. Since I don't use
the debugger features and I don't need to capture exception everywhere (it's
sufficient for me that the test fails completely if an exception is thrown), I'm
more than eager to use these new features.</p>
</div>
<div class="section" id="results">
<h2>Results</h2>
<p>For evaluation, I have compiled the complete test suite of ETL, with 1 thread,
using gcc 4.9.3 with various different options, starting from Catch to doctest
1.1 with all compilation time features. Here are the results, in seconds:</p>
<table border="1" class="docutils">
<colgroup>
<col width="29%">
<col width="12%">
<col width="14%">
<col width="22%">
<col width="23%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Version</th>
<th class="head">Time</th>
<th class="head">VS Catch</th>
<th class="head">VS Fast Catch</th>
<th class="head">VS doctest 1.0</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>Catch</td>
<td>724.22</td>
<td> </td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>Fast Catch</td>
<td>464.52</td>
<td>-36%</td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>doctest 1.0</td>
<td>871.54</td>
<td>+20%</td>
<td>+87%</td>
<td> </td>
</tr>
<tr>
<td>doctest 1.1</td>
<td>614.67</td>
<td>-16%</td>
<td>+32%</td>
<td>-30%</td>
</tr>
<tr>
<td>REQUIRE_EQ</td>
<td>493.97</td>
<td>-32%</td>
<td>+6%</td>
<td>-43%</td>
</tr>
<tr>
<td>FAST_REQUIRE_EQ</td>
<td>439.09</td>
<td>-39%</td>
<td>-6%</td>
<td>-50%</td>
</tr>
<tr>
<td>SUPER_FAST_ASSERTS</td>
<td>411.11</td>
<td>-43%</td>
<td>-12%</td>
<td>-53%</td>
</tr>
</tbody>
</table>
<p>As you can see, doctest 1.1 is much faster to compile than doctest 1.0! This is
really great news. Moreover, it is already 16% faster than Catch. When all the
features are used, doctest is 12% faster than my stripped down versions of Catch
macros (and 43% faster than Catch standard macros). This is really cool! It
means that I don't have to do any change in the code (no need to strip macros
myself) and I can gain a lot of compilation time compared to the bare Catch
framework.</p>
<p>I really think the author of doctest did a great job with the new version.
Although this was not of as much interest for me, there are also a lot of
other changes in the new version. You can consult the
<a class="reference external" href="https://github.com/onqtam/doctest/blob/master/CHANGELOG.md">changelog</a> if you want more information.</p>
</div>
<div class="section" id="conclusion-2">
<h2>Conclusion</h2>
<p>Overall, doctest 1.1 is much faster to compile than doctest 1.0. Moreover, it
offers very fast macros for test assertions that are much faster to compile
than Catch versions and even faster than the versions I created myself to reduce
compilation time. I really thing this is a great advance for doctest. When
compiling with all the optimizations, doctest 1.1 saves me 50 seconds in
compilation time compared to the fast version of Catch macro and more than
5 minutes compared to the standard version of Catch macros.</p>
<p>I'll probably start using doctest on my development machine. For now, I'll keep
Catch as well since I need it to generate the unit test reports in XML format
for Sonarqube. Once this feature appears in doctest, I'll probably drop Catch
from ETL and DLL</p>
<p>If you need blazing fast compilation times for your unit tests, doctest 1.1 is
probably the way to go.</p>
</div>
</div>
        </div>
            
        
    <a href="posts/2016/09/blazing-fast-unit-test-compilation-with-doctest-11.html#disqus_thread" data-disqus-identifier="cache/posts/2016/09/blazing-fast-unit-test-compilation-with-doctest-11.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2016/09/short-review-of-bullseye-coverage.html" class="u-url">Short review of Bullseye Coverage</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2016-09-16T13:25:44+02:00">2016-09-16 13:25</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p><a class="reference external" href="http://www.bullseye.com/">Bullseye</a> is a commercial Code Coverage analyzer.
It is fully-featured with an export to HTML, to XML and even a specific GUI to
see the application.It costs about 800$, with a renewal fee of about 200$ per
year.</p>
<p>I'm currently using gcov and passing the results to Sonar. This works well, but
there are several problems. First, I need to use gcovr to generate the XML file,
that means two tools. Then, gcov has no way to merge coverage reports. In my
tests of ETL, I have seven different profiles being tested and I need the
overall coverage report. lcov has a merge feature but it is slow as hell (it
takes longer to merge the coverage files than to compile and run the complete
test suite seven times...). For now, I'm using a C++ program that I wrote to
combine the XML files or a Python script that does that, but neither are perfect
and it needs maintenance. Finally, it's impossible to exclude some code from the
coverage report (there is code that isn't meant to be executed (exceptional
code)). For now, I'm using yet another C++ program  that I wrote to do this from
comments in code.</p>
<p>Bullseye does have all these feature, so I got an evaluation license online and
tried this tool and wrote a short review of it.</p>
<div class="section" id="usage">
<h2>Usage</h2>
<p>The usage is pretty simple. You put the coverage executables in your PATH
variable and activate coverage globally. Then, we you compile, the compiler
calls will be intercepted and a coverage file will be generated. When the
compilation is done, run the program and the coverage measurements will be
filled.</p>
<p>The coverage results can then be exported to HTML (or XML) or visualized using
the CoverageBrowser tool:</p>
<div class="figure align-center">
<img alt="Screenshot of Bullseye main coverage view" src="images/bullseye_view.png"><p class="caption">The main view of the Bullseye tool code coverage results</p>
</div>
<p>It's a pretty good view of the coverage result. You have a breakdown by folders,
by file, by function and finally by condition. You can view directly the source
code:</p>
<div class="figure align-center">
<img alt="Screenshot of Bullseye source code coverage view" src="images/bullseye_source_view.png"><p class="caption">The source view of the Bullseye tool code coverage results</p>
</div>
<p>If you want to exclude some code from your coverage reports, you can use
a pragma:</p>
<pre class="code cpp"><a name="rest_code_42cdf56fb9bc426fb70271b3b9456de9-1"></a><span class="k">switch</span> <span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">{</span>
<a name="rest_code_42cdf56fb9bc426fb70271b3b9456de9-2"></a>    <span class="k">case</span> <span class="mi">1</span><span class="o">:</span> <span class="n">one</span><span class="o">++</span><span class="p">;</span> <span class="k">break</span><span class="p">;</span>
<a name="rest_code_42cdf56fb9bc426fb70271b3b9456de9-3"></a>    <span class="k">case</span> <span class="mi">2</span><span class="o">:</span> <span class="n">two</span><span class="o">++</span><span class="p">;</span> <span class="k">break</span><span class="p">;</span>
<a name="rest_code_42cdf56fb9bc426fb70271b3b9456de9-4"></a>    <span class="k">case</span> <span class="mi">3</span><span class="o">:</span> <span class="n">three</span><span class="o">++</span><span class="p">;</span> <span class="k">break</span><span class="p">;</span>
<a name="rest_code_42cdf56fb9bc426fb70271b3b9456de9-5"></a>    <span class="cp">#pragma BullseyeCoverage off</span>
<a name="rest_code_42cdf56fb9bc426fb70271b3b9456de9-6"></a>    <span class="k">default</span><span class="o">:</span> <span class="n">abort</span><span class="p">();</span>
<a name="rest_code_42cdf56fb9bc426fb70271b3b9456de9-7"></a>    <span class="cp">#pragma BullseyeCoverage on</span>
<a name="rest_code_42cdf56fb9bc426fb70271b3b9456de9-8"></a><span class="p">}</span>
</pre>
<p>So that the condition won't be set as uncovered.</p>
<p>As for the coverage, it's pretty straightforward. For example:</p>
<pre class="code bash"><a name="rest_code_bcfb49983da74725b6451048ebf7933f-1"></a>covmerge -c -ffinal.cov sse.cov avx.cov
</pre>
<p>and it's really fast. Unfortunately, the merging is only done at the function
level, not at the statement or at the condition level. This is a bit
disappointing, especially from a commercial tool. Nevertheless, it works well.</p>
</div>
<div class="section" id="conclusion">
<h2>Conclusion</h2>
<p>To conclude, Bullseye seems to be a pretty good tool. It has more features than
standard gcov coverage and all features are well integrated together. I have
only covered the features I was interested in, there are plenty of other things
you can look at on the <a class="reference external" href="http://www.bullseye.com/">official website</a>.</p>
<p>However, if you don't need the extra features such as the visualizer (or use
something like Sonar for this), or the merge or code excluding, it's probably
not worth paying the price for it. In my case, since the merge is not better
than my C++ tool (both do almost the same and my tool does some basic line
coverage merging as well) and I don't need the visualizer, I won't pay the price
for it. Moreover, they don't have student or open source licensing, therefore,
I'll continue with my complicated toolchain :)</p>
</div>
</div>
        </div>
            
        
    <a href="posts/2016/09/short-review-of-bullseye-coverage.html#disqus_thread" data-disqus-identifier="cache/posts/2016/09/short-review-of-bullseye-coverage.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2016/09/expression-templates-library-etl-10.html" class="u-url">Expression Templates Library (ETL) 1.0</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2016-09-02T16:12:38+02:00">2016-09-02 16:12</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>I've just released the first official version of my Expression Templates Library
(ETL for short): The version 1.0.</p>
<p>Until now, I was using a simple rolling release model, but I think it's now time
to switch to some basic versioning. The project is now at a stable state.</p>
<p>ETL 1.0 has the following main features:</p>
<ul class="simple">
<li>Smart Expression Templates</li>
<li>Matrix and vector (runtime-sized and compile-time-sized)</li>
<li>Simple element-wise operations</li>
<li>Reductions (sum, mean, max, ...)</li>
<li>Unary operations (sigmoid, log, exp, abs, ...)</li>
<li>Matrix multiplication</li>
<li>Convolution (1D and 2D and higher variations)</li>
<li>Max Pooling</li>
<li>Fast Fourrier Transform</li>
<li>Use of SSE/AVX to speed up operations</li>
<li>Use of BLAS/MKL/CUBLAS/CUFFT/CUDNN libraries to speed up operations</li>
<li>Symmetric matrix adapter (experimental)</li>
<li>Sparse matrix (experimental)</li>
</ul>
<div class="section" id="examples">
<h2>Examples</h2>
<p>Here is an example of expressions in ETL:</p>
<pre class="code cpp"><a name="rest_code_725be86787c2458e93c791b0163ebe32-1"></a><span class="n">etl</span><span class="o">::</span><span class="n">fast_matrix</span><span class="o">&lt;</span><span class="kt">float</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="p">{</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">};</span>
<a name="rest_code_725be86787c2458e93c791b0163ebe32-2"></a><span class="n">etl</span><span class="o">::</span><span class="n">fast_matrix</span><span class="o">&lt;</span><span class="kt">float</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="p">{</span><span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">};</span>
<a name="rest_code_725be86787c2458e93c791b0163ebe32-3"></a><span class="n">etl</span><span class="o">::</span><span class="n">fast_matrix</span><span class="o">&lt;</span><span class="kt">float</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="p">{</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">};</span>
<a name="rest_code_725be86787c2458e93c791b0163ebe32-4"></a>
<a name="rest_code_725be86787c2458e93c791b0163ebe32-5"></a><span class="n">etl</span><span class="o">::</span><span class="n">fast_matrix</span><span class="o">&lt;</span><span class="kt">float</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">&gt;</span> <span class="n">d</span><span class="p">(</span><span class="mf">2.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">a</span> <span class="o">&gt;&gt;</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">abs</span><span class="p">(</span><span class="n">c</span><span class="p">)))</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">scale</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">sign</span><span class="p">(</span><span class="n">b</span><span class="p">))</span> <span class="o">/</span> <span class="n">c</span><span class="p">)</span> <span class="o">+</span> <span class="mf">2.111</span> <span class="o">/</span> <span class="n">log</span><span class="p">(</span><span class="n">c</span><span class="p">));</span>
</pre>
<p>Or another I'm using in my neural networks library:</p>
<pre class="code cpp"><a name="rest_code_a56f242bb4244ca8abed25c5963af622-1"></a><span class="n">h</span> <span class="o">=</span> <span class="n">etl</span><span class="o">::</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">b</span> <span class="o">+</span> <span class="n">v</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span>
</pre>
<p>In that case, the vector-matrix multiplication will be executed using a BLAS
kernel (if ETL is configured correclty) and the assignment, the sigmoid and the
addition will be automatically vectorized to use either AVX or SSE depending
on the machine.</p>
<p>Or with a convolutional layer and a ReLU activation function:</p>
<pre class="code cpp"><a name="rest_code_eb3a0525f2da4efeb55e1fcf9b9176c5-1"></a><span class="n">etl</span><span class="o">::</span><span class="n">reshape</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">NH1</span><span class="p">,</span> <span class="n">NH2</span><span class="o">&gt;</span><span class="p">(</span><span class="n">h_a</span><span class="p">)</span> <span class="o">=</span> <span class="n">etl</span><span class="o">::</span><span class="n">conv_4d_valid_flipped</span><span class="p">(</span><span class="n">etl</span><span class="o">::</span><span class="n">reshape</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="n">NC</span><span class="p">,</span> <span class="n">NV1</span><span class="p">,</span> <span class="n">NV2</span><span class="o">&gt;</span><span class="p">(</span><span class="n">v_a</span><span class="p">),</span> <span class="n">w</span><span class="p">);</span>
<a name="rest_code_eb3a0525f2da4efeb55e1fcf9b9176c5-2"></a><span class="n">h</span> <span class="o">=</span> <span class="n">max</span><span class="p">(</span><span class="n">b_rep</span> <span class="o">+</span> <span class="n">h_a</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">);</span>
</pre>
<p>This will automatically be computed either with NVIDIA CUDNN (if available) or
with optimized SSE/AVX kernels.</p>
<p>For more information, you can take a look at the <a class="reference external" href="https://github.com/wichtounet/etl/wiki">Reference</a> on the wiki.</p>
</div>
<div class="section" id="next-version">
<h2>Next version</h2>
<p>For the next version, I'll focus on several things:</p>
<ul class="simple">
<li>Improve matrix-matrix multiplication kernels when BLAS is not available. There
is a lot of room for improvement here</li>
<li>Complete support for symmetric matrices (currently experimental)</li>
<li>Maybe some new adapters such as Hermitian matrices</li>
<li>GPU improvements for some operations that can be done entirely on GPU</li>
<li>New convolution performanceimprovements</li>
<li>Perhaps more complete parallel support for some implementations</li>
<li>Drop some compiler support to use full C++14 support</li>
</ul>
</div>
<div class="section" id="download-etl">
<h2>Download ETL</h2>
<p>You can download ETL <a class="reference external" href="https://github.com/wichtounet/etl">on Github</a>. If you
only interested in the 1.0 version, you can look at the
<a class="reference external" href="https://github.com/wichtounet/etl/releases">Releases pages</a> or clone the tag
1.0. There are several branches:</p>
<ul class="simple">
<li>
<em>master</em> Is the eternal development branch, may not always be stable</li>
<li>
<em>stable</em> Is a branch always pointing to the last tag, no development here</li>
</ul>
<p>For the future release, there always will tags pointing to the corresponding
commits. I'm not following the git flow way, I'd rather try to have a more
linear history with one eternal development branch, rather than an useless
develop branch or a load of other branches for releases.</p>
<p>Don't hesitate to comment this post if you have any comment on this library or
any question. You can also open an Issue on Github if you have a problem using
this library or propose a Pull Request if you have any contribution you'd like
to make to the library.</p>
<p>Hope this may be useful to some of you :)</p>
</div>
</div>
        </div>
            
        
    <a href="posts/2016/09/expression-templates-library-etl-10.html#disqus_thread" data-disqus-identifier="cache/posts/2016/09/expression-templates-library-etl-10.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2016/08/asgard-home-automation-project.html" class="u-url">Asgard: Home Automation project</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2016-08-27T22:28:16+02:00">2016-08-27 22:28</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>I have updated my asgard project to make it finally useful for me, so I figured
I'd present the project now.</p>
<p>Asgard is my project of home automation based on a Raspberry Pi. I started this
project after Ninja Blocks kickstarter company went down and I was left with
useless sensors. So I figured why not have fun creating my own :P I know there
are some other projects out there that are pretty good, but I wanted to do some
more low level stuff for once, so what the hell.</p>
<p>Of course, everything is written in C++, no surprise here. The project is built
upon a server / drivers architecture. The drivers and the server are talking via
network sockets, so they can be on different machines.  The server is displaying
the data it got on a web interface and also provide a way to trigger actions of
drivers either from the web interface or through the integrated rules engine.
The data are stored in a database, accessed with CPPSqlite3 (probably going to
be replaced by sqlcpp11) and the web server is handled with mongoose (with a c++
interface).</p>
<p>I must mention that most of the web part of the project was made by a student of
mine, Stéphane Ly, who work on it as part of his study.</p>
<p>Here is a picture of the Raspberry Pi system (not very pretty ;) ):</p>
<img alt="Asgard automation system hardware" class="align-center" src="images/asgard_hardware.jpg"><p>I plan to try to fit at least some of it on a nicer box with nicer cables and
such. Moreover, I also plan to add real antennas to the RF transmitter and
receiver, but I haven't received them so far.</p>
<div class="section" id="sensors">
<h2>Sensors</h2>
<p>asgard support several sensors:</p>
<ul class="simple">
<li>DHT11 Temperature/Humdity Sensor</li>
<li>WT450 Temperature/Humdity Sensor</li>
<li>RF Button</li>
<li>IR Remote</li>
<li>CPU Temperature Sensor</li>
</ul>
<p>You can see the sensors data displayed on the web interface:</p>
<img alt="Asgard automation system home page" class="align-center" src="images/asgard_home.png">
</div>
<div class="section" id="actions">
<h2>Actions</h2>
<p>There are currently a few actions provided by the drivers:</p>
<ul class="simple">
<li>Wake-On-Lan a computer by its MAC Address</li>
<li>ITT-1500 smart plugs ON and OFF</li>
<li>Kodi actions: Pause / Play / Next / Previous on Kodi</li>
</ul>
<p>Here are the rules engine:</p>
<img alt="Asgard automation system rules page" class="align-center" src="images/asgard_rules.png">
</div>
<div class="section" id="my-home-automation">
<h2>My home automation</h2>
<p>I'm currently using this system to monitor the temperature in my appartment.
Nothing great so far because I don't have enough sensors yet. And now, I'm also
using a wireless button to turn on my power socket, wait 2 seconds and then
power on my Kodi Home Theater with wake on lan.</p>
<p>It's nothing fancy so far, but it's already better than what I had with Ninja
Blocks, except for the ugly hardware ;).</p>
</div>
<div class="section" id="future-2">
<h2>Future</h2>
<p>There are still tons of work on the project and on integration in my home.</p>
<ul class="simple">
<li>I'm really dissatisfied with the WT450 sensor, I've ordered new Oregon sensors to try to do better.</li>
<li>I've ordered a few new sensors: Door intrusion detector and motion detector</li>
<li>The rules system needs to be improve to support multiple conditions</li>
<li>I plan to add a simple state system to the asgard server</li>
<li>There are a lot of refactorings necessary in the code and</li>
</ul>
<p>However, I don't know when I'll work on this again, my work on this project is
pretty episodic to say the least.</p>
</div>
<div class="section" id="code">
<h2>Code</h2>
<p>The code is, as always, available on Github. There are multiple repositories:
<a class="reference external" href="https://github.com/search?q=user%3Awichtounet+asgard">all asgard repositories</a>.
It's not that much code for now, about 2000 lines of code, but some of it may be
useful. If you plan to use the system, keep in mind that it was never tested out
of my environment and that there is no documentation so far, but don't hesitate
to open Issues on Github if you have questions or post a comment here.</p>
</div>
</div>
        </div>
            
        
    <a href="posts/2016/08/asgard-home-automation-project.html#disqus_thread" data-disqus-identifier="cache/posts/2016/08/asgard-home-automation-project.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2016/08/update-thor-thesis-and-publications.html" class="u-url">Update: Thor, Thesis and Publications</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2016-08-23T07:40:13+02:00">2016-08-23 07:40</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>Since it's been a real while since the last post I've written here, I wanted to
write a short status update.</p>
<p>I had to serve one month in the army, which does not help at all for
productivity :P Since the update to Boost Spirit X3, I haven't worked on my
eddic compiler again, but I've switched back to my operating system project:
thor. I'm having a lot of fun with it again and it's in much better state than
before.</p>
<p>We also have been very productive on the publication side, with four new
publications this year in various conferences. I'll update the blog when the
proceedings are published. I'll be going to ICANN 2016 and ANNPR 2016 next week
and probably to ICFHR in October. And of course, I'll go back to Meeting C++ in
November :) As for my thesis, it's finally going great, I've started writing
regularly and it's taking form!</p>
<div class="section" id="thor">
<h2>Thor</h2>
<p>My project Thor Operating System now has much more features than before:</p>
<ul class="simple">
<li>64bit operating system</li>
<li>Preemptive Multiprocessing</li>
<li>Keyboard / Mouse driver</li>
<li>Full ACPI support with ACPICA</li>
<li>Read/Write ATA driver</li>
<li>FAT32 file system support</li>
<li>HPET/RTC/PIT drivers</li>
<li>Basic PCI support</li>
<li>Multi stage booting with FAT32</li>
</ul>
<p>Since last time, I've fixed tons of bug in the system. Although there are still
some culprit, it's much more stable than before. They were a lot of bugs in the
scheduler with loads of race conditions. I hope I've working through most of
them now.</p>
<p>I'm currently working on the network stack. I'm able to receive and send packets
using the Realtek 8139 card. I have working support for Ethernet, IP and ARP.
I'm currently working on adding ICMP support. I've come to realize that the
hardest part is not to develop the code here but to find a way to test it.
Network in Qemu is a huge pain in the ass to configure. And then, you need tools
to generate some packets or at least answer to packets send by the virtual
machine, and it's really bad... Nevertheless, it's pretty fun overall :)</p>
<p>Aside from this, I'm also working on a window manager. I'll try to post an
update on this.</p>
<p>You can take a look at the <a class="reference external" href="https://github.com/wichtounet/thor-os">thor sources</a> if you're interested.</p>
</div>
<div class="section" id="future">
<h2>Future</h2>
<p>For the time being, I'll focus my effort on the thor project. I also have some
development to do on my home automation system: <a class="reference external" href="https://github.com/wichtounet/asgard-server">asgard-server</a> that I plan to finalize and deploy in a useful way this weekend in my apartment. You can also expect some updates on my deep learning library where I've started work to make it more user-friendly (kind of). I'm also still waiting on the first stable version of doctest for a new comparison with Catch.</p>
<p>I really want to try to publish again some more posts on the blog. I'll
especially try to publish some more updates about Thor.</p>
</div>
</div>
        </div>
            
        
    <a href="posts/2016/08/update-thor-thesis-and-publications.html#disqus_thread" data-disqus-identifier="cache/posts/2016/08/update-thor-thesis-and-publications.html">Comments</a>


        </article><nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-30.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-28.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="blogwichtounet";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div> <!-- col -->
    </div>
<!-- row  -->
</div>
<!-- container-fluid -->

<!-- End of Menubar -->

<!-- Footer -->

<footer>
    Contents © 2017         <a href="mailto:baptistewicht@gmail.com">Baptiste Wicht</a> - Powered by         <a href="http://getnikola.com" rel="nofollow">Nikola</a>         - License: 
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="padding-left:5px;border-width:0" src="assets/img/cc.png"></a>
        <ul class="footer_inline_ul"></ul></footer><!-- Late loading stuff  --><script src="assets/js/all-nocdn.js"></script><script type="text/javascript">
      $(document).ready(function() {
        $.getJSON("/assets/js/tx3_tag_cloud.json", function(data){
            var items = [];
            $.each(data, function(key, val){
                var count = val[0];
                var url = val[1];
                var posts = val[2];

                if(count > 9){
                    items.push("<li data-weight='" + count + "'><a href='" + url + "'>" + key + "</a></li>");
                }
            });

            $("<ul/>", {
                "id": "tag_cloud_left",
                html: items.join("")
            }).appendTo("#tag_cloud_left_container");

            $("#tag_cloud_left").tx3TagCloud({
                multiplier: 0.8 // default multiplier is "1"
            });
        });
      });
    </script><!-- Google platform JS -->
</body>
</html>