<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Tutorials and short posts about programming, C++, Java, Assembly, Operating Systems Development, Compilers, ...">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Blog blog("Baptiste Wicht");</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="http://baptiste-wicht.com/index.html">
<link rel="next" href="index-30.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-2175227-7']);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script><link href="favicon.ico" rel="icon" type="image/x-icon">
<link rel="publisher" href="https://plus.google.com/+BaptisteWicht">
</head>
<body>

<!-- Menubar -->

<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<div class="container-fluid">
<!-- This keeps the margins nice -->
    <div class="row">
        <div class="col-sm-3 col-lg-2">
            <nav class="navbar navbar-inverse navbar-fixed-side"><div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="http://baptiste-wicht.com/">
                        <span id="blog-title">Blog blog("Baptiste Wicht");</span>
                    </a>
                </div>
<!-- /.navbar-header -->

                <div class="collapse navbar-collapse navbar-ex1-collapse">
                    <ul class="nav navbar-nav">
<li>
<a href="stories/about.html">About</a>
                </li>
<li>
<a href="stories/publications.html">Publications</a>
                </li>
<li>
<a href="stories/projects.html">Projects</a>
                </li>
<li>
<a href="categories/index.html">Tags</a>
                </li>
<li>
<a href="archive.html">Archives</a>
                </li>
<li>
<a href="http://feeds.feedburner.com/BaptisteWicht">RSS</a>


                            </li>
<li class="navbar-content">
                                <h3>Tags</h3>
                            </li>
                            <li class="navbar-empty">
                                <div id="tag_cloud_left_container" style="line-height: 18px !important;"></div>
                            </li>
                            <li class="navbar-block">


                        <li class="wicht-navbar-right">
                            <a target="_blank" title="Follow @wichtounet on Twitter" href="https://twitter.com/wichtounet">
                                <img src="assets/img/twitter.png" alt="Follow @wichtounet on Twitter"></a>
                        </li>

                        <li class="wicht-navbar-right">
                            <a target="_blank" title="Follow +BaptisteWicht on Google+" href="https://plus.google.com/+BaptisteWicht">
                                <img src="assets/img/google_plus.png" alt="Follow +BaptisteWicht on Google+"></a>
                        </li>


                    </ul>
</div>
<!-- /.navbar-collapse -->
            </nav>
</div> <!-- col -->
        <div class="col-sm-9 col-lg-10">
            <div id="content"></div>
            
        <article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/05/speed-up-tensorflow-inference-compiling-from-source.html" class="u-url">Speed up TensorFlow inference by compiling it from source</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-05-10T14:18:33+02:00">2017-05-10 14:18</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>The most simple way to install TensorFlow is to work in a virtual Python
environment and simply to use either the TensorFlow official packages in pip or
use one of the official wheels for distributions.  There is one big problem with
that technique and it's the fact that the binaries are precompiled so that they
fit as many hardware configuration as possible. This is normal from Google since
generating precompiled binaries for all the possible combinations of processor
capabilities would be a nightmare. This is not a problem for GPU
since the CUDA Libraries will take care of the difference from one graphics card
to another. But it is a problem with CPU performance. Indeed, different
processors have different capabilities. For instance, the vectorization
capabilities are different from processor to processor (SSE, AVX, AVX2,
AVX-512F, FMA, ...). All those options can make a significant difference in the
performance of the programs. Although most of the machine learning training
occurs on GPU most of the time, the inference is mostly done on the CPU.
Therefore, it probably remains important to be as fast as possible on CPU.</p>
<p>So if you care about performance on CPU, you should install TensorFlow from
sources directly yourself. This will allow compilation of the TensorFlow sources
with -march=native which will enable all the hardware capabilities of machine on
which you are compiling the library.</p>
<p>Depending on your problem, this may give you some nice speedup. In my case, on
a very small Recurrent Neural Network, it made inference about 20% faster.  On
a larger problem and depending on your processor, you may gain much more than
that. If you are training on CPU, this may make a very large difference in total
time.</p>
<p>Installing TensorFlow is sometimes a bit cumbersome. You'll likely have to
compile Bazel from sources as well and depending on your processor, it may take
a long time to finish. Nevertheless, I have successfully compiled TensorFlow
from sources on several machines now without too many problems. Just pay close
attention to the options you are setting while configuring TensorFlow, for
instance CUDA configuration if you want GPU support.</p>
<p>I hope this little trick will help you gain some time :)</p>
<p>Here is the <a class="reference external" href="https://www.tensorflow.org/install/install_sources">link to compile TensorFlow from source</a>.</p>
</div>
        </div>
            
        
    <a href="posts/2017/05/speed-up-tensorflow-inference-compiling-from-source.html#disqus_thread" data-disqus-identifier="cache/posts/2017/05/speed-up-tensorflow-inference-compiling-from-source.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/05/update-on-expression-templates-library-etl.html" class="u-url">Update on Expression Templates Library (ETL)</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-05-06T21:31:48+02:00">2017-05-06 21:31</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>It's been a while since I've <a class="reference external" href="https://baptiste-wicht.com/posts/2016/09/expression-templates-library-etl-10.html">released the version 1.0 of ETL</a>. There is some work to do before I release the next version, but I wanted to give you a quick update on what has been going on for ETL in the last months. There has been a lot of changes in the library and the next version will be a major update when I'm done with some refactorings and improvements.</p>
<p>Thanks to my thesis supervisor, the project now has a logo:</p>
<img alt="ETL Logo" class="align-center" src="images/logo.png"><p>There are quite a few new features, although probably nothing really major. The
support for square root has been improved with cubic root and inverse root.
Vectors can now be transformed using floor and ceil. Cross product of vector has
been implemented as well. Batched outer product and batched bias averaging (for
machine learning) are now supported. Reductions have also been improved with
absolute sum and mean (asum/asum) support and min_index and max_index. argmax
can now be used to get the max index in each sub dimensions. Matrix can now be
decomposed into their Q/R decomposition rather than only their PALU
decomposition. The matrices can now be sliced by getting only a sub part of the
matrix. The pooling operators  have also been improved with stride and padding
support. Matrices and vectors can also be shuffled. Moreover, a few adapters
are now available for hermitian matrices, symmetric matrices and lower and upper
matrices. So far the support for these adapters is not huge, but they are
guaranteed to validate their constraints.</p>
<p>Several operations have been optimized for speed. All the pooling and upsample
operators are now parallelized and the most used kernel (2x2 pooling) is now
more optimized. 4D convolution kernels (for machine learning) have been greatly
improved. There are now very specialized vectorized kernels for classic kernel
configurations (for instance 3x3 or 5x5) and the selection of implementations is
now smarter than before. The support of padding now much better than before for
small amount of padding. Moreover, for small kernels the full convolution can
now be evaluated using the valid convolution kernels directly with some padding,
for much faster overall performance. Matrix-matrix multiplication with
transposed matrices is now much faster when using BLAS kernels. Indeed, the
transposition is not performed but handled inside the kernels. Moreover, the
performance of the transposition itself is also much faster. Finally, accesses
to 3D and 4D matrices is now much faster than before.</p>
<p>The parallelization feature of ETL has been completely reworked. Before, there
was a thread pool for each algorithm that was parallelized. Now, there is
a global thread engine with one thread pool. Since parallelization is not nested
in ETL, this improves performance slightly by greatly diminishing the number of
threads that are created throughout an application.</p>
<p>Vectorization has also been greatly improved in ETL. Integer operations are now
automatically vectorized on processors that support this. The automatic
vectorizer now is able to use non-temporal stores for very large operations.
A non-temporal store bypasses the cache, thus gaining some time. Since very
large matrices do not fit in cache, this is a net gain. Moreover, the alignment
detection in the automatic vectorizer has also been improved. Support for
Fused-Multiply-Add (FMA) operations has also been integrated in the algorithms
that can make use of it. The matrix-matrix multiplications and vector-matrix
multiplications now have optimized vectorized kernels. They also have versions
for column-major matrices now. The old egblas version of the gemm, based on BLIS
kernels, has been removed since it was only supporting double-precision and was
not faster than the new vectorized algorithm. I plan to reintegrate a version of
the GEMM based on BLIS in the future but with more optimizations and support for
all precisions and integers. The sum and the dot product now also have
specialized vectorized implementations. The min and max operations are now
automatically-vectorized.</p>
<p>The GPU has also been almost completely reworked. Now, operations can be chained
without any copies between GPU and CPU. Several new operations have also been
added with support to GPU. Moreover, to complement operations that are not
available in any of the supported NVIDIA libraries, I've created a simple
library that can be used to add a few more GPU operations. Nevertheless a lot of
operations are still missing and only algorithms are available not expressions
(such as c = a + b * 1.0) that are entirely computed on CPU. I have plans to
improve that further, but probably not before the version 1.2.</p>
<p>There also have been a lot of refactorings in the code of the library. A lot of
expressions now have less overhead and are specialized for performance.
Moreover, temporary expressions are currently being reworked in order to be more
simple and maintainable and easier to optimize in the future.</p>
<p>Finally, there also was quite a few bug fixes. Most of them have been found by
the use of the library in the Deep Learning Library (DLL) project.</p>
</div>
        </div>
            
        
    <a href="posts/2017/05/update-on-expression-templates-library-etl.html#disqus_thread" data-disqus-identifier="cache/posts/2017/05/update-on-expression-templates-library-etl.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/04/home-automation-power-meter-integration-zwave-domoticz.html" class="u-url">Home Automation: Power Meter and Integration of Zwave into Domoticz</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-04-23T16:43:13+02:00">2017-04-23 16:43</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>I've improved a bit my home automation installation. It's been a while since
the last upgrade, but unfortunately I cannot afford as many upgrades as I would
like :P</p>
<p>For a long time I wanted to monitor the power consumption of a few of my
appliances in my apartment. Especially my Linux servers so that I could try to
improve the consumption and reduce my bill on the long run. Unfortunately, there
are very few options for power meter in Switzerland due to the special type of
plug we have. The only option I found is a Zwave power plug. For a while,
I waited to see if I could find other options because Zwave devices are quite
expensive and I would have rather preferred to stay with simpler and cheaper
RF-433 appliances. Since I didn't find anything, I ordered a ZWave USB
controller from  Aeon Labs (the generation 5). I also ordered two Aeon Labs
Swiss Smart Plug with power meter.</p>
<p>Here is an image of the Aeon Labs key:</p>
<img alt="Aeon Labs ZWave USB Key" class="align-center" src="images/zwave_usb.jpg"><p>And of the power meter in usage:</p>
<img alt="ZWave power meter" class="align-center" src="images/power_meter.jpg"><p>Integration of ZWave into Domoticz was extremely easy. I just plugged the USB
key, restarted Domoticz (seems necessary for it to pick the new tty) and added
new hardware "OpenZWave USB" with the correct serial port. From there, there are
two main ways to add new devices. The first is to remove the USB key and use the
synchronization button on both the key and the device close to each other. The
other way is to use the "Include Node" option on Domoticz and then press the
synchronization button on the device to detect the new device. I used the second
option since it seemed simpler and it worked perfectly. I did that for my two
plugs and it worked fine. Directly after this, 5 new devices were added for each
of the plug. One for the voltage, one for the current , two for the  usage (I
don't know why there is two, but they are both reporting the same value) and one
for the switch on/off. I was a bit afraid that only the On/Off part of the smart
plug would work on Domoticz, but I had absolutely no problem.</p>
<p>Here is for instance the power usage of last 24 hours on my television system:</p>
<img alt="Power usage on television system" class="align-center" src="images/domoticz_power_usage.png"><p>For now, I haven't integrated this information on any rule, but I plan to
monitor this information in the coming weeks and try to improve my consumption,
especially for my servers. I also plan to purchase more of these plugs once my
home automation budget can be upgraded.</p>
<p>On another note, I also purchased a Chacon wall remote switch working in RF-433.
Although it is quite cheap, I'm very disappointed by the quality of this switch.
I add to straighten myself the pins that are attached to the battery because
there was no contact. After that, it worked correctly and it is able to work
with the RFLink module.</p>
<p>I have to say that I'm quite satisfied with ZWave devices with this experience.
Even though I still feel it is way too expensive, it is high quality and have
a good finishing. I'll probably purchase more ZWave devices in the future. I'm
especially interested in The Aeotec 6 in 1 sensor for temperature humidity,
motion, light, UV and vibration. This would allow me to have much information in
each room with only one sensor in place of several sensors in each room like
I currently have.</p>
<p>I still have a few Milight Bulbs and LEDS to install with a secondary Milight
bridge that I will install in the coming week, but I probably won't do a post
about this.</p>
</div>
        </div>
            
        
    <a href="posts/2017/04/home-automation-power-meter-integration-zwave-domoticz.html#disqus_thread" data-disqus-identifier="cache/posts/2017/04/home-automation-power-meter-integration-zwave-domoticz.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/04/publications-deep-learning-features-handwritten-keyword-spotting.html" class="u-url">Publications: Deep Learning Features for Handwritten Keyword Spotting</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-04-21T20:29:39+02:00">2017-04-21 20:29</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>After my previous post about my publication on CPU performance optimization,
I wanted to talk a bit about two publications on Handwritten Keyword Spotting,
in which we extract features with Convolutional RBM RBM</p>
<p>We published two different papers:</p>
<ul class="simple">
<li>
<a class="reference external" href="https://www.researchgate.net/publication/306081095_Keyword_Spotting_with_Convolutional_Deep_Belief_Networks_and_Dynamic_Time_Warping">Keyword Spotting With Convolutional Deep Belief Networks and Dynamic Time Warping</a>, in the Proceedings of the International Conference on Artificial Neural Networks (ICANN-2016), Barcelona, Spain</li>
<li>Mixed Handwritten and printed digit recognition in Sudoku With Convolutional Deep Belief Network (Link will come), in the Proceedings of the International Conference on Pattern Recognition (ICPR-2016), Cancun, Mexico</li>
</ul>
<p>The second paper is mostly a large extension of the first one, so I'll focus on
the complete version.</p>
<p>On a side note, I also co-authored a third paper:</p>
<ul class="simple">
<li>
<a class="reference external" href="https://www.researchgate.net/publication/312486359_Inkball_Models_as_Features_for_Handwriting_Recognition">Inkball Models as Features for Handwriting Recognition</a>, in the Proceedings of the International Conference on Frontiers of Handwriting Recognition (ICFHR-2016), Shenzen, China</li>
</ul>
<p>We mostly used our existing system to generate features for a comparison between
different set of features for handwritten keyword spotting. It was my first time
in China and I enjoyed the stay a lot. I also had the chance to meet my
girlfriend in Shenzen, all the more reason to mention this publication :)</p>
<p>Back on the main subject. The idea behind these publications is to
a Convolutional Deep Belief Network (CDBN) to extract features from the images
and then pass these features to either a Dynamic Time Warping (DTW) algorithm or
an Hidden Markov Model (HMM). The following image describe the overall system:</p>
<img alt="Keyword Spotting System" class="align-center" src="images/kws_system.png"><p>The features are extracted from preprocessed normalized binary images. Using
a sliding window, moving from left to right, one pixel at a time, the features
are extracted on each window. The feature extractor is a Convolutional Deep
Belief Network, trained fully unsupervised. The features are then normalized so
that each feature group sum to one and then each has zero-mean and
unit-variance. The network used for feature extraction is depicted in the
following image:</p>
<img alt="Convolutional Deep Belief Network features" class="align-center" src="images/kws_network.png"><p>Two Convolutional Restricted Boltzmann Machines (CRBMs) are used, each followed
by a max pooling layer.</p>
<p>Once the features are extracted, they can be passed to the classifier for
keyword spotting scoring. We tested our features with two different approaches
for word scoring. The first one is a template matching strategy, Dynamic Time
Warping (DTW), is a very simple measure of distance between two sequences of
different length. The two sequences are warped non-linearly to minimize the
distance between each pair of features. A template from the training set is
compared to the word image being evaluated. This works pretty well for simple
data sets but fails when the writing styles of the test set are not known in the
training set. The second classifier is more powerful and trained, a Hidden
Markov Model (HMM). Character models are trained using the entire training set.
From these character models, a keyword model as well as an unconstrained model
(the filler model) are constructed. The probability of these two models is
computed using Viterbi and the final score is computed using log-odds scoring of
these two models using the filler model as a form of normalization.</p>
<p>This technique was evaluated on three datasets (George Washington (GW), Parzival
(PAR) and IAM offline database (IAM)). Our features were compared with three
reference feature sets, one heuristic and two local feature sets.</p>
<p>The results for DTW:</p>
<img alt="Keyword Spotting Results with Dynamic Time Warping" class="align-center" src="images/kws_results_dtw.png"><p>Overall, our features exhibit better performance than the other reference.
Except for the Mean Average Precision on the PAR data set. The very low
performance on PAR with DTW is explained by the fact mentioned earlier that it
has poor generalization to unknown writing styles.</p>
<p>The results for HMM:</p>
<img alt="Keyword Spotting Results with Hidden Markov Model" class="align-center" src="images/kws_results_hmm.png"><p>With HMM, our features are always better than the other feature sets. However,
the margin of improvement is smaller than when using DTW.</p>
<p>Overall, the proposed system proved quite powerful and was able to outperform
the three tested feature sets on three datasets for keyword spotting.</p>
<p>You can find the <a class="reference external" href="https://github.com/wichtounet/word_spotting">C++ implementation on Github</a>.</p>
<p>As for my thesis, I have finished the writings about a month ago and it is now
in the hands on my supervisor.</p>
<p>If you want to have a look, the
<a class="reference external" href="http://baptiste-wicht.com/stories/publications.html">list of my publications</a>
is available on this website.</p>
<p>If you want more details on this project, don't hesitate to ask here or on
Github, or read the papers :)</p>
<p>I hope the next post about my publications will be about the finalization of my
thesis :)</p>
</div>
        </div>
            
        
    <a href="posts/2017/04/publications-deep-learning-features-handwritten-keyword-spotting.html#disqus_thread" data-disqus-identifier="cache/posts/2017/04/publications-deep-learning-features-handwritten-keyword-spotting.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/03/partial-type-erasing-deep-learning-library-dll-improve-compilation-time.html" class="u-url">Partial type erasing in Deep Learning Library (DLL) to improve compilation time</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-03-15T07:43:44+01:00">2017-03-15 07:43</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>In a previous post, I compared the <a class="reference external" href="https://baptiste-wicht.com/posts/2017/03/disappointing-zapcc-performance-on-deep-learning-library-dll.html">compilation time on my Deep Learning Library (DLL) project with different compilers</a>. I realized that the compilation times were quickly going unreasonable for this library, especially for compiling the unit cases which clearly hurts the development of the library. Indeed, you want to be able to run the unit tests reasonably quickly after you integrated new changes.</p>
<div class="section" id="reduce-the-compilation-time">
<h2>Reduce the compilation time</h2>
<p>The first thing I did was to split the compilation in three executables: one for
the unit tests, one for the various performance tests and one for the various other
miscellaneous tests. With this, it is much faster to compile only the unit test
cases.</p>
<p>But this can be improved significantly more. In DLL a network is a variadic
template containing the list of layers, in order. In DLL, there are two main
different ways of declaring a neural networks. In the first version, the fast
version, the layers directly know their sizes:</p>
<pre class="code cpp"><a name="rest_code_99b44b58301048718fc70e69c4781cbf-1"></a><span class="k">using</span> <span class="n">network_t</span> <span class="o">=</span>
<a name="rest_code_99b44b58301048718fc70e69c4781cbf-2"></a>    <span class="n">dll</span><span class="o">::</span><span class="n">dbn_desc</span><span class="o">&lt;</span>
<a name="rest_code_99b44b58301048718fc70e69c4781cbf-3"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dbn_layers</span><span class="o">&lt;</span>
<a name="rest_code_99b44b58301048718fc70e69c4781cbf-4"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">rbm_desc</span><span class="o">&lt;</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_99b44b58301048718fc70e69c4781cbf-5"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">rbm_desc</span><span class="o">&lt;</span><span class="mi">500</span>    <span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_99b44b58301048718fc70e69c4781cbf-6"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">rbm_desc</span><span class="o">&lt;</span><span class="mi">400</span>    <span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">hidden</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">unit_type</span><span class="o">::</span><span class="n">SOFTMAX</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="o">&gt;</span><span class="p">,</span>
<a name="rest_code_99b44b58301048718fc70e69c4781cbf-7"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">trainer</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">sgd_trainer</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;&gt;::</span><span class="n">dbn_t</span><span class="p">;</span>
<a name="rest_code_99b44b58301048718fc70e69c4781cbf-8"></a>
<a name="rest_code_99b44b58301048718fc70e69c4781cbf-9"></a><span class="k">auto</span> <span class="n">network</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">network_t</span><span class="o">&gt;</span><span class="p">();</span>
<a name="rest_code_99b44b58301048718fc70e69c4781cbf-10"></a><span class="n">network</span><span class="o">-&gt;</span><span class="n">pretrain</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">training_images</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
<a name="rest_code_99b44b58301048718fc70e69c4781cbf-11"></a><span class="n">network</span><span class="o">-&gt;</span><span class="n">fine_tune</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">training_images</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">training_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
</pre>
<p>In my opinion, this is the best way to use DLL. This is the fastest and the
clearest. Moreover, the dimensions of the network can be validated at compile
time, which is always better than at runtime. However, the dimensions of the
network cannot be changed at runtime.  For this, there is a different version,
the dynamic version:</p>
<pre class="code cpp"><a name="rest_code_0b601520b84e4681b1524f522bffc46e-1"></a><span class="k">using</span> <span class="n">network_t</span> <span class="o">=</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-2"></a>    <span class="n">dll</span><span class="o">::</span><span class="n">dbn_desc</span><span class="o">&lt;</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-3"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dbn_layers</span><span class="o">&lt;</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-4"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">dyn_rbm_desc</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-5"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">dyn_rbm_desc</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-6"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">dyn_rbm_desc</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">hidden</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">unit_type</span><span class="o">::</span><span class="n">SOFTMAX</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="o">&gt;</span><span class="p">,</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-7"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">trainer</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">sgd_trainer</span><span class="o">&gt;&gt;::</span><span class="n">dbn_t</span><span class="p">;</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-8"></a>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-9"></a><span class="k">auto</span> <span class="n">network</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">network_t</span><span class="o">&gt;</span><span class="p">();</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-10"></a>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-11"></a><span class="n">network</span><span class="o">-&gt;</span><span class="k">template</span> <span class="n">layer_get</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">().</span><span class="n">init_layer</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">500</span><span class="p">);</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-12"></a><span class="n">network</span><span class="o">-&gt;</span><span class="k">template</span> <span class="n">layer_get</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">().</span><span class="n">init_layer</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">400</span><span class="p">);</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-13"></a><span class="n">network</span><span class="o">-&gt;</span><span class="k">template</span> <span class="n">layer_get</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">().</span><span class="n">init_layer</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-14"></a><span class="n">network</span><span class="o">-&gt;</span><span class="k">template</span> <span class="n">layer_get</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">().</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">;</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-15"></a><span class="n">network</span><span class="o">-&gt;</span><span class="k">template</span> <span class="n">layer_get</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">().</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">;</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-16"></a><span class="n">network</span><span class="o">-&gt;</span><span class="k">template</span> <span class="n">layer_get</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">().</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">;</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-17"></a>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-18"></a><span class="n">network</span><span class="o">-&gt;</span><span class="n">pretrain</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">training_images</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
<a name="rest_code_0b601520b84e4681b1524f522bffc46e-19"></a><span class="n">network</span><span class="o">-&gt;</span><span class="n">fine_tune</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">training_images</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">training_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
</pre>
<p>This is a bit more verbose, but the configuration can be changed at runtime with
this system. Moreover, this is also faster to compile. On the other hand, there
is some performance slowdown.</p>
<p>There is also a third version that is a hybrid of the first version:</p>
<pre class="code cpp"><a name="rest_code_cc10af0436b64f1fb4fe27f5716ba6e1-1"></a><span class="k">using</span> <span class="n">network_t</span> <span class="o">=</span>
<a name="rest_code_cc10af0436b64f1fb4fe27f5716ba6e1-2"></a>    <span class="n">dll</span><span class="o">::</span><span class="n">dyn_dbn_desc</span><span class="o">&lt;</span>
<a name="rest_code_cc10af0436b64f1fb4fe27f5716ba6e1-3"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dbn_layers</span><span class="o">&lt;</span>
<a name="rest_code_cc10af0436b64f1fb4fe27f5716ba6e1-4"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">rbm_desc</span><span class="o">&lt;</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_cc10af0436b64f1fb4fe27f5716ba6e1-5"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">rbm_desc</span><span class="o">&lt;</span><span class="mi">500</span>    <span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_cc10af0436b64f1fb4fe27f5716ba6e1-6"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">rbm_desc</span><span class="o">&lt;</span><span class="mi">400</span>    <span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">hidden</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">unit_type</span><span class="o">::</span><span class="n">SOFTMAX</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="o">&gt;</span><span class="p">,</span>
<a name="rest_code_cc10af0436b64f1fb4fe27f5716ba6e1-7"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">trainer</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">sgd_trainer</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;&gt;::</span><span class="n">dbn_t</span><span class="p">;</span>
<a name="rest_code_cc10af0436b64f1fb4fe27f5716ba6e1-8"></a>
<a name="rest_code_cc10af0436b64f1fb4fe27f5716ba6e1-9"></a><span class="k">auto</span> <span class="n">network</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">network_t</span><span class="o">&gt;</span><span class="p">();</span>
<a name="rest_code_cc10af0436b64f1fb4fe27f5716ba6e1-10"></a><span class="n">network</span><span class="o">-&gt;</span><span class="n">pretrain</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">training_images</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
<a name="rest_code_cc10af0436b64f1fb4fe27f5716ba6e1-11"></a><span class="n">network</span><span class="o">-&gt;</span><span class="n">fine_tune</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">training_images</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">training_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
</pre>
<p>Only one line was changed compared to the first version, <code>dbn_desc</code>
becomes <code>dyn_dbn_desc</code>. What this changes is that all the layers are
automatically transformed into their dynamic versions and all the parameters are
propagated at runtime. This is a form a type erasing since the sizes will not be
propagated at compilation time. But this is simple since the types are simply
transformed from one type to another directly. Behind the scene, it's the
dynamic version using the front-end of the fast version. This is almost as fast
to compile as the dynamic version, but the code is much better. It executes the
same as the dynamic version.</p>
<p>If we compare the compilation time of the three versions when compiling a single
network and 5 different networks with different architectures, we get the
following results (with clang):</p>
<table border="1" class="docutils">
<colgroup>
<col width="52%">
<col width="48%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Model</th>
<th class="head">Time [s]</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>1 Fast</td>
<td>30</td>
</tr>
<tr>
<td>1 Dynamic</td>
<td>16.6</td>
</tr>
<tr>
<td>1 Hybrid</td>
<td>16.6</td>
</tr>
<tr>
<td>5 Fast</td>
<td>114</td>
</tr>
<tr>
<td>5 Dynamic</td>
<td>16.6</td>
</tr>
<tr>
<td>5 Hybrid</td>
<td>21.9</td>
</tr>
</tbody>
</table>
<p>Even with one single network, the compilation time is reduced by 44%. When five
different networks are compilation, time is reduced by 85%. This can be
explained easily. Indeed, for the hybrid and dynamic versions, the layers will
have the same type and therefore a lot of template instantiations will only be
done once instead of five times. This makes a lot of difference since almost
everything is template inside the library.</p>
<p>Unfortunately, this also has an impact on the runtime of the network:</p>
<table border="1" class="docutils">
<colgroup>
<col width="26%">
<col width="41%">
<col width="32%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Model</th>
<th class="head">Pretrain [s]</th>
<th class="head">Train [s]</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>Fast</td>
<td>195</td>
<td>114</td>
</tr>
<tr>
<td>Dynamic</td>
<td>203</td>
<td>123</td>
</tr>
<tr>
<td>Hybrid</td>
<td>204</td>
<td>122</td>
</tr>
</tbody>
</table>
<p>On average, for dense models, the slowdown is between 4% and 8%. For
convolutional models, it is between 10% and 25%. I will definitely work on
trying to make the dynamic and especially the hybrid version faster in the
future, most on the work should be on the matrix library (ETL) that is used.</p>
<p>Since for test cases, a 20% increase in runtime is not really a problem, tests
being fast already, I decided to add an option to DLL so that everything can be
compiled by default in hybrid model. By using a compilation flag, all the
<code>dbn_desc</code> are becoming <code>dyn_dbn_desc</code> and therefore each used
network is becoming a hybrid network. Without a single change in the code, the
compilation time of the entire library can be significantly improved, as seen in
the next section.  This can also be used in user code to improve compilation
time during debugging and experiments and can be turned off for the final
training.</p>
<p>On my Continuous Integration system, I will build the system in both
configurations. This is not really an issue, since my personal machine at home
is more powerful than what I have available here.</p>
</div>
<div class="section" id="results">
<h2>Results</h2>
<p>On a first experiment, I measured the difference before and after this change on
the three executables of the library, with gcc:</p>
<table border="1" class="docutils">
<colgroup>
<col width="23%">
<col width="26%">
<col width="26%">
<col width="26%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Model</th>
<th class="head">Unit [s]</th>
<th class="head">Perf [s]</th>
<th class="head">Misc [s]</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>Before</td>
<td>1029</td>
<td>192</td>
<td>937</td>
</tr>
<tr>
<td>After</td>
<td>617</td>
<td>143</td>
<td>619</td>
</tr>
<tr>
<td>Speedup</td>
<td>40.03%</td>
<td>25.52%</td>
<td>33.93%</td>
</tr>
</tbody>
</table>
<p>It is clear that the speedups are very significant! The compilation is between
25% and 40% faster with the new option. Overall, this is a speedup of 36%!
I also noticed that the compilation takes significantly less memory than before.
Therefore, I decided to rerun the compiler benchmark on the library. In the
previous experiment, zapcc was taking so much memory that it was impossible to
use more than one thread. Let's see how it is faring now. The time to compile
the full unit tests is computed for each compiler. Let's start in debug mode:</p>
<table border="1" class="docutils">
<colgroup>
<col width="23%">
<col width="19%">
<col width="19%">
<col width="19%">
<col width="19%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Debug</th>
<th class="head">-j1</th>
<th class="head">-j2</th>
<th class="head">-j3</th>
<th class="head">-j4</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>clang-3.9</td>
<td>527</td>
<td>268</td>
<td>182</td>
<td>150</td>
</tr>
<tr>
<td>gcc-4.9.3</td>
<td>591</td>
<td>303</td>
<td>211</td>
<td>176</td>
</tr>
<tr>
<td>gcc-5.3.0</td>
<td>588</td>
<td>302</td>
<td>209</td>
<td>175</td>
</tr>
<tr>
<td>zapcc-1.0</td>
<td><strong>375</strong></td>
<td><strong>187</strong></td>
<td><strong>126</strong></td>
<td><strong>121</strong></td>
</tr>
</tbody>
</table>
<p>This time, zapcc is able to scale to four threads without problems. Moreover, it
is always the fastest compiler, by a significant margin, in this configuration.
It is followed by clang and then by gcc for which both versions are about the
same speed.</p>
<p>If we compile again in release mode:</p>
<table border="1" class="docutils">
<colgroup>
<col width="24%">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="16%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Release</th>
<th class="head">-j1</th>
<th class="head">-j2</th>
<th class="head">-j3</th>
<th class="head">-j4</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>clang-3.9</td>
<td>1201</td>
<td>615</td>
<td>421</td>
<td>356</td>
</tr>
<tr>
<td>gcc-4.9.3</td>
<td>1041</td>
<td>541</td>
<td>385</td>
<td>321</td>
</tr>
<tr>
<td>gcc-5.3.0</td>
<td>1114</td>
<td>579</td>
<td>412</td>
<td>348</td>
</tr>
<tr>
<td>zapcc-1.0</td>
<td><strong>897</strong></td>
<td><strong>457</strong></td>
<td><strong>306</strong></td>
<td><em>306</em></td>
</tr>
</tbody>
</table>
<p>The difference in compilation time is very large, it's twice slower to compile
with all optimizations enabled. It also takes significantly more memory. Indeed,
zapcc was not able to compile with 4 threads. Nevertheless, even the results
with three threads are better than the other compilers using four threads. zapcc
is clearly the winner again on this test, followed by gcc4-9 which is faster
than gcc-5.3 which is itself faster than clang. It seems that while clang is
better at frontend than gcc, it is slower for optimizations. Note that this may
also be an indication that clang performs more optimizations than gcc and may
not be slower.</p>
</div>
<div class="section" id="conclusion">
<h2>Conclusion</h2>
<p>By using some form of type erasing to simplify the templates types at compile
time, I was able to reduce the overall compilation time of my Deep Learning
Library (DLL) by 36%. Moreover, this can be done by switching a simple
compilation flag. This also very significantly reduce the memory used during the
compilation, allowing zapcc to to compile with up to three threads, compared
with only one before. This makes zapcc the fastest compiler again on this
benchmark. Overall, this will make debugging much easier on this library and
will save me a lot of time.</p>
<p>In the future, I plan to try to improve compilation time even more. I have a few
ideas, especially in ETL that should significantly improve the compilation time
but that will require a lot of time to implement, so that will likely have to
wait a while. In the coming days, I plan to work on the performance of DLL,
especially for stochastic gradient descent.</p>
<p>If you want more information on DLL, you can check out the
<a class="reference external" href="https://github.com/wichtounet/dll">dll Github repository</a>.</p>
</div>
</div>
        </div>
            
        
    <a href="posts/2017/03/partial-type-erasing-deep-learning-library-dll-improve-compilation-time.html#disqus_thread" data-disqus-identifier="cache/posts/2017/03/partial-type-erasing-deep-learning-library-dll-improve-compilation-time.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/03/clang-tidy-static-analysis-integration-in-sonarqube.html" class="u-url">Use clang-tidy for static analysis and integration in Sonarqube</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-03-11T09:54:00+01:00">2017-03-11 09:54</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>clang-tidy is an extensive linter C++. It provides a complete framework for
analysis of C++ code. Some of the checks are very simple but some of them are
very complete and most of the checks from the clang-static-analyzer are
integrated into clang-tidy.</p>
<div class="section" id="usage">
<h2>Usage</h2>
<p>If you want to see the list of checks available on clang-tidy, you can use the
list-checks options:</p>
<pre class="code text"><a name="rest_code_4873f800a63e48208350b5e1bd01002c-1"></a>clang-tidy -list-checks
</pre>
<p>You can then choose the tests you are interested in and perform an analysis of
your code. For, it is highly recommended to use a Clang compilation database,
you can have a look at Bear to generate this compilation database if you don't
have it yet. The usage of clang-tidy, is pretty simple, you set the list of
checks you want, the header on which you want to have warnings reported and the
list of source files to analyse:</p>
<pre class="code text"><a name="rest_code_2372917bf86b4df0a1e23386e10a2041-1"></a>clang-tidy -checks='*' -header-filter="^include" -p . src/*.cpp
</pre>
<p>You'll very likely see a lot of warnings. And you will very likely see a lot of
false positives and a lot of warnings you don't agree too. For insance, there
are a lot of warnings from the CPP Core Guidelines and the Google Guidelines
that I don't follow in my coding. You should not take the complete list of tests
as rule, you should devise your own list of what you really want to fix in your
code. If you want to disable one check X, you can use the - operation:</p>
<pre class="code text"><a name="rest_code_8e74b4dd14794fad813defae8441f9e8-1"></a>clang-tidy -checks='*,-X' -header-filter="^include" -p . src/*.cpp
</pre>
<p>You can also enable the checks one by one or parts of them with *:</p>
<pre class="code text"><a name="rest_code_841d344046224ed1a38654338e5a3ba4-1"></a>clang-tidy -checks='google-*' -header-filter="^include" -p . src/*.cpp
</pre>
<p>One problem with the clang-tidy tool is that it is utterly slow, especially if
you enable the clang-static-analyzer checks. Moreover, if you use it like it is
set before, it will only use one thread for the complete set of files. This may
not be an issue on small projects, but this will definitely be a big issue for
large projects and template-heavy code (like my ETL project). You could create
an implicit target into your Makefile to use it on each file independently and
then use the -j option of make to make them in parallel, but it not really
practical.</p>
<p>For this, I just discovered that clang propose a Python script,
run-clang-tidy.py that does it all for us! On Gentoo, it is installed at
/usr/share/run-clang-tidy.py.</p>
<pre class="code text"><a name="rest_code_9251ff48a9c940b8a10c9fd535c14444-1"></a>run-clang-tidy.py -checks='*' -header-filter="^include" -p . -j9
</pre>
<p>This will automatically run clang-tidy on each file from the compilation
database and use 9 threads to perform the checks. This is definitely much
faster. For me, this is the best way to run clang-tidy.</p>
<p>One small point I don't like is that the script always print the list of enabled
checks. For, this I changed this line in the script:</p>
<pre class="code python"><a name="rest_code_7db30eb0c1c84bbf8c265d0b843a75f1-1"></a><span class="n">invocation</span> <span class="o">=</span> <span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">clang_tidy_binary</span><span class="p">,</span> <span class="s1">'-list-checks'</span><span class="p">]</span>
</pre>
<p>with:</p>
<pre class="code python"><a name="rest_code_aa4b8e3d2f1e49179ae432fc5d189562-1"></a><span class="n">invocation</span> <span class="o">=</span> <span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">clang_tidy_binary</span><span class="p">]</span>
</pre>
<p>This makes it more quiet.</p>
<p>One thing I didn't mention is that clang-tidy is able to fix some of the errors
directly if you use the -fix option. Personally, I don't like this, but for
a large code base and a carefully selected set of checks, this could be really
useful. Note that not all the checks are automatically fixable by clang-tidy.</p>
</div>
<div class="section" id="results">
<h2>Results</h2>
<p>I have run clang-tidy on my cpp-utils library and here some interesting results.
I have not run all the checks, here is the command I used:</p>
<pre class="code text"><a name="rest_code_04d5f91813de48bf994d6742facb15f4-1"></a>/usr/share/clang/run-clang-tidy.py -p . -header-filter '^include/cpp_utils' -checks='cert-*,cppcoreguidelines-*,google-*,llvm-*,misc-*,modernize-*,performance-*,readility-*,-cppcoreguidelines-pro-type-reinterpret-cast,-cppcoreguidelines-pro-bounds-pointer-arithmetic,-google-readability-namespace-comments,-llvm-namespace-comment,-llvm-include-order,-google-runtime-references' -j9 2&gt;/dev/null  | /usr/bin/zgrep -v "^clang-tidy"
</pre>
<p>Let's go over some warnings I got:</p>
<pre class="code text"><a name="rest_code_28edd264ff064b1a8758c122b7d99c4b-1"></a>include/cpp_utils/assert.hpp:91:103: warning: consider replacing 'long' with 'int64' [google-runtime-int]
<a name="rest_code_28edd264ff064b1a8758c122b7d99c4b-2"></a>void assertion_failed_msg(const CharT* expr, const char* msg, const char* function, const char* file, long line) {
<a name="rest_code_28edd264ff064b1a8758c122b7d99c4b-3"></a>                                                                                                      ^
</pre>
<p>I got this one several times. It is indeed more portable to use <code>int64</code> rather than <code>long</code>.</p>
<pre class="code text"><a name="rest_code_357ca8bc750848be9c071fbcc2e8038b-1"></a>include/cpp_utils/aligned_allocator.hpp:53:9: warning: use 'using' instead of 'typedef' [modernize-use-using]
<a name="rest_code_357ca8bc750848be9c071fbcc2e8038b-2"></a>        typedef aligned_allocator&lt;U, A&gt; other;
<a name="rest_code_357ca8bc750848be9c071fbcc2e8038b-3"></a>        ^
</pre>
<p>This one is part of the modernize checks, indicating that one should use
<code>using</code> rather than a <code>typedef</code> and I completely agree.</p>
<pre class="code cpp"><a name="rest_code_01b4b74415424c50b71aa0b59006ecd3-1"></a><span class="n">include</span><span class="o">/</span><span class="n">cpp_utils</span><span class="o">/</span><span class="n">aligned_allocator</span><span class="p">.</span><span class="nl">hpp</span><span class="p">:</span><span class="mi">79</span><span class="o">:</span><span class="mi">5</span><span class="o">:</span> <span class="nl">warning</span><span class="p">:</span> <span class="n">use</span> <span class="err">'</span><span class="o">=</span> <span class="k">default</span><span class="err">'</span> <span class="n">to</span> <span class="n">define</span> <span class="n">a</span> <span class="n">trivial</span> <span class="k">default</span> <span class="n">constructor</span> <span class="p">[</span><span class="n">modernize</span><span class="o">-</span><span class="n">use</span><span class="o">-</span><span class="k">default</span><span class="p">]</span>
<a name="rest_code_01b4b74415424c50b71aa0b59006ecd3-2"></a>    <span class="n">aligned_allocator</span><span class="p">()</span> <span class="p">{}</span>
<a name="rest_code_01b4b74415424c50b71aa0b59006ecd3-3"></a>    <span class="o">^</span>
<a name="rest_code_01b4b74415424c50b71aa0b59006ecd3-4"></a>                        <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</pre>
<p>Another one from the modernize checks that I really like. This is completely
true.</p>
<!-- code.:

include/cpp_utils/maybe_parallel.hpp:33:5: warning: constructors that are callable with a single argument must be marked explicit to avoid unintentional implicit conversions [google-explicit-constructor]
    thread_pool(Args... /*args*/){
    ^
    explicit -->
<p>I don't agree that every constructor with one argument should be explicit,
sometimes you want implicit conversion. Nevertheless, this particular case is
very interesting since it is variadic, it can have one template argument and as
thus it can be implicitly converted from anything, which is pretty bad I think.</p>
<pre class="code cpp"><a name="rest_code_61886842f19741e0aeb610157e71796a-1"></a><span class="n">test</span><span class="o">/</span><span class="n">array_wrapper</span><span class="p">.</span><span class="nl">cpp</span><span class="p">:</span><span class="mi">15</span><span class="o">:</span><span class="mi">18</span><span class="o">:</span> <span class="nl">warning</span><span class="p">:</span> <span class="n">C</span><span class="o">-</span><span class="n">style</span> <span class="n">casts</span> <span class="n">are</span> <span class="n">discouraged</span><span class="p">;</span> <span class="n">use</span> <span class="k">reinterpret_cast</span> <span class="p">[</span><span class="n">google</span><span class="o">-</span><span class="n">readability</span><span class="o">-</span><span class="n">casting</span><span class="p">]</span>
<a name="rest_code_61886842f19741e0aeb610157e71796a-2"></a>    <span class="kt">float</span><span class="o">*</span> <span class="n">mem</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="mi">8</span><span class="p">);</span>
<a name="rest_code_61886842f19741e0aeb610157e71796a-3"></a>                 <span class="o">^</span>
<a name="rest_code_61886842f19741e0aeb610157e71796a-4"></a>                 <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span>         <span class="p">)</span>
</pre>
<p>On this one, I completely agree, C-style casts should be avoided and much
clearer C++ style casts should be preferred.</p>
<pre class="code cpp"><a name="rest_code_4297a265a7c94bdc9a7437f6b1de083c-1"></a><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">wichtounet</span><span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">cpp_utils_test</span><span class="o">/</span><span class="n">include</span><span class="o">/</span><span class="n">cpp_utils</span><span class="o">/</span><span class="n">aligned_allocator</span><span class="p">.</span><span class="nl">hpp</span><span class="p">:</span><span class="mi">126</span><span class="o">:</span><span class="mi">19</span><span class="o">:</span> <span class="nl">warning</span><span class="p">:</span> <span class="n">thrown</span> <span class="n">exception</span> <span class="n">type</span> <span class="n">is</span> <span class="n">not</span> <span class="n">nothrow</span> <span class="n">copy</span> <span class="n">constructible</span> <span class="p">[</span><span class="n">cert</span><span class="o">-</span><span class="n">err60</span><span class="o">-</span><span class="n">cpp</span><span class="p">]</span>
<a name="rest_code_4297a265a7c94bdc9a7437f6b1de083c-2"></a>            <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">length_error</span><span class="p">(</span><span class="s">"aligned_allocator&lt;T&gt;::allocate() - Integer overflow."</span><span class="p">);</span>
<a name="rest_code_4297a265a7c94bdc9a7437f6b1de083c-3"></a>                  <span class="o">^</span>
</pre>
<p>This is one of the checks I don't agree with. Even though it makes sense to
prefer exception that are nothrow copy constructible, they should be caught by
const reference anyway. Moreover, this is here an exception from the standard
library.</p>
<pre class="code text"><a name="rest_code_c19d5acc8e08417cada0c13f496b2625-1"></a>/home/wichtounet/dev/cpp_utils_test/include/cpp_utils/aligned_allocator.hpp:141:40: warning: do not use const_cast [cppcoreguidelines-pro-type-const-cast]
<a name="rest_code_c19d5acc8e08417cada0c13f496b2625-2"></a>        free((reinterpret_cast&lt;void**&gt;(const_cast&lt;std::remove_const_t&lt;T&gt;*&gt;(ptr)))[-1]);
<a name="rest_code_c19d5acc8e08417cada0c13f496b2625-3"></a>                                       ^
</pre>
<p>In general, I agree that using const_cast should be avoided as much as possible.
But there are some cases where they make sense. In this particular case, I don't
modify the object itself but some memory before the object that is unrelated and
I initialize myself.</p>
<p>I also had a few false positives, but overall nothing too bad. I'm quite
satisfied with the quality of the results. I'll fix these warnings in the coming
week.</p>
<p>Integration in Sonarqube</p>
<p>The sonar-cxx plugin just integrated support for clang-tidy in main. You need
to build the version yourself, the 0.9.8-SNAPSHOT version. You then can use
something like this in your sonar-project.properties file:</p>
<pre class="code text"><a name="rest_code_ad7190aee6584f44945437adc9ffab55-1"></a>sonar.cxx.clangtidy.reportPath=clang-tidy-report
</pre>
<p>and sonar-cxx will parse the results and integrate the issues in your sonar
report.</p>
<p>Here is an example:</p>
<img alt="/images/sonar-cxx-clang-tidy.png" src="images/sonar-cxx-clang-tidy.png"><p>You can see two of the warnings from clang-tidy :)</p>
<p>For now, I haven't integrate this in my Continuous Integration system because
I'm still having issues with clang-tidy and the compilation database. Because
the compilation contains absolute paths to the file and to the current
directory, it cannot be shared directly between servers. I have to find a way to
fix that so that clang-tidy can use on the other computer. I'll probably wait
till the sonar-cxx 0.9.8 version is released before integrating all this in
Sonarqube, but this is a great news for this plugin :)</p>
</div>
<div class="section" id="conclusion">
<h2>Conclusion</h2>
<p>clang-tidy is C++ linter that can analyze your code and checks for hundreds of
problems in it. With it, I have found some very interesting problems in the code
of my cpp_utils library. Moreover, you can now integrate it Sonarqube by using
the sonar-cxx plugin. Since it is a bit slow, I'll probably not integrate it in
my bigger projects, but I'll integrate at least in the cpp_utils library when
sonar-cxx 0.9.8 will be released.</p>
</div>
</div>
        </div>
            
        
    <a href="posts/2017/03/clang-tidy-static-analysis-integration-in-sonarqube.html#disqus_thread" data-disqus-identifier="cache/posts/2017/03/clang-tidy-static-analysis-integration-in-sonarqube.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/03/disappointing-zapcc-performance-on-deep-learning-library-dll.html" class="u-url">Disappointing zapcc performance on Deep Learning Library (DLL)</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-03-09T13:41:06+01:00">2017-03-09 13:41</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>One week ago, zapcc 1.0 was released and I've observed it to be much faster than the other
compilers in terms of compile time. This can be seen when
<a class="reference external" href="http://baptiste-wicht.com/posts/2017/03/release-zapcc-10-fast-cpp-compiler.html">I tested it on my Expression Templates Library (ETL)</a>. It was almost four
times faster than clang 3.9 and about 2.5 times faster than GCC.</p>
<p>The ETL library is quite heavy to compile, but still reasonable. This is not the
case for my Deep Learning Library (DLL) where compiling all the test cases takes
a very long time. I have to admit that I have been going overboard with
templates and such and I have now to pay the price. In practice, for the users
of the library, this is not a big problem since only one or two neural networks
will be compiled (and it will take hours to train), but in the test cases, there
are hundreds of them and this is a huge pain. Anyway, enough with the ramble,
I figured it would be very good to test zapcc on it and see what I can gain from
using it.</p>
<p>In this article, when I speak of a compiler thread, I mean an instance of the
processor, so it's really a process in the Linux world.</p>
<div class="section" id="results">
<h2>Results</h2>
<p>However, I soon realized that I would have more issues than I thought. The first
problem is the memory consumed by zapcc. Indeed, it is based on clang and
I always had problem with huge memory consumption from clang on this library and
zapcc has even bigger memory consumption because some information is cached
between runs. The amount of memory that zapcc is able to cache can be configured
in the configuration file. By default, it can use 1.5Go of memory. When zapcc
goes over the memory limit, it simply wipes out its caches. This means that all
the gain for the next compilation will be lost, since the cache will have to be
rebuilt from scratch. This is not a hard limit for the compilation itself.
Indeed, if the compilation itself takes 3Go, it will still be able to complete
it, but it is likely that the cache will be wiped after the compilation.</p>
<p>When I tried compiling using several threads, it soon used all my memory and
crashed. The same occurs with clang but I can still compile with 3 or 4 threads
without too much issues on this computer. The same also occurs with GCC but it
can still handle 4 or 5 threads (depending on the order of the compilation
units).</p>
<p>The tests are performed on my desktop computer at work, which is not really
good... I have 12Go of RAM (I had to ask for extra...) and an old Sandy Bridge
processor, but at least I have an SSD (also had to ask for extra).</p>
<p>I started with testing with only one compiler thread. For zapcc, I set the
maximum memory limit to 8Go. Even with such a limit, the zapcc server restarted
more than 10 times during the compilation of the 84 test cases. After this first
experiment, I increased the number of threads to 2 for each compiler, using 4Go
limit for zapcc. The limit is for each server and each parallel thread will
spawn a new server, so the effective limit is the number of threads times the
limit. Even with two threads, I was unable to finish a compilation with zapcc.
This is quite disappoint for me since clang is able to run with 4 threads in
parallel. Moreover, a big problem with that is that the servers are not always
killed when there is no no more memory, they just hang and use all the memory of
the computer, which is evidently really inconvenient for service processes. When
this happens with clang or gcc, the compiler simply crashes and the memory is
released and make is interrupted. Since zapcc is not able to work with more than
one thread on this computer, the results are the ones with one thread. I was
also surprised to be able to compile the library with clang and four threads,
this was not possible before clang-3.9.</p>
<table border="1" class="docutils">
<colgroup>
<col width="36%">
<col width="17%">
<col width="17%">
<col width="15%">
<col width="15%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Compiler</th>
<th class="head">-j1</th>
<th class="head">-j2</th>
<th class="head">-j3</th>
<th class="head">-j4</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>gcc-4.9.3</td>
<td>2250.95</td>
<td>1256.36</td>
<td>912.67</td>
<td>760.84</td>
</tr>
<tr>
<td>gcc-5.3.0</td>
<td>2305.37</td>
<td>1279.49</td>
<td>918.08</td>
<td>741.38</td>
</tr>
<tr>
<td>clang-3.9</td>
<td>2047.61</td>
<td><strong>1102.93</strong></td>
<td><strong>899.13</strong></td>
<td><strong>730.42</strong></td>
</tr>
<tr>
<td>zapcc-1.0</td>
<td><strong>1483.73</strong></td>
<td>1483.73</td>
<td>1483.73</td>
<td>1483.73</td>
</tr>
<tr>
<td>Difference against Clang</td>
<td>-27.55%</td>
<td>+25.69%</td>
<td>+39.37%</td>
<td>+50.77%</td>
</tr>
<tr>
<td>Speedup VS GCC-5.3</td>
<td>-35.66%</td>
<td>+13.75%</td>
<td>+38.09%</td>
<td>+50.03%</td>
</tr>
<tr>
<td>Speedup VS GCC-4.9</td>
<td>-34.08%</td>
<td>+15.30%</td>
<td>+38.50%</td>
<td>+48.75%</td>
</tr>
</tbody>
</table>
<p>If we look at the results with only one thread, we can see that there still are
some significant improvements when using zapcc, but nowhere near as good as what
was seen in the compilation of ETL. Here, the compilation time is reduced by 34%
compared to gcc and by 27% compared to clang. This is not bad, since it is
faster than the other compilers, but I would have expected better speedups. We
can see that g++-4.9 is slightly faster than g++-5.3, but this is not really
a significant difference. I'm actually very surprised to find that clang is
faster than g++ on this experiment. On ETL, it is always very significantly
slower and before, it was also significantly slower on DLL. I was so used to
this, that I stopped using it on this project. I may have to reconsider my
position when working on this project.</p>
<p>Let's look at the results with more than two threads. Even with two threads,
every compiler is faster than zapcc. Indeed, zapcc is slower than Clang by 25%
and slower than GCC by about 15%. If we use more threads, the other compilers
are becoming even faster and the slowdowns of zapcc are more important. When
using four threads, zapcc is about 48% slower than gcc and about 50% slower than
clang. This is really showing one big downside of zapcc that has a very large
memory consumption. When it is used to compile really heavy template code, it is
failing very early to use more processes. And even when there is enough memory,
the speedups are not as great as for relatively simpler code.</p>
<p>One may argue that this is not a fair comparison since zapcc does not have the
same numbers of threads. However, considering that this is the best zapcc can do
on this machine, I would argue that this is a fair comparison in this limited
experimental setting. If we were to have a big machine for compilation, which
I don't have at work, the zapcc results would likely be more interesting, but in
this specific limited case, it shows that zapcc suffers from its high memory
consumption. It should also be taken into account that this experiment was done
with almost nothing else running on the machine (no browser for instance) to
have as much memory as possible available for the compilers. This is not
a common use case.  Most of the days, when I compile something, I have my
browser open, which makes a large difference in memory available, and several
other applications (but consoles and vim instances do not really consume memory
:D).</p>
<p>This experiment made me realize that the compilation times for this library were
quickly becoming crazy. Most of the time, the complete test suite is only
compiled on my Continuous Integration machine at home which has a much faster
processor and much more RAM. Therefore, it is relatively fast since it uses more
threads to compile.  Nevertheless, this is not a good point that the unit tests
takes so much time to compile. I plan to split the test cases in several sets.
Because, currently the real unit tests are compiled with the performance tests
and other various tests. I'll probably end up generating three executables. This
will help greatly during development. Moreover, I also have a technique to
decrease the compilation time by erasing some template parameters at compilation
time. This is already ready, but has currently a runtime overhead that I will
try to remove and then use this technique everywhere to get back to reasonable
compilation times. I'll also try to see if I can find obvious compilation
bottlenecks in the code.</p>
</div>
<div class="section" id="conclusion">
<h2>Conclusion</h2>
<p>To conclude, while zapcc brings some very interesting compilation speedups in
some cases like in my ETL library, it also has some downsides, namely
<strong>huge memory consumption</strong>. This memory consumption may prevent the use of several
compiler threads and render zapcc much less interesting than other compilers.</p>
<p>When trying to compile my DLL library on a machine with 12Go of RAM with two
zapcc threads, it was impossible for me to make it complete. While zapcc was
faster with one thread than the other compilers, they were able to use up to
four threads and in the end <strong>zapcc was about twice slower than clang</strong>.</p>
<p>I knew that zapcc memory consumption was very large, but I would have not have
expected something so critical. Another feature that would be interesting in
zapcc would be to set a max memory hard limit for the server instead of simply
a limit on the cache they are able to keep in memory. This would prevent hanging
the complete computer when something goes wrong.</p>
<p>I had a good surprise with clang that was actually faster than GCC and also able
to work with four threads in parallel. This was not the case with previous
version of clang. On ETL, it is still significantly slower than GCC though.</p>
<p>For now, I'll continue using clang on this DLL project and use zapcc only on my
ETL project. I'll also focus on improving the compilation time on this project
and make it reasonable again.</p>
</div>
</div>
        </div>
            
        
    <a href="posts/2017/03/disappointing-zapcc-performance-on-deep-learning-library-dll.html#disqus_thread" data-disqus-identifier="cache/posts/2017/03/disappointing-zapcc-performance-on-deep-learning-library-dll.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/03/migrated-from-owncloud-5-to-nextcloud-11.html" class="u-url">Migrated from owncloud 5 to Nextcloud 11</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-03-03T09:08:58+01:00">2017-03-03 09:08</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>For several years now I've been using Owncloud running on one of my servers.
I'm using simply using as a simple synchronization, I don't use any of the tons
of fancy features they keep adding. Except from several synchronization issues,
I haven't had too much issues with it.</p>
<p>However, I have had a very bad time with updates of Owncloud. The last time
I tried, already long ago, was to upgrade from 5.0 to 6.0 and I never succeeded
without losing all the configuration and having to do the resync. Therefore,
I've still an Owncloud 5.0 running. From this time, I had to say that I've been
lazy and didn't try again to upgrade it. Recently, I've received several mails
indicating that this is a security threat.</p>
<p>Since I was not satisfied with updates in Owncloud and its security has been
challenged recently, I figured it would be a good moment to upgrade to Nextcloud
which is a very active fork of Owncloud that was forked by developers of
Owncloud.</p>
<p>I haven't even tried to do an upgrade from such an old version to the last
version of Nextcloud, it was doomed to fail. Therefore, I made a new clean
installation. Since I only use the sync feature of the tool, it does not really
matter, it is just some time lost to sync everything again, but nothing too bad.</p>
<p>I configured a new PostgreSQL on one of my servers for the new database and then
installed Nextcloud 11 on Gentoo. It's a bit a pain to have a working Nginx
configuration for Nextcloud, I don't advice to do it by hand, better take one
from the official documentation, you'll also gain some security. One very bad
thing in the installation process is that you cannot choose the database prefix,
it's set like Owncloud. The problem with that is that you cannot install both
Owncloud and Nextcloud on the same database which would be more practical for
testing purpose. It's a bit retarded in my opinion, but not a big problem in the
end. Other than these two points, everything went well and it was installation
pretty nicely. Then, you should have your user ready to go.</p>
<img alt="Nextcloud view" class="align-center" src="images/nextcloud.png"><p>As for the interface, I don't think there is a lot to tell here. Most of it is
what you would except from this kind of tool. Moreover, I very rarely use the
web interface or any of the feature that are not the sync feature. One thing
that is pretty cool I think is the monitoring graphs in the Admin section of the
interface. You can the number of users connected, the memory used and the CPU
load. It's pretty useful if you share your Nextcloud between a lot of different
users.</p>
<p>I didn't have any issue with the sync either. I used the nextcloud-client
package on Gentoo directly and it worked perfectly directly. It took about 10
minutes to sync everything again (about 5GB). I'll have to do the same thing on
my other computer as well, but I don't think I'll have any issue.</p>
<p>So far, I cannot say that this is better than Owncloud, I just hope the next
upgrade will fare better than they did on Owncloud. Moreover, I also hope that
the security that they promise is really here and I won't have any problem with
it. I'll see in the future!</p>
</div>
        </div>
            
        
    <a href="posts/2017/03/migrated-from-owncloud-5-to-nextcloud-11.html#disqus_thread" data-disqus-identifier="cache/posts/2017/03/migrated-from-owncloud-5-to-nextcloud-11.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/03/release-zapcc-10-fast-cpp-compiler.html" class="u-url">Release of zapcc 1.0 - Fast C++ compiler</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-03-02T14:50:04+01:00">2017-03-02 14:50</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>If you remember, I recently wrote about <a class="reference external" href="http://baptiste-wicht.com/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html">zapcc C++ compilation speed against gcc 5.4 and clang 3.9</a> in which I was comparing the beta version of zapcc against gcc and clang.</p>
<p>I just been informed that zapcc was just released in version 1.0. I though it
was a good occasion to test it again. It will be compared against gcc-4.9,
gcc-5.3 and clang-3.9. This version is based on the trunk of clang-5.0.</p>
<p>Again, I will use my Expression Template Library (<a class="reference external" href="https://github.com/wichtounet/etl/">ETL</a>) project. This is a purely header-only
library with lots of templates. I'm going to compile the full test cases. This
is a perfect example for long compilation times.</p>
<p>The current tests are made on the last version of the library and with slightly
different parameters for compilation, therefore the absolute times are not
comparable, but the speedups should be comparable.</p>
<p>Just like last time, I have configured zapcc to let is use 2Go RAM per caching
server, which is the maximum allowed. Moreover, I killed the servers before each
tests.</p>
<div class="section" id="debug-results">
<h2>Debug results</h2>
<p>Let's start with a debug build, with no optimizations enabled. Every build will
use four threads. This is the equivalent of doing make -j4 debug/bin/etl_test
without the link step.</p>
<table border="1" class="docutils">
<colgroup>
<col width="73%">
<col width="27%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Compiler</th>
<th class="head"> </th>
</tr></thead>
<tbody valign="top">
<tr>
<td>g++-4.9.3</td>
<td>190.09s</td>
</tr>
<tr>
<td>g++-5.3.0</td>
<td>200.92s</td>
</tr>
<tr>
<td>clang++-3.9</td>
<td>313.85</td>
</tr>
<tr>
<td>zapcc++</td>
<td>81.25</td>
</tr>
<tr>
<td>Speedup VS Clang</td>
<td>3.86</td>
</tr>
<tr>
<td>Speedup VS GCC-5.3</td>
<td>2.47</td>
</tr>
<tr>
<td>Speedup VS GCC-4.9</td>
<td>2.33</td>
</tr>
</tbody>
</table>
<p>The speedups are even more impressive than last time! zapcc is <strong>almost four
times fast than clang-3.9</strong> and around <strong>2.5 times faster than GCC-5.3</strong>.
Interestingly, we can see that gcc-5.3 is slighly slower than GCC-4.9.</p>
<p>It seems that they have the compiler even faster!</p>
</div>
<div class="section" id="release-results">
<h2>Release results</h2>
<p>Let's look now how the results are looking with optimizations enabled. Again,
every build will use four threads. This is the equivalent of doing make -j4
release_debug/bin/etl_test without the link step.</p>
<table border="1" class="docutils">
<colgroup>
<col width="75%">
<col width="25%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Compiler</th>
<th class="head"> </th>
</tr></thead>
<tbody valign="top">
<tr>
<td>g++-4.9.3</td>
<td>252.99</td>
</tr>
<tr>
<td>g++-5.3.0</td>
<td>264.96</td>
</tr>
<tr>
<td>clang++-3.9</td>
<td>361.65</td>
</tr>
<tr>
<td>zapcc++</td>
<td>237.96</td>
</tr>
<tr>
<td>Speedup VS Clang</td>
<td>1.51</td>
</tr>
<tr>
<td>Speedup VS GCC-5.3</td>
<td>1.11</td>
</tr>
<tr>
<td>Speedup VS GCC-4.9</td>
<td>1.06</td>
</tr>
</tbody>
</table>
<p>We can see that this time the speedups are not as interesting as they were.
Very interestingly, it's the compiler that suffers the more from the
optimization overhead. Indeed, zapcc is three times slower in release mode than
it was in debug mode. Nevertheless, it still manages to beat the three other
compilers, by about 10% for Gcc and 50% than clang, which is already
interesting.</p>
<div class="section" id="conclusion">
<h3>Conclusion</h3>
<p>To conclude, we have observed that zapcc is always faster than the three
compilers tested in this experiment. Moreover, in debug mode, the speedups are
very significant, it was almost 4 times faster than clang and around 2.5 faster
than gcc.</p>
<p>I haven't seen any problem with the tool, it's like clang and it should generate
code of the same performance, but just compile it much faster. One problem
I have with zapcc is that it is not based on an already released version of
clang but on the trunk. That means it is hard to be compare with the exact same
version of clang and it is also a risk of running into clang bugs.</p>
<p>Although the prices have not been published yet, it is indicated on the website
that zapcc is free for non-commercial entities. Which is really great.</p>
<p>If you want more information, you can go to the
<a class="reference external" href="https://www.zapcc.com/">official website of zapcc</a></p>
</div>
</div>
</div>
        </div>
            
        
    <a href="posts/2017/03/release-zapcc-10-fast-cpp-compiler.html#disqus_thread" data-disqus-identifier="cache/posts/2017/03/release-zapcc-10-fast-cpp-compiler.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/02/home-automation-first-attempt-voice-control-jarvis.html" class="u-url">Home Automation: First attempt at voice control with Jarvis</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-02-14T18:34:19+01:00">2017-02-14 18:34</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>I have several devices in my home that can be controller via Domoticz, a few
power outlets, a few lights (more are coming), my Kodi home theater. And I have
a lot of sensors and information gathered by Domoticz. All of this is working
quite well, but I have only a few actuators and intelligence (motion sensor,
button and some automation via Lua script).</p>
<p>My next objective was to add voice control to my system. If I was living in
United States or United Kingdom I would directly an Amazon Dot or even an Amazon
Echo, but they are not available in Switzerland. I could have arranged for
delivery, but if I want my system to be useful to several people, I need to have
in French. It's the same problem with the Google Home system. So, no other way
than custom solutions.</p>
<p>Since I had an extra Raspberry Pi 2, I based my system on this. I bought a Trust
Mico microphone and a Trust Compact speakers and installed them on the Pi. Both
peripherals are working quite well.</p>
<p>You can have a closer look at my microphone:</p>
<img alt="Trust Mico microphone for Jarvis Home Automation Voice Control" class="align-center" src="images/jarvis_mic.jpg"><p>and the complete installation:</p>
<img alt="Jarvis Home Automation Voice Control around my TV" class="align-center" src="images/jarvis_full.jpg"><p>The Raspberry Pi is on the bottom, the speakers below the TV, left and right and
the microphone on the top right.</p>
<p>For the voice control software, I decided to go with Jarvis. It seems to me that
this is the best suited software for this kind of project. Moreover, it supports
French natively which seems good. I also tried Jasper, but this has been such
a pain to install that I gave up.</p>
<p>Jarvis is reasonably easy to install if you have a recent Raspbian image. It
took some time to install the dependencies, but in the end it was not difficult.
The installation process has a step-by-step wizard help so it's really easy to
configure everything.</p>
<p>However, even if it's easy to install, it's easy to configure correctly. The
first thing is to configure the hotword to activate commands. There are several
options, but I used snowboy which is offline and is made for hotword
recognition. This worked quite well, you just have to train a model with the
hotword to recognize the voice. After this, the problems started... You then
have to configure audio for the commands themselves. There are 6 parameters for
audio capture (noise levels to start and stop the capture, silence levels, ...)
and no help to tune them. So basically, I tried a lot of combinations until
I had something working reasonably well. When you are in debug mode, you can
listen to what the system captured. These parameters are dependent on your
environment and on your microphone and on your voice. I may be dumb but it took
several hours and a lot of tries to get a configuration working. After this, you
have to choose the engine for recognition of the commands. Unfortunately, all
the good options are online so everything you'll say as commands after the
hotword will be sent online. I first tried Bing, but I had very poor recognition
rate. I then switched to wit.ai which gave me better results. In the end, I have
about 60% recognition rate, which is not great at all, but at least some phrases
are working almost all the time while others are always failing. Another problem
I have with this is the large delay between commands and action. It takes almost
five seconds between the end of my sentence and the time where the lights in my
living room are tuned on or off by Jarvis via Domoticz.</p>
<p>So far, I'm a bit disappointed by the quality of the system, but maybe I was
hoping for too much. I have been able to control a few of my appliances but not
really reliably. Another thing I have realized is that when counting the
Raspberry Pi, its enclosure the Microphone and the speakers, this system is more
costly than an Amazon Dot and seem highly inferior (and is much less good
looking).</p>
<p>I'll try to improve the current system with better configuration and commands in
the coming days and I will maybe try another system for voice control. I still
hope Amazon Alexa systems or Google Home are made available in
France/Switzerland not too far in the future, since I believe these systems are
a better solution than custom made systems, at least for now. Moreover, next
month, I plan to integrate ZWave into my systems with a few sensors, complete
the lighting installation and add new motion sensors. This should make it more
useful. And hopefully, by this time, I should have a good voice control system,
but I'm not too hopeful.</p>
<p>Don't hesitate to comment or contact me if you have questions about this
installation or want to share experience about voice control in home automation.
If you want more details about this, dont' hesitate to ask as well ;)</p>
</div>
        </div>
            
        
    <a href="posts/2017/02/home-automation-first-attempt-voice-control-jarvis.html#disqus_thread" data-disqus-identifier="cache/posts/2017/02/home-automation-first-attempt-at-voice-control-with-jarvis.html">Comments</a>


        </article><nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-30.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="blogwichtounet";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div> <!-- col -->
    </div>
<!-- row  -->
</div>
<!-- container-fluid -->

<!-- End of Menubar -->

<!-- Footer -->

<footer>
    Contents © 2017         <a href="mailto:baptistewicht@gmail.com">Baptiste Wicht</a> - Powered by         <a href="http://getnikola.com" rel="nofollow">Nikola</a>         - License: 
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="padding-left:5px;border-width:0" src="assets/img/cc.png"></a>
        <ul class="footer_inline_ul"></ul></footer><!-- Late loading stuff  --><script src="assets/js/all-nocdn.js"></script><script type="text/javascript">
      $(document).ready(function() {
        $.getJSON("/assets/js/tx3_tag_cloud.json", function(data){
            var items = [];
            $.each(data, function(key, val){
                var count = val[0];
                var url = val[1];
                var posts = val[2];

                if(count > 9){
                    items.push("<li data-weight='" + count + "'><a href='" + url + "'>" + key + "</a></li>");
                }
            });

            $("<ul/>", {
                "id": "tag_cloud_left",
                html: items.join("")
            }).appendTo("#tag_cloud_left_container");

            $("#tag_cloud_left").tx3TagCloud({
                multiplier: 0.8 // default multiplier is "1"
            });
        });
      });
    </script><!-- Google platform JS -->
</body>
</html>
