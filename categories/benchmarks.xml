<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Blog blog("Baptiste Wicht"); (Posts about Benchmarks)</title><link>http://baptiste-wicht.com/</link><description></description><atom:link rel="self" type="application/rss+xml" href="http://baptiste-wicht.com/categories/benchmarks.xml"></atom:link><language>en</language><lastBuildDate>Sun, 04 Jun 2017 20:17:48 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>C++ Containers Benchmark: vector/list/deque and plf::colony</title><link>http://baptiste-wicht.com/posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Already more than three years ago, I've written a &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/12/cpp-benchmark-vector-list-deque.html"&gt;benchmark of some of the STL containers&lt;/a&gt;,
namely the vector, the list and the deque. Since this article was very popular,
I decided to improve the benchmarks and collect again all the results. There are
now more benchmarks and some problems have been fixed in the benchmark code.
Moreover, I have also added a new container, the plf::colony. Therefore, there
are four containers tested:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The std::vector: This is a dynamically-resized array of elements. All the
elements are contiguous in memory. If an element is inserted or removed it at
a position other than the end, the following elements will be moved to fill
the gap or to open a gap. Elements can be accessed at random position in
constant time. The array is resized so that it can several more elements, not
resized at each insert operation. This means that insertion at the end of the
container is done in amortized constant time.&lt;/li&gt;
&lt;li&gt;The std::deque: The deque is a container that offer constant time insertion
both at the front and at the back of the collection. In current c++ libraries,
it is implementation as a collection of dynamically allocated fixed-size
array. Not all elements are contiguous, but depending on the size of the data
type, this still has good data locality. Access to a random element is also
done in constant time, but with more overhead than the vector. For insertions
and removal at random positions, the elements are shifted either to the front
or to the back meaning that it is generally faster than the vector, by twice
in average.&lt;/li&gt;
&lt;li&gt;The std::list: This is a doubly-linked list. It supports constant time
insertions at any position of the collection. However, it does not support
constant time random access. The elements are obviously not contiguous, since
they are all allocated in nodes. For small elements, this collection has
a very big memory overhead.&lt;/li&gt;
&lt;li&gt;The plf::colony: This container is a non-standard container which is
unordered, it means that the insertion order will not necessarily be
preserved. It provides strong iterators guarantee, pointers to non-erased
element are not invalidated by insertion or erasure. It is especially tailored
for high-insertion/erasure workloads. Moreover, it is also specially optimized
for non-scalar types, namely structs and classes with relatively large data
size (greater than 128 bits on the official documentation). Its implementation
is more complicated than the other containers. It is also implemented as
a list of memory blocks, but they are of increasingly large sizes. When
elements are erased, there position is not removed, but marked as erased so
that it can be reused for fast insertion later on. This container uses the
same conventions as the standard containers and was proposed for inclusion to
the standard library, which is the main reason why it's included in this
benchmark. If you want more information, you can consult the
&lt;a class="reference external" href="http://plflib.org/colony.htm"&gt;official website&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the text and results, the namespaces will be omitted. Note that I have only
included sequence containers in my test. These are the most common containers in
practices and these also the containers I'm the most familiar with. I could have
included multiset in this benchmark, but the interface and purpose being
different, I didn't want the benchmark to be confusing.&lt;/p&gt;
&lt;p&gt;All the examples are compiled with g++-4.9.4 (-std=c++11 -march=native -O2) and
run on a Gentoo Linux machine with an Intel Core i7-4770 at 3.4GHz.&lt;/p&gt;
&lt;p&gt;For each graph, the vertical axis represent the amount of time necessary to
perform the operations, so the lower values are the better. The horizontal axis
is always the number of elements of the collection. For some graph, the
logarithmic scale could be clearer, a button is available after each graph to
change the vertical scale to a logarithmic scale.&lt;/p&gt;
&lt;p&gt;The tests are done with several different data types. The trivial data types are
varying in size, they hold an array of longs and the size of the array varies to
change the size of the data type. The non-trivial data type is composed of
a string (just long enough to avoid SSO (Small String Optimization) (even though
I'm using GCC)). The non-trivial data types comes in a second version with
noexcept move operations.  Not all results are presented for each data types if
there are not significant differences between in order to keep this article
relatively short (it's already probably too long :P).&lt;/p&gt;
&lt;p class="more"&gt;&lt;a href="http://baptiste-wicht.com/posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html"&gt;Read moreâ€¦&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>C++</category><category>C++11</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html</guid><pubDate>Sun, 21 May 2017 10:46:23 GMT</pubDate></item><item><title>Continuous Performance Management with CPM for C++</title><link>http://baptiste-wicht.com/posts/2015/06/continuous-performance-management-with-cpm-for-cpp.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;For some time, I have wanted some tool to monitor the performance of some of my projects. There are plenty of tools for Continuous Integration and Sonar is really great for continuous monitoring of code quality, but I haven't found anything satisfying for monitoring performance of C++ code. So I decided to write my own. &lt;a class="reference external" href="https://github.com/wichtounet/cpm"&gt;Continous Performance Monitor (CPM)&lt;/a&gt; is a simple C++ tool that helps you running benchmarks for your C++ programs and generate web reports based on the results. In this article, I will present this tool. CPM is especially made to benchmark several sub parts of libraries, but it perfectly be used to benchmark a whole program as well.&lt;/p&gt;
&lt;p&gt;The idea is to couple it with a Continuous Integration tool (I use Jenkins for instance) and run the benchmarks for every new push in a repository. With that, you can check if you have performance regression for instance or simply see if your changes were really improving the performance as much as you thought.&lt;/p&gt;
&lt;p&gt;It is made of two separate parts:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;A header-only library that you can use to benchmark your program and that will give you the performance results. It will also generate a JSON report of the collected data.&lt;/li&gt;
&lt;li&gt;A program that will generate a web version of the reports with analysis over time, over different compilers or over different configurations.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;CPM is especially made to benchmark functions that takes input data and which runtime depends on the dimensions of the input data. For each benchmark, CPM will execute it with several different input sizes. There are different ways to define a benchmark:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;two_pass&lt;/em&gt;: The benchmark is made of two part, the initialization part is called once for each input size and then the benchmark part is repeated several times for the measure. This is the most general version.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;global&lt;/em&gt;: The benchmark will be run with different input sizes but uses global data that will be randomized before each measure&lt;/li&gt;
&lt;li&gt;&lt;em&gt;simple&lt;/em&gt;: The benchmark will be run with different input sizes, data will not be randomized&lt;/li&gt;
&lt;li&gt;&lt;em&gt;once&lt;/em&gt;: The benchmark will be run with no input size.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: The randomization of the data can be disabled.&lt;/p&gt;
&lt;p&gt;You can run independent benchmarks or you can run sections of benchmarks. A section is used to compared different implementations of the same thing. For instance, I use them to compare different implementation of convolution or to see how ETL compete with other Expression Templates library.&lt;/p&gt;
&lt;img alt="/images/cpm_large.png" src="http://baptiste-wicht.com/images/cpm_large.png"&gt;
&lt;div class="section" id="examples"&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;I've uploaded three generated reports so that you can have look at some results:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Results of the ETL benchmark (one page per benchmark): &lt;a class="reference external" href="http://baptiste-wicht.com/cpm/etl/"&gt;Site 1&lt;/a&gt; &lt;a class="reference external" href="https://github.com/wichtounet/etl/blob/master/workbench/benchmark.cpp"&gt;Source 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Results of simple ETL / Blaze / Eigen3 comparison: &lt;a class="reference external" href="http://baptiste-wicht.com/cpm/etl_blaze_eigen/"&gt;Site 2&lt;/a&gt; &lt;a class="reference external" href="https://github.com/wichtounet/etl_vs_blaze/blob/master/src/simple.cpp"&gt;Source 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Results of the CPM dummy examples: &lt;a class="reference external" href="http://baptiste-wicht.com/cpm/examples/"&gt;Site 3&lt;/a&gt; &lt;a class="reference external" href="https://github.com/wichtounet/cpm/blob/master/examples/simple.cpp"&gt;Source 3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="run-benchmarks"&gt;
&lt;h2&gt;Run benchmarks&lt;/h2&gt;
&lt;p&gt;There are two ways of running CPM. You can directly use the library to run the benchmarks or you can use the macro facilities to make it easier. I recommend to use the second way since it is easier and I'm gonna try to keep it stable while the library can change. If you want an example of using the library directly, you can take a look at &lt;a class="reference external" href="https://github.com/wichtounet/cpm/blob/master/examples/simple.cpp"&gt;this example&lt;/a&gt;. In this chapter, I'm gonna focus on the macro-way.&lt;/p&gt;
&lt;p&gt;The library is available &lt;a class="reference external" href="https://github.com/wichtounet/cpm"&gt;here&lt;/a&gt;, you can either include as a submodule of your projects or install it globally to have access to its headers.&lt;/p&gt;
&lt;p&gt;The first thing to do is to include the CPM header:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_636b80cd143a4e72ba27a682a77ab659-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define CPM_BENCHMARK "Example Benchmarks"&lt;/span&gt;
&lt;a name="rest_code_636b80cd143a4e72ba27a682a77ab659-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"cpm/cpm.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;You have to name your benchmark. This will automatically creates a main and will run all the declared benchmark.&lt;/p&gt;
&lt;div class="section" id="define-benchmarks"&gt;
&lt;h3&gt;Define benchmarks&lt;/h3&gt;
&lt;p&gt;Benchmarks can be defined either in a CPM_BENCH functor or in the global scope with &lt;code&gt;CPM_DIRECT_BENCH&lt;/code&gt;.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;simple&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_fc5a9e6f992d458295011dde4f212027-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_DIRECT_BENCH_SIMPLE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"bench_name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The first argument is the name of the benchmark and the second argument is the function that will be benchmarked by the system, this function takes the input size as input.&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;global&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_f08f6cdda910473a95e3001f7df77d39-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_BENCH&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_f08f6cdda910473a95e3001f7df77d39-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_f08f6cdda910473a95e3001f7df77d39-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_GLOBAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"bench_name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_f08f6cdda910473a95e3001f7df77d39-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The first argument is the name of the benchmark, the second is the function being benchmarked and the following arguments must be references to global data that will be randomized by CPM.&lt;/p&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;two_pass&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_72c822830c0c43c3921a3c7c21be2daa-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_DIRECT_BENCH_TWO_PASS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"bench_name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_72c822830c0c43c3921a3c7c21be2daa-2"&gt;&lt;/a&gt;    &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;});&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
&lt;a name="rest_code_72c822830c0c43c3921a3c7c21be2daa-3"&gt;&lt;/a&gt;    &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_72c822830c0c43c3921a3c7c21be2daa-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Again, the first argument is the name. The second argument is the initialization functor. This functor must returns a tuple with all the information that will be passed (unpacked) to the third argument (the benchmark functor). Everything that is being returned by the initialization functor will be randomized.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="select-the-input-sizes"&gt;
&lt;h3&gt;Select the input sizes&lt;/h3&gt;
&lt;p&gt;By default, CPM will invoke your benchmarks with values from 10 to 1000000, multiplying it by 10 each step. This can be tuned for each benchmark and section independently. Each benchmark macro has a _P suffix that allows you to set the size policy:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_d4708b93bf6b433aad3e32ada84c7187-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_SIMPLE_P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_d4708b93bf6b433aad3e32ada84c7187-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;VALUES_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_d4708b93bf6b433aad3e32ada84c7187-3"&gt;&lt;/a&gt;    &lt;span class="s"&gt;"simple_a_n"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_d4708b93bf6b433aad3e32ada84c7187-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;});&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;You can also have several sizes (for multidimensional data structures or algorithms):&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_7fa86a2bb9054c15a6ed0d42ad5bcae6-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_DIRECT_BENCH_TWO_PASS_P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_7fa86a2bb9054c15a6ed0d42ad5bcae6-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;NARY_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VALUES_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;VALUES_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
&lt;a name="rest_code_7fa86a2bb9054c15a6ed0d42ad5bcae6-3"&gt;&lt;/a&gt;    &lt;span class="s"&gt;"convmtx2"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_7fa86a2bb9054c15a6ed0d42ad5bcae6-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dmat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dmat&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;d1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
&lt;a name="rest_code_7fa86a2bb9054c15a6ed0d42ad5bcae6-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="cm"&gt;/*d1*/&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dmat&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dmat&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;convmtx2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_7fa86a2bb9054c15a6ed0d42ad5bcae6-6"&gt;&lt;/a&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="configure-benchmarks"&gt;
&lt;h3&gt;Configure benchmarks&lt;/h3&gt;
&lt;p&gt;By default, each benchmark is run 10 times for warmup and then repeated 50 times, but you can define your own values:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_a27ffa33be2e4608bf8f446d9038795b-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define CPM_WARMUP 3&lt;/span&gt;
&lt;a name="rest_code_a27ffa33be2e4608bf8f446d9038795b-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define CPM_REPEAT 10&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This must be done before the inclusion of the header.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="define-sections"&gt;
&lt;h3&gt;Define sections&lt;/h3&gt;
&lt;p&gt;Sections are simply a group of benchmarks, so instead of putting several benchmarks inside a &lt;code&gt;CPM_BENCH&lt;/code&gt;, you can put them inside a &lt;code&gt;CPM_SECTION&lt;/code&gt;. For instance:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_SECTION&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"mmul"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_SIMPLE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"std"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_SIMPLE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"fast"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_SIMPLE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"common"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-5"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_SECTION&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"conv"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_TWO_PASS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"std"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-8"&gt;&lt;/a&gt;        &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;});&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-9"&gt;&lt;/a&gt;        &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-10"&gt;&lt;/a&gt;        &lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_TWO_PASS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"fast"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-12"&gt;&lt;/a&gt;        &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;});&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-13"&gt;&lt;/a&gt;        &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-14"&gt;&lt;/a&gt;        &lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_cc0158d0ba8447aeb63147d2e3c0821d-15"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;You can also set different warmup and repeat values for each section by using &lt;code&gt;CPM_SECTION_O&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_df87c617efff48909c6aeac06cd1b55a-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_SECTION_O&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"fft"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;51&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_df87c617efff48909c6aeac06cd1b55a-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_df87c617efff48909c6aeac06cd1b55a-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_df87c617efff48909c6aeac06cd1b55a-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_GLOBAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"std"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_df87c617efff48909c6aeac06cd1b55a-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_GLOBAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"mkl"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_df87c617efff48909c6aeac06cd1b55a-6"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;will be warmup 11 times and run 51 times.&lt;/p&gt;
&lt;p&gt;The size policy can also be changed for the complete section (cannot be changed independently for benchmarks inside the section):&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_89f4468365d3444e92b021ad026f90a4-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_SECTION_P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"mmul"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_89f4468365d3444e92b021ad026f90a4-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;NARY_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;STD_STOP_POLICY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;VALUES_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;VALUES_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_89f4468365d3444e92b021ad026f90a4-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_89f4468365d3444e92b021ad026f90a4-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_89f4468365d3444e92b021ad026f90a4-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_GLOBAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"std"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d3&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="cm"&gt;/* Something */&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_89f4468365d3444e92b021ad026f90a4-6"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_GLOBAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"mkl"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d3&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="cm"&gt;/* Something */&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_89f4468365d3444e92b021ad026f90a4-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_GLOBAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"bla"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d3&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="cm"&gt;/* Something */&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_89f4468365d3444e92b021ad026f90a4-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="run"&gt;
&lt;h3&gt;Run&lt;/h3&gt;
&lt;p&gt;Once your benchmarks and sections are defined, you can build you program as a normal C++ main and run it. You can pass several options:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_7f4a739ac3f04b099ae55a3cd7fe60d9-1"&gt;&lt;/a&gt;./debug/bin/full -h
&lt;a name="rest_code_7f4a739ac3f04b099ae55a3cd7fe60d9-2"&gt;&lt;/a&gt;Usage:
&lt;a name="rest_code_7f4a739ac3f04b099ae55a3cd7fe60d9-3"&gt;&lt;/a&gt;  ./debug/bin/full [OPTION...]
&lt;a name="rest_code_7f4a739ac3f04b099ae55a3cd7fe60d9-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_7f4a739ac3f04b099ae55a3cd7fe60d9-5"&gt;&lt;/a&gt;  -n, --name arg           Benchmark name
&lt;a name="rest_code_7f4a739ac3f04b099ae55a3cd7fe60d9-6"&gt;&lt;/a&gt;  -t, --tag arg            Tag name
&lt;a name="rest_code_7f4a739ac3f04b099ae55a3cd7fe60d9-7"&gt;&lt;/a&gt;  -c, --configuration arg  Configuration
&lt;a name="rest_code_7f4a739ac3f04b099ae55a3cd7fe60d9-8"&gt;&lt;/a&gt;  -o, --output arg         Output folder
&lt;a name="rest_code_7f4a739ac3f04b099ae55a3cd7fe60d9-9"&gt;&lt;/a&gt;  -h, --help               Print help
&lt;/pre&gt;&lt;p&gt;The tag is used to distinguish between runs, I recommend that you use a SCM identifier for the tag. If you want to run your program with different configurations (compiler options for instance), you'll have to set the configuration with the --configuration option.&lt;/p&gt;
&lt;p&gt;Here is a possible output:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-1"&gt;&lt;/a&gt; Start CPM benchmarks
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-2"&gt;&lt;/a&gt;    Results will be automatically saved in /home/wichtounet/dev/cpm/results/10.cpm
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-3"&gt;&lt;/a&gt;    Each test is warmed-up 10 times
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-4"&gt;&lt;/a&gt;    Each test is repeated 50 times
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-5"&gt;&lt;/a&gt;    Time Sun Jun 14 15:33:51 2015
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-7"&gt;&lt;/a&gt;    Tag: 10
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-8"&gt;&lt;/a&gt;    Configuration:
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-9"&gt;&lt;/a&gt;    Compiler: clang-3.5.0
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-10"&gt;&lt;/a&gt;    Operating System: Linux x86_64 3.16.5-gentoo
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-13"&gt;&lt;/a&gt; simple_a(10) : mean: 52.5us (52.3us,52.7us) stddev: 675ns min: 48.5us max: 53.3us througput: 190KEs
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-14"&gt;&lt;/a&gt; simple_a(100) : mean: 50.1us (48us,52.2us) stddev: 7.53us min: 7.61us max: 52.3us througput: 2MEs
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-15"&gt;&lt;/a&gt; simple_a(1000) : mean: 52.7us (52.7us,52.7us) stddev: 48.7ns min: 52.7us max: 53us througput: 19MEs
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-16"&gt;&lt;/a&gt; simple_a(10000) : mean: 62.6us (62.6us,62.7us) stddev: 124ns min: 62.6us max: 63.5us througput: 160MEs
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-17"&gt;&lt;/a&gt; simple_a(100000) : mean: 161us (159us,162us) stddev: 5.41us min: 132us max: 163us througput: 622MEs
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-18"&gt;&lt;/a&gt; simple_a(1000000) : mean: 1.16ms (1.16ms,1.17ms) stddev: 7.66us min: 1.15ms max: 1.18ms througput: 859MEs
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-19"&gt;&lt;/a&gt;
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-20"&gt;&lt;/a&gt;-----------------------------------------
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-21"&gt;&lt;/a&gt;|            gemm |       std |     mkl |
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-22"&gt;&lt;/a&gt;-----------------------------------------
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-23"&gt;&lt;/a&gt;|           10x10 | 51.7189us | 64.64ns |
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-24"&gt;&lt;/a&gt;|         100x100 | 52.4336us | 63.42ns |
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-25"&gt;&lt;/a&gt;|       1000x1000 | 56.0097us |  63.2ns |
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-26"&gt;&lt;/a&gt;|     10000x10000 | 95.6123us | 63.52ns |
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-27"&gt;&lt;/a&gt;|   100000x100000 | 493.795us | 63.48ns |
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-28"&gt;&lt;/a&gt;| 1000000x1000000 | 4.46646ms |  63.8ns |
&lt;a name="rest_code_9378d07dd08446fe9b344d4b0915cd51-29"&gt;&lt;/a&gt;-----------------------------------------
&lt;/pre&gt;&lt;p&gt;The program will give you for each benchmark, the mean duration (with confidence interval), the standard deviation of the samples, the min and max duration and an estimated throughput. The throughput is simply using the size and the mean duration. Each section is directly compared with an array-like output. Once the benchmark is run, a JSON report will be generated inside the output folder.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="continuous-monitoring"&gt;
&lt;h2&gt;Continuous Monitoring&lt;/h2&gt;
&lt;p&gt;Once you have run the benchmark, you can use the CPM program to generate the web reports. It will generate:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;1 performance graph for each benchmark and section&lt;/li&gt;
&lt;li&gt;1 graph comparing the performances over time of your benchmark sections if you have run the benchmark several time&lt;/li&gt;
&lt;li&gt;1 graph comparing different compiler if you have compiled your program with different compiler&lt;/li&gt;
&lt;li&gt;1 graph comparing different configuration if you have run the benchmark with different configuration&lt;/li&gt;
&lt;li&gt;1 table summary for each benchmark / section&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First you have to build and install the CPM program (you can have a look at the &lt;a class="reference external" href="https://github.com/wichtounet/cpm/blob/master/README.rst"&gt;Readme&lt;/a&gt; for more informations.&lt;/p&gt;
&lt;p&gt;Several options are available:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-1"&gt;&lt;/a&gt;Usage:
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-2"&gt;&lt;/a&gt;  cpm [OPTION...]  results_folder
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-4"&gt;&lt;/a&gt;      --time-sizes             Display multiple sizes in the time graphs
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-5"&gt;&lt;/a&gt;  -t, --theme arg              Theme name [raw,bootstrap,boostrap-tabs] (default:bootstrap)
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-6"&gt;&lt;/a&gt;  -c, --hctheme theme_name     Highcharts Theme name [std,dark_unica] (default:dark_unica)
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-7"&gt;&lt;/a&gt;  -o, --output output_folder   Output folder (default:reports)
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-8"&gt;&lt;/a&gt;      --input arg              Input results
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-9"&gt;&lt;/a&gt;  -s, --sort-by-tag            Sort by tag instaed of time
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-10"&gt;&lt;/a&gt;  -p, --pages                  General several HTML pages (one per bench/section)
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-11"&gt;&lt;/a&gt;  -d, --disable-time           Disable time graphs
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-12"&gt;&lt;/a&gt;      --disable-compiler       Disable compiler graphs
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-13"&gt;&lt;/a&gt;      --disable-configuration  Disable configuration graphs
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-14"&gt;&lt;/a&gt;      --disable-summary        Disable summary table
&lt;a name="rest_code_b2eea5446f1f4af9bccdf10a38c2bd34-15"&gt;&lt;/a&gt;  -h, --help                   Print help
&lt;/pre&gt;&lt;p&gt;There are 3 themes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;bootstrap&lt;/em&gt;: The default theme, using Bootstrap to make a responsive interface.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;bootstrap-tabs&lt;/em&gt;: Similar to the &lt;em&gt;bootstrap&lt;/em&gt; theme except that only is displayed at the same time for each benchmark, with tabs.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;raw&lt;/em&gt; : A very basic theme, only using Highcharts library for graphs. It is very minimalistic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, here are how the reports are generated for the ETL benchmark:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_0f819bc5d4bd49f4a25e3d9fc0d62097-1"&gt;&lt;/a&gt;cpm -p -s -t bootstrap -c dark_unica -o reports results
&lt;/pre&gt;&lt;p&gt;Here is the graph generated for the "R = A + B + C" benchmark and different compilers:&lt;/p&gt;
&lt;img alt="/images/cpm_etl_compiler.png" src="http://baptiste-wicht.com/images/cpm_etl_compiler.png"&gt;
&lt;p&gt;and its summary:&lt;/p&gt;
&lt;img alt="/images/cpm_etl_summary.png" src="http://baptiste-wicht.com/images/cpm_etl_summary.png"&gt;
&lt;p&gt;Here is the graph for a 2D convolution with ETL:&lt;/p&gt;
&lt;img alt="/images/cpm_etl_section.png" src="http://baptiste-wicht.com/images/cpm_etl_section.png"&gt;
&lt;p&gt;And the graph for different configurations of ETL and the dense matrix matrix multiplication:&lt;/p&gt;
&lt;img alt="/images/cpm_etl_configuration.png" src="http://baptiste-wicht.com/images/cpm_etl_configuration.png"&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion-and-future-work"&gt;
&lt;h2&gt;Conclusion and Future Work&lt;/h2&gt;
&lt;p&gt;Although CPM is already working, there are several things that could be done to improve it further:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The generated web report could benefit from a global summary.&lt;/li&gt;
&lt;li&gt;The throughput evaluation should be evaluated more carefully.&lt;/li&gt;
&lt;li&gt;The tool should automatically evaluate the number of times that each tests should be run to have a good result instead of global warmup and repeat constants.&lt;/li&gt;
&lt;li&gt;A better bootstrapping procedure should be used to determine the quality of the results and compute the confidence intervals.&lt;/li&gt;
&lt;li&gt;The performances of the website with lots of graphs should be improved.&lt;/li&gt;
&lt;li&gt;Make CPM more general-purpose to support larger needs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here it is, I have summed most of the features of the CPM Continuous Performance Analysis tool. I hope that it will be helpful to some of you as well.&lt;/p&gt;
&lt;p&gt;If you have other ideas or want to contribute something to the project, you can directly open an issue or a pull request on &lt;a class="reference external" href="https://github.com/wichtounet/cpm"&gt;Github&lt;/a&gt;. Or contact me via this site or Github.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>C++</category><category>Performances</category><category>Sonar</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2015/06/continuous-performance-management-with-cpm-for-cpp.html</guid><pubDate>Sun, 14 Jun 2015 12:02:57 GMT</pubDate></item><item><title>Improving eddic Boost Spirit parser performances</title><link>http://baptiste-wicht.com/posts/2013/06/improving-eddic-boost-spirit-parser-performances.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;After the last changes on the data-flow framework, the parsing pass of the eddic compiler became the slowest one. I was sure there was some area of optimizations, so I decided to improve its performances.&lt;/p&gt;
&lt;p&gt;In this post, I will present the different techniques I applied and their results.&lt;/p&gt;
&lt;h4&gt;Static grammar&lt;/h4&gt;

&lt;p&gt;The first optimization that I tried was to make the grammar static, meaning that we can declare it static and it will be constructed only once and will be allocated on the data segment.It is indeed heavy to build the lexer especially but also the grammar. I would like to thank &lt;a title="sehe on github" href="https://github.com/sehe"&gt;sehe&lt;/a&gt; for this optimization, he found it after I posted a question on Stackoverflow.&lt;/p&gt;
&lt;p&gt;The lexer was very easy to make static (only add static keyword :) ), but the parser was a bit more complicated because it needs the lexer iterator to get the current position in the file. This problem has been resolved by saving the iterator into the qi::locals of the rules.&lt;/p&gt;
&lt;p&gt;The result of this optimization are amazing. It saved 33% of the time.&lt;/p&gt;
&lt;h4&gt;Expectation points&lt;/h4&gt;

&lt;p&gt;Expectation points have the interesting point that they disallow backtracking and so can improve performances in some cases. Moreover, they are always interesting because they make the grammar clearer and the error messages much better.&lt;/p&gt;
&lt;p&gt;I tried adding more expectation points to the grammar. Unfortunately, there weren't a lot of them to add. Moreover, it seems that there are some quite weird behavior with them because some times it is impossible to add them (causes compilation failure) and sometimes it just make the code don't work anymore the same way, though I don't understand why.&lt;/p&gt;
&lt;p&gt;Anyway, I have been able to add some to the grammar. These changes improve the performance by a bit more than 1%. It is not a lot, but it is still an improvement. Moreover, I'm quite sure that there are more expectation points that can be added to the code. I will take some time again later to try to add more and to understand them better.&lt;/p&gt;
&lt;h4&gt;Less skips&lt;/h4&gt;

&lt;p&gt;In my grammar, I've a special parser for getting the current position in the file to add "debug information" to the AST. This special parser was skipping over its content, but it has no content, since it is artificial. Removing it improved the performance by about half a percent.&lt;/p&gt;
&lt;h4&gt;Improve Position (debug information)&lt;/h4&gt;

&lt;p&gt;As said before, there is a special parser to get the current position in the file. This information is then stored into an eddic::ast::Position structure. This structure was holding the line number, the column, the file name and the contents of the line. The first two were ints and the last two were std::string. Each time, a copy of the strings were necessary.&lt;/p&gt;
&lt;p&gt;I avoided storing the std::string directly by storing only the number of the line as well as the index of the file. Then, the content of the file is stored in the global context and can be accessed if it is necessary to display the line where the error happened.&lt;/p&gt;
&lt;p&gt;This change gave an increase of 10% of the parsing performance.&lt;/p&gt;
&lt;h4&gt;Auto Rules&lt;/h4&gt;

&lt;p&gt;Rules in Boost Spirit have an overhead due to the cost of the virtual calls that are necessary. In theory, auto rules can improve the efficiency of the rules by removing the cost of virtual calls. Moreover, auto rules should also avoid code bloat when the rules are compiled. The rules can be inlined and better optimized.&lt;/p&gt;
&lt;p&gt;I transformed some rules to auto rules to improve performances. Unfortunately, I found that this did not improve the performances. Moreover, transforming some rules to auto rules made the performance worse. I still did let some of the rules as auto rules. I have to say that I was very disappointed by this result, I was really expecting more from this :(&lt;/p&gt;
&lt;h4&gt;Generated Static Lexer&lt;/h4&gt;

&lt;p&gt;The first time the lexer is used, it has to generate the Deterministic Finite Automaton (DFA) that is used to identify the different tokens. This process takes time. There is way to avoid this by using the static model of Boost Spirit Lex. With that, the code is generated with the complete DFA and then it doesn't have to be initialized again.&lt;/p&gt;
&lt;p&gt;I was not expecting a lot from this because the lexer was already static and so was initialized only once. Indeed, it resulted in less than half a percent improvement.&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;Even if I've been able to largely reduce the overhead of the parsing by more than 40%, it still has a big overhead. Indeed, it still represents 36 percent of the whole process of compiling a source file. I think it is still too much.&lt;/p&gt;
&lt;p&gt;Moreover, an interesting fact is that the optimization I would have thought to be very effective (auto rules especially) did not have the expected effect, but making the grammar static, which I would not have thought of, was very effective.&lt;/p&gt;
&lt;p&gt;When profiled, the final version shows that quite some time is spent in destructing the multi_pass, which is quite odd. And it also seems that transforming the string operators to ast::Operator is not very effective, but I do not know how to improve that at this point.&lt;/p&gt;
&lt;p&gt;I won't probably work on that again for the version 1.2.4 of eddic, but I will eventually take some time again for the version 1.3.0 to improve it again.&lt;/p&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>Boost</category><category>C++11</category><category>Compilers</category><category>EDDI</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2013/06/improving-eddic-boost-spirit-parser-performances.html</guid><pubDate>Mon, 10 Jun 2013 06:16:42 GMT</pubDate></item><item><title>C++ benchmark â€“ std::vector VS std::list VS std::deque</title><link>http://baptiste-wicht.com/posts/2012/12/cpp-benchmark-vector-list-deque.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;script type="text/javascript" src="https://www.google.com/jsapi"&gt;&lt;/script&gt;

&lt;script type="text/javascript"&gt;google.load('visualization','1',{packages:['corechart']});&lt;/script&gt;

&lt;p&gt;Last week, I wrote a benchmark comparing the performance of std::vector and std::list on different workloads. This previous article received a lot of comments and several suggestions to improve it. The present article is an improvement over the previous article.&lt;/p&gt;
&lt;p&gt;In this article, I will compare the performance of std::vector, std::list and std::deque on several different workloads and with different data types. In this article, when I talk about a list refers to std::list, a vector refers to std::vector and deque to std::deque.&lt;/p&gt;
&lt;p&gt;It is generally said that a list should be used when random insert and remove will be performed (performed in O(1) versus O(n) for a vector or a deque). If we look only at the complexity, the scale of linear search in both data structures should be equivalent, complexity being in O(n). When random insert/replace operations are performed on a vector or a deque, all the subsequent data needs to be moved and so each element will be copied. That is why the size of the data type is an important factor when comparing those two data structures. Because the size of the data type will play an important role on the cost of copying an element.&lt;/p&gt;
&lt;p&gt;However, in practice, there is a huge difference: the usage of the memory caches. All the data in a vector is contiguous where the std::list allocates separately memory for each element. How does that change the results in practice ? The deque is a data structure aiming at having the advantages of both data structures without their drawbacks, we will see how it perform in practice. Complexity analysis does not take the memory hierarchy into level. I believe that in practice, memory hierarchy usage is as important as complexity analysis.&lt;/p&gt;
&lt;p&gt;Keep in mind that all the tests performed are made on vector, list and deque even if other data structures could be better suited to the given workload.&lt;/p&gt;
&lt;p&gt;In the graphs and in the text, &lt;em&gt;n&lt;/em&gt; is used to refer to the number of elements of the collection.&lt;/p&gt;
&lt;p&gt;All the tests performed have been performed on an Intel Core i7 Q 820 Â @ 1.73GHz. The code has been compiled in 64 bits with GCC 4.7.2 with -02 and -march=native. The code has been compiled with C++11 support (-std=c++11).&lt;/p&gt;
&lt;p&gt;For each graph, the vertical axis represent the amount of time necessary to perform the operations, so the lower values are the better. The horizontal axis is always the number of elements of the collection. For some graph, the logarithmic scale could be clearer, a button is available after each graph to change the vertical scale to a logarithmic scale.&lt;/p&gt;
&lt;p&gt;The data types are varying in size, they hold an array of longs and the size of the array varies to change the size of the data type. The non-trivial data type is made of two longs and has very stupid assignment operator and copy constructor that just does some maths (totally meaningless but costly). One may argue that is not a common copy constructor neither a common assignment operator and one will be right, however, the important point here is that it is costly operators which is enough for this benchmark.&lt;/p&gt;
&lt;h3&gt;Fill&lt;/h3&gt;

&lt;p&gt;The first test that is performed is to fill the data structures by adding elements to the back of the container (using &lt;em&gt;push_back&lt;/em&gt;). Two variations of vector are used, &lt;em&gt;vector_pre&lt;/em&gt; being a std::vector using vector::reserve at the beginning, resulting in only one allocation of memory.&lt;/p&gt;
&lt;p&gt;Lets see the results with a very small data type:&lt;/p&gt;
&lt;div id="graph_0" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_0" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_0(){var graph=new google.visualization.LineChart(document.getElementById('graph_0'));var data=google.visualization.arrayToDataTable([['x','list','vector','deque','vector_pre'],['100000',2545,271,2012,317],['200000',4927,552,998,334],['300000',7310,944,1707,595],['400000',9463,936,2056,1099],['500000',12591,1140,2642,1058],['600000',14351,1894,3125,1237],['700000',16561,1995,3686,1208],['800000',18820,2648,4291,1365],['900000',20832,2777,4962,2268],['1000000',23430,3015,5396,2585],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"fill_back - 8 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"us",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_0');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The pre-allocated vector is the fastest by a small margin and the list is 3 times slower than a vector. deque and vector.&lt;/p&gt;
&lt;p&gt;If we consider higher data type:&lt;/p&gt;
&lt;div id="graph_1" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_1" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_1(){var graph=new google.visualization.LineChart(document.getElementById('graph_1'));var data=google.visualization.arrayToDataTable([['x','list','vector','deque','vector_pre'],['100000',104867,55545,66852,21738],['200000',226215,108289,136035,42532],['300000',340910,198343,153446,60317],['400000',445035,217325,269316,80616],['500000',559619,236576,189613,101371],['600000',688422,391354,303729,122447],['700000',799902,405771,426373,138868],['800000',921441,415707,537057,160637],['900000',1006331,439635,263650,177052],['1000000',1113690,464416,372000,199434],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"fill_back - 4096 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"us",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_1');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;This time vector and list are performing at about the same speed. The deque is a bit faster than list and vector. The pre-allocated vector is clearly the winner here. The variations in the results of deque and vector are probably coming from my system that doesn't like allocating so much memory back and forth at this speed.&lt;/p&gt;
&lt;p&gt;Finally, if we use a non-trivial data type:&lt;/p&gt;
&lt;div id="graph_2" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_2" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_2(){var graph=new google.visualization.LineChart(document.getElementById('graph_2'));var data=google.visualization.arrayToDataTable([['x','list','vector','deque','vector_pre'],['100000',8093,8123,10251,8095],['200000',15433,15305,16061,13897],['300000',25964,24643,24450,19954],['400000',33414,30322,32148,27171],['500000',40416,37817,40752,35058],['600000',48991,48594,48785,41049],['700000',55059,55124,55092,47609],['800000',63688,61360,64505,55659],['900000',70550,67636,72329,60952],['1000000',79271,73533,79522,67787],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"fill_back - 16 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"us",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_2');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;All data structures are performing more or less the same, with vector_pre being the fastest.&lt;/p&gt;
&lt;p&gt;For push_back operations, pre-allocated vectors is a very good choice if the size is known in advance. The others performs more of less the same.&lt;/p&gt;
&lt;p&gt;I would have expected a better result for pre-allocated vector. If someone find an explanation for such a small margin, I'm interested.&lt;/p&gt;
&lt;h3&gt;Linear Search&lt;/h3&gt;

&lt;p&gt;The first operation is that is tested is the search. The container is filled with all the numbers in [0, N] and shuffled. Then, each number in [0,N] is searched in the container with std::find that performs a simple linear search. In theory, all the data structures should perform the same if we consider their complexity.&lt;/p&gt;
&lt;div id="graph_3" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_3" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_3(){var graph=new google.visualization.LineChart(document.getElementById('graph_3'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['1000',593,1098,318],['2000',2927,5307,1271],['3000',5891,12228,3020],['4000',8663,24415,5081],['5000',12859,36316,8066],['6000',18493,55057,11463],['7000',25057,74344,16022],['8000',38980,99990,21051],['9000',44951,127575,26650],['10000',52281,158216,32557],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"linear_search - 8 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"us",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_3');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;It is clear from the graph that the list has very poor performance for searching. The growth is much worse for a list than for a vector or a deque.&lt;/p&gt;
&lt;p&gt;The only reason is the usage of the cache line. When a data is accessed, the data is fetched from the main memory to the cache. Not only the accessed data is accessed, but a whole cacheline is fetched. As the elements in a vector are contiguous, when you access an element, the next element is automatically in the cache. As the main memory is orders of magnitude slower than the cache, this makes a huge difference. In the list case, the processor spends its whole time waiting for data being fetched from memory to the cache, at each fetch, the processor fetches a lot of unnecessary data that are almost always useless.&lt;/p&gt;
&lt;p&gt;The deque is a bit slower than the vector, that is logical because here there are more cache misses due to the segmented parts.&lt;/p&gt;
&lt;p&gt;If we take a bigger data type:&lt;/p&gt;
&lt;div id="graph_4" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_4" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_4(){var graph=new google.visualization.LineChart(document.getElementById('graph_4'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['1000',1116,2683,776],['2000',4983,16675,3537],['3000',12255,44379,10874],['4000',23212,83026,20189],['5000',37392,133353,33609],['6000',55295,193428,47636],['7000',74877,261314,63911],['8000',100903,340157,84647],['9000',126299,435816,107922],['10000',156386,545160,135680],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"linear_search - 128 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"us",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_4');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The list is still much slower than the others, but what is interesting is that gap between the deque and the array is decreasing. Let's try with a 4KB data type:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;/p&gt;&lt;div id="graph_5" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;&lt;input id="button_graph_5" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_5(){var graph=new google.visualization.LineChart(document.getElementById('graph_5'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['1000',4258,7190,4445],['2000',20584,38411,19825],['3000',48236,113189,55341],['4000',87475,223174,118453],['5000',136945,362421,191967],['6000',197856,530943,281252],['7000',273359,726323,387940],['8000',351223,954463,511276],['9000',447525,1211581,652269],['10000',551556,1497916,807161],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"linear_search - 4096 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"us",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_5');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;
&lt;/blockquote&gt;
&lt;p&gt;The performance of the list are still poor but the gap is decreasing. The interesting point is that deque is now faster than vector. I'm not really sure of the reason of this result. It is possible that it comes only from this special size. One thing is sure, the bigger the data size, the more cache misses the processor will get because elements don't fit in cache lines.&lt;/p&gt;
&lt;p&gt;For search, list is clearly slow where deque and vector have about the same performance. It seems that deque is faster than a vector for very large data sizes.&lt;/p&gt;
&lt;h3&gt;Random Insert (+Linear Search)&lt;/h3&gt;

&lt;p&gt;In the case of random insert, in theory, the list should be much faster, its insert operation being in O(1) versus O(n) for a vector or a deque.&lt;/p&gt;
&lt;p&gt;The container is filled with all the numbers in [0, N] and shuffled. Then, 1000 random values are inserted at a random position in the container. The random position is found by linear search. In both cases, the complexity of the search is O(n), the only difference comes from the insert that follow the search. We saw before that the performance of the list were poor for searching, so we'll see if the fast insertion can compensate the slow search.&lt;/p&gt;
&lt;div id="graph_6" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_6" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_6(){var graph=new google.visualization.LineChart(document.getElementById('graph_6'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['10000',8,27,8],['20000',15,45,14],['30000',22,63,21],['40000',29,74,27],['50000',37,87,38],['60000',43,105,44],['70000',50,114,48],['80000',61,130,55],['90000',66,139,61],['100000',70,155,68],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"random_insert - 8 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_6');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;List is clearly slower than the other two data structures that exhibit the same performance. This comes from the very slow linear search. Even if the two other data structures have to move a lot of data, the copy is cheap for small data types.&lt;/p&gt;
&lt;p&gt;Let's increase the size a bit:&lt;/p&gt;
&lt;div id="graph_7" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_7" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_7(){var graph=new google.visualization.LineChart(document.getElementById('graph_7'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['10000',21,53,25],['20000',39,80,48],['30000',57,103,68],['40000',71,122,90],['50000',88,146,112],['60000',102,165,130],['70000',124,190,152],['80000',140,214,175],['90000',157,238,195],['100000',174,268,213],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"random_insert - 32 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_7');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The result are interesting. The list is still the slowest but with a smaller margin. This time deque is faster than the vector by a small margin.&lt;/p&gt;
&lt;p&gt;Again, increasing the data size:&lt;/p&gt;
&lt;div id="graph_8" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_8" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_8(){var graph=new google.visualization.LineChart(document.getElementById('graph_8'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['10000',64,80,89],['20000',108,128,154],['30000',158,182,248],['40000',212,248,347],['50000',281,348,469],['60000',402,443,735],['70000',569,643,1034],['80000',767,775,1347],['90000',978,1002,1614],['100000',1190,1202,1962],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"random_insert - 128 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_8');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;This time, the vector is clearly the looser and deque and list have the same performance. We can say that with a size of 128 bytes, the time to move a lot of the elements is more expensive than searching in the list.&lt;/p&gt;
&lt;p&gt;A huge data type gives us clearer results:&lt;/p&gt;
&lt;div id="graph_9" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_9" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_9(){var graph=new google.visualization.LineChart(document.getElementById('graph_9'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['10000',4430,178,8074],['20000',7918,311,14121],['30000',11043,444,20014],['40000',13806,555,26783],['50000',17421,694,33519],['60000',20663,904,39175],['70000',23599,1147,45111],['80000',26736,1470,50887],['90000',29524,1940,60139],['100000',32005,2534,65098],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"random_insert - 4096 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_9');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The list is more than 20 times faster than the vector and an order of magnitude faster than the deque ! The deque is also twice faster than the vector.&lt;/p&gt;
&lt;p&gt;The fact than the deque is faster than vector is quite simple. When an insertion is made in a deque, the elements can either moved to the end or the beginning. The closer point will be chosen. An insert in the middle is the most costly operation with O(n/2) complexity. It is always more efficient to insert elements in a deque than in vector because at least twice less elements will be moved.&lt;/p&gt;
&lt;p&gt;If we look at the non-trivial data type:&lt;/p&gt;
&lt;div id="graph_10" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_10" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_10(){var graph=new google.visualization.LineChart(document.getElementById('graph_10'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['10000',230,41,425],['20000',376,65,705],['30000',552,84,1054],['40000',692,101,1345],['50000',862,119,1661],['60000',1003,141,1984],['70000',1186,155,2277],['80000',1358,172,2681],['90000',1540,186,2965],['100000',1658,203,3236],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"random_insert - 16 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_10');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The results are about the same as for the previous graph, but the data type is only 16B. The cost of the copy constructors and assignment operators is very important for vector and deque. The list doesn't care because no copy neither assignment of the existing elements is made during insertions (only the inserted element is copied).&lt;/p&gt;
&lt;h3&gt;Random Remove&lt;/h3&gt;

&lt;p&gt;In theory, random remove is the same case than random insert. Now that we've seen the results with random insert, we could expect the same behavior for random remove.&lt;/p&gt;
&lt;p&gt;The container is filled with all the numbers in [0, N] and shuffled. Then, 1000 random values are removed from a random position in the container.&lt;/p&gt;
&lt;p&gt;If we take the same data sizes as the random insert case:&lt;/p&gt;
&lt;div id="graph_11" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_11" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_11(){var graph=new google.visualization.LineChart(document.getElementById('graph_11'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['10000',6,19,5],['20000',12,41,11],['30000',20,55,18],['40000',27,68,25],['50000',34,81,33],['60000',43,101,40],['70000',49,113,45],['80000',59,126,52],['90000',67,138,61],['100000',72,157,65],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"random_remove - 8 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_11');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;div id="graph_12" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_12" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_12(){var graph=new google.visualization.LineChart(document.getElementById('graph_12'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['10000',40,40,63],['20000',85,83,134],['30000',127,132,198],['40000',181,189,282],['50000',245,263,473],['60000',363,376,664],['70000',524,502,960],['80000',743,688,1343],['90000',977,812,1639],['100000',1228,1017,2004],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"random_remove - 128 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_12');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;div id="graph_13" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_13" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_13(){var graph=new google.visualization.LineChart(document.getElementById('graph_13'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['10000',2906,109,5649],['20000',6190,233,11760],['30000',9379,359,18218],['40000',12840,490,23634],['50000',16027,585,30046],['60000',18918,773,36100],['70000',22213,999,42453],['80000',25788,1317,48793],['90000',28975,1762,55043],['100000',30860,2128,59791],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"random_remove - 4096 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_13');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;div id="graph_14" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_14" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_14(){var graph=new google.visualization.LineChart(document.getElementById('graph_14'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['10000',149,27,294],['20000',319,50,608],['30000',481,68,934],['40000',638,89,1236],['50000',794,108,1547],['60000',954,120,1894],['70000',1101,144,2185],['80000',1253,160,2513],['90000',1399,177,2812],['100000',1595,194,3108],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"random_remove - 16 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_14');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The behavior of random remove is the same as the behavior of random insert, for the same reasons. The results are not very interesting, so, let's get to the next workload.&lt;/p&gt;
&lt;h3&gt;Push Front&lt;/h3&gt;

&lt;p&gt;The next operation that we will compare is inserting elements in front of the collection. This is the worst case for vector, because after each insertion, all the previously inserted will be moved and copied. For a list or a deque, it does not make a difference compared to pushing to the back.&lt;/p&gt;
&lt;p&gt;So let's see the results:&lt;/p&gt;
&lt;div id="graph_15" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_15" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_15(){var graph=new google.visualization.LineChart(document.getElementById('graph_15'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['10000',0,0,33],['20000',0,0,135],['30000',0,0,313],['40000',0,0,585],['50000',0,1,913],['60000',0,1,1327],['70000',0,1,1823],['80000',0,1,2405],['90000',0,2,3107],['100000',0,2,4017],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"fill_front - 8 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_15');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The results are crystal-clear and as expected, vector is very bad at inserting elements to the front. The list and the deque results are almost invisible in the graph because it is a free operation for the two data structures. This does not need further explanations. There is no need to change the data size, it will only make vector much slower and my processor hotter.&lt;/p&gt;
&lt;h3&gt;Sort&lt;/h3&gt;

&lt;p&gt;The next operation that is tested is the time necessary to sort the data structures. For the vector and the deque std::sort is used and for a list the member function sort is used.&lt;/p&gt;
&lt;div id="graph_16" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_16" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_16(){var graph=new google.visualization.LineChart(document.getElementById('graph_16'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['100000',9,25,6],['200000',19,61,14],['300000',29,115,22],['400000',40,175,30],['500000',50,233,39],['600000',60,321,48],['700000',71,378,57],['800000',85,457,66],['900000',95,517,74],['1000000',108,593,83],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"sort - 8 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_16');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;For a small data type, the list is several times slower than the other two data structures. This is again due to the very poor spatial locality of the list during the search. vector is slightly faster than a deque, but the difference is not very significant.&lt;/p&gt;
&lt;p&gt;If we increase the size:&lt;/p&gt;
&lt;div id="graph_17" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_17" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_17(){var graph=new google.visualization.LineChart(document.getElementById('graph_17'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['100000',25,32,20],['200000',65,80,48],['300000',103,143,80],['400000',136,197,113],['500000',180,246,149],['600000',223,340,181],['700000',274,396,222],['800000',302,469,266],['900000',358,514,303],['1000000',395,579,337],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"sort - 128 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_17');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The order remains the same but the difference between the list and the other is decreasing.&lt;/p&gt;
&lt;p&gt;With a 1KB data type:&lt;/p&gt;
&lt;div id="graph_18" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_18" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_18(){var graph=new google.visualization.LineChart(document.getElementById('graph_18'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['100000',176,39,168],['200000',389,94,376],['300000',620,168,595],['400000',859,228,823],['500000',1100,285,1059],['600000',1355,392,1301],['700000',1609,452,1555],['800000',1844,539,1797],['900000',2111,597,2054],['1000000',2397,670,2278],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"sort - 1024 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_18');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The list is almost five times faster than the vector and the deque which are both performing the same (with a very slight advantage for vector).&lt;/p&gt;
&lt;p&gt;If we use the non-trivial data type:&lt;/p&gt;
&lt;div id="graph_19" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_19" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_19(){var graph=new google.visualization.LineChart(document.getElementById('graph_19'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['100000',92,26,89],['200000',195,70,188],['300000',301,135,296],['400000',410,195,399],['500000',519,255,510],['600000',638,350,623],['700000',763,410,729],['800000',858,492,846],['900000',971,552,954],['1000000',1090,628,1072],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"sort - 16 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_19');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;Again, the cost of the operators of this type have a strong impact on the vector and deque.&lt;/p&gt;
&lt;h3&gt;Destruction&lt;/h3&gt;

&lt;p&gt;The next test is to calculate the time necessary to the destruction of a container. The containers are dynamically allocated, are filled with n numbers and then their destruction time (via delete) is computed.&lt;/p&gt;
&lt;div id="graph_20" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_20" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_20(){var graph=new google.visualization.LineChart(document.getElementById('graph_20'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['100000',34,1489,0],['200000',70,2838,0],['300000',102,4677,0],['400000',142,6072,0],['500000',173,7737,0],['600000',215,8828,0],['700000',353,10599,1],['800000',321,12115,0],['900000',355,13932,1],['1000000',410,15345,0],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"destruction - 8 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"us",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_20');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The results are already interesting. The vector is almost free to destroy, which is logical because that incurs only freeing one array and the vector itself. The deque is slower due to the freeing of each segments. But the list is much more costly than the other two, more than an order of magnitude slower. This is expected because the list have to free the dynamic memory of each node and also has to iterate through all the elements which we saw was slow.&lt;/p&gt;
&lt;p&gt;If we increase the data type:&lt;/p&gt;
&lt;div id="graph_21" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_21" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_21(){var graph=new google.visualization.LineChart(document.getElementById('graph_21'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['100000',898,4403,1],['200000',2488,8499,1],['300000',4091,12499,1430],['400000',5461,16379,1909],['500000',6729,21128,2459],['600000',8164,25719,2729],['700000',9517,31046,3227],['800000',10871,34550,3756],['900000',12392,37176,4163],['1000000',13762,40119,4523],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"destruction - 128 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"us",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_21');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;This time we can see that the deque is three times slower than a vector and that the list is still an order of magnitude slower than a vector ! However, the is less difference than before.&lt;/p&gt;
&lt;p&gt;With our biggest data type, now:&lt;/p&gt;
&lt;div id="graph_22" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_22" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_22(){var graph=new google.visualization.LineChart(document.getElementById('graph_22'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['100000',20575,22434,15499],['200000',44234,47254,29848],['300000',67196,69374,39818],['400000',89253,91128,54229],['500000',108689,112557,68090],['600000',131751,135764,75063],['700000',150801,155610,90761],['800000',172365,176957,102830],['900000',192575,193897,112728],['1000000',211507,215274,126348],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"destruction - 4096 bytes",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"us",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_22');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;There is no more difference between list and deque. The vector is still twice faster than them.&lt;/p&gt;
&lt;p&gt;Even if the vector is always faster than the list and deque, keep in mind that the graphs for destruction are in microseconds and so the operations are not very costly. It could make a difference is very time-sensitive application but unlikely in most applications. Moreover, destruction is made only once per data structure, generally, it is not a very important operation.&lt;/p&gt;
&lt;h3&gt;Number Crunching&lt;/h3&gt;

&lt;p&gt;Finally, we can also test a number crunching operation. Here, random elements are inserted into the container that is kept sorted. It means, that the position where the element has to be inserted is first searched by iterating through elements and the inserted. As we talk about number crunching, only 8 bytes elements are tested.&lt;/p&gt;
&lt;div id="graph_23" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_23" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_23(){var graph=new google.visualization.LineChart(document.getElementById('graph_23'));var data=google.visualization.arrayToDataTable([['x','deque','list','vector'],['10000',39,187,33],['20000',150,1247,134],['30000',339,3380,310],['40000',623,6513,547],['50000',958,10757,864],['60000',1394,16098,1257],['70000',1894,22623,1713],['80000',2479,30656,2249],['90000',3162,39451,2858],['100000',3932,49906,3576],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"number_crunching",width:'600px',height:'400px',hAxis:{title:"Number of elements",slantedText:true},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_23');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;Even if there is only 100'000 elements, the list is already an order of magnitude slower than the other two data structures. If we look a the curves of the results, it is easy to see that this will be only worse with higher collection sizes. The list is absolutely not adapted for number crunching operations due to its poor spatial locality.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;To conclude, we can get some facts about each data structure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;std::list is very very slow to iterate through the collection due to its very poor spatial locality.&lt;/li&gt;
&lt;li&gt;std::vector and std::deque perform always faster than std::list with very small data&lt;/li&gt;
&lt;li&gt;std::list handles very well large elements&lt;/li&gt;
&lt;li&gt;std::deque performs better than a std::vector for inserting at random positions (especially at the front, which is constant time)&lt;/li&gt;
&lt;li&gt;std::deque and std::vector do not support very well data types with high cost of copy/assignment&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This draw simple conclusions on usage of each data structure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number crunching: use std::vector or std::deque&lt;/li&gt;
&lt;li&gt;Linear search: use std::vector or std::deque&lt;/li&gt;
&lt;li&gt;Random Insert/Remove:&lt;ul&gt;
&lt;li&gt;Small data size: use std::vector&lt;/li&gt;
&lt;li&gt;Large element size: use std::list (unless if intended principally for searching)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Non-trivial data type: use std::list unless you need the container especially for searching. But for multiple modifications of the container, it will be very slow.&lt;/li&gt;
&lt;li&gt;Push to front: use std::deque or std::list&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I have to say that before writing this new version of the benchmark I did not know std::deque a lot. This is a very good data structure that is very good at inserting at both ends and even in the middle while exposing a very good spatial locality. Even if sometimes slower than a vector, when the operations involves both searching and inserting in the middle, I would say that this structure should be preferred over vectors, especially for data types of medium sizes.&lt;/p&gt;
&lt;p&gt;If you have the time, in practice, the best way to decide is always to benchmark each version, or even to try another data structures. Two operations with the same Big O complexity can perform quite differently in practice.&lt;/p&gt;
&lt;p&gt;I hope that you found this article interesting. If you have any comment or have an idea about an other workload that you would like to test, don't hesitate to post a comment ;) If you have a question on results, don't hesitate as well.&lt;/p&gt;
&lt;p&gt;The code source of the benchmark is available online: &lt;a href="https://github.com/wichtounet/articles/blob/master/src/vector_list/bench.cpp" title="Source code of the benchmark"&gt;https://github.com/wichtounet/articles/blob/master/src/vector_list/bench.cpp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The older version of the article is still available: &lt;a href="http://www.baptiste-wicht.com/2012/11/cpp-benchmark-vector-vs-list/" title="C++ benchmark â€“ std::vector VS std::list"&gt;C++ benchmark â€“ std::vector VS std::list&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;function draw_visualization(){draw_graph_0();draw_graph_1();draw_graph_2();draw_graph_3();draw_graph_4();draw_graph_5();draw_graph_6();draw_graph_7();draw_graph_8();draw_graph_9();draw_graph_10();draw_graph_11();draw_graph_12();draw_graph_13();draw_graph_14();draw_graph_15();draw_graph_16();draw_graph_17();draw_graph_18();draw_graph_19();draw_graph_20();draw_graph_21();draw_graph_22();draw_graph_23();}google.setOnLoadCallback(draw_visualization);&lt;/script&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>C++</category><category>C++11</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2012/12/cpp-benchmark-vector-list-deque.html</guid><pubDate>Mon, 03 Dec 2012 07:58:29 GMT</pubDate></item><item><title>C++ benchmark - std::vector VS std::list</title><link>http://baptiste-wicht.com/posts/2012/11/cpp-benchmark-vector-vs-list.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;script type="text/javascript" src="https://www.google.com/jsapi"&gt;&lt;/script&gt;

&lt;script type="text/javascript"&gt;google.load('visualization','1',{packages:['corechart']});&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;A updated version of this article is available: &lt;a title="C++ benchmark â€“ std::vector VS std::list VS std::deque" href="http://www.baptiste-wicht.com/2012/12/cpp-benchmark-vector-list-deque/"&gt;C++ benchmark â€“ std::vector VS std::list VS std::deque&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In C++, the two most used data structures are the std::vector and the std::list. In this article, we will compare the performance in practice of these two data structures on several different workloads. In this article, when I talk about a list it is the std::list implementation and vector refers to the std::vector implementation.&lt;/p&gt;
&lt;p&gt;It is generally said that a list should be used when random insert and remove will be performed (performed in O(1) versus O(n) for a vector). If we look only at the complexity, search in both data structures should be roughly equivalent, complexity being in O(n). When random insert/replace operations are performed on a vector, all the subsequent data needs to be moved and so each element will be copied. That is why the size of the data type is an important factor when comparing those two data structures.&lt;/p&gt;
&lt;p&gt;However, in practice, there is a huge difference, the usage of the memory caches. All the data in a vector is contiguous where the std::list allocates separately memory for each element. How does that change the results in practice ?&lt;/p&gt;
&lt;p&gt;Keep in mind that all the tests performed are made on vector and list even if other data structures could be better suited to the given workload.&lt;/p&gt;
&lt;p&gt;In the graphs and in the text, &lt;em&gt;n&lt;/em&gt; is used to refer to the number of elements of the collection.&lt;/p&gt;
&lt;p&gt;All the tests performed have been performed on an Intel Core i7 Q 820 Â @ 1.73GHz. The code has been compiled in 64 bits with GCC 4.7.2 with -02 and -march=native. The code has been compiled with C++11 support (-std=c++11).&lt;/p&gt;
&lt;h3&gt;Fill&lt;/h3&gt;

&lt;p&gt;The first test that is performed is to fill the data structures by adding elements to the back of the container. Two variations of vector are used, vector_pre being a std::vector with the size passed in parameters to the constructor, resulting in only one allocation of memory.&lt;/p&gt;
&lt;div id="graph_0" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_0" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_0(){var graph=new google.visualization.LineChart(document.getElementById('graph_0'));var data=google.visualization.arrayToDataTable([['x','vector_pre','vector','list'],['1000',0,0,1],['10000',0,1,10],['100000',4,11,100],['1000000',7,234,1023]]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"Fill (8 bytes)",width:'600px',height:'400px',vAxis:{title:"Milliseconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_0');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;&lt;div id="graph_1" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;&lt;input id="button_graph_1" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_1(){var graph=new google.visualization.LineChart(document.getElementById('graph_1'));var data=google.visualization.arrayToDataTable([['x','vector_pre','vector','list'],['1000',0,9,1],['10000',12,245,18],['100000',949,2635,1153],['1000000',9138,23654,11270]]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"Fill (1024 bytes)",width:'600px',height:'400px',vAxis:{title:"Milliseconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_1');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;
&lt;p&gt;All data structures are impacted the same way when the data size increases, because there will be more memory to allocate. The vector_pre is clearly the winner of this test, being one order of magnitude faster than a list and about twice faster than a vector without pre-allocation. The result are directly linked to the allocations that have to be performed, allocation being slow. Whatever the data size is, push_back to a vector will always be faster than to a list. This is logical becomes vector allocates more memory than necessary and so does not need to allocate memory for each element.&lt;/p&gt;
&lt;p&gt;But this test is not very interesting, generally building the data structure is not critical. What is critical is the operations that are performed on the data structure. That will be tested in the next sections.&lt;/p&gt;
&lt;!--nextpage--&gt;

&lt;h3&gt;Random Find&lt;/h3&gt;

&lt;p&gt;The first operation is that is tested is the search. The container is filled with all the numbers in [0, N] and shuffled. Then, each number in [0,N] is searched in the container with std::find that performs a simple linear search.&lt;/p&gt;
&lt;div id="graph_2" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_2" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_2(){var graph=new google.visualization.LineChart(document.getElementById('graph_2'));var data=google.visualization.arrayToDataTable([['x','vector','list'],['100',0,11],['1000',0,1545],['5000',0,35886],['10000',0,150865],['20000',0,614496]]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"Find (8 bytes)",width:'600px',height:'400px',vAxis:{title:"Microseconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_2');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;Yes, vector is present in the graph, its line is the same as the x line ! Performing a &lt;strong&gt;linear search in a vector is several orders of magnitude faster than in a list&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The only reason is the usage of the cache line. When a data is accessed, the data is fetched from the main memory to the cache. Not only the accessed data is accessed, but a whole cacheline is fetched. As the elements in a vector are contiguous, when you access an element, the next element is automatically in the cache. As the main memory is orders of magnitude slower than the cache, this makes a huge difference. In the list case, the processor spends its whole time waiting for data being fetched from memory to the cache.&lt;/p&gt;
&lt;p&gt;If we augment the size of the data type to 1KB, the results remain the same, but slower:&lt;/p&gt;
&lt;div id="graph_3" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_3" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_3(){var graph=new google.visualization.LineChart(document.getElementById('graph_3'));var data=google.visualization.arrayToDataTable([['x','vector','list'],['100',0,11],['1000',0,3551],['5000',0,195429],['10000',0,829631],['20000',0,3356432]]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"Find (1024 bytes)",width:'600px',height:'400px',vAxis:{title:"Microseconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_3');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;!--nextpage--&gt;

&lt;h3&gt;Random Insert&lt;/h3&gt;

&lt;p&gt;In the case of random insert, in theory, the list should be much faster, its insert operation being in O(1) versus O(n) for a vector.&lt;/p&gt;
&lt;p&gt;The container is filled with all the numbers in [0, N] and shuffled. Then, 1000 random values are inserted at a random position in the container. The random position is found by linear search. In both cases, the complexity of the search is O(n), the only difference comes from the insert that follow the search.&lt;/p&gt;
&lt;div id="graph_4" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_4" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_4(){var graph=new google.visualization.LineChart(document.getElementById('graph_4'));var data=google.visualization.arrayToDataTable([['x','vector','list'],['1000',9,85],['2000',9,85],['4000',10,94],['6000',12,98],['8000',13,106],['10000',14,106]]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"Insert (8 bytes)",width:'600px',height:'400px',vAxis:{title:"Milliseconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_4');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;When, the vector should be slower than the list, it is almost an order of magnitude faster. Again, this is because finding the position in a list is much slower than copying a lot of small elements.&lt;/p&gt;
&lt;p&gt;If we increase the size:&lt;/p&gt;
&lt;div id="graph_5" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_5" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_5(){var graph=new google.visualization.LineChart(document.getElementById('graph_5'));var data=google.visualization.arrayToDataTable([['x','vector','list'],['1000',27,120],['2000',30,113],['4000',34,122],['6000',37,140],['8000',42,145],['10000',47,155]]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"Insert (32 bytes)",width:'600px',height:'400px',vAxis:{title:"Milliseconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_5');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The two lines are getting closer, but vector is still faster.&lt;/p&gt;
&lt;p&gt;Increase it to 1KB:&lt;/p&gt;
&lt;div id="graph_6" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_6" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_6(){var graph=new google.visualization.LineChart(document.getElementById('graph_6'));var data=google.visualization.arrayToDataTable([['x','vector','list'],['1000',1821,167],['2000',1941,163],['4000',2383,191],['6000',2679,207],['8000',2960,214],['10000',3308,228]]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"Insert (1024 bytes)",width:'600px',height:'400px',vAxis:{title:"Milliseconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_6');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;This time, list outperforms vector by an order of magnitude ! The performance of random insert in a list are not impacted much by the size of the data type, where vector suffers a lot when big sizes are used. We can also see that list doesn't seem to care about the size of the collection. It is because the size of the collection only impact the search and not the insertion and as few search are performed, it does not change the results a lot.&lt;/p&gt;
&lt;p&gt;If the iterator was already known (no need for linear search), it would be faster to insert into a list than into the vector.&lt;/p&gt;
&lt;h3&gt;Random Remove&lt;/h3&gt;

&lt;p&gt;In theory, random remove is the same case than random insert. Now that we've seen the results with random insert, we could expect the same behavior for random remove.&lt;/p&gt;
&lt;p&gt;The container is filled with all the numbers in [0, N] and shuffled. Then, 1000 random values are removed from a random position in the container.&lt;/p&gt;
&lt;div id="graph_7" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_7" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_7(){var graph=new google.visualization.LineChart(document.getElementById('graph_7'));var data=google.visualization.arrayToDataTable([['x','vector','list'],['100',0,0],['1000',0,0],['10000',40,0],['50000',949,2],['100000',3937,4],['200000',16003,9],['300000',42393,12]]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"Push front (8 bytes)",width:'600px',height:'400px',vAxis:{title:"Milliseconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_7');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;Again, vector is several times faster and looks to scale better. Again, this is because it is very cheap to copy small elements.&lt;/p&gt;
&lt;p&gt;Let's increase it directly to 1KB element.&lt;/p&gt;
&lt;div id="graph_8" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_8" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_8(){var graph=new google.visualization.LineChart(document.getElementById('graph_8'));var data=google.visualization.arrayToDataTable([['x','vector','list'],['1000',0,0],['10000',2,26],['100000',163,684],['1000000',2147,15950],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"Sort (8 bytes)",width:'600px',height:'400px',vAxis:{title:"Milliseconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_8');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The two lines have been reversed !&lt;/p&gt;
&lt;p&gt;The behavior of random remove is the same as the behavior of random insert, for the same reasons.&lt;/p&gt;
&lt;!--nextpage--&gt;

&lt;h3&gt;Push Front&lt;/h3&gt;

&lt;p&gt;The next operation that we will compare is inserting elements in front of the collection. This is the worst case for vector, because after each insertion, all the previously inserted will be moved and copied. For a list, it does not make a difference compared to pushing to the back.&lt;/p&gt;
&lt;div id="graph_9" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_9" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_9(){var graph=new google.visualization.LineChart(document.getElementById('graph_9'));var data=google.visualization.arrayToDataTable([['x','vector','list'],['100',0,0],['1000',0,0],['10000',40,0],['50000',949,2],['100000',3937,4],['200000',16003,9],['300000',42393,12]]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"Push front (8 bytes)",width:'600px',height:'400px',vAxis:{title:"Milliseconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_9');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The results are crystal-clear and as expected. vector is very bad at inserting elements to the front. This does not need further explanations. There is no need to change the data size, it will only make vector much slower.&lt;/p&gt;
&lt;h3&gt;Sort&lt;/h3&gt;

&lt;p&gt;The next operation that is tested is the performance of sorting a vector or a list. For a vector std::sort is used and for a list the member function sort is used.&lt;/p&gt;
&lt;div id="graph_10" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_10" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_10(){var graph=new google.visualization.LineChart(document.getElementById('graph_10'));var data=google.visualization.arrayToDataTable([['x','vector','list'],['1000',0,0],['10000',2,26],['100000',163,684],['1000000',2147,15950],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"Sort (8 bytes)",width:'600px',height:'400px',vAxis:{title:"Milliseconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_10');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;We can see that sorting a list is several times slower. It comes from the poor usage of the cache.&lt;/p&gt;
&lt;p&gt;If we increase the size of the element to 1KB:&lt;/p&gt;
&lt;div id="graph_11" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_11" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_11(){var graph=new google.visualization.LineChart(document.getElementById('graph_11'));var data=google.visualization.arrayToDataTable([['x','vector','list'],['1000',2,0],['10000',224,50],['100000',4289,1083],['1000000',50973,17975],]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"Sort (1024 bytes)",width:'600px',height:'400px',vAxis:{title:"Milliseconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_11');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;This time the list is faster than the vector. It is not very clear on the graph, but the values for the list are almost the same as for the previous results. That is because std::list::sort() does not perform any copy, only pointers to the elements are changed. On the other hand, swapping two elements in a vector involves at least three copies, so the cost of sorting will increase as the cost of copying increases.&lt;/p&gt;
&lt;!--nextpage--&gt;

&lt;h3&gt;Number Crunching&lt;/h3&gt;

&lt;p&gt;Finally, we can also test a number crunching operation. Here, random elements are inserted into the container that is kept sorted. It means, that the position where the element has to be inserted is first searched by iterating through elements and the inserted. As we talk about number crunching, only 8 bytes elements are tested.&lt;/p&gt;
&lt;div id="graph_12" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_12" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_12(){var graph=new google.visualization.LineChart(document.getElementById('graph_12'));var data=google.visualization.arrayToDataTable([['x','vector','list'],['1000',0,0],['10000',45,166],['50000',928,10665],['100000',3753,50766],['200000',15185,231480],['300000',34293,715892]]);var options={curveType:"function",animation:{duration:1200,easing:"in"},title:"Random Sorted Insert (8 bytes)",width:'600px',height:'400px',vAxis:{title:"Milliseconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_12');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see that vector is more than an order of magnitude faster than list and this will only be more as the size of the collection increase. This is because traversing the list is much more expensive than copying the elements of the vector.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;To conclude, we can get some facts about each data structure:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;std::vector is insanely faster than std::list to find an element&lt;/li&gt;
    &lt;li&gt;std::vector performs always faster than std::list with very small data&lt;/li&gt;
    &lt;li&gt;std::vector is always faster to push elements at the back than std::list&lt;/li&gt;
    &lt;li&gt;std::list handles very well large elements, especially for sorting or inserting in the front&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This draw simple conclusions on usage of each data structure:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;Number crunching: use std::vector&lt;/li&gt;
    &lt;li&gt;Linear search: use std::vector&lt;/li&gt;
    &lt;li&gt;Random Insert/Remove: use std::list (if data size very small (&amp;lt; 64B on my computer), use std::vector)&lt;/li&gt;
    &lt;li&gt;Big data size: use std::list (not if intended for searching)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you have the time, in practice, the best way to decide is always to benchmark both versions, or even to try another data structures.&lt;/p&gt;
&lt;p&gt;I hope that you found this article interesting. If you have any comment or have an idea about an other workload that you would like to test, don't hesitate to post a comment ;) If you have a question on results, don't hesitate as well.&lt;/p&gt;
&lt;p&gt;The code source of the benchmark is available online: &lt;a title="Source code of the benchmark" href="https://github.com/wichtounet/articles/blob/master/src/vector_list/bench.cpp"&gt;https://github.com/wichtounet/articles/blob/master/src/vector_list/bench.cpp&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;function draw_visualization(){draw_graph_0();draw_graph_1();draw_graph_2();draw_graph_3();draw_graph_4();draw_graph_5();draw_graph_6();draw_graph_7();draw_graph_8();draw_graph_9();draw_graph_10();draw_graph_11();draw_graph_12();}google.setOnLoadCallback(draw_visualization);&lt;/script&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>C++</category><category>C++11</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2012/11/cpp-benchmark-vector-vs-list.html</guid><pubDate>Mon, 26 Nov 2012 07:47:35 GMT</pubDate></item><item><title>GCC 4.7 vs CLang 3.1 on eddic</title><link>http://baptiste-wicht.com/posts/2012/11/gcc-4-7-clang-3-1-eddic.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;script type="text/javascript" src="https://www.google.com/jsapi"&gt;&lt;/script&gt;

&lt;script type="text/javascript"&gt;google.load('visualization','1',{packages:['corechart']});&lt;/script&gt;

&lt;p&gt;&lt;a href="http://www.baptiste-wicht.com/2012/11/eddic-compiles-with-clang-3-1/" title="eddic compiles with CLang 3.1"&gt;Now that eddic can be compiled with CLang&lt;/a&gt;, I wanted to compare the differences in compilation time and in performance of the generated executable between those two compilers. The tests are done using GCC 4.7.2 and CLang 3.1 on Gentoo.
&lt;/p&gt;&lt;h3&gt;Compilation Time&lt;/h3&gt;
&lt;p&gt;The first thing that I tested has been the compilation time of the two compilers to compile eddic with different flags. I tested the compilation in debug mode and with -O2 and -O3.&lt;/p&gt;
&lt;div id="graph_0" style="width: 400px; height: 300px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_0" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_0(){var graph=new google.visualization.ColumnChart(document.getElementById('graph_0'));var data=google.visualization.arrayToDataTable([['Options','GCC','CLang'],['-g',234.59,119.59],['-O2',273.02,178.22],['-O3',276.87,183.78],]);var options={title:"Compilation Time - Less is better",animation:{duration:1200,easing:"in"},width:'400px',height:'300px',hAxis:{title:"Options"},vAxis:{title:"Seconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_0');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The most interesting fact in these results is that CLang is much faster than GCC. It takes twice less times to compile eddic with CLang in debug mode than with GCC. The impact on optimizations on CLang's compilation is also more important than on GCC. For both compilers, -O3 does not seems to add a lot of overhead.&lt;/p&gt;
&lt;h3&gt;Runtime performance&lt;/h3&gt;

&lt;p&gt;Then, I tested the performance of the generated executable. I tested it on three things, the whole test suite and two test cases that I know are the slowest for the EDDI Compiler. For each case, I took the slowest value of 5 consecutive executions.&lt;/p&gt;
&lt;div id="graph_1" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_1" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_1(){var graph=new google.visualization.ColumnChart(document.getElementById('graph_1'));var data=google.visualization.arrayToDataTable([['Compiler','GCC -O2','GCC -O3','CLang -O2','CLang -O3'],['testsuite',6.58,6.59,6.74,6.58],['assembly',1.2,1.2,1.2,1.2],['linked_list',0.51,0.5,0.49,0.49],]);var options={title:"Runtime Performance - Less is better",animation:{duration:1200,easing:"in"},width:'600px',height:'400px',hAxis:{title:"Options"},vAxis:{title:"Seconds",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_1');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The difference are very small. In -02, GCC performs a bit better, but in -O3, the performance are equivalent. I was a bit disappointed by the results, because I thought that there would be higher differences. It seems that CLang is not as far from GCC that some people would like to say. It also certainly depends on the program being compiled.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;It is clear that CLang is much faster than GCC to compile eddic. Moreover, the performance of the generated executable are almost similar.&lt;/p&gt;
&lt;p&gt;I will continue to use CLang as my development compiler and switches between the two when I'm doing performance benchmarking. I will try to update the benchmark once new versions of GCC / CLang are available.&lt;/p&gt;
&lt;script type="text/javascript"&gt;function draw_visualization(){draw_graph_0();draw_graph_1();}google.setOnLoadCallback(draw_visualization);&lt;/script&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>clang</category><category>Compilers</category><category>EDDI</category><category>gcc</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2012/11/gcc-4-7-clang-3-1-eddic.html</guid><pubDate>Mon, 12 Nov 2012 08:28:44 GMT</pubDate></item><item><title>Integer Linear Time Sorting Algorithms</title><link>http://baptiste-wicht.com/posts/2012/11/integer-linear-time-sorting-algorithms.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;script type="text/javascript" src="https://www.google.com/jsapi"&gt;&lt;/script&gt;

&lt;script type="text/javascript"&gt;google.load('visualization','1',{packages:['corechart']});&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: The code is now more C++&lt;/p&gt;
&lt;p&gt;Most of the sorting algorithms that are used are generally comparison sort. It means that each element of the collection being sorted will be compared to see which one is the first one. A comparison must have a lower bound of Î©(n log n) comparisons. That is why there are no comparison-based sorting algorithm better than O(n log n).&lt;/p&gt;
&lt;p&gt;On the other hand, there are also sorting algorithms that are performing better. This is the family of the integer sorting algorithms. These algorithms are using properties of integer to sort them without comparing them. They can be only be used to sort integers. Nevertheless, a hash function can be used to assign a unique integer to any value and so sort any value. All these algorithms are using extra space. There are several of these algorithms. In this article, we will see three of them and I will present an implementation in C++. At the end of the article, I will compare them to &lt;em&gt;std::sort&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In the article, I will use &lt;em&gt;n&lt;/em&gt; as the size of the array to sort and &lt;em&gt;m&lt;/em&gt; as the max number that is permitted in the array.&lt;/p&gt;
&lt;h3&gt;Bin Sort&lt;/h3&gt;

&lt;p&gt;Bin Sort, or Bucket Sort, is a very simple algorithm that partition all the input numbers into a number of buckets. Then, all the buckets are outputted in order in the array, resulting in a sorting array. I decided to implement the simplest case of Bin Sort where each number goes in its own bucket, so there are &lt;em&gt;m&lt;/em&gt; buckets.&lt;/p&gt;
&lt;p&gt;The implementation is pretty straightforward:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;binsort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MAX&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;SIZE&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
        &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]].&lt;/span&gt;&lt;span class="n"&gt;push_back&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;current&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;MAX&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="nl"&gt;item&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]){&lt;/span&gt;
            &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;current&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;B is the array of buckets. Each bucket is implemented as a std::vector. The algorithm starts by filling each buckets with the numbers from the input array. Then, it outputs them in order in the array.&lt;/p&gt;
&lt;p&gt;This algorithm works in &lt;em&gt;O(n + m)&lt;/em&gt; and requires &lt;em&gt;O(m)&lt;/em&gt; extra memory. With these properties, it makes a very limited algorithm, because if you don't know the maximum number and you have to use the maximum number of the array type, you will have to allocate for instance 2^32 buckets. That won't be possible.&lt;/p&gt;
&lt;h3&gt;Couting Sort&lt;/h3&gt;

&lt;p&gt;An interesting fact about binsort is that each bucket contains only the same numbers. The size of the bucket would be enough. That is exactly what Counting Sort. It counts the number of times an element is present instead of the elements themselves. I will present two versions. The first one is a version using a secondary array and then copying again into the input array and the second one is an in-place sort.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;counting_sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SIZE&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MAX&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;SIZE&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
        &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]];&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;MAX&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
        &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SIZE&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
        &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]];&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;SIZE&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
        &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;The algorithm is also simple. It starts by counting the number of elements in each bucket. Then, it aggregates the number by summing them to obtain the position of the element in the final sorted array. Then, all the elements are copied in the temporary array. Finally, the temporary array is copied in the final array. This algorithms works in &lt;em&gt;O(m + n)&lt;/em&gt; and requires &lt;em&gt;O(m + n)&lt;/em&gt;. This version is presented only because it is present in the literature. We can do much better by avoiding the temporary array and optimizing it a bit:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;in_place_counting_sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MAX&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;SIZE&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
        &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]];&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;current&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;MAX&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
            &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;current&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;The temporary array is removed and the elements are directly written in the sorted array. The counts are not used directly as position, so there is no need to sum them. This version still works in &lt;em&gt;O(m + n)&lt;/em&gt; but requires only &lt;em&gt;O(m)&lt;/em&gt; extra memory. It is much faster than the previous version.&lt;/p&gt;
&lt;h3&gt;Radix Sort&lt;/h3&gt;

&lt;p&gt;The last version that I will discuss here is a Radix Sort. This algorithm sorts the number digit after digit in a specific radix. It is a form of bucket sort, where there is a bucket by digit. Like Counting Sort, only the counts are necessary. For example, if you use radix sort in base 10. It will first sort all the numbers by their first digit, then the second, .... It can work in any base and that is its force. With a well chosen base, it can be very powerful. Here, we will focus on radix that are in the form 2^r. These radix have good properties, we can use shifts and mask to perform division and modulo, making the algorithm much faster.&lt;/p&gt;
&lt;p&gt;The implementation is a bit more complex than the other implementations:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;digits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;             &lt;span class="c1"&gt;//Digits&lt;/span&gt;
&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;                 &lt;span class="c1"&gt;//Bits&lt;/span&gt;
&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;radix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;         &lt;span class="c1"&gt;//Bins&lt;/span&gt;
&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;radix&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;radix_sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SIZE&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;cnt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;radix&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shift&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shift&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;radix&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
            &lt;span class="n"&gt;cnt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;SIZE&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
            &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;cnt&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;shift&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;radix&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
            &lt;span class="n"&gt;cnt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;cnt&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SIZE&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
            &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;cnt&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;shift&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;SIZE&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
           &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;&lt;em&gt;r&lt;/em&gt; indicates the power of two used as the radix (2^r). The mask is used to compute modulo faster. The algorithm repeats the steps for each digit. Here &lt;em&gt;digits&lt;/em&gt; equals 2. It means that we support 2^32 values. A 32 bits value is sorted in two pass. The steps are very similar to counting sort. Each value of the digit is counted and then the counts are summed to give the position of the number. Finally, the numbers are put in order in the temporary array and copied into A.&lt;/p&gt;
&lt;p&gt;This algorithm works in &lt;em&gt;O(digits (m + radix))&lt;/em&gt; and requires &lt;em&gt;O(n + radix)&lt;/em&gt; extra memory. A very good thing is that the algorithm does not require space based on the maximum value, only based on the radix.&lt;/p&gt;
&lt;h3&gt;Results&lt;/h3&gt;

&lt;p&gt;It's time to compare the different implementations in terms of runtime. For each size, each version is tested 25 times on different random arrays. The arrays are the same for each algorithm. The number is the time necessary to sort the 25 arrays. The benchmark has been compiler with GCC 4.7.&lt;/p&gt;
&lt;p&gt;The first test is made with very few duplicates (m = 10n).&lt;/p&gt;
&lt;div id="graph_0" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_0" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_0(){var graph=new google.visualization.ColumnChart(document.getElementById('graph_0'));var data=google.visualization.arrayToDataTable([['x','std::sort','counting_sort','in_place_counting_sort','bin_sort','radix_sort'],['100000',171,182,105,945,89],['500000',993,2229,970,6435,461],['1000000',2175,4812,2046,14096,1068],['5000000',11791,27050,10202,81255,6148],]);var options={title:"m = 10n",animation:{duration:1200,easing:"in"},width:'600px',height:'400px',hAxis:{title:"n"},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_0');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;Radix Sort comes to be the fastest in this case, &lt;strong&gt;twice faster as &lt;em&gt;std::sort&lt;/em&gt;&lt;/strong&gt;. In place counting sort has almost the same performance as &lt;em&gt;std::sort&lt;/em&gt;. The other are performing worse.&lt;/p&gt;
&lt;p&gt;The second test is made with few duplicates (m ~= n).&lt;/p&gt;
&lt;div id="graph_1" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_1" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_1(){var graph=new google.visualization.ColumnChart(document.getElementById('graph_1'));var data=google.visualization.arrayToDataTable([['x','std::sort','counting_sort','in_place_counting_sort','bin_sort','radix_sort'],['100000',186,73,37,309,90],['500000',991,611,189,3126,455],['1000000',2235,2171,547,7978,1038],['5000000',12184,18470,4516,49056,5791],]);var options={title:"m ~= n",animation:{duration:1200,easing:"in"},width:'600px',height:'400px',hAxis:{title:"n"},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_1');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;The numbers are impressive. In place &lt;strong&gt;counting sort is between 3-4 times faster than &lt;em&gt;std::sort&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;radix sort is twice faster than &lt;em&gt;std::sort&lt;/em&gt;&lt;/strong&gt; ! Bin Sort does not performs very well and counting sort even if generally faster than &lt;em&gt;std::sort&lt;/em&gt; does not scale very well.&lt;/p&gt;
&lt;p&gt;Let's test with more duplicates (m = n / 2).&lt;/p&gt;
&lt;div id="graph_2" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_2" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_2(){var graph=new google.visualization.ColumnChart(document.getElementById('graph_2'));var data=google.visualization.arrayToDataTable([['x','std::sort','counting_sort','in_place_counting_sort','bin_sort','radix_sort'],['100000',178,65,25,262,90],['500000',979,450,143,2332,461],['1000000',2171,1480,321,6240,1041],['5000000',11978,16205,3453,41709,5890],]);var options={title:"m = n / 2",animation:{duration:1200,easing:"in"},width:'600px',height:'400px',hAxis:{title:"n"},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_2');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;std::sort&lt;/em&gt; and radix sort performance does not change a lot but the other sort are performing better. In-place counting sort is still the leader with a higher margin.&lt;/p&gt;
&lt;p&gt;Finally, with a lot of duplicates (m = n / 10).&lt;/p&gt;
&lt;div id="graph_3" style="width: 600px; height: 400px;"&gt;&lt;/div&gt;

&lt;p&gt;&lt;input id="button_graph_3" type="button" value="Logarithmic scale"&gt;&lt;script type="text/javascript"&gt;function draw_graph_3(){var graph=new google.visualization.ColumnChart(document.getElementById('graph_3'));var data=google.visualization.arrayToDataTable([['x','std::sort','counting_sort','in_place_counting_sort','bin_sort','radix_sort'],['100000',161,46,12,144,74],['500000',918,322,76,1023,449],['1000000',2062,824,167,2721,1041],['5000000',10789,8534,1030,24026,5686],]);var options={title:"m = n / 10n",animation:{duration:1200,easing:"in"},width:'600px',height:'400px',hAxis:{title:"n"},vAxis:{title:"ms",viewWindow:{min:0}}};graph.draw(data,options);var button=document.getElementById('button_graph_3');button.onclick=function(){if(options.vAxis.logScale){button.value="Logarithmic Scale";}else{button.value="Normal scale";}options.vAxis.logScale=!options.vAxis.logScale;graph.draw(data,options);};}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;Again, &lt;em&gt;std::sort&lt;/em&gt; and radix sort performance are stable, but in-place counting is now &lt;strong&gt;ten times faster than &lt;em&gt;std::sort&lt;/em&gt;&lt;/strong&gt; !&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;To conclude, we have seen that these algorithms can outperforms &lt;em&gt;std::sort&lt;/em&gt; by a high factor (10 times for In place Counting Sort when there m &amp;lt;&amp;lt; n). If you have to sort integers, you should consider these two cases:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;m &amp;gt; n or m is unknown : Use radix sort that is about twice faster than &lt;em&gt;std::sort&lt;/em&gt;.&lt;/li&gt;
    &lt;li&gt;m &amp;lt;&amp;lt; n : Use in place counting sort that can be much faster than &lt;em&gt;std::sort&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope you found this article interesting. The implementation can be found on Github: https://github.com/wichtounet/articles/tree/master/src/linear_sorting&lt;/p&gt;
&lt;script type="text/javascript"&gt;function draw_visualization(){draw_graph_0();draw_graph_1();draw_graph_2();draw_graph_3();}google.setOnLoadCallback(draw_visualization);&lt;/script&gt;&lt;/div&gt;</description><category>Algorithm</category><category>Benchmarks</category><category>C++</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2012/11/integer-linear-time-sorting-algorithms.html</guid><pubDate>Wed, 07 Nov 2012 08:02:46 GMT</pubDate></item><item><title>C++11 Synchronization Benchmark</title><link>http://baptiste-wicht.com/posts/2012/07/c11-synchronization-benchmark.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;In the previous parts of this serie, we saw some C++11 Synchronization techniques: locks, lock guards and atomic references.&lt;/p&gt;
&lt;p&gt;In this small post, I will present the results of a little benchmark I did run to compare the different techniques. In this benchmark, the critical section is a single increment to an integer. The critical section is protected using three techniques:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;A single std::mutex with calls to lock() and unlock()&lt;/li&gt;
    &lt;li&gt;A single std::mutex locked with std::lock_guard&lt;/li&gt;
    &lt;li&gt;An atomic reference on the integer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The tests have been made with 1, 2, 4, 8, 16, 32, 64 and 128 threads. Each test is repeated 5 times.&lt;/p&gt;
&lt;p&gt;The results are presented in the following figure:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.baptiste-wicht.com/2012/07/c11-synchronization-benchmark/synchronization_cpp_benchmarks/" rel="attachment wp-att-2071"&gt;&lt;img class=" wp-image-2071  " title="C++11 Synchronization Benchmark Result" src="http://baptiste-wicht.com/wp-content/uploads/2012/07/synchronization_cpp_benchmarks-300x230.png" alt="C++11 Synchronization Benchmark Result" width="300" height="230"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As expected, the mutex versions are much slower than the atomic one. An interesting point is that the the atomic version has not a very good scalability. I would have expected that the impact of adding one thread would not be that high.&lt;/p&gt;
&lt;p&gt;I'm also surprised that the lock guard version has a non-negligibleÂ overhead when there are few threads.&lt;/p&gt;
&lt;p&gt;In conclusion, do not locks when all you need is modifying integral types. For that, std::atomic is much faster. Good Lock-Free algorithms are almost always faster than the algorithms with lock.&lt;/p&gt;
&lt;p&gt;The sources of the benchmark are available on Github:Â &lt;a href="https://github.com/wichtounet/articles/tree/master/src/threads/benchmark"&gt;https://github.com/wichtounet/articles/tree/master/src/threads/benchmark&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>C++</category><category>C++11 Concurrency Tutorial</category><category>Concurrency</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2012/07/c11-synchronization-benchmark.html</guid><pubDate>Thu, 26 Jul 2012 06:47:59 GMT</pubDate></item><item><title>My Java Benchmarks on GitHub</title><link>http://baptiste-wicht.com/posts/2010/09/my-java-benchmarks-on-github.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've created a new github repository for my &lt;strong&gt;Java Benchmarks&lt;/strong&gt; : &lt;a title="java-benchmarks github repository" href="http://github.com/wichtounet/java-benchmarks" target="_blank"&gt;java-benchmarks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;From now all my benchmarks will be pushed to this repository. This is more simple for me to manage and more secure also.&lt;/p&gt;
&lt;p&gt;At this time, there is seven benchmarks on the repository :&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;Closest Pair Search Benchmark : A benchmark to test two closest pair point search algorithms : the naive one and the &lt;strong&gt;sweeping plane&lt;/strong&gt; one. &lt;a title="Closest Pair Search Benchmark Results" href="http://www.baptiste-wicht.com/2010/04/closest-pair-of-point-plane-sweep-algorithm/" target="_blank"&gt;Results&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;File Copy Benchmark : A benchmark on the different ways to make &lt;strong&gt;file copy&lt;/strong&gt; in Java. &lt;a title="Java File Copy Benchmark Results" href="http://www.baptiste-wicht.com/2010/08/file-copy-in-java-benchmark/" target="_blank"&gt;Results&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;Iteration Remove Benchmark :Â A simple benchmark to test if it's interesting to remove the read elements from a list when we make severalÂ iterations over the list.&lt;/li&gt;
    &lt;li&gt;Reflection Benchmark : A little benchmark to test the performances of &lt;strong&gt;reflection&lt;/strong&gt; versus switch cases and direct invocations.&lt;/li&gt;
    &lt;li&gt;Short Indexes Loop Benchmark : A benchmark to test which &lt;strong&gt;primitive type&lt;/strong&gt; is the most performing using as iteration index.Â &lt;a title="Short Indexes Loop Benchmark Results" href="http://www.baptiste-wicht.com/2010/01/dont-use-shorts-in-loop/" target="_blank"&gt;Results&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;Synchronization Benchmark : A benchmark to test the performances of the different &lt;strong&gt;synchronization mechanisms&lt;/strong&gt; available in Java to provide mutual exclusion. &lt;a title="Synchronization Benchmark Results" href="http://www.baptiste-wicht.com/2010/09/java-synchronization-mutual-exclusion-benchmark/" target="_blank"&gt;Results&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;Unmodifiable Benchmark : A benchmark to test the performances of &lt;strong&gt;unmodifiable collection&lt;/strong&gt; versus creating a copy of the list.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I hope you'll find these sources interesting. If you found errors or improvements, don't hesitate to comment to tell me what.&lt;/p&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>Git</category><category>Java</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2010/09/my-java-benchmarks-on-github.html</guid><pubDate>Fri, 03 Sep 2010 05:16:20 GMT</pubDate></item><item><title>Java Synchronization (Mutual Exclusion) Benchmark</title><link>http://baptiste-wicht.com/posts/2010/09/java-synchronization-mutual-exclusion-benchmark.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've created another benchmark. This time, I've benchmarked the different ways of synchronizing a little code using &lt;strong&gt;mutual exclusion&lt;/strong&gt; on this code.&lt;/p&gt;
&lt;p&gt;The code to protect will be very simple. It's a simple counter :&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;//Init&lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;counter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; 
&lt;span class="c1"&gt;//Critical section&lt;/span&gt;
&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="o"&gt;++;&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;The critical section, if not protected with synchronization system, will not function properly due to possible &lt;strong&gt;interleavings&lt;/strong&gt; (read &lt;a href="http://www.baptiste-wicht.com/2010/08/java-concurrrency-synchronization-locks/" target="_blank"&gt;the article on synchronization&lt;/a&gt; if you don't know what is &lt;strong&gt;interleaving&lt;/strong&gt;).&lt;/p&gt;
&lt;p class="more"&gt;&lt;a href="http://baptiste-wicht.com/posts/2010/09/java-synchronization-mutual-exclusion-benchmark.html"&gt;Read moreâ€¦&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>Concurrency</category><category>Java</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2010/09/java-synchronization-mutual-exclusion-benchmark.html</guid><pubDate>Wed, 01 Sep 2010 05:13:18 GMT</pubDate></item><item><title>Java File Copy Benchmark Updates (once again)</title><link>http://baptiste-wicht.com/posts/2010/08/file-copy-benchmark-updates-once-again.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've made another updates to my file copy benchmark.&lt;/p&gt;
&lt;p&gt;First of all, I used my &lt;a title="Utility class to create graphs of benchmark results" href="http://www.baptiste-wicht.com/2010/08/generate-graphs-benchmarks-easily/" target="_blank"&gt;little utility class to automatically create the graphs&lt;/a&gt;. The graph are a little less clean, but I spare a lot of time not creating them myself.&lt;/p&gt;
&lt;p&gt;Then, I've also made some corrections on the code :&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;I''ve used a buffer size of 8192 instead of 4096&lt;/li&gt;
    &lt;li&gt;I've made some corrections using the channels because the old code can forgot to write some portions of the file&lt;/li&gt;
    &lt;li&gt;I used allocateDirect() instead of allocate() for the ByteBuffer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And I've added a new method using Java 7 : Path.copyTo(Path path).&lt;/p&gt;
&lt;p&gt;So the new results are all based on a Java 7 Virtual Machine.&lt;/p&gt;
&lt;p&gt;You'll find all the new informations and result, on the original post : &lt;a title="Java File Copy Benchmark" href="http://www.baptiste-wicht.com/2010/08/file-copy-in-java-benchmark/" target="_self"&gt;File Copy in Java - Benchmark&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I hope this new informations will interest you.&lt;/p&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>I/O</category><category>Java</category><category>Java 7</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2010/08/file-copy-benchmark-updates-once-again.html</guid><pubDate>Wed, 25 Aug 2010 05:26:11 GMT</pubDate></item><item><title>Generate graphs benchmarks easily</title><link>http://baptiste-wicht.com/posts/2010/08/generate-graphs-benchmarks-easily.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;After launching a lot of benchmarks for &lt;a title="Java File Copy Benchmark" href="http://www.baptiste-wicht.com/2010/08/file-copy-in-java-benchmark/" target="_blank"&gt;file copy benchmark&lt;/a&gt; and always generating the graphs from the results in Excel, I realized that I was loosing a lot of time to do that. So like any Java developer, I decided to create a little tool that do the work automatically for me.&lt;/p&gt;
&lt;p&gt;For creating benchmarks, I'm using a little micro-benchmarking framework, &lt;a title="How to write correct benchmarks" href="http://www.baptiste-wicht.com/2010/04/write-corrects-benchmarks/" target="_blank"&gt;described here&lt;/a&gt;. After the results are generated, I automatically generate a bar chart of the result using JFreeChart.&lt;/p&gt;
&lt;p&gt;Here is an example of graph generated by the tool :&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/graph-example.png"&gt;&lt;img class="size-full wp-image-931" title="Example graph" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/graph-example.png" alt="Example graph" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p class="more"&gt;&lt;a href="http://baptiste-wicht.com/posts/2010/08/generate-graphs-benchmarks-easily.html"&gt;Read moreâ€¦&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>Java</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2010/08/generate-graphs-benchmarks-easily.html</guid><pubDate>Wed, 11 Aug 2010 05:04:50 GMT</pubDate></item><item><title>Java File Copy Benchmarks Update</title><link>http://baptiste-wicht.com/posts/2010/08/java-file-copy-benchmarks-update.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've made an update of my benchmark about file copy methods in Java. I've been asked for new informations about this benchmark and for new test, so I've included more results and informations.&lt;/p&gt;
&lt;p&gt;This new version include two new complete benchmarks :&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;Benchmark on the same disk (Ext4)&lt;/li&gt;
    &lt;li&gt;Benchmark between two disks (Ext4 -&amp;gt; Ext4)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And of course the old benchmark is always here : Benchmark between two disks (Ext4 -&amp;gt; NTFS).&lt;/p&gt;
&lt;p&gt;I've also included more informations about the disk and the benchmark. The statistics informations about the results are also included in the post. So you can found the standard deviation of the results and the confidence intervals of the results stats.&lt;/p&gt;
&lt;p&gt;And last but not least I've included a new method to copy files using the cp executable of Linux.&lt;/p&gt;
&lt;p&gt;The results are always available at the same place : &lt;a title="File Copy in Java - Benchmark" href="http://www.baptiste-wicht.com/2010/08/file-copy-in-java-benchmark/" target="_self"&gt;File Copy in Java - Benchmark&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>I/O</category><category>Java</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2010/08/java-file-copy-benchmarks-update.html</guid><pubDate>Wed, 04 Aug 2010 07:04:35 GMT</pubDate></item><item><title>File copy in Java - Benchmark</title><link>http://baptiste-wicht.com/posts/2010/08/file-copy-in-java-benchmark.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Yesterday I wondered if the copyFile method in JTheque Utils was the best method or if I need to change. So I decided to do a benchmark.&lt;/p&gt;
&lt;p&gt;So I searched all the methods to copy a File in Java, even the bad methods and found 5 methods :&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;ol&gt;
    &lt;li&gt;&lt;strong&gt;Native Copy&lt;/strong&gt; : Make the copy using the cp executable of Linux&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Naive Streams Copy&lt;/strong&gt; : Open two streams, one to read, one to write and transfer the content byte by byte.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Naive Readers Copy&lt;/strong&gt; : Open two readers, one to read, one to write and transfer the content character by character.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Buffered Streams Copy&lt;/strong&gt; : Same as the first but using buffered streams instead of simple streams.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Buffered Readers Copy&lt;/strong&gt; : Same as the second but using buffered readers instead of simple readers.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Custom Buffer Stream Copy&lt;/strong&gt; : Same as the first but reading the file not byte by byte but using a simple byte array as buffer.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Custom Buffer Reader Copy&lt;/strong&gt; : Same as the fifth but using a Reader instead of a stream.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Custom Buffer Buffered Stream Copy&lt;/strong&gt; : Same as the fifth but using buffered streams.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Custom Buffer Buffered Reader Copy&lt;/strong&gt; : Same as the sixth but using buffered readers.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;NIO Buffer Copy&lt;/strong&gt; : Using NIO Channel and using a ByteBuffer to make the transfer.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;NIO Transfer Copy&lt;/strong&gt; : Using NIO Channel and direct transfer from one channel to other.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Path (Java 7) Copy&lt;/strong&gt; : Using the Path class of Java 7 and its method copyTo()&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think, this is the principal methods to copy a file to another file. The different methods are available at the end of the post. Pay attention that the methods with Readers only works with text files because Readers are using character by character reading so it doesn't work on a binary file like an image. Here I used a buffer size of 4096 bytes. Of course, use a higher value improve the performances of custom buffer strategies.&lt;/p&gt;
&lt;p&gt;For the benchmark, I made the tests using different files.&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;Little file (5 KB)&lt;/li&gt;
    &lt;li&gt;Medium file (50 KB)&lt;/li&gt;
    &lt;li&gt;Big file (5 MB)&lt;/li&gt;
    &lt;li&gt;Fat file (50 MB)&lt;/li&gt;
    &lt;li&gt;And an enormous file (1.3 GB) only binary&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And I made the tests first using text files and then using binary files. I made the tests using in three modes :&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;On the same hard disk. It's an IDE Hard Disk of 250 GB with 8 MB of cache. It's formatted in Ext4.&lt;/li&gt;
    &lt;li&gt;Between two disk. I used the first disk and an other SATA Hard Disk of 250 GB with 16 MB of cache. It's formatted in Ext4.&lt;/li&gt;
    &lt;li&gt;Between two disk. I used the first disk and an other SATA Hard Disk of 1 TB with 32 MB of cache. It's formatted using NTFS.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I used a benchmark framework, &lt;a title="Micro-Benchmarking framework" href="http://www.baptiste-wicht.com/2010/04/write-corrects-benchmarks/" target="_blank"&gt;described here&lt;/a&gt;, to make the tests of all the methods. The tests have been made on my personal computer (Ubuntu 10.04 64 bits, Intel Core 2 Duo 3.16 GHz, 6 Go DDR2, SATA Hard Disks). The Java version used is a Java 7 64 bits Virtual Machine.&lt;/p&gt;
&lt;p&gt;I've cut the post into several pages due to the length of the post :&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;Introduction about the benchmark&lt;/li&gt;
    &lt;li&gt;Benchmark on the same disk&lt;/li&gt;
    &lt;li&gt;Benchmark between Ext4 and Ext4&lt;/li&gt;
    &lt;li&gt;Benchmark between Ext4 and NTFS&lt;/li&gt;
    &lt;li&gt;Conclusions about the benchmark results&lt;/li&gt;
&lt;/ol&gt;

&lt;!--nextpage--&gt;

&lt;h4&gt;Benchmark on the same disk (Ext4)&lt;/h4&gt;

&lt;p&gt;So let's start with the results of the benchmarking using the same disk.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/little-text-same-disk.png"&gt;&lt;img class="size-full wp-image-964" title="Little Text Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/little-text-same-disk.png" alt="Little Text Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can see that here the native and naive streams methods are a lot slower than the other methods. So lets remove the naive Â streams method from the graph to have a better view on the other methods :&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/little-text-same-disk-sub.png"&gt;&lt;img class="size-full wp-image-965" title="Little Text Benchmark Sub Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/little-text-same-disk-sub.png" alt="Little Text Benchmark Sub Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first conclusion we can do is that the naive readers is a lot faster than the naive streams. It's because Reader use a buffer internally and this is not the case in streams. The others methods are closer, so we'll see with the next sizes what happens.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/medium-text-same-disk-sub.png"&gt;&lt;img class="size-full wp-image-967" title="Medium Text Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/medium-text-same-disk-sub.png" alt="Medium Text Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here, we have removed the two naive methods because they are too slows compared to the others.&lt;/p&gt;
&lt;p&gt;The readers methods are slower than the equivalent streams methods because readers are working on chars, so they must make characters conversion for every char of the file, so this is a cost to add.&lt;/p&gt;
&lt;p&gt;Another observation is that the custom buffer strategy is faster than the buffering of the streams and than using custom buffer with a buffered stream or a single stream doesn't change anything. The same observation can be made using the custom buffer using readers, it's the same with buffered readers or not. This is logical, because with custom buffer we made 4096 (size of the buffer) times less invocations to the read method and because we ask for a complete buffer we have not a lot of I/O operations. So the buffer of the streams (or the readers) is not useful here.&lt;/p&gt;
&lt;p&gt;The NIO Buffer, NIO Transfer and Path strategies are almost equivalent to custom buffer.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/big-text-same-disk-sub.png"&gt;&lt;img class="size-full wp-image-969" title="Big Text Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/big-text-same-disk-sub.png" alt="Big Text Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here we see the limits of the simple buffered stream (and readers methods). And another really interesting thing we see is that the native is now faster than buffered streams and readers. Native method must start an external program and this has a cost not negligible. But the copy using the cp executable is really fast and that's because when the file size grows, the native method becomes interesting. All the other methods except the readers are almost equivalent.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/fat-text-same-disk.png"&gt;&lt;img class="size-full wp-image-970" title="Fat Text Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/fat-text-same-disk.png" alt="Fat Text Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This time we can see that the native copy method is here as fast as the custom buffer streams. The fast method is the NIO Transfer method.&lt;/p&gt;
&lt;p&gt;It's interesting to see that it doesn't take 100 ms to copy a 50 MB file.&lt;/p&gt;
&lt;p&gt;We'll see with binary now. We'll directly start with a 5 MB file.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/big-binary-same-disk.png"&gt;&lt;img class="size-full wp-image-971" title="Big Binary Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/big-binary-same-disk.png" alt="Big Binary Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We see exactly the same results as with a text file. The native method start to be interesting. We see precisely that the NIOand Path methods are really interesting here.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/fat-binary-same-disk.png"&gt;&lt;img class="size-full wp-image-972" title="Fat Binary Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/fat-binary-same-disk.png" alt="Fat Binary Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can see that all the methods are really, really close, but the native, NIO Buffer, NIO Transfer and Path methods are the best. Just to be sure of these results, let's test with a bigger file :&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/enormous-binary-same-disk.png"&gt;&lt;img class="size-full wp-image-973" title="Enormous Binary Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/enormous-binary-same-disk.png" alt="Enormous Binary Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here we can see that the native method become to be the fastest one. The other method are really close. I thought the NIO Transfer will be normally faster. Due to the size of the file the benchmark has been made only a little number of times, so the number can be inaccurate. We see that he Path method is really close to the other.&lt;/p&gt;
&lt;p&gt;The detailed informations (standard deviation, confidence intervals and other stats stuff) are available in the conclusion page.&lt;/p&gt;
&lt;!--nextpage--&gt;

&lt;h4&gt;Benchmark between two disks (Ext4 -&amp;gt; Ext4)&lt;/h4&gt;

&lt;p&gt;Here are the results of the same tests but using two hard disk with the same formatting (Ext4).&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/little-text-same-disk1.png"&gt;&lt;img class="size-full wp-image-975" title="Little Text Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/little-text-same-disk1.png" alt="Little Text Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We see exactly the same results as in the first benchmark. The naive streams iscompletely useless for little files. So let's remove itand see what happens for interesting methods :&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/little-text-between-disks-sub.png"&gt;&lt;img class="size-full wp-image-976" title="Little Text Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/little-text-between-disks-sub.png" alt="Little Text Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here again, the conclusion are the same and the times are not enough big to make global conclusions.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/medium-text-between-disks-sub.png"&gt;&lt;img class="size-full wp-image-977" title="Medium Text Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/medium-text-between-disks-sub.png" alt="Medium Text Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here, we have the limits of the buffered strategy and see a real advantage of custom buffer strategy. We also see that the NIO Transfer and Path methods are taking a little advantage. But again, the times are really short.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/big-text-between-disks-sub.png"&gt;&lt;img class="size-full wp-image-978" title="Big Text Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/big-text-between-disks-sub.png" alt="Big Text Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can see the reintroduction of the native method on the interesting methods.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/fat-text-between-disks.png"&gt;&lt;img class="size-full wp-image-979" title="Fat Text Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/fat-text-between-disks.png" alt="Fat Text Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So we covered the text files. If we compare the times between the first benchmark (the same disk) and this method (between two disk), we can see that the times are almost the same, just a little slower for some methods. So let's watch the big binary files :&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/fat-binary-between-disks.png"&gt;&lt;img class="size-full wp-image-980" title="Fat Binary Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/fat-binary-between-disks.png" alt="Fat Binary Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Again, the results are close to using the same disk. So let's see with the last file :&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/enormous-binary-between-disks.png"&gt;&lt;img class="size-full wp-image-981" title="Enormous Binary Benchmark Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/enormous-binary-between-disks.png" alt="Enormous Binary Benchmark Results" width="500" height="400"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This time, the differences are impressive. The native and NIO Buffer methods are the fastest methods. The NIO Transfer is a little slower but the Path method is a lot slower here.&lt;/p&gt;
&lt;p&gt;This transfer is a lot faster than on the same disk. I'm not sure of the cause of these results. The only reason I can found is that the operating system can made the two things at the same time, reading on the first disk and writing on the second disk. If someone has a better conclusion, don't hesitate to comment the post.&lt;/p&gt;
&lt;p&gt;The detailed informations (standard deviation, confidence intervals and other stats stuff) are available in the conclusion page.&lt;/p&gt;
&lt;!--nextpage--&gt;

&lt;h4&gt;Benchmark between two disks (Ext4 -&amp;gt; NTFS)&lt;/h4&gt;

&lt;p&gt;Here are the results of the first version of this post. The first disk is always the same, but the second disk is a NTFS. For concision, I removed some graphes. I've also removed the conclusion that are the same as the first two benchmarks. The native method is not covered in these results.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/little-text-file-2.png"&gt;&lt;img class="size-full wp-image-876" title="Little Text File - Best results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/little-text-file-2.png" alt="Little Text File - Best results" width="483" height="411"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The best two versions are the Buffered Streams and Buffered Readers. Here this is because the buffered streams and readers can write the file in only one operation. Here the times are in microseconds, so there is really little differences between the methods. So the results are not really relevant.&lt;/p&gt;
&lt;p&gt;Now, let's test with a bigger file.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/medium-text-file.png"&gt;&lt;img class="size-full wp-image-877" title="Medium Text File" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/medium-text-file.png" alt="Medium Text File" width="488" height="371"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can see that the versions with the Readers are a little slower than the version with the streams. This is because Readers works on character and for every read() operation, a char conversion must be made, and the same conversion must be made on the other side.&lt;/p&gt;
&lt;p&gt;Another observation is that the custom buffer strategy is faster than the buffering of the streams and than using custom buffer with a buffered stream or a single stream doesn't change anything. The same observation can be made using the custom buffer using readers, it's the same with buffered readers or not. This is logical, because with custom buffer we made 4096 (size of the buffer) times less invocations to the read method and because we ask for a complete buffer we have not a lot of I/O operations. So the buffer of the streams (or the readers) is not useful here. The NIO buffer strategy is almost equivalent to custom buffer. And the direct transfer using NIO is here slower than the custom buffer methods. I think this is because here the cost of invoking native methods in the operating system level is higher than simply the cost of making the file copy.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/big-text-file-2.png"&gt;&lt;img class="size-full wp-image-879" title="Big Text File - Best results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/big-text-file-2.png" alt="Big Text File - Best results" width="483" height="391"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here, it's now clear that the custom buffer strategy is a better than the simple buffered streams or readers and that using custom buffer and buffered streams is really useful for bigger files. The Custom Buffer Readers method is better than Custom Buffer Streams because FileReader use a buffer internally.&lt;/p&gt;
&lt;p&gt;And now, continue with a bigger file :&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/fat-text-file.png"&gt;&lt;img class="size-full wp-image-880" title="Fat Text File Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/fat-text-file.png" alt="Fat Text File Results" width="483" height="392"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can see that it doesn't take 500 ms to copy a 50 MB file using the custom buffer strategy and that it even doesn't take 400 ms with the NIO Transfer method. Really quick isn't it ? We can see that for a big file, the NIO Transfer start to show an advantage, we'll better see that in the binary file benchmarks. We will directly start with a big file (5 MB) for this benchmark :&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/big-binary-file.png"&gt;&lt;img class="size-full wp-image-881" title="Big Binary File Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/big-binary-file.png" alt="Big Binary File Results" width="483" height="291"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So we can make the same conclusion as for the text files, of course, the buffered streams methods is not fast. The other methods are really close.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/fat-binary-file.png"&gt;&lt;img class="size-full wp-image-883" title="Fat Binary File Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/fat-binary-file.png" alt="Fat Binary File Results" width="483" height="291"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We see here again that the NIO Transfer is gaining advantages more the files is bigger.&lt;/p&gt;
&lt;p&gt;And just for the pleasure, a great file (1.3 GB) :&lt;/p&gt;
&lt;p&gt;&lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/enormous-binary-file.png"&gt;&lt;img class="size-full wp-image-882" title="Enormous Binary File Results" src="http://baptiste-wicht.com/wp-content/uploads/2010/08/enormous-binary-file.png" alt="Enormous Binary File Results" width="483" height="291"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We see that all the methods are really close, but the NIO Transfer method has an advantage of 500 ms. It's not negligible.&lt;/p&gt;
&lt;p&gt;A conclusion we can make is that transfering a file from Ext4 to Ext4 is a lot faster than from Ext4 to NTFS. I think it's logical because the operating system must made conversions. I think it's not because of the disk, because the NTFS disk is the faster I've.&lt;/p&gt;
&lt;!--nextpage--&gt;

&lt;h4&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;In conclusion, the NIO Transfer method is the best one for big files but it's not the fastest for little files (&amp;lt; 5 MB). But the custom buffer strategy (and the NIO Buffer too) are also really fast methods to copy files. We've also see that the method using the native utility tools to make the copy is faster as NIO for big files (&amp;lt; 1 GB) but it's really slow for little files because of the cost of invoking an external program.&lt;/p&gt;
&lt;p&gt;So perhaps, the best method is a method that make a custom buffer strategy on the little files and a NIO Transfer on the big ones and perhaps use the native executable on the really bigger ones. But it will be interesting to also make the tests on an other computer and operating system.&lt;/p&gt;
&lt;p&gt;We can take several rules from this benchmark :&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;Never made a copy of file byte by byte (or char by char)&lt;/li&gt;
    &lt;li&gt;Prefer a buffer in your side more than in the stream to make less invocations of the read method, but don't forget the buffer in the side of the streams&lt;/li&gt;
    &lt;li&gt;Pay attention to the size of the buffers&lt;/li&gt;
    &lt;li&gt;Don't use char conversion if you only need to tranfer the content of a file, so don't use Reader if you need only streams.&lt;/li&gt;
    &lt;li&gt;Don't hesitate to use channels to make file transfer, it's the fastest way to make a file transfer.&lt;/li&gt;
    &lt;li&gt;Consider the native executable invocation only for really bigger files.&lt;/li&gt;
    &lt;li&gt;The new Path method of Java 7 is really fast except for the transfer of an enormous file between two disks.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I hope this benchmark (and its results) interested you.&lt;/p&gt;
&lt;p&gt;Here are the sources of the benchmark :Â &lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/FileCopyBenchmark2.java"&gt;File Copy Benchmark Version 3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here are the informations complete for the benchmark between two disks : &lt;a href="http://baptiste-wicht.com/wp-content/uploads/2010/08/results.txt"&gt;Complete results of first two benchmarks&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>I/O</category><category>Java</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2010/08/file-copy-in-java-benchmark.html</guid><pubDate>Mon, 02 Aug 2010 05:18:44 GMT</pubDate></item><item><title>Find closest pair of point with Plane Sweep Algorithm in O(n ln n)</title><link>http://baptiste-wicht.com/posts/2010/04/closest-pair-of-point-plane-sweep-algorithm.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Finding the closest pair of Point in a given collection of points is a standard problem in computational geometry. In this article I'll explain an efficient algorithm using plane sweep, compare it to the naive implementation and discuss its complexity.&lt;/p&gt;
&lt;p class="more"&gt;&lt;a href="http://baptiste-wicht.com/posts/2010/04/closest-pair-of-point-plane-sweep-algorithm.html"&gt;Read moreâ€¦&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Algorithm</category><category>Benchmarks</category><category>Conception</category><category>Java</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2010/04/closest-pair-of-point-plane-sweep-algorithm.html</guid><pubDate>Tue, 27 Apr 2010 14:08:10 GMT</pubDate></item><item><title>How to write correct benchmarks</title><link>http://baptiste-wicht.com/posts/2010/04/write-corrects-benchmarks.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Several months ago, I wrote an article to &lt;a href="http://www.baptiste-wicht.com/2010/01/dont-use-shorts-in-loop/"&gt;compare the performances of short indexes for loops&lt;/a&gt;. I wrote that code to achieve my goal :&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;package&lt;/span&gt; &lt;span class="nn"&gt;com.wicht.old&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;TestShortInt&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;){&lt;/span&gt;
        &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;startTime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;nanoTime&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;

        &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;resultInt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++){&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="mi"&gt;32760&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;++){&lt;/span&gt;
                &lt;span class="n"&gt;resultInt&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
            &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;

        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Temp pour int : "&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;nanoTime&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;startTime&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;1000000&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;" ms"&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;

        &lt;span class="n"&gt;startTime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;nanoTime&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;

        &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;resultShort&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;++){&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;short&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="mi"&gt;32760&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;++){&lt;/span&gt;
                &lt;span class="n"&gt;resultShort&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
            &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;

        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Temp pour short : "&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;nanoTime&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;startTime&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;1000000&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;" ms"&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;

        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;resultInt&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;resultShort&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;And i found as a result that short was two times slower than int and I was convinced of these results until a week ago.&lt;/p&gt;
&lt;p&gt;At this time, a reader (Jean) criticized the results of my tests and gave me links to several articles about &lt;strong&gt;micro-benchmarking&lt;/strong&gt;. I've read these articles and understand why my results were incorrect.&lt;/p&gt;
&lt;p&gt;In fact, my test doesn't pay attention to several things that can change results of tests :&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;JVM warmup&lt;/strong&gt; : Due to several parameters, the code is first often slow and becomes faster and faster when the execution time grows until it goes to steady-state.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Class loading&lt;/strong&gt; : The first time you launch a benchmark, all the used classes must be loaded, increasing the execution time.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Just In Time Compiler&lt;/strong&gt; : When the JVM identify a hot part of the code&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Garbage Collector&lt;/strong&gt; : A garbage collection can happen during the benchmark and with that the time can increase a lot.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Due to all these factors, the first runs (perhaps 10 seconds of run) are slower than the other and than can make your benchmarks completely false.&lt;/p&gt;
&lt;p&gt;So, how can we do to have good benchmarks results ?&lt;/p&gt;
&lt;p&gt;It's really difficult, but we can have help using a benchmark framework introduced by Brent Boyer, a software developer from Elliptic Group. This framework take care of all the previously introduced factors and made good benchmarks.&lt;/p&gt;
&lt;p&gt;The use of this framework is really simple, you just have to create a new instance of the Benchmark class passing to it a Callable or a Runnable and the test is directly launched. Here is the example with the test of short and int in loop indexes :&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ShortIndexesLoop&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;Callable&lt;/span&gt; &lt;span class="n"&gt;callableInt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="o"&gt;(){&lt;/span&gt;
            &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Long&lt;/span&gt; &lt;span class="nf"&gt;call&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="kd"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;Exception&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
                &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

                &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="mi"&gt;32760&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;++){&lt;/span&gt;
                      &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;444&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
                  &lt;span class="o"&gt;}&lt;/span&gt;

                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
            &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;};&lt;/span&gt;

        &lt;span class="n"&gt;Callable&lt;/span&gt; &lt;span class="n"&gt;callableShort&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="o"&gt;(){&lt;/span&gt;
            &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Long&lt;/span&gt; &lt;span class="nf"&gt;call&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="kd"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;Exception&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
                &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

                &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;short&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="mi"&gt;32760&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;++){&lt;/span&gt;
                      &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;444&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
                  &lt;span class="o"&gt;}&lt;/span&gt;

                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
            &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;};&lt;/span&gt;

        &lt;span class="k"&gt;try&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;Benchmark&lt;/span&gt; &lt;span class="n"&gt;intBenchmark&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Benchmark&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;callableInt&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;

            &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Result with int "&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
            &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;intBenchmark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;

            &lt;span class="n"&gt;Benchmark&lt;/span&gt; &lt;span class="n"&gt;shortBenchmark&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Benchmark&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;callableShort&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;

            &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Result short "&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
            &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shortBenchmark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;catch&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Exception&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;printStackTrace&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;To get the results, you can use Benchmark.toString() or Benchmark.toStringFull() for more statistics. You can also directly access some stats like standard deviation using Benchmark.getSd() or directly with Benchmark.getStats() to get all the stats.&lt;/p&gt;
&lt;p&gt;Here is the result with the preceding code :&lt;/p&gt;
&lt;pre&gt;Result int
first = 807.056 us, mean = 46.032 us (CI deltas: -261.393 ns, +408.932 ns), sd = 230.929 us (CI deltas: -68.201 us, +105.262 us)
Result short
first = 721.912 us, mean = 48.234 us (CI deltas: -198.625 ns, +254.774 ns), sd = 160.196 us (CI deltas: -32.764 us, +37.882 us)&lt;/pre&gt;

&lt;p&gt;As you can see, the short version is only 104.78% slower than the int. That show that the first results were completely false.&lt;/p&gt;
&lt;p&gt;Here is the full results of the int version :&lt;/p&gt;
&lt;pre&gt;action statistics: first = 807.056 us, mean = 46.032 us (CI deltas: -261.393 ns, +408.932 ns), sd = 230.929 us (CI deltas: -68.201 us, +105.262 us) WARNING: EXECUTION TIMES HAVE EXTREME OUTLIERS, SD VALUES MAY BE INACCURATE
    ----------
    --the action statistics were calculated from block statistics
    --each block measured 32768 task executions
    --the user says that task internally performs m = 1 actions
    --then the number of actions per block measurement is a = 32768
    --block statistics: mean = 1.508 s (CI deltas: -8.565 ms, +13.400 ms), sd = 41.803 ms (CI deltas: -12.346 ms, +19.054 ms)
    --the forumla used to convert block statistics to action statistics (mean scales as 1/a, sd scales as 1/sqrt(a)) assumes that the action execution times are iid
    ----------
    --each confidence interval (CI) is reported as either +- deltas from the point estimate, or as a closed interval ([x, y])
    --each confidence interval has confidence level = 0.95
    ----------
    --EXECUTION TIMES APPEAR TO HAVE OUTLIERS
    --this was determined using the boxplot algorithm with median = 1.498 s, interquantileRange = 34.127 ms
    --3 are EXTREME (on the high side): #57 = 1.621 s, #58 = 1.647 s, #59 = 1.688 s
    --2 are mild (on the high side): #55 = 1.570 s, #56 = 1.582 s
    ----------
    --block sd values MAY NOT REFLECT TASK'S INTRINSIC VARIATION
    --guesstimate: environmental noise explains at least 55.89418621876822% of the measured sd
    ----------
    --action sd values ALMOST CERTAINLY GROSSLY INFLATED by outliers
    --they cause at least 98.95646276911543% of the measured VARIANCE according to a equi-valued outlier model
    --model quantities: a = 32768.0, muB = 1.5083895562166663, sigmaB = 0.04180264914581472, muA = 4.603239612477619E-5, sigmaA = 2.3092919283255957E-4, tMin = 0.0, muGMin = 2.3016198062388096E-5, sigmaG = 5.754049515597024E-6, cMax1 = 1252, cMax2 = 322, cMax = 322, cOutMin = 322, varOutMin = 0.0017292260645147487, muG(cOutMin) = 2.3034259031465023E-5, U(cOutMin) = 0.002363416110812895&lt;/pre&gt;

&lt;p&gt;Like you can perhaps see when you use this framework, it gives you some warnings when by example you have extreme outliers that can make the standard deviation completely false.&lt;/p&gt;
&lt;p&gt;You can download this framework on &lt;a href="http://www.ellipticgroup.com/html/benchmarkingArticle.html"&gt;the web page of the Elliptic Group&lt;/a&gt;. I found it really powerful and easy to use and I'll use it everytime I have to do a benchmark.&lt;/p&gt;
&lt;p&gt;To conclude, I must also say that even if you use that kind of framework, you can make very bad benchmarks if you don't test the right part of the code. Here are two really interesting articles from Brent Boyer :&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;a href="http://www.ibm.com/developerworks/java/library/j-benchmark1.html" target="_blank"&gt;Robust Java benchmarking, Part 1: Issues&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href="http://www.ibm.com/developerworks/java/library/j-benchmark2/index.html" target="_blank"&gt;Robust Java benchmarking, Part 2: Statistics and solutions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>Java</category><category>Libraries</category><category>Performances</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2010/04/write-corrects-benchmarks.html</guid><pubDate>Mon, 26 Apr 2010 04:49:45 GMT</pubDate></item><item><title>Tip : Donâ€™t use shorts for loop indexes !</title><link>http://baptiste-wicht.com/posts/2010/01/dont-use-shorts-in-loop.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;After a post I read on a french forum, i asked myself of the performances using shorts as loop indexes for loop with few iterations (less than 32768).&lt;/p&gt;
&lt;p&gt;At first view, it can be tempting because we save 2 octets, so why use an int instead a short ?&lt;/p&gt;
&lt;p&gt;But, when we think of that, we see that the int is more adapted. Indeed, it's more performant.&lt;/p&gt;
&lt;p&gt;Why ?&lt;/p&gt;
&lt;p&gt;In Java language, all the operations on integers are made in int. Thus, if we use a short as loop index, at each iterations, a typecasting will be made, that's really heavier than a simple affectation to int.&lt;/p&gt;
&lt;p&gt;Here were my first results :&lt;/p&gt;
&lt;pre&gt;Time for int : 1441 ms
Time for short : 3015 ms&lt;/pre&gt;

&lt;p&gt;The short version is two times slower !&lt;/p&gt;
&lt;p&gt;But like Jean have said, my first test was not correct at all because it doesn't consider some issues that could occurs with micro-benchmarks.&lt;/p&gt;
&lt;p&gt;So I write a new test using a Java Benchmarking framework from Brent Boyer. Here is a little test with that framework :&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;package&lt;/span&gt; &lt;span class="nn"&gt;com.wicht&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;bb.util.Benchmark&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;java.util.concurrent.Callable&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ShortIndexesLoop&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;Callable&lt;/span&gt; &lt;span class="n"&gt;callableInt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="o"&gt;(){&lt;/span&gt;
            &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Long&lt;/span&gt; &lt;span class="nf"&gt;call&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="kd"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;Exception&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
                &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

                &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="mi"&gt;32760&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;++){&lt;/span&gt;
                      &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
                  &lt;span class="o"&gt;}&lt;/span&gt;

                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
            &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;};&lt;/span&gt;

        &lt;span class="n"&gt;Callable&lt;/span&gt; &lt;span class="n"&gt;callableShort&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="o"&gt;(){&lt;/span&gt;
            &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Long&lt;/span&gt; &lt;span class="nf"&gt;call&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="kd"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;Exception&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
                &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

                &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;short&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;amp&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;span class="n"&gt;lt&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="mi"&gt;32760&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;++){&lt;/span&gt;
                      &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
                  &lt;span class="o"&gt;}&lt;/span&gt;

                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
            &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;};&lt;/span&gt;

        &lt;span class="k"&gt;try&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Result with int "&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Benchmark&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;callableInt&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
            &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Result with short "&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Benchmark&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;callableShort&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;catch&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Exception&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;printStackTrace&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;And here are the results :&lt;/p&gt;
&lt;pre&gt;Result with int first = 695.181 us, mean = 43.233 us (CI deltas: -49.358 ns, +74.073 ns), sd = 42.938 us (CI deltas: -11.634 us, +16.426 us)
Result with short first = 733.224 us, mean = 45.679 us (CI deltas: -62.975 ns, +63.932 ns), sd = 45.567 us (CI deltas: -5.877 us, +8.020 us)&lt;/pre&gt;

&lt;p&gt;So the results are lower. This time, the short version is 5.657% slower than the int version. Note that this can vary a lot depending on your configuration.&lt;/p&gt;
&lt;p&gt;I talk here of for loops, but the case is the same when you use while loops with indexes.&lt;/p&gt;
&lt;p&gt;Here are also the results with long, double and float versions :&lt;/p&gt;
&lt;pre&gt;Result with long first = 816.555 us, mean = 104.771 us (CI deltas: -236.563 ns, +344.219 ns), sd = 143.295 us (CI deltas: -39.149 us, +63.700 us)
Result with float first = 1.018 ms, mean = 58.055 us (CI deltas: -87.036 ns, +113.537 ns), sd = 70.757 us (CI deltas: -14.269 us, +19.962 us)
Result with double first = 912.115 us, mean = 57.918 us (CI deltas: -66.644 ns, +91.312 ns), sd = 55.185 us (CI deltas: -12.617 us, +25.160 us)&lt;/pre&gt;

&lt;p&gt;We can see that the long version is the slowest one and that float and double are equivalent but slower than int and short.&lt;/p&gt;
&lt;p&gt;To conclude, always use int as loop indexes add a very little improvements of performances, but not a great thing and do not use long for loops indexes.&lt;/p&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>Java</category><category>Performances</category><category>Tips</category><guid>http://baptiste-wicht.com/posts/2010/01/dont-use-shorts-in-loop.html</guid><pubDate>Sun, 10 Jan 2010 14:12:33 GMT</pubDate></item></channel></rss>