<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Blog blog("Baptiste Wicht"); (Posts about LVM)</title><link>http://baptiste-wicht.com/</link><description></description><atom:link type="application/rss+xml" rel="self" href="http://baptiste-wicht.com/categories/lvm.xml"></atom:link><language>en</language><lastBuildDate>Sat, 12 Aug 2017 10:42:56 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>How to speed up RAID (5-6) growing with mdadm ?</title><link>http://baptiste-wicht.com/posts/2015/03/how-to-speed-up-raid-5-6-growing-with-mdadm.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Yesterday, I added my 11th disk to my RAID 6 array. As the last time it took my more than 20 hours, I spent some time investigating how to speed things up and this post contains some tips on how to achieve good grow performances. With these tips, I have been able to reach a speed of about 55K in average during reshape. It did finish in about 13 hours.&lt;/p&gt;
&lt;p&gt;First, take into account that some of these tips may depend on your configuration. In my case, this server is only used for this RAID, so I don't care if the CPU is used a lot during rebuild or if other processes are suffering from the load. This may not be the case with your configuration. Moreover, I speak only of hard disks, if you use SSD RAID, there are probably better way of tuning the rebuild (or perhaps it is fast enough). Finally, you have know that a RAID reshape is going to be slow, there is no way you'll grow a 10+ RAID array in one hour. G&lt;/p&gt;
&lt;p&gt;In the examples, I use /dev/md0 as the raid array, you'll have to change this to your array name.&lt;/p&gt;
&lt;p&gt;The first 3 tips can be used even after the rebuild has started and you should see the differences in real-time. But, these 3 tips will also be erased after each reboot.&lt;/p&gt;
&lt;div class="section" id="increase-speed-limits"&gt;
&lt;h2&gt;Increase speed limits&lt;/h2&gt;
&lt;p&gt;The easiest thing to do is to increase the system speed limits on raid. You can see the current limits on your system by using these commands:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_a72e40a4d3b346bbb58b387f1be753af-1"&gt;&lt;/a&gt;sysctl dev.raid.speed_limit_min
&lt;a name="rest_code_a72e40a4d3b346bbb58b387f1be753af-2"&gt;&lt;/a&gt;sysctl dev.raid.speed_limit_max
&lt;/pre&gt;&lt;p&gt;These values are set in Kibibytes per second (KiB/s).&lt;/p&gt;
&lt;p&gt;You can put them to high values:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_d5fdf6517c5948a9a7e553caf8534be2-1"&gt;&lt;/a&gt;sysctl -w dev.raid.speed_limit_min&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;100000&lt;/span&gt;
&lt;a name="rest_code_d5fdf6517c5948a9a7e553caf8534be2-2"&gt;&lt;/a&gt;sysctl -w dev.raid.speed_limit_max&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;500000&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;At least with these values, you won't be limited by the system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="increase-stripe-cache-size"&gt;
&lt;h2&gt;Increase stripe cache size&lt;/h2&gt;
&lt;p&gt;By allowing the array to use more memory for its stripe cache, you may improve the performances. In some cases, it can improve performances by up to 6 times. By default, the size of the stripe cache is 256, in pages. By default, Linux uses 4096B pages. If you use 256 pages for the stripe cache and you have 10 disks, the cache would use 10*256*4096=10MiB of RAM. In my case, I have increased it to 4096:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_311bcc62c93d467b9a97d4132c8c525f-1"&gt;&lt;/a&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="m"&gt;4096&lt;/span&gt; &amp;gt; /sys/block/md0/md/stripe_cache_size
&lt;/pre&gt;&lt;p&gt;The maximum value is 32768. If you have many disks, this may well take all your available memory. I don't think values higher than 4096 will improve performance, but feel free to try it ;)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="increase-read-ahead"&gt;
&lt;h2&gt;Increase read-ahead&lt;/h2&gt;
&lt;p&gt;If configured too low, the read-ahead of your array may make things slower.&lt;/p&gt;
&lt;p&gt;You can see get the current read-ahead value with this command:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_ba5e35cedbe4486b94f6f56fe49bf9f5-1"&gt;&lt;/a&gt;blockdev --getra /dev/md0
&lt;/pre&gt;&lt;p&gt;These values are in 512B sector. You can set it to 32MB to be sure:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_3e437f76fd0945379002cd0af450f8d7-1"&gt;&lt;/a&gt;blockdev --setra &lt;span class="m"&gt;65536&lt;/span&gt; /dev/md0
&lt;/pre&gt;&lt;p&gt;This can improve the performances, but don't expect this to be a game-changer unless it was configured really low at the first place.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="tip-reshape-stuck-at-0k-s"&gt;
&lt;h2&gt;Tip: Reshape stuck at 0K/s&lt;/h2&gt;
&lt;p&gt;If reshape starts, but with a speed of 0K/s, you can try to issue this simple
command:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_de6bd13db14849e09a525cd4462ba01e-1"&gt;&lt;/a&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; max &amp;gt; /sys/block/md0/md/sync_max
&lt;/pre&gt;&lt;p&gt;And the reshape should start directly at your maximum speed.&lt;/p&gt;
&lt;p&gt;The solution is the same if you are growing any type of RAID level with parity
(RAID5, RAID6, ...).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="bonus-speed-up-standard-resync-with-a-write-intent-bitmap"&gt;
&lt;h2&gt;Bonus: Speed up standard resync with a write-intent bitmap&lt;/h2&gt;
&lt;p&gt;Although it won't speed up the growing of your array, this is something that you should do after the rebuild has finished. Write-intent bitmaps is a kind of map of what needs to be resynced. This is of great help in several cases:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;When the computer crash (power shutdown for instance)&lt;/li&gt;
&lt;li&gt;If a disk is disconnected, then reconnected.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In these case, it may totally avoid the need of a rebuild which is great in my opinion. Moreover, it does not take any space on the array since it uses space that is not usable by the array.&lt;/p&gt;
&lt;p&gt;Here is how to enable it:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_75fe38a675fd443ea647ac19045a28ba-1"&gt;&lt;/a&gt;mdadm --grow --bitmap&lt;span class="o"&gt;=&lt;/span&gt;internal /dev/md0
&lt;/pre&gt;&lt;p&gt;However, it may cause some write performance degradation. In my case, I haven't seen any noticeable degradation, but if it is the case, you may want to disable it:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_4c8b6fc796584ccf8d338188fcda5b60-1"&gt;&lt;/a&gt;mdadm --grow --bitmap&lt;span class="o"&gt;=&lt;/span&gt;none /dev/md0
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="bonus-monitor-rebuild-process"&gt;
&lt;h2&gt;Bonus: Monitor rebuild process&lt;/h2&gt;
&lt;p&gt;If you want to monitor the build process, you can use the watch command:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_b7239689216d4e0e888ed37631bc6dfd-1"&gt;&lt;/a&gt;watch cat /proc/mdstat
&lt;/pre&gt;&lt;p&gt;With that you'll see the rebuild going in real-time.&lt;/p&gt;
&lt;p&gt;You can also monitor the I/O statistics:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_3dc0db910d214ab4a562397b73144ba4-1"&gt;&lt;/a&gt;watch iostat -k &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="bonus-how-to-grow-a-raid-5-6-array"&gt;
&lt;h2&gt;Bonus: How to grow a RAID 5-6 array&lt;/h2&gt;
&lt;p&gt;As a sidenote, this section indicates how to grow an array. If you  want to add the disk /dev/sdl to the array /dev/md0, you'll first have to add it:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_63f50495ecda401eac74ace70e7cd9e3-1"&gt;&lt;/a&gt;mdadm --add /dev/md0 /dev/sdl
&lt;/pre&gt;&lt;p&gt;This will add the disk as a spare disk. If you had 5 disks before, you'll want to grow it to 6:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_7a054f3cfad34df796939f3c516ad803-1"&gt;&lt;/a&gt;mdadm --grow --backup-file&lt;span class="o"&gt;=&lt;/span&gt;/root/grow_md0_backup_file --raid-devices&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt; /dev/md0
&lt;/pre&gt;&lt;p&gt;The backup file must be on another disk of course. The backup file is optional but improves the chance of success if you have a power shutdown or another form of unexpected shutdown. If you know what you're doing, you can grow it without backup-file:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_2f9c5a918fab4d29b1e7b3e7dec30e1d-1"&gt;&lt;/a&gt;mdadm --grow --raid-devices&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt; /dev/md0
&lt;/pre&gt;&lt;p&gt;This command will return almost instantly, but the actual reshape won't likely be finished for hours (maybe days).&lt;/p&gt;
&lt;p&gt;Once the rebuild is finished, you'll still have to extend the partitions with resize2fs. If you use LVM on top of the array, you'll have to resize the Physical Volume (PV) first:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_2d9b9529177c4abca5edde06f57ca124-1"&gt;&lt;/a&gt;pvresize /dev/md0
&lt;/pre&gt;&lt;p&gt;and then extend the Logical Volume (s) (LV). For instance, if you want to add 1T to a LV named /dev/vgraid/work:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_3e89c3c80a5e4b3e888a95adaae853b3-1"&gt;&lt;/a&gt;vgextend -r -L+1T /dev/vgraid/work
&lt;/pre&gt;&lt;p&gt;The -r option will automatically resize the underlying filesystem. Otherwise, you'd still have to resize it with resize2fs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;These are the changes I have found that speed up the reshape process. There are others that you may test in your case. For instance, in some systems disabling NCQ on each disk may help.&lt;/p&gt;
&lt;p&gt;I hope that these tips will help you doing fast rebuilds in your RAID array :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>Gentoo</category><category>Hardware</category><category>Home Server</category><category>Linux</category><category>LVM</category><guid>http://baptiste-wicht.com/posts/2015/03/how-to-speed-up-raid-5-6-growing-with-mdadm.html</guid><pubDate>Sat, 07 Mar 2015 14:01:56 GMT</pubDate></item><item><title>Changing root hard disk on Gentoo with LVM and Grub2</title><link>http://baptiste-wicht.com/posts/2014/05/changing-root-hard-disk-gentoo-lvm-grub2.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I recently changed my hard disk on one of my servers and I didn't found a simple
and up to date tutorial to do that, so I decided to post here how I did it. &lt;/p&gt;
&lt;p&gt;I do not claim that is the best method, or the easiest or the safest method, but
that is the one I used and it worked quite well. You could do it online with LVM, 
but when changing the boot hard disk, I'd rather be safe. Especially because you
need to reinstall Grub on the new hard disk and probably alter its
configuration. But, doing it online directly with LVM should work. &lt;/p&gt;
&lt;p&gt;My server is using LVM and Grub2. You will need a Gentoo Live CD (or DVD, or
USB, or any other Linux live installation with the necessary tools). If you do
not use LVM, this guide is not for you. Even if I wrote this guide for Gentoo,
this is also gonna work with another system as long as you use LVM and Grub2,
just use your distribution live cd. &lt;/p&gt;
&lt;p&gt;Here I will assume /dev/sda is your old disk and /dev/sdb is your new disk.
I will also assume that you previous VG was vg0. If it is, you'll have to
replace them in the commands. &lt;/p&gt;
&lt;p&gt;1. Obviously, shutdown your computer. &lt;/p&gt;
&lt;p&gt;2. Install the new hard disk in it. &lt;/p&gt;
&lt;p&gt;3. Reboot on the Live CD:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Activate LVM: vgchange -a y&lt;/li&gt;
&lt;li&gt;Create the LVM partition on the new disk (/dev/sdb) with fdisk. &lt;/li&gt;
&lt;li&gt;Create a new PV: pvcreate /dev/sdb1&lt;/li&gt;
&lt;li&gt;Create a new VG: vgcreate vg1 /dev/sdb1&lt;/li&gt;
&lt;li&gt;Recreate all your LV with lvcreate. In my case, I have /dev/vg0/root and
/dev/vg0/data &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, you'll need to copy your data. Here is an example with my root: &lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;mkdir old
mkdir new
mount /dev/vg0/root/ old
mount /dev/vg1/root/ new
cp -ax old/* new
umount old
umount new
&lt;/pre&gt;


&lt;p&gt;You have to do that with for LV you have. &lt;/p&gt;
&lt;p&gt;4. Shutdown the computer&lt;/p&gt;
&lt;p&gt;5. Remove the old hard disk and use the connector of the old hard disk to
connect the new one. &lt;/p&gt;
&lt;p&gt;6. Reboot on the Live CD: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Activate LVM: vchange -a y&lt;/li&gt;
&lt;li&gt;Rename the VG: vgrename vg0 &lt;/li&gt;
&lt;li&gt;Chroot in the new hard disk&lt;/li&gt;
&lt;li&gt;Install grub on the hard disk: grub2-install /dev/sda&lt;/li&gt;
&lt;li&gt;Regenerate the Grub config: grub2-mkconfig -o /boot/grub/grub.cfg&lt;/li&gt;
&lt;li&gt;Exit from chroot&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;7. Reboot on Gentoo&lt;/p&gt;
&lt;p&gt;8. You now have switched hard disk. &lt;/p&gt;
&lt;p&gt;At any point, if something goes wrong, you still have the previous hard disk
completely ready at hand. &lt;/p&gt;
&lt;p&gt;I hope this would useful to some Linux users. &lt;/p&gt;&lt;/div&gt;</description><category>Gentoo</category><category>LVM</category><category>Tips</category><guid>http://baptiste-wicht.com/posts/2014/05/changing-root-hard-disk-gentoo-lvm-grub2.html</guid><pubDate>Tue, 06 May 2014 19:39:57 GMT</pubDate></item></channel></rss>