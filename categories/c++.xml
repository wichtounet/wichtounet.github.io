<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Blog blog("Baptiste Wicht"); (Posts about C++)</title><link>http://baptiste-wicht.com/</link><description></description><atom:link href="http://baptiste-wicht.com/categories/c%2B%2B.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Wed, 13 Sep 2017 14:19:00 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>C++11 Concurrency Tutorial - Part 5: Futures</title><link>http://baptiste-wicht.com/posts/2017/09/cpp11-concurrency-tutorial-futures.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've been recently reminded that a long time ago I was doing a series of
tutorial on C++11 Concurrency. For some reason, I haven't continued these
tutorials.  The next post in the series was supposed to be about Futures, so I'm
finally going to do it :)&lt;/p&gt;
&lt;p&gt;Here are the links to the current posts of the C++11 Concurrency Tutorial:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/03/cpp11-concurrency-part1-start-threads.html"&gt;Part 1: Start Threads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/03/cp11-concurrency-tutorial-part-2-protect-shared-data.html"&gt;Part 2: Protect Shared Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/04/c11-concurrency-tutorial-advanced-locking-and-condition-variables.html"&gt;Part 3: Advanced Locking and condition variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/07/c11-concurrency-tutorial-part-4-atomic-type.htm"&gt;Part 4: Atomic Types&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, we are going to talk about futures, more precisely
&lt;code&gt;std::future&amp;lt;T&amp;gt;&lt;/code&gt;. What is a future ? It's a very nice and simple mechanism
to work with asynchronous tasks. It also has the advantage of decoupling you
from the threads themselves, you can do multithreading without using
&lt;code&gt;std::thread&lt;/code&gt;. The future itself is a structure pointing to a result that
will be computed in the future. How to create a future ? The simplest way is to
use &lt;code&gt;std::async&lt;/code&gt; that will create an asynchronous task and return
a &lt;code&gt;std::future&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let's start with the simplest of the examples:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_545e4b235e7d446f915be0f994c14e94-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;thread&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_545e4b235e7d446f915be0f994c14e94-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;future&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_545e4b235e7d446f915be0f994c14e94-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_545e4b235e7d446f915be0f994c14e94-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_545e4b235e7d446f915be0f994c14e94-5"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_545e4b235e7d446f915be0f994c14e94-6"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;future&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_545e4b235e7d446f915be0f994c14e94-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm a thread"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_545e4b235e7d446f915be0f994c14e94-8"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_545e4b235e7d446f915be0f994c14e94-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_545e4b235e7d446f915be0f994c14e94-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_545e4b235e7d446f915be0f994c14e94-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_545e4b235e7d446f915be0f994c14e94-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_545e4b235e7d446f915be0f994c14e94-13"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Nothing really special here. &lt;code&gt;std::async&lt;/code&gt; will execute the task that we
give it (here a lambda) and return a &lt;code&gt;std::future&lt;/code&gt;. Once you use the
&lt;code&gt;get()&lt;/code&gt; function on a future, it will wait until the result is available
and return this result to you once it is. The &lt;code&gt;get()&lt;/code&gt; function is then
blocking. Since the lambda, is a void lambda, the returned future is of type
&lt;code&gt;std::future&amp;lt;void&amp;gt;&lt;/code&gt; and &lt;code&gt;get()&lt;/code&gt; returns &lt;code&gt;void&lt;/code&gt; as well. It is
very important to know that you cannot call &lt;code&gt;get&lt;/code&gt; several times on the
same future. Once the result is consumed, you cannot consume it again! If you
want to use the result several times, you need to store it yourself after you
called &lt;code&gt;get()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let's see with something that returns a value and actually takes some time
before returning it:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;thread&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;future&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-4"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;chrono&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-6"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;future&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-8"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-9"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-10"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-12"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Do something else ?&lt;/span&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-13"&gt;&lt;/a&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-14"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-15"&gt;&lt;/a&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-16"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_1139a6beed8b4be493bc3c56699d82f9-17"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This time, the future will be of the time &lt;code&gt;std::future&amp;lt;int&amp;gt;&lt;/code&gt; and thus
&lt;code&gt;get()&lt;/code&gt; will also return an &lt;code&gt;int&lt;/code&gt;. &lt;code&gt;std::async&lt;/code&gt; will again
launch a task in an asynchronous way and &lt;code&gt;future.get()&lt;/code&gt; will wait for the
answer. What is interesting, is that you can do something else before the call
to future.&lt;/p&gt;
&lt;p&gt;But &lt;code&gt;get()&lt;/code&gt; is not the only interesting function in &lt;code&gt;std::future&lt;/code&gt;.
You also have &lt;code&gt;wait()&lt;/code&gt; which is almost the same as &lt;code&gt;get()&lt;/code&gt; but does
not consume the result. For instance, you can wait for several futures and then
consume their result together. But, more interesting are the
&lt;code&gt;wait_for(duration)&lt;/code&gt; and &lt;code&gt;wait_until(timepoint)&lt;/code&gt; functions. The
first one wait for the result at most the given time and then returns and the
second one wait for the result at most until the given time point. I think that
&lt;code&gt;wait_for&lt;/code&gt; is more useful in practices, so let's discuss it further.
Finally, an interesting function is &lt;code&gt;bool valid()&lt;/code&gt;. When you use
&lt;code&gt;get()&lt;/code&gt; on the future, it will consume the result, making &lt;code&gt;valid()
returns :code:`false&lt;/code&gt;. So, if you intend to check multiple times for a future,
you should use &lt;code&gt;valid()&lt;/code&gt; first.&lt;/p&gt;
&lt;p&gt;One possible scenario would be if you have several asynchronous tasks, which is
a common scenario. You can imagine that you want to process the results as fast
as possible, so you want to ask the futures for their result several times. If
no result is available, maybe you want to do something else. Here is a possible
implementation:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;thread&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;future&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-4"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;chrono&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-6"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-8"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-9"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-10"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;f2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-14"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-15"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-17"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;f3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-19"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;666&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-20"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-21"&gt;&lt;/a&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-22"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;timeout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;milliseconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-23"&gt;&lt;/a&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-24"&gt;&lt;/a&gt;    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()){&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-25"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wait_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future_status&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ready&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-26"&gt;&lt;/a&gt;            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Task1 is done! "&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-27"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-28"&gt;&lt;/a&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-29"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wait_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future_status&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ready&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-30"&gt;&lt;/a&gt;            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Task2 is done! "&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-31"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-32"&gt;&lt;/a&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-33"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wait_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future_status&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ready&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-34"&gt;&lt;/a&gt;            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Task3 is done! "&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-35"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-36"&gt;&lt;/a&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-37"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm doing my own work!"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-38"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-39"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm done with my own work!"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-40"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-41"&gt;&lt;/a&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-42"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Everything is done, let's go back to the tutorial"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-43"&gt;&lt;/a&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-44"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_8fa0ff2d8afa449b8594feae04482d65-45"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The three tasks are started asynchronously with &lt;code&gt;std::async&lt;/code&gt; and the
resulting &lt;code&gt;std::future&lt;/code&gt; are stored. Then, as long as one of the tasks is
not complete, we query each three task and try to process its result. If no
result is available, we simply do something else. This example is important to
understand, it covers pretty much every concept of the futures.&lt;/p&gt;
&lt;p&gt;One interesting thing that remains is that you can pass parameters to your task
via &lt;code&gt;std::async&lt;/code&gt;. Indeed, all the extra parameters that you pass to
&lt;code&gt;std::async&lt;/code&gt; will be passed to the task itself. Here is an example of
spawning tasks in a loop with different parameters:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;thread&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;future&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-4"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;chrono&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-5"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;vector&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-7"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-11"&gt;&lt;/a&gt;        &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;emplace_back&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-12"&gt;&lt;/a&gt;            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-13"&gt;&lt;/a&gt;            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-14"&gt;&lt;/a&gt;        &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-15"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-17"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Start querying"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-18"&gt;&lt;/a&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-19"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nl"&gt;future&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-20"&gt;&lt;/a&gt;      &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-21"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-22"&gt;&lt;/a&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-23"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_48e4da1f976843769b46da58e02a0259-24"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Pretty practical :) All The created &lt;code&gt;std::future&amp;lt;size_t&amp;gt;&lt;/code&gt; are stored in
a &lt;code&gt;std::vector&lt;/code&gt; and then are all queried for their result.&lt;/p&gt;
&lt;p&gt;Overall, I think &lt;code&gt;std::future&lt;/code&gt; and &lt;code&gt;std::async&lt;/code&gt; are great tool that
can simplify your asynchronous code a lot. They allow you to make pretty
advanced stuff while keeping the complexity of the code to a minimum.&lt;/p&gt;
&lt;p&gt;I hope this long-due post is going to be interesting to some of you :)
The code for this post is available &lt;a class="reference external" href="https://github.com/wichtounet/articles/tree/master/src/threads/part5"&gt;on Github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I do not yet know if there will be a next installment in the series. I've
covered pretty much everything that is available in C++11 for concurrency. I may
cover the parallel algorithms of C++17 in a following post. If you have any
suggestion for the next post, don't hesitate to post a comment or contact me
directly by email.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>C++11 Concurrency Tutorial</category><category>Concurrency</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2017/09/cpp11-concurrency-tutorial-futures.html</guid><pubDate>Tue, 12 Sep 2017 13:05:08 GMT</pubDate></item><item><title>Simplify your type traits with C++14 variable templates</title><link>http://baptiste-wicht.com/posts/2017/08/simplify-your-type-traits-with-c%2B%2B14-variable-templates.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Often if you write templated code, you have to write and use a lot of different
traits. In this article, I'll focus on the traits that are representing values,
typically a boolean value. For instance, std::is_const, std::is_same or
std::is_reference are type traits provided by the STL. They are giving you some
information at compile time for a certain type. If you need to write a type
traits, let's say is_float, here is how you would maybe do it in C++11:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_3f030eff94664381ae032cd2f651991e-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_3f030eff94664381ae032cd2f651991e-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_3f030eff94664381ae032cd2f651991e-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_3f030eff94664381ae032cd2f651991e-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;or a bit nicer with a template type alias and std::integral constant:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_c7769c1aa9ea4815a295f25344e34978-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_c7769c1aa9ea4815a295f25344e34978-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;integral_constant&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;or since is_same is itself a type traits, you can also directly alias it:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_1594b38067274366a7af6c3052811351-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1594b38067274366a7af6c3052811351-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This makes for some very nice syntax, but we still have a type rather than a value.&lt;/p&gt;
&lt;p&gt;Note that in some cases, you cannot use the using technique since it cannot be
specialized and you often need specialization to write some more advanced
traits.&lt;/p&gt;
&lt;p&gt;And then you would use your traits to do something specific based on that
information. For instance with a very basic example:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_0320788fc0594cd48b60e53702a51f7f-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_0320788fc0594cd48b60e53702a51f7f-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_0320788fc0594cd48b60e53702a51f7f-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is_float&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_0320788fc0594cd48b60e53702a51f7f-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm a float"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_0320788fc0594cd48b60e53702a51f7f-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_0320788fc0594cd48b60e53702a51f7f-6"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm not a float"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_0320788fc0594cd48b60e53702a51f7f-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_0320788fc0594cd48b60e53702a51f7f-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Really nothing fancy here, but that will be enough as examples.&lt;/p&gt;
&lt;p&gt;Even though all this works pretty, it can be made better on two points. First,
every time you use a traits, you need to use the value member (via ::value).
Secondly, every time you declare a new traits, you have to declare a new type or
a type alias. But all you want is a boolean value.&lt;/p&gt;
&lt;p&gt;C++14 introduced a new feature, variable templates. As their name indicates,
they are variables, parametrized with a type. This allows us to write type
traits without using a type alias or struct, meaning we have a real value
instead of a type. If we rewrite our is_float traits with variable templates, we
have the following:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_9fd359f4a2294a15860b3410d31b6594-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_9fd359f4a2294a15860b3410d31b6594-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I think it's much nicer, the intent is clearly stated and there is no
unnecessary code. Moreover, it's also nicer to use:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_88f499f874af48dda9c5ca81f20e9a25-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_88f499f874af48dda9c5ca81f20e9a25-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_88f499f874af48dda9c5ca81f20e9a25-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is_float&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_88f499f874af48dda9c5ca81f20e9a25-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm a float"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_88f499f874af48dda9c5ca81f20e9a25-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_88f499f874af48dda9c5ca81f20e9a25-6"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm not a float"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_88f499f874af48dda9c5ca81f20e9a25-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_88f499f874af48dda9c5ca81f20e9a25-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;No more ::value everywhere :) I think it's really cool.&lt;/p&gt;
&lt;p&gt;Note that, unlike type alias template, they can be specialized, either fully or
partially, so no more limitation on that side.&lt;/p&gt;
&lt;p&gt;Interestingly, variable templates are used in C++17 to provide helpers for each
type traits with values. For instance, std::is_same will have a std::is_same_v
helper that is a variable template. With that, we can simplify our traits a bit
more:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_8aa050c4de8044a486486280b891d380-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_8aa050c4de8044a486486280b891d380-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same_v&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Personally, I replaced all the type traits inside ETL using variable templates.
If you don't want to do it, you can also introduce helpers like in the C++17 STL
and start using the wrappers when you see fit so that you don't break any code.&lt;/p&gt;
&lt;p&gt;If you want to use this feature, you need a C++14 compiler, such as any version
from GCC5 family or clang 3.6. Although I haven't tested, it should also work on
Microsoft VS2015 Update 2.&lt;/p&gt;
&lt;p&gt;Unfortunately there is a bug in both clang (fixed in clang 3.7) and GCC (fixed
in GCC 6 only) that you may encounter if you start using variable templates in
template classes or variable templates used in another variable templates. If
you plan to use variable templates inside a template, such as something like
this:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_57e7093394594702a8a308eae380333d-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_57e7093394594702a8a308eae380333d-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;outer_traits&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_57e7093394594702a8a308eae380333d-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_57e7093394594702a8a308eae380333d-4"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;sub_traits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_57e7093394594702a8a308eae380333d-5"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_57e7093394594702a8a308eae380333d-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_57e7093394594702a8a308eae380333d-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_57e7093394594702a8a308eae380333d-8"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;outer_helper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outer_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;sub_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_57e7093394594702a8a308eae380333d-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_57e7093394594702a8a308eae380333d-10"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_57e7093394594702a8a308eae380333d-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;outer_helper&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_57e7093394594702a8a308eae380333d-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_57e7093394594702a8a308eae380333d-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_57e7093394594702a8a308eae380333d-14"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;You will encounter a not-helpful at all error message with GCC5 family, such as:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_c8ab013e26fd4808ae12ac57eb7ffcd8-1"&gt;&lt;/a&gt;test.cpp: In instantiation of ‘constexpr const bool outer_helper&amp;lt;float, float&amp;gt;’:
&lt;a name="rest_code_c8ab013e26fd4808ae12ac57eb7ffcd8-2"&gt;&lt;/a&gt;test.cpp:14:22:   required from here
&lt;a name="rest_code_c8ab013e26fd4808ae12ac57eb7ffcd8-3"&gt;&lt;/a&gt;test.cpp:11:20: error: ‘template&amp;lt;class X&amp;gt; constexpr const bool outer_traits&amp;lt;float&amp;gt;::sub_traits&amp;lt;X&amp;gt;’ is not a function template
&lt;a name="rest_code_c8ab013e26fd4808ae12ac57eb7ffcd8-4"&gt;&lt;/a&gt;     constexpr bool outer_helper = outer_traits&amp;lt;T&amp;gt;::template sub_trait
&lt;a name="rest_code_c8ab013e26fd4808ae12ac57eb7ffcd8-5"&gt;&lt;/a&gt;                    ^
&lt;a name="rest_code_c8ab013e26fd4808ae12ac57eb7ffcd8-6"&gt;&lt;/a&gt;test.cpp:11:20: error: ‘sub_traits&amp;lt;X&amp;gt;’ is not a member of ‘outer_traits&amp;lt;float&amp;gt;’
&lt;/pre&gt;&lt;p&gt;It comes from a bug in the handling of variable templates as dependent names. If
you don't come in this cases, you can use GCC5 family directly, otherwise,
you'll have to use GCC6 family only.&lt;/p&gt;
&lt;p&gt;I hope this can help some of you to improve your type traits or at least to
discover the power of the new variable templates. Personally, I've rewritten all
the traits from the ETL library using this new feature and I'm pretty satisfied
with the result. Of course, that means that the compiler support was reduced,
but since I don't have many users, it's not a real issue.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>Compilers</category><category>etl</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/08/simplify-your-type-traits-with-c%2B%2B14-variable-templates.html</guid><pubDate>Tue, 22 Aug 2017 12:45:11 GMT</pubDate></item><item><title>DLL: Blazing Fast Neural Network Library</title><link>http://baptiste-wicht.com/posts/2017/08/dll-blazing-fast-neural-network-library.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;A few weeks ago, I talked about all
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html"&gt;the new features of my Deep Learning Library (DLL)&lt;/a&gt;
project. I've mentioned that, on several experiments, DLL was always
significantly faster than some popular deep learning frameworks such as
TensorFlow. I'll now go into more details into this comparison and provide all
the results. So far, the paper we wrote about these results has not been
published, so I'll not provide the paper directly yet.&lt;/p&gt;
&lt;p&gt;For those that may not know, DLL is the project I've been developing to support
my Ph.D. thesis. This is a neural network framework  that supports
Fully-Connected Neural Network (FCNN), Convolutional Neural Network (CNN),
Restricted Boltzmann Machine (RBM), Deep Belief Network (DBN), Convolutional RBM
(CRBM) and Convolutional DBN (CDBN). It also supports a large variety of options
such as Dropout, Batch Normalization and Adaptive Learning Rates. You can read
read the
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html"&gt;previous post&lt;/a&gt;
if you want more information about the new features of the framework. And, as those of
you that read my blog frequently may know, I'm a bit obsessed with performance
optimization, so I've spent a considerable amount of time optimizing
the performance of neural network training, on CPU. Since, at the beginning of my
thesis, I had no access to GPU for training, I've focused on CPU. Although there
is now support for GPU, the gains are not yet important enough.&lt;/p&gt;
&lt;div class="section" id="evaluation"&gt;
&lt;h2&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;To see how fast, or not, the library was, it was compared against five popular
machine learning libraries:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Caffe, installed from sources&lt;/li&gt;
&lt;li&gt;TensorFlow 1.0, from pip&lt;/li&gt;
&lt;li&gt;Keras 2.0, from pip&lt;/li&gt;
&lt;li&gt;Torch, installed from sources&lt;/li&gt;
&lt;li&gt;DeepLearning4J 0.7, from Maven&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I've run four different experiments with all these frameworks and compared the
efficiency of each of them for training the same neural networks with the same
options. In each case, the training or testing error have also been compared to
ensure that each framework is doing roughly the same. I wont present here the
details, but in each experiment DLL showed around the same accuracies as the
other frameworks. I will only focus on the speed results in this article.&lt;/p&gt;
&lt;p&gt;Each experiment is done once with only CPU and once with a GPU. For DLL, I only
report the CPU time in both modes, since it's more stable and more optimized.&lt;/p&gt;
&lt;p&gt;The code for the evaluation is available online on the
&lt;a class="reference external" href="https://github.com/wichtounet/frameworks"&gt;Github repository of the frameworks project&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="mnist-fully-connected-neural-network"&gt;
&lt;h2&gt;MNIST: Fully Connected Neural Network&lt;/h2&gt;
&lt;p&gt;The first experiment is performed on The MNIST data set. It consists of 60'000
grayscale images of size 28x28. The goal is to classify each image of a digit
from 0 to 9. To solve this task, I trained a very small fully-connected neural
network with 500 hidden units in the first layer, 250 in the second and 10 final
hidden units (or output units) for classification. The first two layers are
using the logistic sigmoid activation function and the last layer is using the
softmax activation function. The network is trained for 50 epochs with a
categorical cross entropy loss, with mini-batches of 100 images. Here are
results of this experiment:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Training time performance for the different frameworks on the Fully-Connected Neural Network experiment, on MNIST." src="http://baptiste-wicht.com/images/dll_fcnn.png"&gt;
&lt;p class="caption"&gt;Training time performance for the different frameworks on the Fully-Connected
Neural Network experiment, on MNIST. All the times are in seconds.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In DLL mode, the DLL framework is the clear winner here! It's about 35% faster
than TensorFlow and Keras which are coming at the second place. DLL is more than
four times slower than DLL and the last two frameworks (Caffe and
DeepLearning4J) are five times slower than DLL! Once we add a GPU to the system,
the results are very different. Caffe is now the fastest framework, three times
faster than DLL. DLL is less than two times slower than Keras and TensorFlow.
Interestingly, DLL is still faster than Torch and DeepLearning4J.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="mnist-convolutional-neural-network"&gt;
&lt;h2&gt;MNIST: Convolutional Neural Network&lt;/h2&gt;
&lt;p&gt;Although a Fully-Connected Neural Network is an interesting tool, the trend now
is to use Convolutional Neural Network which have proved very efficient at
solving a lot of problems. The second experiment is also using the same data
set. Again, it's a rather small network. The first layer is a convolutional
layer with 8 5x5 kernels, followed by max pooling layer with 2x2 kernel. They
are followed by one more convolutional layers with 8 5x5 kernels and a 2x2 max
pooling layer. These first four layers are followed by two fully-connected
layers, the first with 150 hidden units and the last one with 10 output units.
The activation functions are the same as for the first network, as is the
training procedure. This takes significantly longer to train than the first
network because of the higher complexity of the convolutional layers compared to
the fully-connected layers even though they have much less weights. The results
are present in the next figure:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Training time performance for the different frameworks on the Convolutional Neural Network experiment, on MNIST." src="http://baptiste-wicht.com/images/dll_cnn.png"&gt;
&lt;p class="caption"&gt;Training time performance for the different frameworks on the Convolutional
Neural Network experiment, on MNIST. All the times are in seconds.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Again, on CPU, DLL is the clear winner, by a lot! It's already 3.6 times faster
than the second frameworks Keras and TensorFlow, more than four times faster
than Caffe and Torch and 8 times faster than DeepLearning4J that is proving very
slow on this experiment. Once a GPU is added, Keras and TensorFlow are about
twice faster than DLL. However, DLL is still faster than the other frameworks
even though they are taking advantage of the GPU.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cifar-10"&gt;
&lt;h2&gt;CIFAR-10&lt;/h2&gt;
&lt;p&gt;The second data set that is tested is the CIFAR-10 data set. It's an object
recognition with 10 classes for classification. The training set is composed of
50'000 colour images for 32x32 pixels. The network that is used for this data
set is similar in architecture than the first network, but has more parameters.
The first convolutional layer now has 12 5x5 kernels and the second
convolutional layer has 24 3x3 kernels. The pooling layers are the same. The
first fully-connected has 64 hidden units and the last one has 10 output units.
The last layer again use a softmax activation function while the other layers
are using Rectifier Linear Units (ReLU). The training is done in the same manner
as for the two first networks. Unfortunately, it was not possible to train
DeepLearning4J on this data set, even though there is official support for this
data set. Since I've had no answer to my question regarding this issue, the
results are simply removed from this experiment. It may not seem so but it's
considerably longer to train this network because of the larger number of input
channels and larger number of convolutional kernels in each layer. Let's get to
the results now:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Training time performance for the different frameworks on the Convolutional Neural Network experiment, on CIFAR-10." src="http://baptiste-wicht.com/images/dll_cifar10.png"&gt;
&lt;p class="caption"&gt;Training time performance for the different frameworks on the Convolutional
Neural Network experiment, on CIFAR-10. All the times are in seconds.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;DLL is still the fastest on CPU, but the margin is less than before. It's about
40% faster than TensorFlow and Keras, twice faster than Torch and 2.6 times
faster than Caffe. Once a GPU is added, DLL is about as fast as Torch but slower
than the other three frameworks. TensorFlow and Keras are about four times
faster than DLL while Caffe is about twice faster than DLL. We can see that
with this larger network, the GPU becomes more interesting and that there is
a smaller margin for improvements compared to the other frameworks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="imagenet"&gt;
&lt;h2&gt;ImageNet&lt;/h2&gt;
&lt;p&gt;The last experiment is made on the ImageNet data set. I used the ILSVRC 2012
subset, that consists "only" of about 1.2 million images for training. I've
resized all the images to 256x256 pixels, this makes for 250 times more colour
values than a MNIST image. This dimension and the number of images makes it
impractical to keep the dataset in memory. The images must be loaded in batch
from the disk. No random cropping or mirroring was performed. The network is
much larger to solve this task. The network starts with 5 pairs of convolutional
layers and max pooling layers. The convolutional layers have 3x3 kernels, 16 for
the first two layers and 32 for the three following one. The five max pooling
layers use 2x2 kernels. Each convolutional layer uses zero-padding so that their
output features are the same dimensions as the input. They are followed by two
fully-connected layer. The first one with 2048 hidden units and the last one
with 1000 output units (one for each class). Except for the last layer, using
softmax, the layers all uses ReLU. The network is trained with mini-batches of
128 images (except for DeepLearning4J and Torch, which can only use 64 images on
the amount of RAM available on my machine). To ease the comparison, I report the
time necessary to train one batch of data (or two for DeepLearning4J and Torch).
The results, presented in logarithmic scale because of DeepLearning4J disastrous
results, are as follows:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Training time performance for the different frameworks on the Convolutional Neural Network experiment, on ImageNet." src="http://baptiste-wicht.com/images/dll_imagenet.png"&gt;
&lt;p class="caption"&gt;Training time performance for the different frameworks on the Convolutional
Neural Network experiment, on ImageNet. The times are the time necessary to
train a batch of 128 images. All the times are in milliseconds.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For this final experiment, DLL is again significantly faster than all the other
frameworks. It's about 40% faster than Keras, twice faster than TensorFlow and
Caffe and more than three times faster than Torch. Although 40% may seem not
that much, don't forget that this kind of training may take days, so it can save
you a lot of time. All the frameworks are much faster than DeepLearning4J. Based
on several posts on the internet, I suspect that this comes from the model of
GPU I have been used (GTX 960), but all the other frameworks seem to handle this
card pretty well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I hope this is not too much of a bragging post :P We can see that my efforts to
make the code as fast as possible have paid :) As was shown in the experiments,
my DLL framework is always the fastest framework when the neural network is
trained on CPU. I'm quite pleased with the results since I've done a lot of work
to optimize the speed as much as possible and since I'm competing with
well-known libraries that have been developed by several persons.  Moreover, the
accuracies of the trained networks is similar to that of the networks trained
with the other frameworks. Even when the other frameworks are using GPU, the
library still remains competitive, although never the fastest.&lt;/p&gt;
&lt;p&gt;In the next step (I've no idea when I'll have the time though), I will want to
focus on GPU speed. This will mostly come from a better support of the GPU in
the ETL library on which DLL is based. I have many ideas to improve it a lot,
but it will take me a lot of time.&lt;/p&gt;
&lt;p&gt;If you want more information on the DLL library, you can have a look at
&lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;its Github repository&lt;/a&gt; and especially at
&lt;a class="reference external" href="https://github.com/wichtounet/dll/tree/master/examples/src"&gt;the few examples&lt;/a&gt;.
You can also have a look at &lt;a class="reference external" href="https://baptiste-wicht.com/categories/dll.html"&gt;my posts about DLL&lt;/a&gt;.
Finally, don't hesitate to comment or contact me through Github issues if you
have comments or problems with this post, the library or anything ;)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>dll</category><category>etl</category><category>Machine Learning</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/08/dll-blazing-fast-neural-network-library.html</guid><pubDate>Fri, 11 Aug 2017 09:09:14 GMT</pubDate></item><item><title>Compiler benchmark GCC and Clang on C++ library (ETL)</title><link>http://baptiste-wicht.com/posts/2017/08/compiler-benchmark-gcc-clang-cpp-library-etl.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;It's been a while since I've done a benchmark of different compilers on C++
code. Since I've recently
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/08/expression-templates-library-etl-11.html"&gt;released the version 1.1 of my ETL project&lt;/a&gt;
(an optimized matrix/vector computation library with expression templates), I've
decided to use it as the base of my benchmark. It's a C++14 library with a lot
of templates. I'm going to compile the full test suite (124 test cases). This is
done directly on the last release (1.1) code. I'm going to compile once in debug
mode and once in release_debug (release plus debug symbols and assertions) and
record the times for each compiler. The tests were compiled with support for
every option in ETL to account to maximal compilation time. Each compilation was
made using four threads (make -j4). I'm also going to test a few of the
benchmarks to see the difference in runtime performance between the code
generated by each compiler. The benchmark will be compiled in release mode and
its compilation time recorded as well.&lt;/p&gt;
&lt;p&gt;I'm going to test the following compilers:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;GCC-4.9.4&lt;/li&gt;
&lt;li&gt;GCC-5.4.0&lt;/li&gt;
&lt;li&gt;GCC-6.3.0&lt;/li&gt;
&lt;li&gt;GCC-7.1.0&lt;/li&gt;
&lt;li&gt;clang-3.9.1&lt;/li&gt;
&lt;li&gt;clang-4.0.1&lt;/li&gt;
&lt;li&gt;zapcc-1.0 (commercial, based on clang-5.0 trunk)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All have been installed directly using Portage (Gentoo package manager) except
for clang-4.0.1 that has been installed from sources and zapcc since it does not
have a Gentoo package. Since clang package on Gentoo does not support
multislotting, I had to install one version from source and the other from the
package manager. This is also the reason I'm testing less versions of clang,
simply less practical.&lt;/p&gt;
&lt;p&gt;For the purpose of these tests, the exact same options have been used throughout
all the compilers. Normally, I use different options for clang than for GCC
(mainly more aggressive vectorization options on clang). This may not lead to
the best performance for each compiler, but allows for comparison between the
results with defaults optimization level. Here are the main options used:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;In debug mode: -g&lt;/li&gt;
&lt;li&gt;In release_debug mode: -g -O2&lt;/li&gt;
&lt;li&gt;In release mode: -g -O3 -DNDEBUG -fomit-frame-pointer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In each case, a lot of warnings are enabled and the ETL options are the same.&lt;/p&gt;
&lt;p&gt;All the results have been gathered on a Gentoo machine running on Intel Core
i7-2600 (Sandy Bridge...) @3.4GHz with 4 cores and 8 threads, 12Go of RAM and
a SSD. I do my best to isolate as much as possible the benchmark from
perturbations and that my benchmark code is quite sound, it may well be that
some results are not totally accurate. Moreover, some of the benchmarks are
using multithreading, which may add some noise and unpredictability. When I was
not sure about the results, I ran the benchmarks several time to confirm them
and overall I'm confident of the results.&lt;/p&gt;
&lt;div class="section" id="compilation-time"&gt;
&lt;h2&gt;Compilation Time&lt;/h2&gt;
&lt;p&gt;Let's start with the results of the performance of the compilers themselves:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="31%"&gt;
&lt;col width="15%"&gt;
&lt;col width="31%"&gt;
&lt;col width="23%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;Debug&lt;/th&gt;
&lt;th class="head"&gt;Release_Debug&lt;/th&gt;
&lt;th class="head"&gt;Benchmark&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;402s&lt;/td&gt;
&lt;td&gt;616s&lt;/td&gt;
&lt;td&gt;100s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;403s&lt;/td&gt;
&lt;td&gt;642s&lt;/td&gt;
&lt;td&gt;95s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;399s&lt;/td&gt;
&lt;td&gt;683s&lt;/td&gt;
&lt;td&gt;102s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;371s&lt;/td&gt;
&lt;td&gt;650s&lt;/td&gt;
&lt;td&gt;105s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;380s&lt;/td&gt;
&lt;td&gt;807s&lt;/td&gt;
&lt;td&gt;106s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;260s&lt;/td&gt;
&lt;td&gt;718s&lt;/td&gt;
&lt;td&gt;92s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;221s&lt;/td&gt;
&lt;td&gt;649s&lt;/td&gt;
&lt;td&gt;108s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note: For Release_Debug and Benchmark, I only use three threads with zapcc,
because 12Go of RAM is not enough memory for four threads.&lt;/p&gt;
&lt;p&gt;There are some very significant differences between the different compilers.
Overall, clang-4.0.1 is by far the fastest free compiler for Debug mode. When
the tests are compiled with optimizations however, clang is falling behind.
It's quite impressive how clang-4.0.1 manages to be so much faster than
clang-3.9.1 both in debug mode and release mode. Really great work by the clang
team here! With these optimizations, clang-4.0.1 is almost on par with gcc-7.1
in release mode.  For GCC, it seems that the cost of optimization has been going
up quite significantly. However, GCC 7.1 seems to have made optimization faster
and standard compilation much faster as well. If we take into account zapcc,
it's the fastest compiler on debug mode, but it's slower than several gcc
versions on release mode.&lt;/p&gt;
&lt;p&gt;Overall, I'm quite impressed by the performance of clang-4.0.1 which seems
really fast! I'll definitely make more tests with this new version of the
compiler in the near future. It's also good to see that g++-7.1 also did make
the build faster than gcc-6.3. However, the fastest gcc version for optimization
is still gcc-4.9.4 which is already an old branch with low C++ standard support.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="runtime-performance"&gt;
&lt;h2&gt;Runtime Performance&lt;/h2&gt;
&lt;p&gt;Let's now take a look at the quality of the generated code. For some of the
benchmarks, I've included two versions of the algorithm. &lt;em&gt;std&lt;/em&gt; is the most
simple algorithm (the naive one) and &lt;em&gt;vec&lt;/em&gt; is the hand-crafted vectorized and
optimized implementation. All the tests were done on single-precision floating
points.&lt;/p&gt;
&lt;div class="section" id="dot-product"&gt;
&lt;h3&gt;Dot product&lt;/h3&gt;
&lt;p&gt;The first benchmark that is run is to compute the dot product between two
vectors. Let's look first at the naive version:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="13%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="7%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;dot (std)&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;th class="head"&gt;500&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;10000&lt;/th&gt;
&lt;th class="head"&gt;100000&lt;/th&gt;
&lt;th class="head"&gt;1000000&lt;/th&gt;
&lt;th class="head"&gt;2000000&lt;/th&gt;
&lt;th class="head"&gt;3000000&lt;/th&gt;
&lt;th class="head"&gt;4000000&lt;/th&gt;
&lt;th class="head"&gt;5000000&lt;/th&gt;
&lt;th class="head"&gt;10000000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;64.96ns&lt;/td&gt;
&lt;td&gt;97.12ns&lt;/td&gt;
&lt;td&gt;126.07ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;25.91us&lt;/td&gt;
&lt;td&gt;326.49us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.92ms&lt;/td&gt;
&lt;td&gt;2.55ms&lt;/td&gt;
&lt;td&gt;3.22ms&lt;/td&gt;
&lt;td&gt;6.36ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;72.96ns&lt;/td&gt;
&lt;td&gt;101.62ns&lt;/td&gt;
&lt;td&gt;127.89ns&lt;/td&gt;
&lt;td&gt;1.90us&lt;/td&gt;
&lt;td&gt;23.39us&lt;/td&gt;
&lt;td&gt;357.63us&lt;/td&gt;
&lt;td&gt;1.23ms&lt;/td&gt;
&lt;td&gt;1.91ms&lt;/td&gt;
&lt;td&gt;2.57ms&lt;/td&gt;
&lt;td&gt;3.20ms&lt;/td&gt;
&lt;td&gt;6.32ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;73.31ns&lt;/td&gt;
&lt;td&gt;102.88ns&lt;/td&gt;
&lt;td&gt;130.16ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;24.314us&lt;/td&gt;
&lt;td&gt;339.13us&lt;/td&gt;
&lt;td&gt;1.47ms&lt;/td&gt;
&lt;td&gt;2.16ms&lt;/td&gt;
&lt;td&gt;2.95ms&lt;/td&gt;
&lt;td&gt;3.70ms&lt;/td&gt;
&lt;td&gt;6.69ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;70.20ns&lt;/td&gt;
&lt;td&gt;104.09ns&lt;/td&gt;
&lt;td&gt;130.98ns&lt;/td&gt;
&lt;td&gt;1.90us&lt;/td&gt;
&lt;td&gt;23.96us&lt;/td&gt;
&lt;td&gt;281.47us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.93ms&lt;/td&gt;
&lt;td&gt;2.58ms&lt;/td&gt;
&lt;td&gt;3.19ms&lt;/td&gt;
&lt;td&gt;6.33ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;64.69ns&lt;/td&gt;
&lt;td&gt;98.69ns&lt;/td&gt;
&lt;td&gt;128.60ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;23.33us&lt;/td&gt;
&lt;td&gt;272.71us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.91ms&lt;/td&gt;
&lt;td&gt;2.56ms&lt;/td&gt;
&lt;td&gt;3.19ms&lt;/td&gt;
&lt;td&gt;6.37ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;60.31ns&lt;/td&gt;
&lt;td&gt;96.34ns&lt;/td&gt;
&lt;td&gt;128.90ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;22.87us&lt;/td&gt;
&lt;td&gt;270.21us&lt;/td&gt;
&lt;td&gt;1.23ms&lt;/td&gt;
&lt;td&gt;1.91ms&lt;/td&gt;
&lt;td&gt;2.55ms&lt;/td&gt;
&lt;td&gt;3.18ms&lt;/td&gt;
&lt;td&gt;6.35ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;61.14ns&lt;/td&gt;
&lt;td&gt;96.92ns&lt;/td&gt;
&lt;td&gt;125.95ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;23.84us&lt;/td&gt;
&lt;td&gt;285.80us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.92ms&lt;/td&gt;
&lt;td&gt;2.55ms&lt;/td&gt;
&lt;td&gt;3.16ms&lt;/td&gt;
&lt;td&gt;6.34ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The differences are not very significant between the different compilers. The
clang-based compilers seem to be the compilers producing the fastest code.
Interestingly, there seem to have been a big regression in gcc-6.3 for large
containers, but that has been fixed in gcc-7.1.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="13%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="9%"&gt;
&lt;col width="7%"&gt;
&lt;col width="8%"&gt;
&lt;col width="9%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="9%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;dot (vec)&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;th class="head"&gt;500&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;10000&lt;/th&gt;
&lt;th class="head"&gt;100000&lt;/th&gt;
&lt;th class="head"&gt;1000000&lt;/th&gt;
&lt;th class="head"&gt;2000000&lt;/th&gt;
&lt;th class="head"&gt;3000000&lt;/th&gt;
&lt;th class="head"&gt;4000000&lt;/th&gt;
&lt;th class="head"&gt;5000000&lt;/th&gt;
&lt;th class="head"&gt;10000000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;48.34ns&lt;/td&gt;
&lt;td&gt;80.53ns&lt;/td&gt;
&lt;td&gt;114.97ns&lt;/td&gt;
&lt;td&gt;1.72us&lt;/td&gt;
&lt;td&gt;22.79us&lt;/td&gt;
&lt;td&gt;354.20us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.89ms&lt;/td&gt;
&lt;td&gt;2.52ms&lt;/td&gt;
&lt;td&gt;3.19ms&lt;/td&gt;
&lt;td&gt;6.55ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;47.16ns&lt;/td&gt;
&lt;td&gt;77.70ns&lt;/td&gt;
&lt;td&gt;113.66ns&lt;/td&gt;
&lt;td&gt;1.72us&lt;/td&gt;
&lt;td&gt;22.71us&lt;/td&gt;
&lt;td&gt;363.86us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.89ms&lt;/td&gt;
&lt;td&gt;2.52ms&lt;/td&gt;
&lt;td&gt;3.19ms&lt;/td&gt;
&lt;td&gt;6.56ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;46.39ns&lt;/td&gt;
&lt;td&gt;77.67ns&lt;/td&gt;
&lt;td&gt;116.28ns&lt;/td&gt;
&lt;td&gt;1.74us&lt;/td&gt;
&lt;td&gt;23.39us&lt;/td&gt;
&lt;td&gt;452.44us&lt;/td&gt;
&lt;td&gt;1.45ms&lt;/td&gt;
&lt;td&gt;2.26ms&lt;/td&gt;
&lt;td&gt;2.87ms&lt;/td&gt;
&lt;td&gt;3.49ms&lt;/td&gt;
&lt;td&gt;7.52ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;49.70ns&lt;/td&gt;
&lt;td&gt;80.40ns&lt;/td&gt;
&lt;td&gt;115.77ns&lt;/td&gt;
&lt;td&gt;1.71us&lt;/td&gt;
&lt;td&gt;22.46us&lt;/td&gt;
&lt;td&gt;355.16us&lt;/td&gt;
&lt;td&gt;1.21ms&lt;/td&gt;
&lt;td&gt;1.85ms&lt;/td&gt;
&lt;td&gt;2.49ms&lt;/td&gt;
&lt;td&gt;3.14ms&lt;/td&gt;
&lt;td&gt;6.47ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;46.13ns&lt;/td&gt;
&lt;td&gt;78.01ns&lt;/td&gt;
&lt;td&gt;114.70ns&lt;/td&gt;
&lt;td&gt;1.66us&lt;/td&gt;
&lt;td&gt;22.82us&lt;/td&gt;
&lt;td&gt;359.42us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.88ms&lt;/td&gt;
&lt;td&gt;2.53ms&lt;/td&gt;
&lt;td&gt;3.16ms&lt;/td&gt;
&lt;td&gt;6.50ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;45.59ns&lt;/td&gt;
&lt;td&gt;74.90ns&lt;/td&gt;
&lt;td&gt;111.29ns&lt;/td&gt;
&lt;td&gt;1.57us&lt;/td&gt;
&lt;td&gt;22.47us&lt;/td&gt;
&lt;td&gt;351.31us&lt;/td&gt;
&lt;td&gt;1.23ms&lt;/td&gt;
&lt;td&gt;1.85ms&lt;/td&gt;
&lt;td&gt;2.49ms&lt;/td&gt;
&lt;td&gt;3.12ms&lt;/td&gt;
&lt;td&gt;6.45ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;45.11ns&lt;/td&gt;
&lt;td&gt;75.04ns&lt;/td&gt;
&lt;td&gt;111.28ns&lt;/td&gt;
&lt;td&gt;1.59us&lt;/td&gt;
&lt;td&gt;22.46us&lt;/td&gt;
&lt;td&gt;357.32us&lt;/td&gt;
&lt;td&gt;1.25ms&lt;/td&gt;
&lt;td&gt;1.89ms&lt;/td&gt;
&lt;td&gt;2.53ms&lt;/td&gt;
&lt;td&gt;3.15ms&lt;/td&gt;
&lt;td&gt;6.47ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If we look at the optimized version, the differences are even slower. Again, the
clang-based compilers are producing the fastest executables, but are closely
followed by gcc, except for gcc-6.3 in which we can still see the same
regression as before.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="logistic-sigmoid"&gt;
&lt;h3&gt;Logistic Sigmoid&lt;/h3&gt;
&lt;p&gt;The next test is to check the performance of the sigmoid operation. In that
case, the evaluator of the library will try to use parallelization and
vectorization to compute it. Let's see how the different compilers fare:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="22%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="13%"&gt;
&lt;col width="15%"&gt;
&lt;col width="13%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sigmoid&lt;/th&gt;
&lt;th class="head"&gt;10&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;10000&lt;/th&gt;
&lt;th class="head"&gt;100000&lt;/th&gt;
&lt;th class="head"&gt;1000000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;8.16us&lt;/td&gt;
&lt;td&gt;5.23us&lt;/td&gt;
&lt;td&gt;6.33us&lt;/td&gt;
&lt;td&gt;29.56us&lt;/td&gt;
&lt;td&gt;259.72us&lt;/td&gt;
&lt;td&gt;2.78ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;7.07us&lt;/td&gt;
&lt;td&gt;5.08us&lt;/td&gt;
&lt;td&gt;6.39us&lt;/td&gt;
&lt;td&gt;29.44us&lt;/td&gt;
&lt;td&gt;266.27us&lt;/td&gt;
&lt;td&gt;2.96ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;7.13us&lt;/td&gt;
&lt;td&gt;5.32us&lt;/td&gt;
&lt;td&gt;6.45us&lt;/td&gt;
&lt;td&gt;28.99us&lt;/td&gt;
&lt;td&gt;261.81us&lt;/td&gt;
&lt;td&gt;2.86ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;7.03us&lt;/td&gt;
&lt;td&gt;5.09us&lt;/td&gt;
&lt;td&gt;6.24us&lt;/td&gt;
&lt;td&gt;28.61us&lt;/td&gt;
&lt;td&gt;252.78us&lt;/td&gt;
&lt;td&gt;2.71ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;7.30us&lt;/td&gt;
&lt;td&gt;5.25us&lt;/td&gt;
&lt;td&gt;6.57us&lt;/td&gt;
&lt;td&gt;30.24us&lt;/td&gt;
&lt;td&gt;256.75us&lt;/td&gt;
&lt;td&gt;1.99ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;7.47us&lt;/td&gt;
&lt;td&gt;5.14us&lt;/td&gt;
&lt;td&gt;5.77us&lt;/td&gt;
&lt;td&gt;26.03us&lt;/td&gt;
&lt;td&gt;235.87us&lt;/td&gt;
&lt;td&gt;1.81ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;7.51us&lt;/td&gt;
&lt;td&gt;5.26us&lt;/td&gt;
&lt;td&gt;6.48us&lt;/td&gt;
&lt;td&gt;28.86us&lt;/td&gt;
&lt;td&gt;258.31us&lt;/td&gt;
&lt;td&gt;1.95ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Interestingly, we can see that gcc-7.1 is the fastest for small vectors while
clang-4.0 is the best for producing code for larger vectors. However, except for
the biggest vector size, the difference is not really significantly. Apparently,
there is a regression in zapcc (or clang-5.0) since it's slower than clang-4.0
at the same level as clang-3.9.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="y-alpha-x-y-axpy"&gt;
&lt;h3&gt;y = alpha * x + y (axpy)&lt;/h3&gt;
&lt;p&gt;The third benchmark is the well-known axpy (y = alpha * x + y). This is entirely
resolved by expressions templates in the library, no specific algorithm is used.
Let's see the results:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="24%"&gt;
&lt;col width="13%"&gt;
&lt;col width="13%"&gt;
&lt;col width="11%"&gt;
&lt;col width="13%"&gt;
&lt;col width="13%"&gt;
&lt;col width="14%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;saxpy&lt;/th&gt;
&lt;th class="head"&gt;10&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;10000&lt;/th&gt;
&lt;th class="head"&gt;100000&lt;/th&gt;
&lt;th class="head"&gt;1000000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;38.1ns&lt;/td&gt;
&lt;td&gt;61.6ns&lt;/td&gt;
&lt;td&gt;374ns&lt;/td&gt;
&lt;td&gt;3.65us&lt;/td&gt;
&lt;td&gt;40.8us&lt;/td&gt;
&lt;td&gt;518us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;35.0ns&lt;/td&gt;
&lt;td&gt;58.1ns&lt;/td&gt;
&lt;td&gt;383ns&lt;/td&gt;
&lt;td&gt;3.87us&lt;/td&gt;
&lt;td&gt;43.2us&lt;/td&gt;
&lt;td&gt;479us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;34.3ns&lt;/td&gt;
&lt;td&gt;59.4ns&lt;/td&gt;
&lt;td&gt;371ns&lt;/td&gt;
&lt;td&gt;3.57us&lt;/td&gt;
&lt;td&gt;40.4us&lt;/td&gt;
&lt;td&gt;452us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;34.8ns&lt;/td&gt;
&lt;td&gt;59.7ns&lt;/td&gt;
&lt;td&gt;399ns&lt;/td&gt;
&lt;td&gt;3.78us&lt;/td&gt;
&lt;td&gt;43.1us&lt;/td&gt;
&lt;td&gt;547us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;32.3ns&lt;/td&gt;
&lt;td&gt;53.8ns&lt;/td&gt;
&lt;td&gt;297ns&lt;/td&gt;
&lt;td&gt;3.21us&lt;/td&gt;
&lt;td&gt;38.3us&lt;/td&gt;
&lt;td&gt;466us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;32.4ns&lt;/td&gt;
&lt;td&gt;59.8ns&lt;/td&gt;
&lt;td&gt;296ns&lt;/td&gt;
&lt;td&gt;3.31us&lt;/td&gt;
&lt;td&gt;38.2us&lt;/td&gt;
&lt;td&gt;475us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;32.0ns&lt;/td&gt;
&lt;td&gt;54.0ns&lt;/td&gt;
&lt;td&gt;333ns&lt;/td&gt;
&lt;td&gt;3.32us&lt;/td&gt;
&lt;td&gt;38.7us&lt;/td&gt;
&lt;td&gt;447us&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Even on the biggest vector, this is a very fast operation, once vectorized and
parallelized. At this speed, some of the differences observed may not be highly
significant. Again clang-based versions are the fastest versions on this code,
but by a small margin.  There also seems to be a slight regression in gcc-7.1,
but again quite small.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="matrix-matrix-multiplication-gemm"&gt;
&lt;h3&gt;Matrix Matrix multiplication (GEMM)&lt;/h3&gt;
&lt;p&gt;The next benchmark is testing the performance of a Matrix-Matrix Multiplication,
an operation known as GEMM in the BLAS nomenclature. In that case, we test both
the naive and the optimized vectorized implementation. To save some horizontal
space, I've split the tables in two.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="23%"&gt;
&lt;col width="12%"&gt;
&lt;col width="14%"&gt;
&lt;col width="15%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sgemm (std)&lt;/th&gt;
&lt;th class="head"&gt;10&lt;/th&gt;
&lt;th class="head"&gt;20&lt;/th&gt;
&lt;th class="head"&gt;40&lt;/th&gt;
&lt;th class="head"&gt;60&lt;/th&gt;
&lt;th class="head"&gt;80&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;7.04us&lt;/td&gt;
&lt;td&gt;50.15us&lt;/td&gt;
&lt;td&gt;356.42us&lt;/td&gt;
&lt;td&gt;1.18ms&lt;/td&gt;
&lt;td&gt;3.41ms&lt;/td&gt;
&lt;td&gt;5.56ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;8.14us&lt;/td&gt;
&lt;td&gt;74.77us&lt;/td&gt;
&lt;td&gt;513.64us&lt;/td&gt;
&lt;td&gt;1.72ms&lt;/td&gt;
&lt;td&gt;4.05ms&lt;/td&gt;
&lt;td&gt;7.92ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;8.03us&lt;/td&gt;
&lt;td&gt;64.78us&lt;/td&gt;
&lt;td&gt;504.41us&lt;/td&gt;
&lt;td&gt;1.69ms&lt;/td&gt;
&lt;td&gt;4.02ms&lt;/td&gt;
&lt;td&gt;7.87ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;7.95us&lt;/td&gt;
&lt;td&gt;65.00us&lt;/td&gt;
&lt;td&gt;508.84us&lt;/td&gt;
&lt;td&gt;1.69ms&lt;/td&gt;
&lt;td&gt;4.02ms&lt;/td&gt;
&lt;td&gt;7.84ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;3.58us&lt;/td&gt;
&lt;td&gt;28.59us&lt;/td&gt;
&lt;td&gt;222.36us&lt;/td&gt;
&lt;td&gt;0.73ms&lt;/td&gt;
&lt;td&gt;1.77us&lt;/td&gt;
&lt;td&gt;3.41ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;4.00us&lt;/td&gt;
&lt;td&gt;25.47us&lt;/td&gt;
&lt;td&gt;190.56us&lt;/td&gt;
&lt;td&gt;0.61ms&lt;/td&gt;
&lt;td&gt;1.45us&lt;/td&gt;
&lt;td&gt;2.80ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;4.00us&lt;/td&gt;
&lt;td&gt;25.38us&lt;/td&gt;
&lt;td&gt;189.98us&lt;/td&gt;
&lt;td&gt;0.60ms&lt;/td&gt;
&lt;td&gt;1.43us&lt;/td&gt;
&lt;td&gt;2.81ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="15%"&gt;
&lt;col width="9%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;col width="7%"&gt;
&lt;col width="7%"&gt;
&lt;col width="7%"&gt;
&lt;col width="7%"&gt;
&lt;col width="7%"&gt;
&lt;col width="8%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sgemm (std)&lt;/th&gt;
&lt;th class="head"&gt;200&lt;/th&gt;
&lt;th class="head"&gt;300&lt;/th&gt;
&lt;th class="head"&gt;400&lt;/th&gt;
&lt;th class="head"&gt;500&lt;/th&gt;
&lt;th class="head"&gt;600&lt;/th&gt;
&lt;th class="head"&gt;700&lt;/th&gt;
&lt;th class="head"&gt;800&lt;/th&gt;
&lt;th class="head"&gt;900&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;1200&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;44.16ms&lt;/td&gt;
&lt;td&gt;148.88ms&lt;/td&gt;
&lt;td&gt;455.81ms&lt;/td&gt;
&lt;td&gt;687.96ms&lt;/td&gt;
&lt;td&gt;1.47s&lt;/td&gt;
&lt;td&gt;1.98s&lt;/td&gt;
&lt;td&gt;2.81s&lt;/td&gt;
&lt;td&gt;4.00s&lt;/td&gt;
&lt;td&gt;5.91s&lt;/td&gt;
&lt;td&gt;9.52s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;63.17ms&lt;/td&gt;
&lt;td&gt;213.01ms&lt;/td&gt;
&lt;td&gt;504.83ms&lt;/td&gt;
&lt;td&gt;984.90ms&lt;/td&gt;
&lt;td&gt;1.70s&lt;/td&gt;
&lt;td&gt;2.70s&lt;/td&gt;
&lt;td&gt;4.03s&lt;/td&gt;
&lt;td&gt;5.74s&lt;/td&gt;
&lt;td&gt;7.87s&lt;/td&gt;
&lt;td&gt;14.905&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;64.04ms&lt;/td&gt;
&lt;td&gt;212.12ms&lt;/td&gt;
&lt;td&gt;502.95ms&lt;/td&gt;
&lt;td&gt;981.74ms&lt;/td&gt;
&lt;td&gt;1.69s&lt;/td&gt;
&lt;td&gt;2.69s&lt;/td&gt;
&lt;td&gt;4.13s&lt;/td&gt;
&lt;td&gt;5.85s&lt;/td&gt;
&lt;td&gt;8.10s&lt;/td&gt;
&lt;td&gt;14.08s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;62.57ms&lt;/td&gt;
&lt;td&gt;210.72ms&lt;/td&gt;
&lt;td&gt;499.68ms&lt;/td&gt;
&lt;td&gt;974.94ms&lt;/td&gt;
&lt;td&gt;1.68s&lt;/td&gt;
&lt;td&gt;2.67s&lt;/td&gt;
&lt;td&gt;3.99s&lt;/td&gt;
&lt;td&gt;5.68s&lt;/td&gt;
&lt;td&gt;7.85s&lt;/td&gt;
&lt;td&gt;13.49s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;27.48ms&lt;/td&gt;
&lt;td&gt;90.85ms&lt;/td&gt;
&lt;td&gt;219.34ms&lt;/td&gt;
&lt;td&gt;419.53ms&lt;/td&gt;
&lt;td&gt;0.72s&lt;/td&gt;
&lt;td&gt;1.18s&lt;/td&gt;
&lt;td&gt;1.90s&lt;/td&gt;
&lt;td&gt;2.44s&lt;/td&gt;
&lt;td&gt;3.36s&lt;/td&gt;
&lt;td&gt;5.84s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;22.01ms&lt;/td&gt;
&lt;td&gt;73.90ms&lt;/td&gt;
&lt;td&gt;175.02ms&lt;/td&gt;
&lt;td&gt;340.70ms&lt;/td&gt;
&lt;td&gt;0.58s&lt;/td&gt;
&lt;td&gt;0.93s&lt;/td&gt;
&lt;td&gt;1.40s&lt;/td&gt;
&lt;td&gt;1.98s&lt;/td&gt;
&lt;td&gt;2.79s&lt;/td&gt;
&lt;td&gt;4.69s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;22.33ms&lt;/td&gt;
&lt;td&gt;75.80ms&lt;/td&gt;
&lt;td&gt;181.27ms&lt;/td&gt;
&lt;td&gt;359.13ms&lt;/td&gt;
&lt;td&gt;0.63s&lt;/td&gt;
&lt;td&gt;1.02s&lt;/td&gt;
&lt;td&gt;1.52s&lt;/td&gt;
&lt;td&gt;2.24s&lt;/td&gt;
&lt;td&gt;3.21s&lt;/td&gt;
&lt;td&gt;5.62s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This time, the differences between the different compilers are very significant.
The clang compilers are leading the way by a large margin here, with clang-4.0
being the fastest of them (by another nice margin). Indeed, clang-4.0.1 is
producing code that is, on average, about twice faster than the code generated
by the best GCC compiler. Very interestingly as well, we can see a huge
regression starting from GCC-5.4 and that is still here in GCC-7.1. Indeed, the
best GCC version, in the tested versions, is again GCC-4.9.4. Clang is really
doing an excellent job of compiling the GEMM code.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="21%"&gt;
&lt;col width="14%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;col width="14%"&gt;
&lt;col width="14%"&gt;
&lt;col width="13%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sgemm (vec)&lt;/th&gt;
&lt;th class="head"&gt;10&lt;/th&gt;
&lt;th class="head"&gt;20&lt;/th&gt;
&lt;th class="head"&gt;40&lt;/th&gt;
&lt;th class="head"&gt;60&lt;/th&gt;
&lt;th class="head"&gt;80&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;264.27ns&lt;/td&gt;
&lt;td&gt;0.95us&lt;/td&gt;
&lt;td&gt;3.28us&lt;/td&gt;
&lt;td&gt;14.77us&lt;/td&gt;
&lt;td&gt;23.50us&lt;/td&gt;
&lt;td&gt;60.37us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;271.41ns&lt;/td&gt;
&lt;td&gt;0.99us&lt;/td&gt;
&lt;td&gt;3.31us&lt;/td&gt;
&lt;td&gt;14.811us&lt;/td&gt;
&lt;td&gt;24.116us&lt;/td&gt;
&lt;td&gt;61.00us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;279.72ns&lt;/td&gt;
&lt;td&gt;1.02us&lt;/td&gt;
&lt;td&gt;3.27us&lt;/td&gt;
&lt;td&gt;15.39us&lt;/td&gt;
&lt;td&gt;24.29us&lt;/td&gt;
&lt;td&gt;61.99us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;273.74ns&lt;/td&gt;
&lt;td&gt;0.96us&lt;/td&gt;
&lt;td&gt;3.81us&lt;/td&gt;
&lt;td&gt;15.55us&lt;/td&gt;
&lt;td&gt;31.35us&lt;/td&gt;
&lt;td&gt;71.11us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;296.67ns&lt;/td&gt;
&lt;td&gt;1.34us&lt;/td&gt;
&lt;td&gt;4.18us&lt;/td&gt;
&lt;td&gt;19.93us&lt;/td&gt;
&lt;td&gt;33.15us&lt;/td&gt;
&lt;td&gt;82.60us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;322.68ns&lt;/td&gt;
&lt;td&gt;1.38us&lt;/td&gt;
&lt;td&gt;4.17us&lt;/td&gt;
&lt;td&gt;20.19us&lt;/td&gt;
&lt;td&gt;34.17us&lt;/td&gt;
&lt;td&gt;83.64us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;307.49ns&lt;/td&gt;
&lt;td&gt;1.41us&lt;/td&gt;
&lt;td&gt;4.10us&lt;/td&gt;
&lt;td&gt;19.72us&lt;/td&gt;
&lt;td&gt;33.72us&lt;/td&gt;
&lt;td&gt;84.80us&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="14%"&gt;
&lt;col width="10%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sgemm (vec)&lt;/th&gt;
&lt;th class="head"&gt;200&lt;/th&gt;
&lt;th class="head"&gt;300&lt;/th&gt;
&lt;th class="head"&gt;400&lt;/th&gt;
&lt;th class="head"&gt;500&lt;/th&gt;
&lt;th class="head"&gt;600&lt;/th&gt;
&lt;th class="head"&gt;700&lt;/th&gt;
&lt;th class="head"&gt;800&lt;/th&gt;
&lt;th class="head"&gt;900&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;1200&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;369.52us&lt;/td&gt;
&lt;td&gt;1.62ms&lt;/td&gt;
&lt;td&gt;2.91ms&lt;/td&gt;
&lt;td&gt;7.17ms&lt;/td&gt;
&lt;td&gt;11.74ms&lt;/td&gt;
&lt;td&gt;22.91ms&lt;/td&gt;
&lt;td&gt;34.82ms&lt;/td&gt;
&lt;td&gt;51.67ms&lt;/td&gt;
&lt;td&gt;64.36ms&lt;/td&gt;
&lt;td&gt;111.15ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;387.54us&lt;/td&gt;
&lt;td&gt;1.60ms&lt;/td&gt;
&lt;td&gt;2.97ms&lt;/td&gt;
&lt;td&gt;7.36ms&lt;/td&gt;
&lt;td&gt;12.11ms&lt;/td&gt;
&lt;td&gt;24.37ms&lt;/td&gt;
&lt;td&gt;35.37ms&lt;/td&gt;
&lt;td&gt;52.27ms&lt;/td&gt;
&lt;td&gt;65.72ms&lt;/td&gt;
&lt;td&gt;112.74ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;384.43us&lt;/td&gt;
&lt;td&gt;1.74ms&lt;/td&gt;
&lt;td&gt;3.12ms&lt;/td&gt;
&lt;td&gt;7.16ms&lt;/td&gt;
&lt;td&gt;12.44ms&lt;/td&gt;
&lt;td&gt;24.15ms&lt;/td&gt;
&lt;td&gt;34.87ms&lt;/td&gt;
&lt;td&gt;52.59ms&lt;/td&gt;
&lt;td&gt;70.074ms&lt;/td&gt;
&lt;td&gt;119.22ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;458.05us&lt;/td&gt;
&lt;td&gt;1.81ms&lt;/td&gt;
&lt;td&gt;3.44ms&lt;/td&gt;
&lt;td&gt;7.86ms&lt;/td&gt;
&lt;td&gt;13.43ms&lt;/td&gt;
&lt;td&gt;24.70ms&lt;/td&gt;
&lt;td&gt;36.54ms&lt;/td&gt;
&lt;td&gt;53.47ms&lt;/td&gt;
&lt;td&gt;66.87ms&lt;/td&gt;
&lt;td&gt;117.25ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;494.52us&lt;/td&gt;
&lt;td&gt;1.96ms&lt;/td&gt;
&lt;td&gt;4.80ms&lt;/td&gt;
&lt;td&gt;8.88ms&lt;/td&gt;
&lt;td&gt;18.20ms&lt;/td&gt;
&lt;td&gt;29.37ms&lt;/td&gt;
&lt;td&gt;41.24ms&lt;/td&gt;
&lt;td&gt;60.72ms&lt;/td&gt;
&lt;td&gt;72.28ms&lt;/td&gt;
&lt;td&gt;123.75ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;511.24us&lt;/td&gt;
&lt;td&gt;2.04ms&lt;/td&gt;
&lt;td&gt;4.11ms&lt;/td&gt;
&lt;td&gt;9.46ms&lt;/td&gt;
&lt;td&gt;15.34ms&lt;/td&gt;
&lt;td&gt;27.23ms&lt;/td&gt;
&lt;td&gt;38.27ms&lt;/td&gt;
&lt;td&gt;58.14ms&lt;/td&gt;
&lt;td&gt;72.78ms&lt;/td&gt;
&lt;td&gt;128.60ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;492.28us&lt;/td&gt;
&lt;td&gt;2.03ms&lt;/td&gt;
&lt;td&gt;3.90ms&lt;/td&gt;
&lt;td&gt;9.00ms&lt;/td&gt;
&lt;td&gt;14.31ms&lt;/td&gt;
&lt;td&gt;25.72ms&lt;/td&gt;
&lt;td&gt;37.09ms&lt;/td&gt;
&lt;td&gt;55.79ms&lt;/td&gt;
&lt;td&gt;67.88ms&lt;/td&gt;
&lt;td&gt;119.92ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As for the optimized version, it seems that the two families are reversed.
Indeed, GCC is doing a better job than clang here, and although the margin is
not as big as before, it's still significant. We can still observe a small
regression in GCC versions because the 4.9 version is again the fastest. As for
clang versions, it seems that clang-5.0 (used in zapcc) has had some performance
improvements for this case.&lt;/p&gt;
&lt;p&gt;For this case of matrix-matrix multiplication, it's very impressive that the
differences in the non-optimized code are so significant. And it's also
impressive that each family of compilers has its own strength, clang being
seemingly much better at handling unoptimized code while GCC is better at
handling vectorized code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="convolution-2d"&gt;
&lt;h3&gt;Convolution (2D)&lt;/h3&gt;
&lt;p&gt;The last benchmark that I considered is the case of the valid convolution on 2D
images. The code is quite similar to the GEMM code but more complicated to
optimized due to cache locality.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="19%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sconv2_valid (std)&lt;/th&gt;
&lt;th class="head"&gt;100x50&lt;/th&gt;
&lt;th class="head"&gt;105x50&lt;/th&gt;
&lt;th class="head"&gt;110x55&lt;/th&gt;
&lt;th class="head"&gt;115x55&lt;/th&gt;
&lt;th class="head"&gt;120x60&lt;/th&gt;
&lt;th class="head"&gt;125x60&lt;/th&gt;
&lt;th class="head"&gt;130x65&lt;/th&gt;
&lt;th class="head"&gt;135x65&lt;/th&gt;
&lt;th class="head"&gt;140x70&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;27.93ms&lt;/td&gt;
&lt;td&gt;33.68ms&lt;/td&gt;
&lt;td&gt;40.62ms&lt;/td&gt;
&lt;td&gt;48.23ms&lt;/td&gt;
&lt;td&gt;57.27ms&lt;/td&gt;
&lt;td&gt;67.02ms&lt;/td&gt;
&lt;td&gt;78.45ms&lt;/td&gt;
&lt;td&gt;92.53ms&lt;/td&gt;
&lt;td&gt;105.08ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;37.60ms&lt;/td&gt;
&lt;td&gt;44.94ms&lt;/td&gt;
&lt;td&gt;54.24ms&lt;/td&gt;
&lt;td&gt;64.45ms&lt;/td&gt;
&lt;td&gt;76.63ms&lt;/td&gt;
&lt;td&gt;89.75ms&lt;/td&gt;
&lt;td&gt;105.08ms&lt;/td&gt;
&lt;td&gt;121.66ms&lt;/td&gt;
&lt;td&gt;140.95ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;37.10ms&lt;/td&gt;
&lt;td&gt;44.99ms&lt;/td&gt;
&lt;td&gt;54.34ms&lt;/td&gt;
&lt;td&gt;64.54ms&lt;/td&gt;
&lt;td&gt;76.54ms&lt;/td&gt;
&lt;td&gt;89.87ms&lt;/td&gt;
&lt;td&gt;105.35ms&lt;/td&gt;
&lt;td&gt;121.94ms&lt;/td&gt;
&lt;td&gt;141.20ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;37.55ms&lt;/td&gt;
&lt;td&gt;45.08ms&lt;/td&gt;
&lt;td&gt;54.39ms&lt;/td&gt;
&lt;td&gt;64.48ms&lt;/td&gt;
&lt;td&gt;76.51ms&lt;/td&gt;
&lt;td&gt;92.02ms&lt;/td&gt;
&lt;td&gt;106.16ms&lt;/td&gt;
&lt;td&gt;125.67ms&lt;/td&gt;
&lt;td&gt;143.57ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;15.42ms&lt;/td&gt;
&lt;td&gt;18.59ms&lt;/td&gt;
&lt;td&gt;22.21ms&lt;/td&gt;
&lt;td&gt;26.40ms&lt;/td&gt;
&lt;td&gt;31.03ms&lt;/td&gt;
&lt;td&gt;36.26ms&lt;/td&gt;
&lt;td&gt;42.35ms&lt;/td&gt;
&lt;td&gt;48.87ms&lt;/td&gt;
&lt;td&gt;56.29ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;15.48ms&lt;/td&gt;
&lt;td&gt;18.67ms&lt;/td&gt;
&lt;td&gt;22.34ms&lt;/td&gt;
&lt;td&gt;26.50ms&lt;/td&gt;
&lt;td&gt;31.27ms&lt;/td&gt;
&lt;td&gt;36.58ms&lt;/td&gt;
&lt;td&gt;42.61ms&lt;/td&gt;
&lt;td&gt;49.33ms&lt;/td&gt;
&lt;td&gt;56.80ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;15.29ms&lt;/td&gt;
&lt;td&gt;18.37ms&lt;/td&gt;
&lt;td&gt;22.00ms&lt;/td&gt;
&lt;td&gt;26.10ms&lt;/td&gt;
&lt;td&gt;30.75ms&lt;/td&gt;
&lt;td&gt;35.95ms&lt;/td&gt;
&lt;td&gt;41.85ms&lt;/td&gt;
&lt;td&gt;48.42ms&lt;/td&gt;
&lt;td&gt;55.74ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In that case, we can observe the same as for the GEMM. The clang-based versions
are much producing significantly faster code than the GCC versions. Moreover, we
can also observe the same large regression starting from GCC-5.4.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="21%"&gt;
&lt;col width="11%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sconv2_valid (vec)&lt;/th&gt;
&lt;th class="head"&gt;100x50&lt;/th&gt;
&lt;th class="head"&gt;105x50&lt;/th&gt;
&lt;th class="head"&gt;110x55&lt;/th&gt;
&lt;th class="head"&gt;115x55&lt;/th&gt;
&lt;th class="head"&gt;120x60&lt;/th&gt;
&lt;th class="head"&gt;125x60&lt;/th&gt;
&lt;th class="head"&gt;130x65&lt;/th&gt;
&lt;th class="head"&gt;135x65&lt;/th&gt;
&lt;th class="head"&gt;140x70&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;878.32us&lt;/td&gt;
&lt;td&gt;1.07ms&lt;/td&gt;
&lt;td&gt;1.20ms&lt;/td&gt;
&lt;td&gt;1.68ms&lt;/td&gt;
&lt;td&gt;2.04ms&lt;/td&gt;
&lt;td&gt;2.06ms&lt;/td&gt;
&lt;td&gt;2.54ms&lt;/td&gt;
&lt;td&gt;3.20ms&lt;/td&gt;
&lt;td&gt;4.14ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;853.73us&lt;/td&gt;
&lt;td&gt;1.03ms&lt;/td&gt;
&lt;td&gt;1.15ms&lt;/td&gt;
&lt;td&gt;1.36ms&lt;/td&gt;
&lt;td&gt;1.76ms&lt;/td&gt;
&lt;td&gt;2.05ms&lt;/td&gt;
&lt;td&gt;2.44ms&lt;/td&gt;
&lt;td&gt;2.91ms&lt;/td&gt;
&lt;td&gt;3.13ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;847.95us&lt;/td&gt;
&lt;td&gt;1.02ms&lt;/td&gt;
&lt;td&gt;1.14ms&lt;/td&gt;
&lt;td&gt;1.35ms&lt;/td&gt;
&lt;td&gt;1.74ms&lt;/td&gt;
&lt;td&gt;1.98ms&lt;/td&gt;
&lt;td&gt;2.43ms&lt;/td&gt;
&lt;td&gt;2.90ms&lt;/td&gt;
&lt;td&gt;3.12ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;795.82us&lt;/td&gt;
&lt;td&gt;0.93ms&lt;/td&gt;
&lt;td&gt;1.05ms&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.60ms&lt;/td&gt;
&lt;td&gt;1.77ms&lt;/td&gt;
&lt;td&gt;2.20ms&lt;/td&gt;
&lt;td&gt;2.69ms&lt;/td&gt;
&lt;td&gt;2.81ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;782.46us&lt;/td&gt;
&lt;td&gt;0.93ms&lt;/td&gt;
&lt;td&gt;1.05ms&lt;/td&gt;
&lt;td&gt;1.26ms&lt;/td&gt;
&lt;td&gt;1.60ms&lt;/td&gt;
&lt;td&gt;1.84ms&lt;/td&gt;
&lt;td&gt;2.21ms&lt;/td&gt;
&lt;td&gt;2.65ms&lt;/td&gt;
&lt;td&gt;2.84ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;767.58us&lt;/td&gt;
&lt;td&gt;0.92ms&lt;/td&gt;
&lt;td&gt;1.04ms&lt;/td&gt;
&lt;td&gt;1.25ms&lt;/td&gt;
&lt;td&gt;1.59ms&lt;/td&gt;
&lt;td&gt;1.83ms&lt;/td&gt;
&lt;td&gt;2.20ms&lt;/td&gt;
&lt;td&gt;2.62ms&lt;/td&gt;
&lt;td&gt;2.83ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;782.49us&lt;/td&gt;
&lt;td&gt;0.94ms&lt;/td&gt;
&lt;td&gt;1.06ms&lt;/td&gt;
&lt;td&gt;1.27ms&lt;/td&gt;
&lt;td&gt;1.62ms&lt;/td&gt;
&lt;td&gt;1.83ms&lt;/td&gt;
&lt;td&gt;2.24ms&lt;/td&gt;
&lt;td&gt;2.65ms&lt;/td&gt;
&lt;td&gt;2.85ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This time, clang manages to produce excellent results. Indeed, all the produced
executables are significantly faster than the versions produced by GCC, except
for GCC-7.1 which is producing similar results. The other versions of GCC are
falling behind it seems. It seems that it was only for the GEMM that clang was
having a lot of troubles handling the optimized code.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Clang seems to have recently done a lot of optimizations regarding compilation
time. Indeed, clang-4.0.1 is much faster for compilation than clang-3.9.
Although GCC-7.1 is faster than GCC-6.3, all the GCC versions are slower than
GCC-4.9.4 which is the fastest at compiling code with optimizations. GCC-7.1 is
the fastest GCC version for compiling code in debug mode.&lt;/p&gt;
&lt;p&gt;In some cases, there is almost no difference between different compilers in the
generated code. However, in more  complex algorithms such as the matrix-matrix
multiplication or the two-dimensional convolution, the differences can be quite
significant. In my tests, Clang have shown to be much better at compiling
unoptimized code. However, and especially in the GEMM case, it seems to be worse
than GCC at handling hand-optimized. I will investigate that case and try to
tailor the code so that clang is having a better time with it.&lt;/p&gt;
&lt;p&gt;For me, it's really weird that the GCC regression, apparently starting from
GCC-5.4, has still not been fixed in GCC 7.1. I was thinking of dropping support
for GCC-4.9 in order to go full C++14 support, but now I may have to reconsider
my position. However, seeing that GCC is generally the best at handling
optimized code (especially for GEMM), I may be able to do the transition, since
the optimized code will be used in most cases.&lt;/p&gt;
&lt;p&gt;As for zapcc, although it is still the fastest compiler in debug mode, with the
new speed of clang-4.0.1, its margin is quite small. Moreover, on optimized
build, it's not as fast as GCC. If you use clang and can have access to zapcc,
it's still quite a good option to save some time.&lt;/p&gt;
&lt;p&gt;Overall, I have been quite pleased by clang-4.0.1 and GCC-7.1, the most recent
versions I have been testing. It seems that they did quite some good work.
I will definitely run some more tests with them and try to adapt the code. I'm
still considering whether I will drop support for some older compilers.&lt;/p&gt;
&lt;p&gt;I hope this comparison was interesting :) My next post will probably be about
the difference in performance between my machine learning framework and other
frameworks to train neural networks.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>C++14</category><category>clang</category><category>Compilers</category><category>etl</category><category>gcc</category><category>Performance</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/08/compiler-benchmark-gcc-clang-cpp-library-etl.html</guid><pubDate>Mon, 07 Aug 2017 07:16:21 GMT</pubDate></item><item><title>Expression Templates Library (ETL) 1.1</title><link>http://baptiste-wicht.com/posts/2017/08/expression-templates-library-etl-11.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;img alt="ETL Logo" class="align-center" src="http://baptiste-wicht.com/images/logo.png"&gt;
&lt;p&gt;It took me longer than I thought, but I'm glad to announce the release of the
version 1.1 of my Expression Templates Library (ETL) project. This is a major
new release with many improvements and new features. It's been almost one month
since the last, and first, release (1.0) was released. I should have done some
minor releases in the mean time, but at least now the library is in a good shape
for major version.&lt;/p&gt;
&lt;p&gt;It may be interesting to note that my machine learning framework (DLL), based on
the ETL library, has shown to be faster than all the tested popular frameworks
(Tensorflow, Keras, Caffee, Torch, DeepLearning4J) for training various neural
networks on CPU. I'll post more details on another post on the coming weeks, but
that shows that special attention to performance has been done in this library
and that it is well adapted to machine learning.&lt;/p&gt;
&lt;p&gt;For those of you that don't follow my blog, ETL is a library providing
Expression Templates for computations on matrix and vector. For instance, if you
have three matrices A, B and C you could write C++ code like this:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_d38daad28de441e9beed92d52c7d243f-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Or given vectors b, v, h and a matrix W, you could write code like this:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_a4234283bacf4383bd8c54dc33fb7721-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The goal of such library is two-fold. First, this makes the expression more
readable and as close to math as possible. And then, it allows the library to
compute the expressions as fast as possible.  In the first case, the framework
will compute the sum using a vectorized algorithm and then compute the overall
expression using yet again vectorized code. The expression can also be computed
in parallel if the matrices are big enough. In the second case, the
vector-matrix multiplication will be computed first using either hand-code
optimized vectorized or a BLAS routine (depending on configuration options).
Then, all the expression will be executed using vectorized code.&lt;/p&gt;
&lt;div class="section" id="features"&gt;
&lt;h2&gt;Features&lt;/h2&gt;
&lt;p&gt;Many new features have been integrated into the library.&lt;/p&gt;
&lt;p&gt;The support for machine learning operations has been improved. There are now
specific helpers for machine learning in the etl::ml namespace which have names
that are standard to machine learning. A real transposed convolution has been
implemented with support for padding and stride. Batched outer product and
batched bias averaging are also now supported. The activation function support
has also been improved and the derivatives have been reviewed. The pooling
operators have also been improved with stride and padding support. Unrelated to
machine learning, 2D and 3D pooling can also be done in higher dimensional
matrix now.&lt;/p&gt;
&lt;p&gt;New functions are also available for matrices and vectors. The support for
square root has been improved with cubic root and inverse root. Support has also
been added for floor and ceil. Moreover, comparison operators are now available
as well as global functions such as approx_equals.&lt;/p&gt;
&lt;p&gt;New reductions have also been added with support for absolute sum and mean
(asum/asum) and for min_index and max_index, which returns the index of the
minimum element, respectively the maximum. Finally, argmax can now be used to
get the max index in each sub dimensions of a matrix. argmax on a vector is
equivalent to max_index.&lt;/p&gt;
&lt;p&gt;Support for shuffling has also been added. By default, shuffling a vector means
shuffling all elements and shuffling a matrix means shuffling by shuffling the
sub matrices (only the first dimension is shuffled), but shuffling a matrix as
a vector is also possible. Shuffle of two vectors or two matrices in parallel,
is also possible. In that case, the same permutation is applied to both
containers. As a side note, all operations using random generation are also
available with an addition parameter for the random generator, which can help to
improve reproducibility or simply tune the random generator.&lt;/p&gt;
&lt;p&gt;I've also included support for adapters matrices. There are adapters for
hermitian matrices, symmetric matrices and lower and upper triangular matrices.
For now, the framework does not take advantage of this information, this will be
done later, but the framework guarantee the different constrain on the content.&lt;/p&gt;
&lt;p&gt;There are also a few new more minor features. Maybe not so minor, matrices can
now be sliced into sub matrices. With that a matrix can be divided into several
sub matrices and modifying the sub matrices will modify the source matrix. The
sub matrices are available in 2D, 3D and 4D for now. There are also some other
ways of slicing matrix and vectors. It is possible to obtain a slice of its
memory or obtain a slice of its first dimension. Another new feature is that it
is now possible compute the cross product of vectors now. Matrices can be
decomposed into their Q/R decomposition rather than only their PALU
decomposition. Special support has been integrated for matrix and vectors of
booleans. In that case, they support logical operators such as and, not and or.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="performance"&gt;
&lt;h2&gt;Performance&lt;/h2&gt;
&lt;p&gt;I've always considered the performance of this library to be a feature itself.
I consider the library to be quite fast, especially its convolution support,
even though there is still room for improvement. Therefore, many improvements
have been made to the performance of the library since the last release. As said
before, this library was used in a machine learning framework which then proved
faster than most popular neural network frameworks on CPU. I'll present here
the most important new improvements to performance, in no real particular order,
every bit being important in my opinion.&lt;/p&gt;
&lt;p&gt;First, several operations have been optimized to be faster.&lt;/p&gt;
&lt;p&gt;Multiplication of matrices or matrices and vectors are now much faster if one of
the matrix is transposed. Instead of performing the slow transposition,
different kernels are used in order to maximize performance without doing any
transposition, although sometimes transposition is performed when it is faster.
This leads to very significant improvements, up to 10 times faster in the best
case. This is performed for vectorized kernels and also for BLAS and CUBLAS
calls. These new kernels are also directly used when matrices of different
storage order are used. For instance, multiplying a column major matrix with
a row major matrix and storing the result in a column major matrix is now much
more efficient than before. Moreover, the performance of the transpose operation
itself is also much faster than before.&lt;/p&gt;
&lt;p&gt;A lot of machine learning operations have also been highly optimized. All the
pooling and upsample operators are now parallelized and the most used kernel
(2x2 pooling) is now more optimized. 4D convolution kernels (for machine
learning) have been greatly improved. There are now very specialized vectorized
kernels for classic kernel configurations (for instance 3x3 or 5x5) and the
selection of implementations is now smarter than before. The support of padding
is now much better than before for small amount of padding. Moreover, for small
kernels the full convolution can now be evaluated using the valid convolution
kernels directly with some padding, for much faster overall performance. The
exponential operation is now vectorized which allows operations such as sigmoid
or softmax to be much faster.&lt;/p&gt;
&lt;p&gt;Matrices and vector are automatically using aligned memory. This means that
vectorized code can use aligned operations, which may be slightly faster.
Moreover, matrices and vectors are now padded to a multiple of the vector size.
This allows to remove the final non-vectorized remainder loop from the
vectorized code. This is only done for the end of matrices, when they are
accessed in flat way. Contrary to some frameworks, inner dimensions of the
matrix are not padded.  Finally, accesses to 3D and 4D matrices is now much
faster than before.&lt;/p&gt;
&lt;p&gt;Then, the parallelization feature of ETL has been completely reworked. Before,
there was a thread pool for each algorithm that was parallelized. Now, there is
a global thread engine with one thread pool. Since parallelization is not nested
in ETL, this improves performance slightly by greatly diminishing the number of
threads that are created throughout an application. Another big difference in
parallel dispatching is that now it can detect good split based on alignment so
that each split are aligned. This then allows the vectorization process to use
aligned stores and loads instead of unaligned ones which may be faster on some
processors.&lt;/p&gt;
&lt;p&gt;Vectorization has also been greatly improved in ETL. Integer operations are now
automatically vectorized on processors that support this. Before, only floating
points operations were vectorized. The automatic vectorizer now is able to use
non-temporal stores for very large operations. A non-temporal store bypasses the
cache, thus gaining some time. Since very large matrices do not fit in cache
anyway and the cache would end up being overwritten anyway, this is a net gain.
Moreover, the alignment detection in the automatic vectorizer has also been
improved. Support for Fused-Multiply-Add (FMA) operations has also been
integrated in the algorithms that can make use of it (multiplications and
convolutions). The matrix-matrix multiplications and vector-matrix
multiplications now have highly optimized vectorized kernels. They also have
versions for column-major matrices now.  I plan to reintegrate a version of the
GEMM based on BLIS in the future but with more optimizations and support for all
precisions and integers, For my version is still slower than the simple
vectorized version. The sum and the dot product operations now also have
specialized vectorized implementations. The min and max operations are now
automatically-vectorized. Several others algorithms have also their own
vectorized implementations.&lt;/p&gt;
&lt;p&gt;Last, but not least, the GPU support has also been almost completely reworked.
Now, several operations can be chained without any copies between GPU and CPU.
Several new operations have also been added with support to GPU (convolutions,
pooling, sigmoid, ReLU, ...). Moreover, to complement operations that are not
available in any of the supported NVIDIA libraries, I've created a simple
library that can be used to add a few more GPU operations.  Nevertheless a lot
of operations are still missing and only algorithms are available not
expressions (such as c = a + b * 1.0) that are entirely computed on CPU. I have
plans to improve that further, probably for version 1.2. The different contexts
necessary for NVIDIA library can now be cached (using an option from ETL),
leading to much faster code. Only the main handle can be cached so far, I plan
to try to cache all the descriptors, but I don't know yet when that will be
ready. Finally, an option is also available to reuse GPU memory instead of
directly releasing it to CUDA. This is using a custom memory pool and can save
some time. Since this needs to be cleaned (by a call to etl::exit() or using
ETL_PROLOGUE), this is only activated on demand.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="other-changes"&gt;
&lt;h2&gt;Other changes&lt;/h2&gt;
&lt;p&gt;There also have been a lot of refactorings in the code of the library. A lot of
expressions now have less overhead and are specialized for performance.
Moreover, temporary expressions have been totally reworked to be more simple and
maintainable and easier to optimize in the future. It's also probably easier to
add new expressions to the framework now, although that could be even more
simple. There are also less duplicated code now in the different expressions.
Especially, now there are now more SSE and AVX variants in the code. All the
optimized algorithms are now using the vectorization system of the library.&lt;/p&gt;
&lt;p&gt;I also tried my best to reduce the compilation time, based on the unit tests.
This is still not great but better than before. For user code, the next version
should be much faster to compile since I plan to disable forced selection of
implementations by default and only enable it on demand.&lt;/p&gt;
&lt;p&gt;Finally, there also was quite a few bug fixes. Most of them have been found by
the use of the library in the Deep Learning Library (DLL) project. Some were
very small edge cases. For instance, the transposition algorithm was not working
on GPU on rectangular column major matrices. There also was a slight bug in the
Q/R decomposition and in the pooling of 4D matrices.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="what-s-next"&gt;
&lt;h2&gt;What's next ?&lt;/h2&gt;
&lt;p&gt;Next time, I may do some minor release, but I don't yet have a complete plan.
For the next major release (1.2 probably), here is what is planned:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Review the system for selection of algorithms to reduce compilation time&lt;/li&gt;
&lt;li&gt;Review the GPU system to allow more complete support for standard operators&lt;/li&gt;
&lt;li&gt;Switch to C++17: there are many improvements that could be done to the code with C++17 features&lt;/li&gt;
&lt;li&gt;Add support for convolution on mixed types (float/double)&lt;/li&gt;
&lt;li&gt;More tests for sparse matrix&lt;/li&gt;
&lt;li&gt;More algorithms support for sparse matrix&lt;/li&gt;
&lt;li&gt;Reduce the compilation time of the library in general&lt;/li&gt;
&lt;li&gt;Reduce the compilation and execution time of the unit tests&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are pretty big changes, especially the first two, so maybe it'll be split
into several releases. It will really depend on the time I have. As for C++17,
I really want to try it and I have a lot of points that could profit from the
switch, but that will means setting GCC 7.1 and Clang 3.9 as minimum
requirement, which may not be reasonable for every user.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="download-etl"&gt;
&lt;h2&gt;Download ETL&lt;/h2&gt;
&lt;p&gt;You can download ETL &lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;on Github&lt;/a&gt;. If you
only interested in the 1.1 version, you can look at the
&lt;a class="reference external" href="https://github.com/wichtounet/etl/releases"&gt;Releases pages&lt;/a&gt; or clone the tag
1.1. There are several branches:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;master&lt;/em&gt; Is the eternal development branch, may not always be stable&lt;/li&gt;
&lt;li&gt;&lt;em&gt;stable&lt;/em&gt; Is a branch always pointing to the last tag, no development here&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the future release, there always will tags pointing to the corresponding
commits. I'm not following the git flow way, I'd rather try to have a more
linear history with one eternal development branch, rather than an useless
develop branch or a load of other branches for releases.&lt;/p&gt;
&lt;p&gt;The documentation is a bit sparse. There are a few examples and the Wiki, but
there still is work to be done. If you have questions on how to use or configure
the library, please don't hesitate.&lt;/p&gt;
&lt;p&gt;Don't hesitate to comment this post if you have any comment on this library or
any question. You can also open an Issue on Github if you have a problem using
this library or propose a Pull Request if you have any contribution you'd like
to make to the library.&lt;/p&gt;
&lt;p&gt;Hope this may be useful to some of you :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>C++17</category><category>Compilers</category><category>Performance</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/08/expression-templates-library-etl-11.html</guid><pubDate>Fri, 04 Aug 2017 13:13:03 GMT</pubDate></item><item><title>C++ Containers Benchmark: vector/list/deque and plf::colony</title><link>http://baptiste-wicht.com/posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Already more than three years ago, I've written a &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/12/cpp-benchmark-vector-list-deque.html"&gt;benchmark of some of the STL containers&lt;/a&gt;,
namely the vector, the list and the deque. Since this article was very popular,
I decided to improve the benchmarks and collect again all the results. There are
now more benchmarks and some problems have been fixed in the benchmark code.
Moreover, I have also added a new container, the plf::colony. Therefore, there
are four containers tested:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The std::vector: This is a dynamically-resized array of elements. All the
elements are contiguous in memory. If an element is inserted or removed it at
a position other than the end, the following elements will be moved to fill
the gap or to open a gap. Elements can be accessed at random position in
constant time. The array is resized so that it can several more elements, not
resized at each insert operation. This means that insertion at the end of the
container is done in amortized constant time.&lt;/li&gt;
&lt;li&gt;The std::deque: The deque is a container that offer constant time insertion
both at the front and at the back of the collection. In current c++ libraries,
it is implementation as a collection of dynamically allocated fixed-size
array. Not all elements are contiguous, but depending on the size of the data
type, this still has good data locality. Access to a random element is also
done in constant time, but with more overhead than the vector. For insertions
and removal at random positions, the elements are shifted either to the front
or to the back meaning that it is generally faster than the vector, by twice
in average.&lt;/li&gt;
&lt;li&gt;The std::list: This is a doubly-linked list. It supports constant time
insertions at any position of the collection. However, it does not support
constant time random access. The elements are obviously not contiguous, since
they are all allocated in nodes. For small elements, this collection has
a very big memory overhead.&lt;/li&gt;
&lt;li&gt;The plf::colony: This container is a non-standard container which is
unordered, it means that the insertion order will not necessarily be
preserved. It provides strong iterators guarantee, pointers to non-erased
element are not invalidated by insertion or erasure. It is especially tailored
for high-insertion/erasure workloads. Moreover, it is also specially optimized
for non-scalar types, namely structs and classes with relatively large data
size (greater than 128 bits on the official documentation). Its implementation
is more complicated than the other containers. It is also implemented as
a list of memory blocks, but they are of increasingly large sizes. When
elements are erased, there position is not removed, but marked as erased so
that it can be reused for fast insertion later on. This container uses the
same conventions as the standard containers and was proposed for inclusion to
the standard library, which is the main reason why it's included in this
benchmark. If you want more information, you can consult the
&lt;a class="reference external" href="http://plflib.org/colony.htm"&gt;official website&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the text and results, the namespaces will be omitted. Note that I have only
included sequence containers in my test. These are the most common containers in
practices and these also the containers I'm the most familiar with. I could have
included multiset in this benchmark, but the interface and purpose being
different, I didn't want the benchmark to be confusing.&lt;/p&gt;
&lt;p&gt;All the examples are compiled with g++-4.9.4 (-std=c++11 -march=native -O2) and
run on a Gentoo Linux machine with an Intel Core i7-4770 at 3.4GHz.&lt;/p&gt;
&lt;p&gt;For each graph, the vertical axis represent the amount of time necessary to
perform the operations, so the lower values are the better. The horizontal axis
is always the number of elements of the collection. For some graph, the
logarithmic scale could be clearer, a button is available after each graph to
change the vertical scale to a logarithmic scale.&lt;/p&gt;
&lt;p&gt;The tests are done with several different data types. The trivial data types are
varying in size, they hold an array of longs and the size of the array varies to
change the size of the data type. The non-trivial data type is composed of
a string (just long enough to avoid SSO (Small String Optimization) (even though
I'm using GCC)). The non-trivial data types comes in a second version with
noexcept move operations.  Not all results are presented for each data types if
there are not significant differences between in order to keep this article
relatively short (it's already probably too long :P).&lt;/p&gt;
&lt;p class="more"&gt;&lt;a href="http://baptiste-wicht.com/posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html"&gt;Read more…&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>C++</category><category>C++11</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html</guid><pubDate>Sun, 21 May 2017 10:46:23 GMT</pubDate></item><item><title>Update on Expression Templates Library (ETL)</title><link>http://baptiste-wicht.com/posts/2017/05/update-on-expression-templates-library-etl.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;It's been a while since I've &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2016/09/expression-templates-library-etl-10.html"&gt;released the version 1.0 of ETL&lt;/a&gt;. There is some work to do before I release the next version, but I wanted to give you a quick update on what has been going on for ETL in the last months. There has been a lot of changes in the library and the next version will be a major update when I'm done with some refactorings and improvements.&lt;/p&gt;
&lt;p&gt;Thanks to my thesis supervisor, the project now has a logo:&lt;/p&gt;
&lt;img alt="ETL Logo" class="align-center" src="http://baptiste-wicht.com/images/logo.png"&gt;
&lt;p&gt;There are quite a few new features, although probably nothing really major. The
support for square root has been improved with cubic root and inverse root.
Vectors can now be transformed using floor and ceil. Cross product of vector has
been implemented as well. Batched outer product and batched bias averaging (for
machine learning) are now supported. Reductions have also been improved with
absolute sum and mean (asum/asum) support and min_index and max_index. argmax
can now be used to get the max index in each sub dimensions. Matrix can now be
decomposed into their Q/R decomposition rather than only their PALU
decomposition. The matrices can now be sliced by getting only a sub part of the
matrix. The pooling operators  have also been improved with stride and padding
support. Matrices and vectors can also be shuffled. Moreover, a few adapters
are now available for hermitian matrices, symmetric matrices and lower and upper
matrices. So far the support for these adapters is not huge, but they are
guaranteed to validate their constraints.&lt;/p&gt;
&lt;p&gt;Several operations have been optimized for speed. All the pooling and upsample
operators are now parallelized and the most used kernel (2x2 pooling) is now
more optimized. 4D convolution kernels (for machine learning) have been greatly
improved. There are now very specialized vectorized kernels for classic kernel
configurations (for instance 3x3 or 5x5) and the selection of implementations is
now smarter than before. The support of padding now much better than before for
small amount of padding. Moreover, for small kernels the full convolution can
now be evaluated using the valid convolution kernels directly with some padding,
for much faster overall performance. Matrix-matrix multiplication with
transposed matrices is now much faster when using BLAS kernels. Indeed, the
transposition is not performed but handled inside the kernels. Moreover, the
performance of the transposition itself is also much faster. Finally, accesses
to 3D and 4D matrices is now much faster than before.&lt;/p&gt;
&lt;p&gt;The parallelization feature of ETL has been completely reworked. Before, there
was a thread pool for each algorithm that was parallelized. Now, there is
a global thread engine with one thread pool. Since parallelization is not nested
in ETL, this improves performance slightly by greatly diminishing the number of
threads that are created throughout an application.&lt;/p&gt;
&lt;p&gt;Vectorization has also been greatly improved in ETL. Integer operations are now
automatically vectorized on processors that support this. The automatic
vectorizer now is able to use non-temporal stores for very large operations.
A non-temporal store bypasses the cache, thus gaining some time. Since very
large matrices do not fit in cache, this is a net gain. Moreover, the alignment
detection in the automatic vectorizer has also been improved. Support for
Fused-Multiply-Add (FMA) operations has also been integrated in the algorithms
that can make use of it. The matrix-matrix multiplications and vector-matrix
multiplications now have optimized vectorized kernels. They also have versions
for column-major matrices now. The old egblas version of the gemm, based on BLIS
kernels, has been removed since it was only supporting double-precision and was
not faster than the new vectorized algorithm. I plan to reintegrate a version of
the GEMM based on BLIS in the future but with more optimizations and support for
all precisions and integers. The sum and the dot product now also have
specialized vectorized implementations. The min and max operations are now
automatically-vectorized.&lt;/p&gt;
&lt;p&gt;The GPU has also been almost completely reworked. Now, operations can be chained
without any copies between GPU and CPU. Several new operations have also been
added with support to GPU. Moreover, to complement operations that are not
available in any of the supported NVIDIA libraries, I've created a simple
library that can be used to add a few more GPU operations. Nevertheless a lot of
operations are still missing and only algorithms are available not expressions
(such as c = a + b * 1.0) that are entirely computed on CPU. I have plans to
improve that further, but probably not before the version 1.2.&lt;/p&gt;
&lt;p&gt;There also have been a lot of refactorings in the code of the library. A lot of
expressions now have less overhead and are specialized for performance.
Moreover, temporary expressions are currently being reworked in order to be more
simple and maintainable and easier to optimize in the future.&lt;/p&gt;
&lt;p&gt;Finally, there also was quite a few bug fixes. Most of them have been found by
the use of the library in the Deep Learning Library (DLL) project.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>etl</category><category>Performance</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/05/update-on-expression-templates-library-etl.html</guid><pubDate>Sat, 06 May 2017 19:31:48 GMT</pubDate></item><item><title>Partial type erasing in Deep Learning Library (DLL) to improve compilation time</title><link>http://baptiste-wicht.com/posts/2017/03/partial-type-erasing-deep-learning-library-dll-improve-compilation-time.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;In a previous post, I compared the &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/03/disappointing-zapcc-performance-on-deep-learning-library-dll.html"&gt;compilation time on my Deep Learning Library (DLL) project with different compilers&lt;/a&gt;. I realized that the compilation times were quickly going unreasonable for this library, especially for compiling the unit cases which clearly hurts the development of the library. Indeed, you want to be able to run the unit tests reasonably quickly after you integrated new changes.&lt;/p&gt;
&lt;div class="section" id="reduce-the-compilation-time"&gt;
&lt;h2&gt;Reduce the compilation time&lt;/h2&gt;
&lt;p&gt;The first thing I did was to split the compilation in three executables: one for
the unit tests, one for the various performance tests and one for the various other
miscellaneous tests. With this, it is much faster to compile only the unit test
cases.&lt;/p&gt;
&lt;p&gt;But this can be improved significantly more. In DLL a network is a variadic
template containing the list of layers, in order. In DLL, there are two main
different ways of declaring a neural networks. In the first version, the fast
version, the layers directly know their sizes:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_2db2aec5844b462189fee14b91389a3f-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;a name="rest_code_2db2aec5844b462189fee14b91389a3f-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_2db2aec5844b462189fee14b91389a3f-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_2db2aec5844b462189fee14b91389a3f-4"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_2db2aec5844b462189fee14b91389a3f-5"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_2db2aec5844b462189fee14b91389a3f-6"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;400&lt;/span&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;hidden&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;unit_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;SOFTMAX&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_2db2aec5844b462189fee14b91389a3f-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sgd_trainer&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_2db2aec5844b462189fee14b91389a3f-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_2db2aec5844b462189fee14b91389a3f-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_2db2aec5844b462189fee14b91389a3f-10"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;pretrain&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_2db2aec5844b462189fee14b91389a3f-11"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;In my opinion, this is the best way to use DLL. This is the fastest and the
clearest. Moreover, the dimensions of the network can be validated at compile
time, which is always better than at runtime. However, the dimensions of the
network cannot be changed at runtime.  For this, there is a different version,
the dynamic version:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-4"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-5"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-6"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;hidden&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;unit_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;SOFTMAX&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sgd_trainer&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-11"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;layer_get&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;init_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-12"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;layer_get&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;init_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-13"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;layer_get&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;init_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-14"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;layer_get&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-15"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;layer_get&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-16"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;layer_get&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-18"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;pretrain&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_cb928d65c3d84995990f6e6c9284b0da-19"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is a bit more verbose, but the configuration can be changed at runtime with
this system. Moreover, this is also faster to compile. On the other hand, there
is some performance slowdown.&lt;/p&gt;
&lt;p&gt;There is also a third version that is a hybrid of the first version:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_27593711e5904ac19185da931a28c272-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;a name="rest_code_27593711e5904ac19185da931a28c272-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_dbn_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_27593711e5904ac19185da931a28c272-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_27593711e5904ac19185da931a28c272-4"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_27593711e5904ac19185da931a28c272-5"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_27593711e5904ac19185da931a28c272-6"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;400&lt;/span&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;hidden&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;unit_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;SOFTMAX&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_27593711e5904ac19185da931a28c272-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sgd_trainer&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_27593711e5904ac19185da931a28c272-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_27593711e5904ac19185da931a28c272-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_27593711e5904ac19185da931a28c272-10"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;pretrain&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_27593711e5904ac19185da931a28c272-11"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Only one line was changed compared to the first version, &lt;code&gt;dbn_desc&lt;/code&gt;
becomes &lt;code&gt;dyn_dbn_desc&lt;/code&gt;. What this changes is that all the layers are
automatically transformed into their dynamic versions and all the parameters are
propagated at runtime. This is a form a type erasing since the sizes will not be
propagated at compilation time. But this is simple since the types are simply
transformed from one type to another directly. Behind the scene, it's the
dynamic version using the front-end of the fast version. This is almost as fast
to compile as the dynamic version, but the code is much better. It executes the
same as the dynamic version.&lt;/p&gt;
&lt;p&gt;If we compare the compilation time of the three versions when compiling a single
network and 5 different networks with different architectures, we get the
following results (with clang):&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="52%"&gt;
&lt;col width="48%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Model&lt;/th&gt;
&lt;th class="head"&gt;Time [s]&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;1 Fast&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;1 Dynamic&lt;/td&gt;
&lt;td&gt;16.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;1 Hybrid&lt;/td&gt;
&lt;td&gt;16.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5 Fast&lt;/td&gt;
&lt;td&gt;114&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5 Dynamic&lt;/td&gt;
&lt;td&gt;16.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5 Hybrid&lt;/td&gt;
&lt;td&gt;21.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Even with one single network, the compilation time is reduced by 44%. When five
different networks are compilation, time is reduced by 85%. This can be
explained easily. Indeed, for the hybrid and dynamic versions, the layers will
have the same type and therefore a lot of template instantiations will only be
done once instead of five times. This makes a lot of difference since almost
everything is template inside the library.&lt;/p&gt;
&lt;p&gt;Unfortunately, this also has an impact on the runtime of the network:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="26%"&gt;
&lt;col width="41%"&gt;
&lt;col width="32%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Model&lt;/th&gt;
&lt;th class="head"&gt;Pretrain [s]&lt;/th&gt;
&lt;th class="head"&gt;Train [s]&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Fast&lt;/td&gt;
&lt;td&gt;195&lt;/td&gt;
&lt;td&gt;114&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Dynamic&lt;/td&gt;
&lt;td&gt;203&lt;/td&gt;
&lt;td&gt;123&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Hybrid&lt;/td&gt;
&lt;td&gt;204&lt;/td&gt;
&lt;td&gt;122&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;On average, for dense models, the slowdown is between 4% and 8%. For
convolutional models, it is between 10% and 25%. I will definitely work on
trying to make the dynamic and especially the hybrid version faster in the
future, most on the work should be on the matrix library (ETL) that is used.&lt;/p&gt;
&lt;p&gt;Since for test cases, a 20% increase in runtime is not really a problem, tests
being fast already, I decided to add an option to DLL so that everything can be
compiled by default in hybrid model. By using a compilation flag, all the
&lt;code&gt;dbn_desc&lt;/code&gt; are becoming &lt;code&gt;dyn_dbn_desc&lt;/code&gt; and therefore each used
network is becoming a hybrid network. Without a single change in the code, the
compilation time of the entire library can be significantly improved, as seen in
the next section.  This can also be used in user code to improve compilation
time during debugging and experiments and can be turned off for the final
training.&lt;/p&gt;
&lt;p&gt;On my Continuous Integration system, I will build the system in both
configurations. This is not really an issue, since my personal machine at home
is more powerful than what I have available here.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="results"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;On a first experiment, I measured the difference before and after this change on
the three executables of the library, with gcc:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="23%"&gt;
&lt;col width="26%"&gt;
&lt;col width="26%"&gt;
&lt;col width="26%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Model&lt;/th&gt;
&lt;th class="head"&gt;Unit [s]&lt;/th&gt;
&lt;th class="head"&gt;Perf [s]&lt;/th&gt;
&lt;th class="head"&gt;Misc [s]&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Before&lt;/td&gt;
&lt;td&gt;1029&lt;/td&gt;
&lt;td&gt;192&lt;/td&gt;
&lt;td&gt;937&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;After&lt;/td&gt;
&lt;td&gt;617&lt;/td&gt;
&lt;td&gt;143&lt;/td&gt;
&lt;td&gt;619&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup&lt;/td&gt;
&lt;td&gt;40.03%&lt;/td&gt;
&lt;td&gt;25.52%&lt;/td&gt;
&lt;td&gt;33.93%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is clear that the speedups are very significant! The compilation is between
25% and 40% faster with the new option. Overall, this is a speedup of 36%!
I also noticed that the compilation takes significantly less memory than before.
Therefore, I decided to rerun the compiler benchmark on the library. In the
previous experiment, zapcc was taking so much memory that it was impossible to
use more than one thread. Let's see how it is faring now. The time to compile
the full unit tests is computed for each compiler. Let's start in debug mode:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="23%"&gt;
&lt;col width="19%"&gt;
&lt;col width="19%"&gt;
&lt;col width="19%"&gt;
&lt;col width="19%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Debug&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j3&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;clang-3.9&lt;/td&gt;
&lt;td&gt;527&lt;/td&gt;
&lt;td&gt;268&lt;/td&gt;
&lt;td&gt;182&lt;/td&gt;
&lt;td&gt;150&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;gcc-4.9.3&lt;/td&gt;
&lt;td&gt;591&lt;/td&gt;
&lt;td&gt;303&lt;/td&gt;
&lt;td&gt;211&lt;/td&gt;
&lt;td&gt;176&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;gcc-5.3.0&lt;/td&gt;
&lt;td&gt;588&lt;/td&gt;
&lt;td&gt;302&lt;/td&gt;
&lt;td&gt;209&lt;/td&gt;
&lt;td&gt;175&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc-1.0&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;375&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;187&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;126&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;121&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This time, zapcc is able to scale to four threads without problems. Moreover, it
is always the fastest compiler, by a significant margin, in this configuration.
It is followed by clang and then by gcc for which both versions are about the
same speed.&lt;/p&gt;
&lt;p&gt;If we compile again in release mode:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="24%"&gt;
&lt;col width="20%"&gt;
&lt;col width="20%"&gt;
&lt;col width="20%"&gt;
&lt;col width="16%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Release&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j3&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;clang-3.9&lt;/td&gt;
&lt;td&gt;1201&lt;/td&gt;
&lt;td&gt;615&lt;/td&gt;
&lt;td&gt;421&lt;/td&gt;
&lt;td&gt;356&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;gcc-4.9.3&lt;/td&gt;
&lt;td&gt;1041&lt;/td&gt;
&lt;td&gt;541&lt;/td&gt;
&lt;td&gt;385&lt;/td&gt;
&lt;td&gt;321&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;gcc-5.3.0&lt;/td&gt;
&lt;td&gt;1114&lt;/td&gt;
&lt;td&gt;579&lt;/td&gt;
&lt;td&gt;412&lt;/td&gt;
&lt;td&gt;348&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc-1.0&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;897&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;457&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;306&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;306&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The difference in compilation time is very large, it's twice slower to compile
with all optimizations enabled. It also takes significantly more memory. Indeed,
zapcc was not able to compile with 4 threads. Nevertheless, even the results
with three threads are better than the other compilers using four threads. zapcc
is clearly the winner again on this test, followed by gcc4-9 which is faster
than gcc-5.3 which is itself faster than clang. It seems that while clang is
better at frontend than gcc, it is slower for optimizations. Note that this may
also be an indication that clang performs more optimizations than gcc and may
not be slower.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;By using some form of type erasing to simplify the templates types at compile
time, I was able to reduce the overall compilation time of my Deep Learning
Library (DLL) by 36%. Moreover, this can be done by switching a simple
compilation flag. This also very significantly reduce the memory used during the
compilation, allowing zapcc to to compile with up to three threads, compared
with only one before. This makes zapcc the fastest compiler again on this
benchmark. Overall, this will make debugging much easier on this library and
will save me a lot of time.&lt;/p&gt;
&lt;p&gt;In the future, I plan to try to improve compilation time even more. I have a few
ideas, especially in ETL that should significantly improve the compilation time
but that will require a lot of time to implement, so that will likely have to
wait a while. In the coming days, I plan to work on the performance of DLL,
especially for stochastic gradient descent.&lt;/p&gt;
&lt;p&gt;If you want more information on DLL, you can check out the
&lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;dll Github repository&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>clang</category><category>Compilers</category><category>dll</category><category>etl</category><category>gcc</category><category>zapcc</category><guid>http://baptiste-wicht.com/posts/2017/03/partial-type-erasing-deep-learning-library-dll-improve-compilation-time.html</guid><pubDate>Wed, 15 Mar 2017 06:43:44 GMT</pubDate></item><item><title>Use clang-tidy for static analysis and integration in Sonarqube</title><link>http://baptiste-wicht.com/posts/2017/03/clang-tidy-static-analysis-integration-in-sonarqube.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;clang-tidy is an extensive linter C++. It provides a complete framework for
analysis of C++ code. Some of the checks are very simple but some of them are
very complete and most of the checks from the clang-static-analyzer are
integrated into clang-tidy.&lt;/p&gt;
&lt;div class="section" id="usage"&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;If you want to see the list of checks available on clang-tidy, you can use the
list-checks options:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_761b76656c4f4859b4e472644c366c88-1"&gt;&lt;/a&gt;clang-tidy -list-checks
&lt;/pre&gt;&lt;p&gt;You can then choose the tests you are interested in and perform an analysis of
your code. For, it is highly recommended to use a Clang compilation database,
you can have a look at Bear to generate this compilation database if you don't
have it yet. The usage of clang-tidy, is pretty simple, you set the list of
checks you want, the header on which you want to have warnings reported and the
list of source files to analyse:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_d325dc8ded614855ad6fa41e2635a5fc-1"&gt;&lt;/a&gt;clang-tidy -checks='*' -header-filter="^include" -p . src/*.cpp
&lt;/pre&gt;&lt;p&gt;You'll very likely see a lot of warnings. And you will very likely see a lot of
false positives and a lot of warnings you don't agree too. For insance, there
are a lot of warnings from the CPP Core Guidelines and the Google Guidelines
that I don't follow in my coding. You should not take the complete list of tests
as rule, you should devise your own list of what you really want to fix in your
code. If you want to disable one check X, you can use the - operation:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_497b9a7c62b04acfba8f0f6687cb1630-1"&gt;&lt;/a&gt;clang-tidy -checks='*,-X' -header-filter="^include" -p . src/*.cpp
&lt;/pre&gt;&lt;p&gt;You can also enable the checks one by one or parts of them with *:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_b5e6a1f5265547b7b57f628d95b27910-1"&gt;&lt;/a&gt;clang-tidy -checks='google-*' -header-filter="^include" -p . src/*.cpp
&lt;/pre&gt;&lt;p&gt;One problem with the clang-tidy tool is that it is utterly slow, especially if
you enable the clang-static-analyzer checks. Moreover, if you use it like it is
set before, it will only use one thread for the complete set of files. This may
not be an issue on small projects, but this will definitely be a big issue for
large projects and template-heavy code (like my ETL project). You could create
an implicit target into your Makefile to use it on each file independently and
then use the -j option of make to make them in parallel, but it not really
practical.&lt;/p&gt;
&lt;p&gt;For this, I just discovered that clang propose a Python script,
run-clang-tidy.py that does it all for us! On Gentoo, it is installed at
/usr/share/run-clang-tidy.py.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_1548b67416484a9285a69ba3a4e895d3-1"&gt;&lt;/a&gt;run-clang-tidy.py -checks='*' -header-filter="^include" -p . -j9
&lt;/pre&gt;&lt;p&gt;This will automatically run clang-tidy on each file from the compilation
database and use 9 threads to perform the checks. This is definitely much
faster. For me, this is the best way to run clang-tidy.&lt;/p&gt;
&lt;p&gt;One small point I don't like is that the script always print the list of enabled
checks. For, this I changed this line in the script:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_9bb440cf687f4277ac5ea1a52004a6c5-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;invocation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clang_tidy_binary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'-list-checks'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;with:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_58162589f8114e58a56e7f8aaaed212d-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;invocation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clang_tidy_binary&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This makes it more quiet.&lt;/p&gt;
&lt;p&gt;One thing I didn't mention is that clang-tidy is able to fix some of the errors
directly if you use the -fix option. Personally, I don't like this, but for
a large code base and a carefully selected set of checks, this could be really
useful. Note that not all the checks are automatically fixable by clang-tidy.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="results"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;I have run clang-tidy on my cpp-utils library and here some interesting results.
I have not run all the checks, here is the command I used:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_79b6ff9b0ca74aa2a0b33a392bf5c786-1"&gt;&lt;/a&gt;/usr/share/clang/run-clang-tidy.py -p . -header-filter '^include/cpp_utils' -checks='cert-*,cppcoreguidelines-*,google-*,llvm-*,misc-*,modernize-*,performance-*,readility-*,-cppcoreguidelines-pro-type-reinterpret-cast,-cppcoreguidelines-pro-bounds-pointer-arithmetic,-google-readability-namespace-comments,-llvm-namespace-comment,-llvm-include-order,-google-runtime-references' -j9 2&amp;gt;/dev/null  | /usr/bin/zgrep -v "^clang-tidy"
&lt;/pre&gt;&lt;p&gt;Let's go over some warnings I got:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_d70e5a12ad874e9b97fd6b8cb3fe033b-1"&gt;&lt;/a&gt;include/cpp_utils/assert.hpp:91:103: warning: consider replacing 'long' with 'int64' [google-runtime-int]
&lt;a name="rest_code_d70e5a12ad874e9b97fd6b8cb3fe033b-2"&gt;&lt;/a&gt;void assertion_failed_msg(const CharT* expr, const char* msg, const char* function, const char* file, long line) {
&lt;a name="rest_code_d70e5a12ad874e9b97fd6b8cb3fe033b-3"&gt;&lt;/a&gt;                                                                                                      ^
&lt;/pre&gt;&lt;p&gt;I got this one several times. It is indeed more portable to use &lt;code&gt;int64&lt;/code&gt; rather than &lt;code&gt;long&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_cbda075a6280474f8e8017c4b52c6e7d-1"&gt;&lt;/a&gt;include/cpp_utils/aligned_allocator.hpp:53:9: warning: use 'using' instead of 'typedef' [modernize-use-using]
&lt;a name="rest_code_cbda075a6280474f8e8017c4b52c6e7d-2"&gt;&lt;/a&gt;        typedef aligned_allocator&amp;lt;U, A&amp;gt; other;
&lt;a name="rest_code_cbda075a6280474f8e8017c4b52c6e7d-3"&gt;&lt;/a&gt;        ^
&lt;/pre&gt;&lt;p&gt;This one is part of the modernize checks, indicating that one should use
&lt;code&gt;using&lt;/code&gt; rather than a &lt;code&gt;typedef&lt;/code&gt; and I completely agree.&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_2105a08baac8480aa2298a5e7e03f4ea-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;include&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;cpp_utils&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;aligned_allocator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nl"&gt;hpp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nl"&gt;warning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;use&lt;/span&gt; &lt;span class="err"&gt;'&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;define&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;trivial&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="n"&gt;constructor&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;modernize&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;use&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_2105a08baac8480aa2298a5e7e03f4ea-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;aligned_allocator&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;a name="rest_code_2105a08baac8480aa2298a5e7e03f4ea-3"&gt;&lt;/a&gt;    &lt;span class="o"&gt;^&lt;/span&gt;
&lt;a name="rest_code_2105a08baac8480aa2298a5e7e03f4ea-4"&gt;&lt;/a&gt;                        &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Another one from the modernize checks that I really like. This is completely
true.&lt;/p&gt;
&lt;!-- code.:

include/cpp_utils/maybe_parallel.hpp:33:5: warning: constructors that are callable with a single argument must be marked explicit to avoid unintentional implicit conversions [google-explicit-constructor]
    thread_pool(Args... /*args*/){
    ^
    explicit --&gt;
&lt;p&gt;I don't agree that every constructor with one argument should be explicit,
sometimes you want implicit conversion. Nevertheless, this particular case is
very interesting since it is variadic, it can have one template argument and as
thus it can be implicitly converted from anything, which is pretty bad I think.&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_d31f0e6a518f4f17bd66ca0ca1cb3c6a-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;array_wrapper&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nl"&gt;cpp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nl"&gt;warning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt; &lt;span class="n"&gt;casts&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="n"&gt;discouraged&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;use&lt;/span&gt; &lt;span class="k"&gt;reinterpret_cast&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;google&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;readability&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;casting&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_d31f0e6a518f4f17bd66ca0ca1cb3c6a-2"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mem&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;malloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_d31f0e6a518f4f17bd66ca0ca1cb3c6a-3"&gt;&lt;/a&gt;                 &lt;span class="o"&gt;^&lt;/span&gt;
&lt;a name="rest_code_d31f0e6a518f4f17bd66ca0ca1cb3c6a-4"&gt;&lt;/a&gt;                 &lt;span class="k"&gt;reinterpret_cast&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;*&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;         &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;On this one, I completely agree, C-style casts should be avoided and much
clearer C++ style casts should be preferred.&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_ca51571d3f7e4de2b43363ac4fcb1531-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;home&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;wichtounet&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;cpp_utils_test&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;include&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;cpp_utils&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;aligned_allocator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nl"&gt;hpp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;126&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nl"&gt;warning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;thrown&lt;/span&gt; &lt;span class="n"&gt;exception&lt;/span&gt; &lt;span class="n"&gt;type&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;nothrow&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt; &lt;span class="n"&gt;constructible&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cert&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;err60&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;cpp&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_ca51571d3f7e4de2b43363ac4fcb1531-2"&gt;&lt;/a&gt;            &lt;span class="k"&gt;throw&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;length_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"aligned_allocator&amp;lt;T&amp;gt;::allocate() - Integer overflow."&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_ca51571d3f7e4de2b43363ac4fcb1531-3"&gt;&lt;/a&gt;                  &lt;span class="o"&gt;^&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is one of the checks I don't agree with. Even though it makes sense to
prefer exception that are nothrow copy constructible, they should be caught by
const reference anyway. Moreover, this is here an exception from the standard
library.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_0ffa6715736f426d9650ff4e2515a69a-1"&gt;&lt;/a&gt;/home/wichtounet/dev/cpp_utils_test/include/cpp_utils/aligned_allocator.hpp:141:40: warning: do not use const_cast [cppcoreguidelines-pro-type-const-cast]
&lt;a name="rest_code_0ffa6715736f426d9650ff4e2515a69a-2"&gt;&lt;/a&gt;        free((reinterpret_cast&amp;lt;void**&amp;gt;(const_cast&amp;lt;std::remove_const_t&amp;lt;T&amp;gt;*&amp;gt;(ptr)))[-1]);
&lt;a name="rest_code_0ffa6715736f426d9650ff4e2515a69a-3"&gt;&lt;/a&gt;                                       ^
&lt;/pre&gt;&lt;p&gt;In general, I agree that using const_cast should be avoided as much as possible.
But there are some cases where they make sense. In this particular case, I don't
modify the object itself but some memory before the object that is unrelated and
I initialize myself.&lt;/p&gt;
&lt;p&gt;I also had a few false positives, but overall nothing too bad. I'm quite
satisfied with the quality of the results. I'll fix these warnings in the coming
week.&lt;/p&gt;
&lt;p&gt;Integration in Sonarqube&lt;/p&gt;
&lt;p&gt;The sonar-cxx plugin just integrated support for clang-tidy in main. You need
to build the version yourself, the 0.9.8-SNAPSHOT version. You then can use
something like this in your sonar-project.properties file:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_8a343e7385a641b092a6b49e5ad74e14-1"&gt;&lt;/a&gt;sonar.cxx.clangtidy.reportPath=clang-tidy-report
&lt;/pre&gt;&lt;p&gt;and sonar-cxx will parse the results and integrate the issues in your sonar
report.&lt;/p&gt;
&lt;p&gt;Here is an example:&lt;/p&gt;
&lt;img alt="/images/sonar-cxx-clang-tidy.png" src="http://baptiste-wicht.com/images/sonar-cxx-clang-tidy.png"&gt;
&lt;p&gt;You can see two of the warnings from clang-tidy :)&lt;/p&gt;
&lt;p&gt;For now, I haven't integrate this in my Continuous Integration system because
I'm still having issues with clang-tidy and the compilation database. Because
the compilation contains absolute paths to the file and to the current
directory, it cannot be shared directly between servers. I have to find a way to
fix that so that clang-tidy can use on the other computer. I'll probably wait
till the sonar-cxx 0.9.8 version is released before integrating all this in
Sonarqube, but this is a great news for this plugin :)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;clang-tidy is C++ linter that can analyze your code and checks for hundreds of
problems in it. With it, I have found some very interesting problems in the code
of my cpp_utils library. Moreover, you can now integrate it Sonarqube by using
the sonar-cxx plugin. Since it is a bit slow, I'll probably not integrate it in
my bigger projects, but I'll integrate at least in the cpp_utils library when
sonar-cxx 0.9.8 will be released.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>projects</category><category>Sonar</category><guid>http://baptiste-wicht.com/posts/2017/03/clang-tidy-static-analysis-integration-in-sonarqube.html</guid><pubDate>Sat, 11 Mar 2017 08:54:00 GMT</pubDate></item><item><title>Disappointing zapcc performance on Deep Learning Library (DLL)</title><link>http://baptiste-wicht.com/posts/2017/03/disappointing-zapcc-performance-on-deep-learning-library-dll.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;One week ago, zapcc 1.0 was released and I've observed it to be much faster than the other
compilers in terms of compile time. This can be seen when
&lt;a class="reference external" href="http://baptiste-wicht.com/posts/2017/03/release-zapcc-10-fast-cpp-compiler.html"&gt;I tested it on my Expression Templates Library (ETL)&lt;/a&gt;. It was almost four
times faster than clang 3.9 and about 2.5 times faster than GCC.&lt;/p&gt;
&lt;p&gt;The ETL library is quite heavy to compile, but still reasonable. This is not the
case for my Deep Learning Library (DLL) where compiling all the test cases takes
a very long time. I have to admit that I have been going overboard with
templates and such and I have now to pay the price. In practice, for the users
of the library, this is not a big problem since only one or two neural networks
will be compiled (and it will take hours to train), but in the test cases, there
are hundreds of them and this is a huge pain. Anyway, enough with the ramble,
I figured it would be very good to test zapcc on it and see what I can gain from
using it.&lt;/p&gt;
&lt;p&gt;In this article, when I speak of a compiler thread, I mean an instance of the
processor, so it's really a process in the Linux world.&lt;/p&gt;
&lt;div class="section" id="results"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;However, I soon realized that I would have more issues than I thought. The first
problem is the memory consumed by zapcc. Indeed, it is based on clang and
I always had problem with huge memory consumption from clang on this library and
zapcc has even bigger memory consumption because some information is cached
between runs. The amount of memory that zapcc is able to cache can be configured
in the configuration file. By default, it can use 1.5Go of memory. When zapcc
goes over the memory limit, it simply wipes out its caches. This means that all
the gain for the next compilation will be lost, since the cache will have to be
rebuilt from scratch. This is not a hard limit for the compilation itself.
Indeed, if the compilation itself takes 3Go, it will still be able to complete
it, but it is likely that the cache will be wiped after the compilation.&lt;/p&gt;
&lt;p&gt;When I tried compiling using several threads, it soon used all my memory and
crashed. The same occurs with clang but I can still compile with 3 or 4 threads
without too much issues on this computer. The same also occurs with GCC but it
can still handle 4 or 5 threads (depending on the order of the compilation
units).&lt;/p&gt;
&lt;p&gt;The tests are performed on my desktop computer at work, which is not really
good... I have 12Go of RAM (I had to ask for extra...) and an old Sandy Bridge
processor, but at least I have an SSD (also had to ask for extra).&lt;/p&gt;
&lt;p&gt;I started with testing with only one compiler thread. For zapcc, I set the
maximum memory limit to 8Go. Even with such a limit, the zapcc server restarted
more than 10 times during the compilation of the 84 test cases. After this first
experiment, I increased the number of threads to 2 for each compiler, using 4Go
limit for zapcc. The limit is for each server and each parallel thread will
spawn a new server, so the effective limit is the number of threads times the
limit. Even with two threads, I was unable to finish a compilation with zapcc.
This is quite disappoint for me since clang is able to run with 4 threads in
parallel. Moreover, a big problem with that is that the servers are not always
killed when there is no no more memory, they just hang and use all the memory of
the computer, which is evidently really inconvenient for service processes. When
this happens with clang or gcc, the compiler simply crashes and the memory is
released and make is interrupted. Since zapcc is not able to work with more than
one thread on this computer, the results are the ones with one thread. I was
also surprised to be able to compile the library with clang and four threads,
this was not possible before clang-3.9.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="36%"&gt;
&lt;col width="17%"&gt;
&lt;col width="17%"&gt;
&lt;col width="15%"&gt;
&lt;col width="15%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j3&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;gcc-4.9.3&lt;/td&gt;
&lt;td&gt;2250.95&lt;/td&gt;
&lt;td&gt;1256.36&lt;/td&gt;
&lt;td&gt;912.67&lt;/td&gt;
&lt;td&gt;760.84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;gcc-5.3.0&lt;/td&gt;
&lt;td&gt;2305.37&lt;/td&gt;
&lt;td&gt;1279.49&lt;/td&gt;
&lt;td&gt;918.08&lt;/td&gt;
&lt;td&gt;741.38&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang-3.9&lt;/td&gt;
&lt;td&gt;2047.61&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;1102.93&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;899.13&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;730.42&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc-1.0&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;1483.73&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1483.73&lt;/td&gt;
&lt;td&gt;1483.73&lt;/td&gt;
&lt;td&gt;1483.73&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Difference against Clang&lt;/td&gt;
&lt;td&gt;-27.55%&lt;/td&gt;
&lt;td&gt;+25.69%&lt;/td&gt;
&lt;td&gt;+39.37%&lt;/td&gt;
&lt;td&gt;+50.77%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC-5.3&lt;/td&gt;
&lt;td&gt;-35.66%&lt;/td&gt;
&lt;td&gt;+13.75%&lt;/td&gt;
&lt;td&gt;+38.09%&lt;/td&gt;
&lt;td&gt;+50.03%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC-4.9&lt;/td&gt;
&lt;td&gt;-34.08%&lt;/td&gt;
&lt;td&gt;+15.30%&lt;/td&gt;
&lt;td&gt;+38.50%&lt;/td&gt;
&lt;td&gt;+48.75%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If we look at the results with only one thread, we can see that there still are
some significant improvements when using zapcc, but nowhere near as good as what
was seen in the compilation of ETL. Here, the compilation time is reduced by 34%
compared to gcc and by 27% compared to clang. This is not bad, since it is
faster than the other compilers, but I would have expected better speedups. We
can see that g++-4.9 is slightly faster than g++-5.3, but this is not really
a significant difference. I'm actually very surprised to find that clang is
faster than g++ on this experiment. On ETL, it is always very significantly
slower and before, it was also significantly slower on DLL. I was so used to
this, that I stopped using it on this project. I may have to reconsider my
position when working on this project.&lt;/p&gt;
&lt;p&gt;Let's look at the results with more than two threads. Even with two threads,
every compiler is faster than zapcc. Indeed, zapcc is slower than Clang by 25%
and slower than GCC by about 15%. If we use more threads, the other compilers
are becoming even faster and the slowdowns of zapcc are more important. When
using four threads, zapcc is about 48% slower than gcc and about 50% slower than
clang. This is really showing one big downside of zapcc that has a very large
memory consumption. When it is used to compile really heavy template code, it is
failing very early to use more processes. And even when there is enough memory,
the speedups are not as great as for relatively simpler code.&lt;/p&gt;
&lt;p&gt;One may argue that this is not a fair comparison since zapcc does not have the
same numbers of threads. However, considering that this is the best zapcc can do
on this machine, I would argue that this is a fair comparison in this limited
experimental setting. If we were to have a big machine for compilation, which
I don't have at work, the zapcc results would likely be more interesting, but in
this specific limited case, it shows that zapcc suffers from its high memory
consumption. It should also be taken into account that this experiment was done
with almost nothing else running on the machine (no browser for instance) to
have as much memory as possible available for the compilers. This is not
a common use case.  Most of the days, when I compile something, I have my
browser open, which makes a large difference in memory available, and several
other applications (but consoles and vim instances do not really consume memory
:D).&lt;/p&gt;
&lt;p&gt;This experiment made me realize that the compilation times for this library were
quickly becoming crazy. Most of the time, the complete test suite is only
compiled on my Continuous Integration machine at home which has a much faster
processor and much more RAM. Therefore, it is relatively fast since it uses more
threads to compile.  Nevertheless, this is not a good point that the unit tests
takes so much time to compile. I plan to split the test cases in several sets.
Because, currently the real unit tests are compiled with the performance tests
and other various tests. I'll probably end up generating three executables. This
will help greatly during development. Moreover, I also have a technique to
decrease the compilation time by erasing some template parameters at compilation
time. This is already ready, but has currently a runtime overhead that I will
try to remove and then use this technique everywhere to get back to reasonable
compilation times. I'll also try to see if I can find obvious compilation
bottlenecks in the code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;To conclude, while zapcc brings some very interesting compilation speedups in
some cases like in my ETL library, it also has some downsides, namely
&lt;strong&gt;huge memory consumption&lt;/strong&gt;. This memory consumption may prevent the use of several
compiler threads and render zapcc much less interesting than other compilers.&lt;/p&gt;
&lt;p&gt;When trying to compile my DLL library on a machine with 12Go of RAM with two
zapcc threads, it was impossible for me to make it complete. While zapcc was
faster with one thread than the other compilers, they were able to use up to
four threads and in the end &lt;strong&gt;zapcc was about twice slower than clang&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I knew that zapcc memory consumption was very large, but I would have not have
expected something so critical. Another feature that would be interesting in
zapcc would be to set a max memory hard limit for the server instead of simply
a limit on the cache they are able to keep in memory. This would prevent hanging
the complete computer when something goes wrong.&lt;/p&gt;
&lt;p&gt;I had a good surprise with clang that was actually faster than GCC and also able
to work with four threads in parallel. This was not the case with previous
version of clang. On ETL, it is still significantly slower than GCC though.&lt;/p&gt;
&lt;p&gt;For now, I'll continue using clang on this DLL project and use zapcc only on my
ETL project. I'll also focus on improving the compilation time on this project
and make it reasonable again.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>Compilers</category><category>dll</category><category>gcc</category><category>projects</category><category>zapcc</category><guid>http://baptiste-wicht.com/posts/2017/03/disappointing-zapcc-performance-on-deep-learning-library-dll.html</guid><pubDate>Thu, 09 Mar 2017 12:41:06 GMT</pubDate></item><item><title>Release of zapcc 1.0 - Fast C++ compiler</title><link>http://baptiste-wicht.com/posts/2017/03/release-zapcc-10-fast-cpp-compiler.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;If you remember, I recently wrote about &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html"&gt;zapcc C++ compilation speed against gcc 5.4 and clang 3.9&lt;/a&gt; in which I was comparing the beta version of zapcc against gcc and clang.&lt;/p&gt;
&lt;p&gt;I just been informed that zapcc was just released in version 1.0. I though it
was a good occasion to test it again. It will be compared against gcc-4.9,
gcc-5.3 and clang-3.9. This version is based on the trunk of clang-5.0.&lt;/p&gt;
&lt;p&gt;Again, I will use my Expression Template Library (&lt;a class="reference external" href="https://github.com/wichtounet/etl/"&gt;ETL&lt;/a&gt;) project. This is a purely header-only
library with lots of templates. I'm going to compile the full test cases. This
is a perfect example for long compilation times.&lt;/p&gt;
&lt;p&gt;The current tests are made on the last version of the library and with slightly
different parameters for compilation, therefore the absolute times are not
comparable, but the speedups should be comparable.&lt;/p&gt;
&lt;p&gt;Just like last time, I have configured zapcc to let is use 2Go RAM per caching
server, which is the maximum allowed. Moreover, I killed the servers before each
tests.&lt;/p&gt;
&lt;div class="section" id="debug-results"&gt;
&lt;h2&gt;Debug results&lt;/h2&gt;
&lt;p&gt;Let's start with a debug build, with no optimizations enabled. Every build will
use four threads. This is the equivalent of doing make -j4 debug/bin/etl_test
without the link step.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="73%"&gt;
&lt;col width="27%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt; &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.3&lt;/td&gt;
&lt;td&gt;190.09s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.3.0&lt;/td&gt;
&lt;td&gt;200.92s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9&lt;/td&gt;
&lt;td&gt;313.85&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;81.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;3.86&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC-5.3&lt;/td&gt;
&lt;td&gt;2.47&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC-4.9&lt;/td&gt;
&lt;td&gt;2.33&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The speedups are even more impressive than last time! zapcc is &lt;strong&gt;almost four
times fast than clang-3.9&lt;/strong&gt; and around &lt;strong&gt;2.5 times faster than GCC-5.3&lt;/strong&gt;.
Interestingly, we can see that gcc-5.3 is slighly slower than GCC-4.9.&lt;/p&gt;
&lt;p&gt;It seems that they have the compiler even faster!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="release-results"&gt;
&lt;h2&gt;Release results&lt;/h2&gt;
&lt;p&gt;Let's look now how the results are looking with optimizations enabled. Again,
every build will use four threads. This is the equivalent of doing make -j4
release_debug/bin/etl_test without the link step.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="75%"&gt;
&lt;col width="25%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt; &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.3&lt;/td&gt;
&lt;td&gt;252.99&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.3.0&lt;/td&gt;
&lt;td&gt;264.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9&lt;/td&gt;
&lt;td&gt;361.65&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;237.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;1.51&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC-5.3&lt;/td&gt;
&lt;td&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC-4.9&lt;/td&gt;
&lt;td&gt;1.06&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can see that this time the speedups are not as interesting as they were.
Very interestingly, it's the compiler that suffers the more from the
optimization overhead. Indeed, zapcc is three times slower in release mode than
it was in debug mode. Nevertheless, it still manages to beat the three other
compilers, by about 10% for Gcc and 50% than clang, which is already
interesting.&lt;/p&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;To conclude, we have observed that zapcc is always faster than the three
compilers tested in this experiment. Moreover, in debug mode, the speedups are
very significant, it was almost 4 times faster than clang and around 2.5 faster
than gcc.&lt;/p&gt;
&lt;p&gt;I haven't seen any problem with the tool, it's like clang and it should generate
code of the same performance, but just compile it much faster. One problem
I have with zapcc is that it is not based on an already released version of
clang but on the trunk. That means it is hard to be compare with the exact same
version of clang and it is also a risk of running into clang bugs.&lt;/p&gt;
&lt;p&gt;Although the prices have not been published yet, it is indicated on the website
that zapcc is free for non-commercial entities. Which is really great.&lt;/p&gt;
&lt;p&gt;If you want more information, you can go to the
&lt;a class="reference external" href="https://www.zapcc.com/"&gt;official website of zapcc&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>Compilers</category><category>etl</category><category>gcc</category><category>projects</category><category>zapcc</category><guid>http://baptiste-wicht.com/posts/2017/03/release-zapcc-10-fast-cpp-compiler.html</guid><pubDate>Thu, 02 Mar 2017 13:50:04 GMT</pubDate></item><item><title>Publication: CPU Performance Optimizations for RBM and CRBM</title><link>http://baptiste-wicht.com/posts/2017/02/publication-cpu-performance-optimizations-rbm-crbm.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Recently, we have published a paper about performance optimizations that may
interest you.&lt;/p&gt;
&lt;p&gt;The paper is &lt;a class="reference external" href="https://www.researchgate.net/publication/307908790_On_CPU_Performance_Optimization_of_Restricted_Boltzmann_Machine_and_Convolutional_RBM"&gt;On CPU Performance Optimizations for Restricted Boltzmann Machine and Convolutional RBM&lt;/a&gt;, published in the Proceedings of the Artificial Neural Networks and Pattern Recognition workshop (ANNPR-2016). I've presented this paper in Germany, at Ulm.&lt;/p&gt;
&lt;p&gt;Although most of the performance research going on is focused on GPU, there are
still of research laboratories that are only equipped with CPU and it remains
important to be as fast as possible on CPU. Moreover, this is something
I really like.&lt;/p&gt;
&lt;p&gt;For this publication, I have tried to make my Restricted Boltzmann Machine (RBM)
and Convolutional RBM (CRBM) implementations in my DLL library as fast as
possible.&lt;/p&gt;
&lt;p&gt;The first part of the article is about Restricted Boltzmann Machine (RBM) which
are a form of dense Artificial Neural Network (ANN). Their training is very
similar to that of the ANN with Gradient Descent. Four different network
configurations are being tested.&lt;/p&gt;
&lt;p&gt;First, mini-batch training is shown to be much faster than online training, even
when online training is performed in parallel. Once mini-batch training is used,
BLAS operations are used in order to get as much performance as possible on the
different operations, mainly the Matrix Matrix Multiplication with the use of
the GEMM operation from the Intel Math Kernel Library (MKL). Moreover, the
parallel version of the MKL is also used to get even more performance. When all
these optimizations are performed, speedups of 11 to 30 are obtained compared to
the online training, depending on the network configurations. This final version
is able  to perform one epoch of Contrastive Divergence in 4 to 15 seconds
depending on the network, for 60000 images.&lt;/p&gt;
&lt;p&gt;The second part of the article is about Convolutional Restricted Boltzmann
Machine (CRBM). This is almost the equivalent of a Convolutional Neural Network
(CNN). Again four different networks are evaluated.&lt;/p&gt;
&lt;p&gt;The main problem with CRBM is that there are no standard implementations of the
convolution operation that is really fast. Therefore, it is not possible to
simply use a BLAS library to make the computation as fast as possible. The first
optimization that was tried is to vectorize the convolutions. With this, the
speedups have been between 1.1 and 1.9 times faster. I'm not really satisfied
with these results since in fact per convolution the speedups are much better.
Moreover, I have since been able to obtain better speedups but the deadline was
too short to include them in this paper. I'll try to talk about these
improvements in more details on this blog. What is more interesting to to
parallellize the different convolutions since they are mostly independent. This
can bring a speedup of the amount of cores available on the machine. Since
convolutions are extremely memory hungry, virtual cores with Hyper Threading
generally does not help. An interesting optimization is to use a Matrix
Multiplication to compute several valid convolutions at once.  This can give an
additional speedup between 1.6 and 2.2 compared to the vectorized version. While
it is possible to use the FFT to reduce the full convolution as well, in our
experiment the images were not big enough for this to be interesting. The final
speedups are about 10 times faster with these optimizations.&lt;/p&gt;
&lt;p&gt;We have obtained pretty good and I'm happy we have been published. However, I'm
not very satisfied with these results since I've been able to get even faster
since this and when compared with other frameworks, DLL is actually quite
competitive. I'll try to publish something new in the future.&lt;/p&gt;
&lt;p&gt;If you want more information, you can have a look at the paper. If you want to
look at the code, you can have a look at my projects:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;Expression Templates Library (ETL)&lt;/a&gt;: For
the Matrix Multiplication and Convolutions&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;Deep Learning Library (DLL)&lt;/a&gt;: For the RBM
and CRBM implementations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Don't hesitate to ask any questions if you want more information :)&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>CPU</category><category>crbm</category><category>dbn</category><category>deep learning</category><category>dll</category><category>etl</category><category>Intel</category><category>Performances</category><category>publications</category><category>rbm</category><guid>http://baptiste-wicht.com/posts/2017/02/publication-cpu-performance-optimizations-rbm-crbm.html</guid><pubDate>Tue, 07 Feb 2017 16:33:33 GMT</pubDate></item><item><title>PVS-Studio on C++ Library Review</title><link>http://baptiste-wicht.com/posts/2016/12/pvs-studio-on-cpp-library-review.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;PVS-Studio is a commercial static analyzer for C, C++ and C#. It works in both
Windows and Linux.&lt;/p&gt;
&lt;p&gt;It has been a long time since I wanted to test it on my projects. I contacted
The PVS-Studio team and they gave me a temporary license so that I can test the
tool and make a review.&lt;/p&gt;
&lt;p&gt;I tried the static analyzer on my Expression Templates Library (ETL) project.
This is a heavily-templated C++ library. I tried it on Linux of course.&lt;/p&gt;
&lt;div class="section" id="usage"&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;The installation is very simple, simply untar an archive and put the executables
in your PATH (or use absolute paths). There are also some deb and rpm packages
for some distributions. You need strace to make the analyzer work, it should be
available on any Linux platform.&lt;/p&gt;
&lt;p&gt;The usage of PVS-Studio on Linux should be straightforward. First, you can use the
analyzer directly with make and it will detect the invocations of the compiler.
For instance, here is the command I used for ETL:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_100f2cdeb4d244ce8ad2f18e6d922db9-1"&gt;&lt;/a&gt;pvs-studio-analyzer trace -- make -j5 debug/bin/etl_test
&lt;/pre&gt;&lt;p&gt;Note that you can use several threads without issues, which is really great.
There does not seem to be any slowdown at this stage, probably only collecting
compiler arguments.&lt;/p&gt;
&lt;p&gt;This first step creates a strace_out file that will be used by the next stage.&lt;/p&gt;
&lt;p&gt;Once, the compilation has been analyzed, you can generate the results with the
analyze function, for which you'll need a license. Here is what I did:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_56c4b0ca639d4b8aa05f41ba03ec1ff0-1"&gt;&lt;/a&gt;pvs-studio-analyzer analyze -l ~/pvs_studio/PVS-Studio.lic -j5
&lt;/pre&gt;&lt;p&gt;Unfortunately, this didn't work for me:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_107413d534ce4b7ab28b33e72dd7ac34-1"&gt;&lt;/a&gt;No compilation units found
&lt;a name="rest_code_107413d534ce4b7ab28b33e72dd7ac34-2"&gt;&lt;/a&gt;Analysis finished in 0:00:00.00
&lt;/pre&gt;&lt;p&gt;Apparently, it's not able to use the strace_out it generated itself...&lt;/p&gt;
&lt;p&gt;Another possibility is to use the compilation database from clang to use
PVS-Studio. So I generated my compile_commands.json file again (it was not up to
date...) with &lt;a class="reference external" href="https://github.com/rizsotto/Bear"&gt;Bear&lt;/a&gt;. And then, you only
need to run the analyze step:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_c2ce4129c52247bb87ac300e4b27daeb-1"&gt;&lt;/a&gt;pvs-studio-analyzer analyze -l ~/pvs_studio/PVS-Studio.lic -j5
&lt;/pre&gt;&lt;p&gt;Make sure you have the same compiler configured than the one used to generate
the compilation database to avoid errors with compiler arguments.&lt;/p&gt;
&lt;p&gt;Unfortunately, this just printed a load of crap on my console:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_eada0536dc7e422899c74bddffd85969-1"&gt;&lt;/a&gt;(L8Pu(]-'Lo8h&amp;gt;uo(_uv(uo2(-&amp;gt;'2h_u(uo2(uvU2K h&amp;gt;'o8a=}Lkk;x[G^%cuaa8acr[VS%
&lt;a name="rest_code_eada0536dc7e422899c74bddffd85969-2"&gt;&lt;/a&gt;$ckUaoc8 c'8&amp;gt;_-o-8&amp;gt;U2cu/kau==-8&amp;gt;c-=cU2]Uf=c U2=u%c&amp;amp;kU__-&amp;gt;j}c@uvu2%cJ
&lt;a name="rest_code_eada0536dc7e422899c74bddffd85969-3"&gt;&lt;/a&gt;(L8Pu(]-'Lo8h&amp;gt;uo(_uv(uo2(-&amp;gt;'2h_u(uo2(uvU2K h&amp;gt;'o8a=}Lkk;JVJ^%cuaa8acr[VS%
&lt;a name="rest_code_eada0536dc7e422899c74bddffd85969-4"&gt;&lt;/a&gt;$ckUaoc8 c'8&amp;gt;_-o-8&amp;gt;U2cu/kau==-8&amp;gt;c-=cU2]Uf=c U2=u%c&amp;amp;kU__-&amp;gt;j}c@uvu2%cJ
&lt;a name="rest_code_eada0536dc7e422899c74bddffd85969-5"&gt;&lt;/a&gt;(L8Pu(]-'Lo8h&amp;gt;uo(_uv(uo2(-&amp;gt;'2h_u(uo2(uvU2K h&amp;gt;'o8a=}Lkk;*[G^%cuaa8acr[VS%
&lt;a name="rest_code_eada0536dc7e422899c74bddffd85969-6"&gt;&lt;/a&gt;$ckUaoc8 c'8&amp;gt;_-o-8&amp;gt;U2cu/kau==-8&amp;gt;c-=cU2]Uf=c U2=u%c&amp;amp;kU__-&amp;gt;j}c@uvu2%cJ
&lt;a name="rest_code_eada0536dc7e422899c74bddffd85969-7"&gt;&lt;/a&gt;(L8Pu(]-'Lo8h&amp;gt;uo(_uv(uo2(-&amp;gt;'2h_u(uo2(uvU2K h&amp;gt;'o8a=}Lkk;b[b^%cuaa8acr[VS%
&lt;a name="rest_code_eada0536dc7e422899c74bddffd85969-8"&gt;&lt;/a&gt;$ckUaoc8 c'8&amp;gt;_-o-8&amp;gt;U2cu/kau==-8&amp;gt;c-=cU2]Uf=c U2=u%c&amp;amp;kU__-&amp;gt;j}c@uvu2%cJ
&lt;a name="rest_code_eada0536dc7e422899c74bddffd85969-9"&gt;&lt;/a&gt;(L8Pu(]-'Lo8h&amp;gt;uo(_uv(uo2(-&amp;gt;'2h_u(uo2(uvU2K h&amp;gt;'o8a=}Lkk;[[x^%cuaa8acr[VS%
&lt;a name="rest_code_eada0536dc7e422899c74bddffd85969-10"&gt;&lt;/a&gt;$ckUaoc8 c'8&amp;gt;_-o-8&amp;gt;U2cu/kau==-8&amp;gt;c-=cU2]Uf=c U2=u%c&amp;amp;kU__-&amp;gt;j}c@uvu2%cJ
&lt;/pre&gt;&lt;p&gt;Pretty nice, isn't it ?&lt;/p&gt;
&lt;p&gt;Let's try again in a file:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_27b5c32c3fe64070845b3f6e2141c6ad-1"&gt;&lt;/a&gt;pvs-studio-analyzer analyze -o results.log -l ~/pvs_studio/PVS-Studio.lic -j5
&lt;/pre&gt;&lt;p&gt;The time is quite reasonable for the analysis, it took much less time than the
compilation time. In total, it took 88 seconds to analyze all the files. It's
much faster than the clang static analyzer.&lt;/p&gt;
&lt;p&gt;This time it worked, but the log file is not readable, you need to convert it
again:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_8820870caa0f4657983f76634a2b3944-1"&gt;&lt;/a&gt;plog-converter -t errorfile -o errors results.log
&lt;/pre&gt;&lt;p&gt;And finally, you can read the results of the analysis in the errors file.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="results"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;Overall, PVS-Studio found 236 messages in the ETL library, I was expecting more.
I also wish there was an HTML report that include the source code as well as the
error message. I had to lookup at the code for each message (you could integrate
it in vim and then use the quickfix window to do that). There are some
visualization but in things like QtCreator or LibreOffice which I don't have nor
want on my computer.&lt;/p&gt;
&lt;p&gt;Let's look at the results. For each message, I'll include the message from
PVS-Studio and the code if it's relevant.&lt;/p&gt;
&lt;p&gt;The first is about using the comma:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_d3eb86d1e7064aceb179fd3a446a5ac4-1"&gt;&lt;/a&gt;include/etl/traits.hpp:674:1: error: V521 Such expressions using the ',' operator are dangerous. Make sure the expression is correct.
&lt;a name="rest_code_d3eb86d1e7064aceb179fd3a446a5ac4-2"&gt;&lt;/a&gt;include/etl/traits.hpp:674:1: error: V685 Consider inspecting the return statement. The expression contains a comma.
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_ae832cb2caaa4cefa15d8d3f52f6d1e7-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ae832cb2caaa4cefa15d8d3f52f6d1e7-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;dimensions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;noexcept&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_ae832cb2caaa4cefa15d8d3f52f6d1e7-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;etl_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dimensions&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_ae832cb2caaa4cefa15d8d3f52f6d1e7-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Here I'm simply using the comma operand to ignore expr to avoid a warning. To
make this compile in C++11, you need to do it in one line otherwise it's not
a constexpr function. It's probably not perfect to use this construct, but there
is no problem here.&lt;/p&gt;
&lt;p&gt;There is a bunch of these, let's filter them, it remains 207 warnings. Let's
jump to the next one:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_6c0b909222584d699cc37022005569aa-1"&gt;&lt;/a&gt;include/etl/impl/blas/fft.hpp:29:1: error: V501 There are identical sub-expressions to the left and to the right of the '==' operator: (DFTI_SINGLE) == DFTI_SINGLE
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_88553320fbed4d18bfa89fd8f18ffecc-1"&gt;&lt;/a&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;fft_kernel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;complex&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;*&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;complex&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;*&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_88553320fbed4d18bfa89fd8f18ffecc-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;DFTI_DESCRIPTOR_HANDLE&lt;/span&gt; &lt;span class="n"&gt;descriptor&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_88553320fbed4d18bfa89fd8f18ffecc-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_88553320fbed4d18bfa89fd8f18ffecc-4"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;in_ptr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;const_cast&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="o"&gt;*&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;static_cast&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="o"&gt;*&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;in&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_88553320fbed4d18bfa89fd8f18ffecc-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_88553320fbed4d18bfa89fd8f18ffecc-6"&gt;&lt;/a&gt;    &lt;span class="n"&gt;DftiCreateDescriptor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;descriptor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DFTI_SINGLE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DFTI_COMPLEX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;//Specify size and precision&lt;/span&gt;
&lt;a name="rest_code_88553320fbed4d18bfa89fd8f18ffecc-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;DftiSetValue&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;descriptor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DFTI_PLACEMENT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DFTI_NOT_INPLACE&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;         &lt;span class="c1"&gt;//Out of place FFT&lt;/span&gt;
&lt;a name="rest_code_88553320fbed4d18bfa89fd8f18ffecc-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;DftiCommitDescriptor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;descriptor&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;                                   &lt;span class="c1"&gt;//Finalize the descriptor&lt;/span&gt;
&lt;a name="rest_code_88553320fbed4d18bfa89fd8f18ffecc-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;DftiComputeForward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;descriptor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;in_ptr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;                        &lt;span class="c1"&gt;//Compute the Forward FFT&lt;/span&gt;
&lt;a name="rest_code_88553320fbed4d18bfa89fd8f18ffecc-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;DftiFreeDescriptor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;descriptor&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;                                    &lt;span class="c1"&gt;//Free the descriptor&lt;/span&gt;
&lt;a name="rest_code_88553320fbed4d18bfa89fd8f18ffecc-11"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Unfortunately, the error is inside the MKL library. Here, I really don't think
it's an issue. There is pack of them. I forgot to exclude non-ETL code from the
results. Once filter from all dependencies, 137 messages remain.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_1392e7abed8841239bb335617623ff82-1"&gt;&lt;/a&gt;include/etl/eval_functors.hpp:157:1: warning: V560 A part of conditional expression is always false: !padding.
&lt;/pre&gt;&lt;p&gt;This is true, but not an issue since padding is a configuration constant that
enables the use of padding in vector and matrices. There was 27 of these at
different locations and with different configuration variables.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_60b1cb45b8a54bc6824473c6578b064f-1"&gt;&lt;/a&gt;include/etl/op/sub_view.hpp:161:1: note: V688 The 'i' function argument possesses the same name as one of the class members, which can result in a confusion.
&lt;/pre&gt;&lt;p&gt;This is again true, but not a bug in this particular case. It is still helpful and
I ended up changing these to avoid confusion. Again, there was a few of these.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_bfe6d3e3a1b14bd6939b5df6c6cadca6-1"&gt;&lt;/a&gt;etl/test/src/conv_multi_multi.cpp:23:1: error: V573 Uninitialized variable 'k' was used. The variable was used to initialize itself.
&lt;/pre&gt;&lt;p&gt;This one is in the test code:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_4b4f57b0644f4633955491fa2d79d99a-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_4b4f57b0644f4633955491fa2d79d99a-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_4b4f57b0644f4633955491fa2d79d99a-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;C_ref&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv_2d_valid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt; &lt;span class="c1"&gt;// HERE&lt;/span&gt;
&lt;a name="rest_code_4b4f57b0644f4633955491fa2d79d99a-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_4b4f57b0644f4633955491fa2d79d99a-5"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I don't see any error, k is initialized correctly to zero in the first loop.
This is a &lt;strong&gt;false positive&lt;/strong&gt; for me. There were several of these in different
places. It seems to that the use of the operator() is confusing for PVS-Studio.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_0564490853be4935a6ea3459abc6d50b-1"&gt;&lt;/a&gt;include/etl/traits.hpp:703:1: note: V659 Declarations of functions with 'rows' name differ in the 'const' keyword only, but the bodies of these functions have different composition. This is suspicious and can possibly be an error. Check lines: 693, 703.
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_ad5bb98d964a4560a96212e4326709bd-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cpp_disable_if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;decay_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;is_fast&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ad5bb98d964a4560a96212e4326709bd-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//693&lt;/span&gt;
&lt;a name="rest_code_ad5bb98d964a4560a96212e4326709bd-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;etl_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_ad5bb98d964a4560a96212e4326709bd-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_ad5bb98d964a4560a96212e4326709bd-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_ad5bb98d964a4560a96212e4326709bd-6"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cpp_enable_if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;decay_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;is_fast&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ad5bb98d964a4560a96212e4326709bd-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;noexcept&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//703&lt;/span&gt;
&lt;a name="rest_code_ad5bb98d964a4560a96212e4326709bd-8"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;etl_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_ad5bb98d964a4560a96212e4326709bd-9"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Unfortunately, this is again a &lt;strong&gt;false positive&lt;/strong&gt; because PVS-Studio failed to
recognized SFINAE and therefore the warning is wrong.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_277519ef9f834f9b9bb92cd3ab8639a5-1"&gt;&lt;/a&gt;include/etl/builder/expression_builder.hpp:345:1: note: V524 It is odd that the body of '&amp;gt;&amp;gt;=' function is fully equivalent to the body of '*=' function.
&lt;/pre&gt;&lt;p&gt;This one is interesting indeed. It is true that they are exactly because in ETL
&amp;gt;&amp;gt; is used for scalar element-wise multiplication. This is quite interesting that
PVS-Studio points that out. There was a few of these oddities but all were
normal in the library.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_8a2154c27d814a8dbc77e1c73d34458e-1"&gt;&lt;/a&gt;etl/test/src/compare.cpp:23:1: error: V501 There are identical sub-expressions to the left and to the right of the '!=' operator: a != a
&lt;/pre&gt;&lt;p&gt;Again, it is nice that PVS-Studio finds that, but this is done on purpose on the
tests to compare an object to itself. If I remove all the oddities in the test
cases, there are only 17 left in the headers. None of the warnings on the test
case was serious, but there was no more false positives either, so that's great.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_e56d4b4c9bcf4668b614fbb6764c73f7-1"&gt;&lt;/a&gt;include/etl/impl/vec/sum.hpp:92:1: error: V591 Non-void function should return a value.
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_a0c3cffaddfa4ee893363b73d1b814fc-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cpp_disable_if&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;vec_enabled&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;all_vectorizable&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;vector_mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_a0c3cffaddfa4ee893363b73d1b814fc-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;value_t&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_a0c3cffaddfa4ee893363b73d1b814fc-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_a0c3cffaddfa4ee893363b73d1b814fc-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_a0c3cffaddfa4ee893363b73d1b814fc-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_a0c3cffaddfa4ee893363b73d1b814fc-6"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unreachable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"vec::sum called with invalid parameters"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_a0c3cffaddfa4ee893363b73d1b814fc-7"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This one is interesting. It's not a false positive since indeed the function
does not return a value, but there is a __builtin_unreachable() inside the
function and it cannot be called. In my opinion, the static analyzer should be
able to handle that, but this is really a corner case.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_c1028ad3d14a4a068e65b0f7753a4ade-1"&gt;&lt;/a&gt;include/etl/sparse.hpp:148:1: note: V550 An odd precise comparison: a == 0.0. It's probably better to use a comparison with defined precision: fabs(A - B) &amp;lt; Epsilon.
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_c38b1b1bb3e442e497c09a03ed879b23-1"&gt;&lt;/a&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="nf"&gt;is_zero&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_c38b1b1bb3e442e497c09a03ed879b23-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_c38b1b1bb3e442e497c09a03ed879b23-3"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is not false, but again this is intended because of the comparison to zero
for a sparse matrix. There were 10 of these in the same class.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_48363424aa84475ebd930a0ee73503e0-1"&gt;&lt;/a&gt;include/etl/impl/blas/fft.hpp:562:1: note: V656 Variables 'a_padded', 'b_padded' are initialized through the call to the same function. It's probably an error or un-optimized code. Consider inspecting the 'etl::size(c)' expression. Check lines: 561, 562.
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_cc255293d22842cfab56dbd5e8d38c69-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;dyn_vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;complex&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a_padded&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_cc255293d22842cfab56dbd5e8d38c69-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;dyn_vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;complex&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b_padded&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;It's indeed constructed with the same size, but for me I don't think it's an
odd pattern. I would not consider that as a warning, especially since it's
a constructor and not a assignment.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_e1f3f21a58b34e8b8e99918a8b1c22c8-1"&gt;&lt;/a&gt;include/etl/dyn_base.hpp:312:1: warning: V690 The 'dense_dyn_base' class implements a copy constructor, but lacks the '=' operator. It is dangerous to use such a class.
&lt;/pre&gt;&lt;p&gt;This is again a kind of corner case in the library because it's a base class
and the assignment is different between the sub classes and not a real
assignment in the C++ sense.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_d5d1a426711c4d378b478269f87eb2f0-1"&gt;&lt;/a&gt;include/etl/impl/reduc/conv_multi.hpp:657:1: warning: V711 It is dangerous to create a local variable within a loop with a same name as a variable controlling this loop.
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_ba42a2c4716d4f76a59fcef51f50b351-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_ba42a2c4716d4f76a59fcef51f50b351-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_ba42a2c4716d4f76a59fcef51f50b351-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv_temp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_ba42a2c4716d4f76a59fcef51f50b351-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_ba42a2c4716d4f76a59fcef51f50b351-5"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is again a false positive... It really seems that PVS-Studio is not able to
handle the operator().&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_dfdb10b57449477882261441d843531e-1"&gt;&lt;/a&gt;include/etl/impl/pooling.hpp:396:1: error: V501 There are identical sub-expressions to the left and to the right of the '||' operator: P1 || P2 || P1
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_10f42106c9924cba8829daaa13537e47-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;C1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;C2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;C3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;S1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;S2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;S3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;P1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;P2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;P3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_10f42106c9924cba8829daaa13537e47-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_10f42106c9924cba8829daaa13537e47-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;o1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;C1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;P1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;S1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_10f42106c9924cba8829daaa13537e47-4"&gt;&lt;/a&gt;    &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;o2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;C2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;P2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;S2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_10f42106c9924cba8829daaa13537e47-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;o3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;C3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;P3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;S3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_10f42106c9924cba8829daaa13537e47-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_10f42106c9924cba8829daaa13537e47-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;P1&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;P2&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;P1&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Last but not least, this time, it's entirely true and it's in fact a bug in my
code! The condition should be written like this:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_3a5100065a1848f0ab50f3e4ccc0de2c-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;P1&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;P2&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;P3&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is now fixed in the master of ETL.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The installation was pretty easy, but the usage was not as easy as it could
because the first method by analyzing the build system did not work.
Fortunately, the system supports using the Clang compilation database directly
and therefore it was possible to use.&lt;/p&gt;
&lt;p&gt;Overall, it found 236 warnings on my code base (heavily templated library).
Around 50 of them were in some of the extend libraries, but I forgot to filter
them out. The quality of the results is pretty good in my opinion. It was able
to &lt;strong&gt;find a bug&lt;/strong&gt; in my implementation of pooling with padding. Unfortunately,
there was quite a few false positives, due to SFINAE, bad handling of the
operator() and no handling of __builtin_unreachable. The remaining were all
correct, but were not bug considering their usages.&lt;/p&gt;
&lt;p&gt;To conclude, I think it's a great static analyzer that is really fast compared
to other one in the market. There are a few false positives, but it's really not
bad compared to other tools and some of the messages are really great. An HTML
report including the source code would be great as well.&lt;/p&gt;
&lt;p&gt;If you want more information, you can consult
&lt;a class="reference external" href="http://www.viva64.com/en/pvs-studio/"&gt;the official site&lt;/a&gt;. There is even a way
to use it on open-source code for free, but you have to add comments on top of
each of your files.&lt;/p&gt;
&lt;p&gt;I hope it was helpful ;)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>C++14</category><category>Review</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2016/12/pvs-studio-on-cpp-library-review.html</guid><pubDate>Tue, 20 Dec 2016 08:40:12 GMT</pubDate></item><item><title>C++ Compiler benchmark on Expression Templates Library (ETL)</title><link>http://baptiste-wicht.com/posts/2016/12/cpp-compiler-benchmark-on-expression-templates-library-etl.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;script src="https://code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"&gt;&lt;/script&gt;
&lt;script src="https://code.highcharts.com/highcharts.js"&gt;&lt;/script&gt;
&lt;script src="https://code.highcharts.com/modules/exporting.js"&gt;&lt;/script&gt;&lt;p&gt;In my Expression Templates Library (ETL) project, I have a lot of template heavy
code that needs to run as fast as possible and that is quite intensive to
compile. In this post, I'm going to compare the performance of a few of the
kernels produced by different compilers. I've got GCC 5.4, GCC 6.20 and clang
3.9. I also included zapcc which is based on clang 4.0.&lt;/p&gt;
&lt;p&gt;These tests have been run on an Haswell processor. The automatic parallelization
of ETL has been turned off for these tests.&lt;/p&gt;
&lt;p&gt;Keep in mind that some of the diagrams are presented in logarithmic form.&lt;/p&gt;
&lt;div class="section" id="vector-multiplication"&gt;
&lt;h2&gt;Vector multiplication&lt;/h2&gt;
&lt;p&gt;The first kernel is a very simple one, simple element-wise multiplication of two
vectors. Nothing fancy here.&lt;/p&gt;
&lt;div id="mul_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('mul_container', {
        chart: { type: 'column' },
        title: { text: 'Element-wise Vector Multiplication' },
        xAxis: {
            categories: ['10', '100', '1000', '10000', '100000', '1000000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [0.021, 0.040, 0.215, 2.07, 32.1, 403]
        },
        {
            name: 'g++-6.2', data: [0.021, 0.037, 0.208, 2.17, 32.1, 376]
        },
        {
            name: 'clang-3.9', data: [0.027, 0.045, 0.243, 2.43, 32.7, 389]
        },
        {
            name: 'zapcc-4.0', data: [0.026, 0.047, 0.321, 2.5, 32.8, 411]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;For small vectors, clang is significantly slower than gcc-5.4 and gcc6.2. On
vectors from 100'000 elements, the speed is comparable for each compiler,
depending on the memory bandwidth. Overall, gcc-6.2 produces the fastest code
here. clang-4.0 is slightly slower than clang-3.9, but nothing dramatic.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="vector-exponentiation"&gt;
&lt;h2&gt;Vector exponentiation&lt;/h2&gt;
&lt;p&gt;The second kernel is computing the exponentials of each elements of a vector and
storing them in another vector.&lt;/p&gt;
&lt;div id="exp_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('exp_container', {
        chart: { type: 'column' },
        title: { text: 'Element-wise Vector Exponentiation' },
        xAxis: {
            categories: ['10', '100', '1000', '10000', '100000', '1000000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [0.0478, 0.137, 1.12, 9.79, 97.5, 959]
        },
        {
            name: 'g++-6.2', data: [0.0474, 0.132, 1.11, 9.71, 97, 1000]
        },
        {
            name: 'clang-3.9', data: [0.0492, 0.136, 0.959, 9.24, 92.9, 914]
        },
        {
            name: 'zapcc-4.0', data: [0.0488, 0.142, 0.952, 9.25, 91.9, 915]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;Interestingly, this time, clang versions are significantly faster for medium to
large vectors, from 1000 elements and higher, by about 5%. There is no
significant differences between the different versions of each compiler.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="matrix-matrix-multiplication"&gt;
&lt;h2&gt;Matrix-Matrix Multiplication&lt;/h2&gt;
&lt;p&gt;The next kernel I did benchmark with the matrix-matrix multiplication operation.
In that case, the kernel is hand-unrolled and vectorized.&lt;/p&gt;
&lt;div id="gemm_container_small" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;div id="gemm_container_large" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('gemm_container_small', {
        chart: { type: 'column' },
        title: { text: 'Matrix Matrix Multiplication (small)', },
        xAxis: {
            categories: ['10x10', '20x20', '40x40', '60x60', '80x80', '100x100']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [0.159, 0.815, 2.637, 13.849, 17.281, 78.903]
        },
        {
            name: 'g++-6.2', data: [0.162, 0.802, 2.431, 13.531, 17.274, 74.02]
        },
        {
            name: 'clang-3.9', data: [0.179, 1.218, 2.391, 14.981, 15.142, 61.548]
        },
        {
            name: 'zapcc-4.0', data: [0.159, 0.836, 2.712, 13.426, 15.114, 62.241]
        }
        ]
    });
    Highcharts.chart('gemm_container_large', {
        chart: { type: 'column' },
        title: { text: 'Matrix Matrix Multiplication (large)', },
        xAxis: {
            categories: ['200x200', '300x300', '400x400', '500x500', '600x600', '700x700', '800x800', '900x900', '1000x1000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [275.219, 1371, 1837, 5177, 6667, 14981, 17037, 31492, 32813]
        },
        {
            name: 'g++-6.2', data: [267.776, 1362, 1808, 5297, 6859, 15166, 15664, 30666, 33067]
        },
        {
            name: 'clang-3.9', data: [266.033, 1230, 1789, 4825, 6969, 14488, 15916, 30872, 33186]
        },
        {
            name: 'zapcc-4.0', data: [267.806, 1237, 1820, 4909, 7035, 15191, 18193, 33127, 37346]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;There are few differences between the compilers. The first thing is that for
some sizes such as 80x80 and 100x100, clang is significantly faster than GCC, by
more than 10%. The other interesting fact is that for large matrices
zapcc-clang-4.0 is always slower than clang-3.9 which is itself on par with the
two GCC versions. In my opinion, it comes from a regression in clang trunk but
it could also come from zapcc itself.&lt;/p&gt;
&lt;div id="std_gemm_container_large" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('std_gemm_container_large', {
        chart: { type: 'column' },
        title: { text: 'Matrix Matrix Multiplication (naive)', },
        xAxis: {
            categories: ['200x200', '300x300', '400x400', '500x500', '600x600', '700x700', '800x800', '900x900', '1000x1000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [1.195, 4.891, 10.467, 22.400, 33.399,
            58.401, 77.150, 121.392, 148.469]
        },
        {
            name: 'g++-6.2', data: [1.109, 4.540, 9.964, 21.359, 31.904,
            55.282, 72.690, 113.52, 143.27]
        },
        {
            name: 'clang-3.9', data: [0.893, 3.710, 7.287, 16.244, 23.920,
            43.342, 56.771, 91.870, 112.309]
        },
        {
            name: 'zapcc-4.0', data: [5.088, 16.909, 39.632, 77.194, 133.15,
            214.539, 316.01, 447.715, 612.255]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;The results are much more interesting here! First, there is a huge regression in
clang-4.0 (or in zapcc for that matter). Indeed, it is up to 6 times slower than
clang-3.9. Moreover, the clang-3.9 is always significantly faster than gcc-6.2.
Finally, there is a small improvement in gcc-6.2 compared to gcc 5.4.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fast-fourrier-transform"&gt;
&lt;h2&gt;Fast-Fourrier Transform&lt;/h2&gt;
&lt;p&gt;The following kernel is the performance of a hand-crafted Fast-Fourrier
transform implementation.&lt;/p&gt;
&lt;div id="fft_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('fft_container', {
        chart: { type: 'column' },
        title: { text: 'Fast Fourrier Transform', },
        xAxis: {
            categories: ['100', '1000', '10000', '100000', '1000000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [2.640, 27.515, 308.239, 3427.4, 41695.9]
        },
        {
            name: 'g++-6.2', data: [2.578, 26.194, 298.97, 3348.82, 40783.8]
        },
        {
            name: 'clang-3.9', data: [3.047, 30.514, 333.403, 3569.36,43860.6]
        },
        {
            name: 'zapcc-4.0', data: [3.199,33.304,317.135,4025.18,48445.3]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;On this benchmark, gcc-6.2 is the clear winner. It is significantly faster
than clang-3.9 and clang-4.0. Moreover, gcc-6.2 is also faster than gcc-5.4.
On the contrary, clang-4.0 is significantly slower than clang-3.9 except on one
configuration (10000 elements).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="d-convolution"&gt;
&lt;h2&gt;1D Convolution&lt;/h2&gt;
&lt;p&gt;This kernel is about computing the 1D valid convolution of two vectors.&lt;/p&gt;
&lt;div id="conv1_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('conv1_container', {
        chart: { type: 'column' },
        title: { text: '1D convolution (optimized)', },
        xAxis: {
            categories: ['1000x500', '2000x1000', '3000x1500', '4000x2000',
            '5000x2500', '6000x3000', '7000x3500', '8000x4000', '9000x4500',
            '10000x5000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [11.710, 41.002, 91.201, 158.178,
            248.985, 353.695, 486.676, 634.53, 867.101, 1082.62]
        },
        {
            name: 'g++-6.2', data: [9.307, 40.921, 90.327, 158.734, 248.892,
            354.582, 488.38, 636.899, 869.637, 1084.86]
        },
        {
            name: 'clang-3.9', data: [13.404, 41.409, 95.094, 162.339,
            256.143, 362.34, 498.66, 651.352, 886.465, 1092.24]
        },
        {
            name: 'zapcc-4.0', data: [13.528, 40.886, 94.473, 159.917,
            252.992, 356.63, 493.653, 640.348, 872.282, 1091.36]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;While clang-4.0 is faster than clang-3.9, it is still slightly slower than both
gcc versions. On the GCC side, there is not a lot of difference except on the
1000x500 on which gcc-6.2 is 25% faster.&lt;/p&gt;
&lt;p&gt;And here are the results with the naive implementation:&lt;/p&gt;
&lt;div id="std_conv1_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('std_conv1_container', {
        chart: { type: 'column' },
        title: { text: '1D convolution (naive)', },
        xAxis: {
            categories: ['1000x500', '2000x1000', '3000x1500', '4000x2000',
            '5000x2500', '6000x3000', '7000x3500', '8000x4000', '9000x4500',
            '10000x5000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [0.350, 1.452, 3.260, 5.823, 9.116,
            13.155, 17.922, 23.438, 29.705, 36.683]
        },
        {
            name: 'g++-6.2', data: [0.350, 1.457, 3.262, 5.823, 9.120,
            13.152, 17.922, 23.436, 29.687, 36.665]
        },
        {
            name: 'clang-3.9', data: [0.216, 0.873, 1.974, 3.517, 5.501,
            7.921, 10.793, 14.11, 17.867, 22.068]
        },
        {
            name: 'zapcc-4.0', data: [0.215, 0.873, 1.972, 3.514, 5.501,
            7.928, 10.799, 14.11, 17.879, 22.065]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;Again, on the naive version, clang is much faster than GCC on the naive, by
about 65%. This is a really large speedup.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;2D Convolution&lt;/h2&gt;
&lt;p&gt;This next kernel is computing the 2D valid convolution of two matrices&lt;/p&gt;
&lt;div id="conv2_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('conv2_container', {
        chart: { type: 'column' },
        title: { text: '2D Convolution (optimized)', },
        xAxis: {
            categories: ['100x50', '105x50', '110x55', '115x55', '120x60',
            '125x60', '130x65', '135x65', '140x70']
        },
        yAxis: {
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [327.399, 367.389, 441.457, 576.021,
            762.268, 794, 994.06, 1261.71, 1360.57]
        },
        {
            name: 'g++-6.2', data: [327.764, 367.379, 441.993, 572.241,
            761.741, 784.605, 991.717, 1266.55, 1361.59]
        },
        {
            name: 'clang-3.9', data: [330.199, 364.253, 443.483, 580.676,
            763.772, 777.39, 1000.53, 1267.75, 1375.51]
        },
        {
            name: 'zapcc-4.0', data: [339.358, 364.756, 443.807, 575.917,
            761.248, 784.695, 992.29, 1265.04, 1367.33]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;There is no clear difference between the compilers in this code. Every compiler
here has up and down.&lt;/p&gt;
&lt;p&gt;Let's look at the naive implementation of the 2D convolution (units are
milliseconds here not microseconds):&lt;/p&gt;
&lt;div id="std_conv2_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('std_conv2_container', {
        chart: { type: 'column' },
        title: { text: '2D Convolution (naive)', },
        xAxis: {
            categories: ['100x50', '105x50', '110x55', '115x55', '120x60',
            '125x60', '130x65', '135x65', '140x70']
        },
        yAxis: {
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [9.501,11.458,13.888, 16.489, 19.634,
            22.898, 27.012, 31.246, 36.269]
        },
        {
            name: 'g++-6.2', data: [9.502, 11.464, 13.903, 16.484, 19.642,
            22.994, 27.004, 31.248, 36.26]
        },
        {
            name: 'clang-3.9', data: [5.880, 7.136, 8.610, 10.226, 12.164,
            14.247, 17.024, 19.577, 22.510]
        },
        {
            name: 'zapcc-4.0', data: [5.875, 7.091, 8.661, 10.241, 12.218,
            14.302, 16.777, 19.424, 22.472]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;This time the difference is very large! Indeed, clang versions are about 60%
faster than the GCC versions! This is really impressive. Even though this does
not comes close to the optimized. It seems the vectorizer of clang is much more
efficient than the one from GCC.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id2"&gt;
&lt;h2&gt;4D Convolution&lt;/h2&gt;
&lt;p&gt;The final kernel that I'm testing is the batched 4D convolutions that is used a
lot in Deep Learning. This is not really a 4D convolution, but a large number
of 2D convolutions applied on 4D tensors.&lt;/p&gt;
&lt;div id="conv4_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('conv4_container', {
        chart: { type: 'column' },
        title: { text: '4D Convolution', },
        xAxis: {
            categories: ['2x6x3x28x16', '2x6x3x28x16', '2x6x3x28x16',
            '2x6x3x28x16', '2x6x3x28x16', '2x6x3x28x16', '2x6x3x28x16',
            '2x6x3x28x16', '2x6x3x28x16']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [0.095, 0.402, 1.083, 2.237, 3.988,
            6.474, 9.985, 14.132, 19.539]
        },
        {
            name: 'g++-6.2', data: [0.089, 0.413, 1.081, 2.224, 3.990,
            6.462, 9.815, 14.118, 19.612]
        },
        {
            name: 'clang-3.9', data: [0.090, 0.416, 1.108, 2.277, 4.077,
            6.587, 10.024, 14.359, 20.006]
        },
        {
            name: 'zapcc-4.0', data: [0.088, 0.406, 1.080, 2.237, 3.987,
            6.484, 9.827, 14.130, 19.569]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;Again, there are very small differences between each version. The best versions
are the most recent versions of the compiler gcc-6.2 and clang-4.0 on a tie.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Overall, we can see two trends in these results. First, when working with
highly-optimized code, the choice of compiler will not make a huge difference.
On these kind of kernels, gcc-6.2 tend to perform faster than the other
compilers, but only by a very slight margin, except in some cases. On the other
hand, when working with naive implementations, clang versions really did perform
much better than GCC. The clang compiled versions of the 1D and 2D convolutions
are more than 60% faster than their GCC counter parts. This is really
impressive. Overall, clang-4.0 seems to have several performance regressions,
but since it's not still a work in progress, I would not be suprised if these
regressions are not present in the final version. Since the clang-4.0 version is
in fact the clang version used by zapcc, it's also possible that zapcc is
introducing new performance regressions.&lt;/p&gt;
&lt;p&gt;Overall, my advice would be to use GCC-6.2 (or 5.4) on hand-optimized kernels
and clang when you have mostly naive implementations. However, keep in mind that
at least for the example shown here, the naive version optimized by the compiler
never comes close to the highly-optimized version.&lt;/p&gt;
&lt;p&gt;As ever, takes this with a grain of salt, it's only been tested on one project
and one machine, you may obtain very different results on other projects and on
other processors.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>Compilers</category><category>gcc</category><category>Performance</category><category>templates</category><guid>http://baptiste-wicht.com/posts/2016/12/cpp-compiler-benchmark-on-expression-templates-library-etl.html</guid><pubDate>Sun, 11 Dec 2016 13:17:30 GMT</pubDate></item><item><title>zapcc C++ compilation speed against gcc 5.4 and clang 3.9</title><link>http://baptiste-wicht.com/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;A week ago, I compared the &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html"&gt;compilation time performance of zapcc against gcc-4.9.3 and clang-3.7&lt;/a&gt;. On debug builds, zapcc was about 2 times faster than gcc and 3 times faster than clang. In this post, I'm going to try some more recent compilers, namely gcc 5.4 and clang 3.9 on the same project. If you want more information on zapcc, read the previous posts, this post will concentrate on results.&lt;/p&gt;
&lt;p&gt;Again, I use my Expression Template Library
(&lt;a class="reference external" href="https://github.com/wichtounet/etl/"&gt;ETL&lt;/a&gt;). This is a purely header-only
library with lots of templates. I'm going to compile the full test cases.&lt;/p&gt;
&lt;p&gt;The results of the two articles are not directly comparable, since they were
obtained on two different computers. The one on which the present results are
done has a less powerful and only 16Go of RAM compared to the 32Go of RAM of my
build machine. Also take into account that that the present results were
obtained on a Desktop machine, there can be some perturbations from background
tasks.&lt;/p&gt;
&lt;p&gt;Just like on the previous results, it does not help using more threads than
physical cores, therefore, the results were only computed on up to 4 cores on
this machine.&lt;/p&gt;
&lt;p&gt;The link time is not taken into account on the results.&lt;/p&gt;
&lt;div class="section" id="debug-build"&gt;
&lt;h2&gt;Debug build&lt;/h2&gt;
&lt;p&gt;Let's start with the result of the debug build.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="55%"&gt;
&lt;col width="15%"&gt;
&lt;col width="15%"&gt;
&lt;col width="15%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;469s&lt;/td&gt;
&lt;td&gt;230s&lt;/td&gt;
&lt;td&gt;130s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9&lt;/td&gt;
&lt;td&gt;710s&lt;/td&gt;
&lt;td&gt;371s&lt;/td&gt;
&lt;td&gt;218s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;214s&lt;/td&gt;
&lt;td&gt;112s&lt;/td&gt;
&lt;td&gt;66s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;3.31&lt;/td&gt;
&lt;td&gt;3.31&lt;/td&gt;
&lt;td&gt;3.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC&lt;/td&gt;
&lt;td&gt;2.19&lt;/td&gt;
&lt;td&gt;2.05&lt;/td&gt;
&lt;td&gt;1.96&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The results are almost the same as the previous test. zapcc is 3.3 times faster
to compile than Clang and around 2 times faster than GCC. It seems that GCC 5.4
is a bit faster than GCC 4.9.3 while clang 3.9 is a bit slower than clang 3.7,
but nothing terribly significant.&lt;/p&gt;
&lt;p&gt;Overall, for debug builds, zapcc can bring a very significant improvement to
your compile times.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="release-build"&gt;
&lt;h2&gt;Release build&lt;/h2&gt;
&lt;p&gt;Let's see what is the status of Release builds. Since the results are comparable
between the numbers of threads, the results here are just for one thread.&lt;/p&gt;
&lt;p&gt;This is more time consuming since a lot of optimizations are enabled and more
features from ETL are enabled as well.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="79%"&gt;
&lt;col width="21%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;782s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9&lt;/td&gt;
&lt;td&gt;960s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;640s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC&lt;/td&gt;
&lt;td&gt;1.22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;On a release build, the speedups are much less interesting. Nevertheless, they
are still significant. zapcc is still 1.2 times faster than gcc and 1.5 times
faster than clang. Then speedup against clang 3.9 is significantly higher than
it was on my experiment with clang 3.7, it's possible that clang 3.9 is slower
or simply has new optimization passes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The previous conclusion still holds with modern version of compilers: zapcc is
much faster than other compilers on Debug builds of template heavy code. More
than 3 times faster than clang-3.9 and about 2 times faster than gcc-5.4. Since
it's based on clang, there should not be any issue compiling projects that
already compile with a recent clang. Even though the speedups are less
interesting on a release build, it is still significantly, especially compared
against clang.&lt;/p&gt;
&lt;p&gt;I'm really interested in finding out what will be the pricing for zapcc once
out of the beta or if they will be able to get even faster!&lt;/p&gt;
&lt;p&gt;For the comparison with gcc 4.9.3 and clang 3.7, you can have a look at
&lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html"&gt;this article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want more information about zapcc, you can go to the
&lt;a class="reference external" href="https://www.zapcc.com/"&gt;official website of zapcc&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>Compilers</category><category>etl</category><category>gcc</category><category>meta</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html</guid><pubDate>Mon, 05 Dec 2016 17:46:09 GMT</pubDate></item><item><title>zapcc - a faster C++ compiler</title><link>http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Update: For a comparison against more modern compiler versions, you can read: &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html"&gt;zapcc C++ compilation speed against gcc 5.4 and clang 3.9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I just joined the private beta program of zapcc. Zapcc is a c++ compiler, based
on Clang which aims at being much faster than other C++ compilers. How they are
doing this is using a caching server that saves some of the compiler structures,
which should speed up compilation a lot. The private beta is free, but once the
compiler is ready, it will be a commercial compiler.&lt;/p&gt;
&lt;p&gt;Every C++ developer knows that compilation time can quickly be an issue when
programs are getting very big and especially when working with template-heavy
code.&lt;/p&gt;
&lt;p&gt;To benchmark this new compiler, I use my Expression Template Library
(&lt;a class="reference external" href="https://github.com/wichtounet/etl/"&gt;ETL&lt;/a&gt;). This is a purely header-only
library with lots of templates. There are lots of test cases which is what I'm
going to compile. I'm going to compare against Clang-3.7 and gcc-4.9.3.&lt;/p&gt;
&lt;p&gt;I have configured zapcc to let is use 2Go RAM per caching server, which is the
maximum allowed. Moreover, I killed the servers before each tests.&lt;/p&gt;
&lt;div class="section" id="debug-build"&gt;
&lt;h2&gt;Debug build&lt;/h2&gt;
&lt;p&gt;Let's start with a debug build. In that configuration, there is no optimization
going on and several of the features of the library (GPU, BLAS, ...) are
disabled. This is the fastest way to compile ETL. I gathered this result on
a 4 core, 8 threads, Intel processor, with an SSD.&lt;/p&gt;
&lt;p&gt;The following table presents the results with different number of threads and
the difference of zapcc compared to the other compilers:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="42%"&gt;
&lt;col width="11%"&gt;
&lt;col width="13%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;th class="head"&gt;-j6&lt;/th&gt;
&lt;th class="head"&gt;-j8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.3&lt;/td&gt;
&lt;td&gt;350s&lt;/td&gt;
&lt;td&gt;185s&lt;/td&gt;
&lt;td&gt;104s&lt;/td&gt;
&lt;td&gt;94s&lt;/td&gt;
&lt;td&gt;91s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.7&lt;/td&gt;
&lt;td&gt;513s&lt;/td&gt;
&lt;td&gt;271s&lt;/td&gt;
&lt;td&gt;153s&lt;/td&gt;
&lt;td&gt;145s&lt;/td&gt;
&lt;td&gt;138s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;158s&lt;/td&gt;
&lt;td&gt;87s&lt;/td&gt;
&lt;td&gt;47s&lt;/td&gt;
&lt;td&gt;44s&lt;/td&gt;
&lt;td&gt;42s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;3.24&lt;/td&gt;
&lt;td&gt;3.103&lt;/td&gt;
&lt;td&gt;3.25&lt;/td&gt;
&lt;td&gt;3.29&lt;/td&gt;
&lt;td&gt;3.28&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC&lt;/td&gt;
&lt;td&gt;2.21&lt;/td&gt;
&lt;td&gt;2.12&lt;/td&gt;
&lt;td&gt;2.21&lt;/td&gt;
&lt;td&gt;2.13&lt;/td&gt;
&lt;td&gt;2.16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The result is pretty clear! zapcc is around &lt;strong&gt;three times faster than Clang&lt;/strong&gt; and around
&lt;strong&gt;two times faster than GCC&lt;/strong&gt;. This is pretty impressive!&lt;/p&gt;
&lt;p&gt;For those that think than Clang is always faster than GCC, keep in mind that
this is not the case for template-heavy code such as this library. In all my
tests, Clang has always been slower and much memory hungrier than GCC on
template-heavy C++ code. And sometimes the difference is very significant.&lt;/p&gt;
&lt;p&gt;Interestingly, we can also see that going past the physical cores is not really
interesting on this computer. On some computer, the speedups are interesting,
but not on this one. Always benchmark!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="release-build"&gt;
&lt;h2&gt;Release build&lt;/h2&gt;
&lt;p&gt;We have seen the results on a debug build, let's now compare on something a bit
more timely, a release build with all options of ETL enabled (GPU, BLAS, ...),
which should make it significantly longer to compile.&lt;/p&gt;
&lt;p&gt;Again, the table:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="40%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;th class="head"&gt;-j6&lt;/th&gt;
&lt;th class="head"&gt;-j8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.3&lt;/td&gt;
&lt;td&gt;628s&lt;/td&gt;
&lt;td&gt;336s&lt;/td&gt;
&lt;td&gt;197s&lt;/td&gt;
&lt;td&gt;189s&lt;/td&gt;
&lt;td&gt;184s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.7&lt;/td&gt;
&lt;td&gt;663s&lt;/td&gt;
&lt;td&gt;388s&lt;/td&gt;
&lt;td&gt;215s&lt;/td&gt;
&lt;td&gt;212s&lt;/td&gt;
&lt;td&gt;205s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;515s&lt;/td&gt;
&lt;td&gt;281s&lt;/td&gt;
&lt;td&gt;173s&lt;/td&gt;
&lt;td&gt;168s&lt;/td&gt;
&lt;td&gt;158s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;1.28&lt;/td&gt;
&lt;td&gt;1.38&lt;/td&gt;
&lt;td&gt;1.24&lt;/td&gt;
&lt;td&gt;1.26&lt;/td&gt;
&lt;td&gt;1.29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC&lt;/td&gt;
&lt;td&gt;1.21&lt;/td&gt;
&lt;td&gt;1.30&lt;/td&gt;
&lt;td&gt;1.13&lt;/td&gt;
&lt;td&gt;1.12&lt;/td&gt;
&lt;td&gt;1.16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This time, we can see that the difference is much lower. Zapcc is &lt;strong&gt;between 1.2
and 1.4 times faster than Clang&lt;/strong&gt; and &lt;strong&gt;between 1.1 and 1.3 times faster than
GCC&lt;/strong&gt;. This shows that most of the speedups from zapcc are in the front end of
the compiler. This is not a lot but still significant over long builds,
especially if you have few threads where the absolute difference would be
higher.&lt;/p&gt;
&lt;p&gt;We can also observe that Clang is now almost on par with GCC which shows that
optimization is faster in Clang while front and backend is faster in gcc.&lt;/p&gt;
&lt;p&gt;You also have to keep in mind that zapcc memory usage is higher than Clang
because of all the caching. Moreover, the server are still up in between
compilations, so this memory usage stays between builds, which may not be what
you want.&lt;/p&gt;
&lt;p&gt;As for runtime, I have not seen any significant difference in performance
between the clang version and the zapcc. According to the official benchmarks
and documentation, there should not be any difference in that between zapcc and
the version of clang on which zapcc is based.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="incremental-build"&gt;
&lt;h2&gt;Incremental build&lt;/h2&gt;
&lt;p&gt;Normally, zapcc should shine at incremental building, but I was unable to show
any speedup when changing a single without killing the zapcc servers. Maybe
I did something wrong in my usage of zapcc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, we can see that zapcc is always faster than both GCC and Clang,
on my template-heavy library. Moreover, on debug builds, it is much faster than
any of the two compilers, being more than 2 times faster than GCC and more than
3 times faster than clang. This is really great. Moreover, I have not seen any
issue with the tool so far, it can seamlessly replace Clang without problem.&lt;/p&gt;
&lt;p&gt;It's a bit weird that you cannot allocate more than 2Go to the zapcc servers.&lt;/p&gt;
&lt;p&gt;For a program, that's really impressive. I hope that they are continuing the
good work and especially that this motivates other compilers to improve the
speed of compilation (especially of templates).&lt;/p&gt;
&lt;p&gt;If you want more information, you can go to the
&lt;a class="reference external" href="https://www.zapcc.com/"&gt;official website of zapcc&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>Compilers</category><category>etl</category><category>gcc</category><category>projects</category><category>zapcc</category><guid>http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html</guid><pubDate>Sat, 26 Nov 2016 12:17:50 GMT</pubDate></item><item><title>Blazing fast unit test compilation with doctest 1.1</title><link>http://baptiste-wicht.com/posts/2016/09/blazing-fast-unit-test-compilation-with-doctest-11.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;You may remember &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/06/reduce-compilation-time-by-another-16-with-catch.html"&gt;my quest for faster compilation times&lt;/a&gt;. I had made several changes to the Catch test framework macros in order to save some compilation at the expense of my test code looking a bit less nice:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_508589820f1b4718acdfb618073f3d3e-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;//Before&lt;/span&gt;
&lt;a name="rest_code_508589820f1b4718acdfb618073f3d3e-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE_EQUALS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;//After&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The first line is a little bit better, but using several optimizations, I was
able to dramatically change the compilation time of the test cases of ETL. In
the end, I don't think that the difference between the two lines justifies the
high overhead in compilation times.&lt;/p&gt;
&lt;div class="section" id="doctest"&gt;
&lt;h2&gt;doctest&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/onqtam/doctest"&gt;doctest&lt;/a&gt; is a framework quite similar to
Catch but that claims to be much lighter. I tested doctest 1.0 early on, but at
this point it was actually slower than Catch and especially slower than my
versions of the macro.&lt;/p&gt;
&lt;p&gt;Today, doctest 1.1 was released with promises of being even lighter than before
and providing several new ways of speeding up compilation. If you want the
results directly, you can take a look at the next section.&lt;/p&gt;
&lt;p&gt;First of all, this new version improved the basic macros to make expression
decomposition faster. When you use the standard REQUIRE macro, the expression is
composed by using several template techniques and operator overloading. This is
really slow to compile. By removing the need for this decomposition, the fast
Catch macros are much faster to compile.&lt;/p&gt;
&lt;p&gt;Moreover, doctest 1.1 also introduces CHECK_EQ that does not any expression
decomposition. This is close to what I did in my macros expect that it is
directly integrated into the framework and preserves all its features. It is
also possible to bypass the expression checking code by using FAST_CHECK_EQ
macro. In that case, the exceptions are not captured. Finally, a new
configuration option is introduced (DOCTEST_CONFIG_SUPER_FAST_ASSERTS) that
removes some features related to automatic debugger breaks. Since I don't use
the debugger features and I don't need to capture exception everywhere (it's
sufficient for me that the test fails completely if an exception is thrown), I'm
more than eager to use these new features.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="results"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;For evaluation, I have compiled the complete test suite of ETL, with 1 thread,
using gcc 4.9.3 with various different options, starting from Catch to doctest
1.1 with all compilation time features. Here are the results, in seconds:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="29%"&gt;
&lt;col width="12%"&gt;
&lt;col width="14%"&gt;
&lt;col width="22%"&gt;
&lt;col width="23%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Version&lt;/th&gt;
&lt;th class="head"&gt;Time&lt;/th&gt;
&lt;th class="head"&gt;VS Catch&lt;/th&gt;
&lt;th class="head"&gt;VS Fast Catch&lt;/th&gt;
&lt;th class="head"&gt;VS doctest 1.0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Catch&lt;/td&gt;
&lt;td&gt;724.22&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Fast Catch&lt;/td&gt;
&lt;td&gt;464.52&lt;/td&gt;
&lt;td&gt;-36%&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;doctest 1.0&lt;/td&gt;
&lt;td&gt;871.54&lt;/td&gt;
&lt;td&gt;+20%&lt;/td&gt;
&lt;td&gt;+87%&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;doctest 1.1&lt;/td&gt;
&lt;td&gt;614.67&lt;/td&gt;
&lt;td&gt;-16%&lt;/td&gt;
&lt;td&gt;+32%&lt;/td&gt;
&lt;td&gt;-30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;REQUIRE_EQ&lt;/td&gt;
&lt;td&gt;493.97&lt;/td&gt;
&lt;td&gt;-32%&lt;/td&gt;
&lt;td&gt;+6%&lt;/td&gt;
&lt;td&gt;-43%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;FAST_REQUIRE_EQ&lt;/td&gt;
&lt;td&gt;439.09&lt;/td&gt;
&lt;td&gt;-39%&lt;/td&gt;
&lt;td&gt;-6%&lt;/td&gt;
&lt;td&gt;-50%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;SUPER_FAST_ASSERTS&lt;/td&gt;
&lt;td&gt;411.11&lt;/td&gt;
&lt;td&gt;-43%&lt;/td&gt;
&lt;td&gt;-12%&lt;/td&gt;
&lt;td&gt;-53%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can see, doctest 1.1 is much faster to compile than doctest 1.0! This is
really great news. Moreover, it is already 16% faster than Catch. When all the
features are used, doctest is 12% faster than my stripped down versions of Catch
macros (and 43% faster than Catch standard macros). This is really cool! It
means that I don't have to do any change in the code (no need to strip macros
myself) and I can gain a lot of compilation time compared to the bare Catch
framework.&lt;/p&gt;
&lt;p&gt;I really think the author of doctest did a great job with the new version.
Although this was not of as much interest for me, there are also a lot of
other changes in the new version. You can consult the
&lt;a class="reference external" href="https://github.com/onqtam/doctest/blob/master/CHANGELOG.md"&gt;changelog&lt;/a&gt; if you want more information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Overall, doctest 1.1 is much faster to compile than doctest 1.0. Moreover, it
offers very fast macros for test assertions that are much faster to compile
than Catch versions and even faster than the versions I created myself to reduce
compilation time. I really thing this is a great advance for doctest. When
compiling with all the optimizations, doctest 1.1 saves me 50 seconds in
compilation time compared to the fast version of Catch macro and more than
5 minutes compared to the standard version of Catch macros.&lt;/p&gt;
&lt;p&gt;I'll probably start using doctest on my development machine. For now, I'll keep
Catch as well since I need it to generate the unit test reports in XML format
for Sonarqube. Once this feature appears in doctest, I'll probably drop Catch
from ETL and DLL&lt;/p&gt;
&lt;p&gt;If you need blazing fast compilation times for your unit tests, doctest 1.1 is
probably the way to go.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>Catch</category><category>Compilers</category><category>doctest</category><category>etl</category><category>gcc</category><category>Performances</category><category>Tests</category><category>time</category><guid>http://baptiste-wicht.com/posts/2016/09/blazing-fast-unit-test-compilation-with-doctest-11.html</guid><pubDate>Wed, 21 Sep 2016 19:45:13 GMT</pubDate></item><item><title>Short review of Bullseye Coverage</title><link>http://baptiste-wicht.com/posts/2016/09/short-review-of-bullseye-coverage.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;a class="reference external" href="http://www.bullseye.com/"&gt;Bullseye&lt;/a&gt; is a commercial Code Coverage analyzer.
It is fully-featured with an export to HTML, to XML and even a specific GUI to
see the application.It costs about 800$, with a renewal fee of about 200$ per
year.&lt;/p&gt;
&lt;p&gt;I'm currently using gcov and passing the results to Sonar. This works well, but
there are several problems. First, I need to use gcovr to generate the XML file,
that means two tools. Then, gcov has no way to merge coverage reports. In my
tests of ETL, I have seven different profiles being tested and I need the
overall coverage report. lcov has a merge feature but it is slow as hell (it
takes longer to merge the coverage files than to compile and run the complete
test suite seven times...). For now, I'm using a C++ program that I wrote to
combine the XML files or a Python script that does that, but neither are perfect
and it needs maintenance. Finally, it's impossible to exclude some code from the
coverage report (there is code that isn't meant to be executed (exceptional
code)). For now, I'm using yet another C++ program  that I wrote to do this from
comments in code.&lt;/p&gt;
&lt;p&gt;Bullseye does have all these feature, so I got an evaluation license online and
tried this tool and wrote a short review of it.&lt;/p&gt;
&lt;div class="section" id="usage"&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;The usage is pretty simple. You put the coverage executables in your PATH
variable and activate coverage globally. Then, we you compile, the compiler
calls will be intercepted and a coverage file will be generated. When the
compilation is done, run the program and the coverage measurements will be
filled.&lt;/p&gt;
&lt;p&gt;The coverage results can then be exported to HTML (or XML) or visualized using
the CoverageBrowser tool:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Screenshot of Bullseye main coverage view" src="http://baptiste-wicht.com/images/bullseye_view.png"&gt;
&lt;p class="caption"&gt;The main view of the Bullseye tool code coverage results&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It's a pretty good view of the coverage result. You have a breakdown by folders,
by file, by function and finally by condition. You can view directly the source
code:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Screenshot of Bullseye source code coverage view" src="http://baptiste-wicht.com/images/bullseye_source_view.png"&gt;
&lt;p class="caption"&gt;The source view of the Bullseye tool code coverage results&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you want to exclude some code from your coverage reports, you can use
a pragma:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_7c36b65c22c0406784d03e9b371463f5-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;switch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_7c36b65c22c0406784d03e9b371463f5-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;one&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_7c36b65c22c0406784d03e9b371463f5-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;two&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_7c36b65c22c0406784d03e9b371463f5-4"&gt;&lt;/a&gt;    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;three&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_7c36b65c22c0406784d03e9b371463f5-5"&gt;&lt;/a&gt;    &lt;span class="cp"&gt;#pragma BullseyeCoverage off&lt;/span&gt;
&lt;a name="rest_code_7c36b65c22c0406784d03e9b371463f5-6"&gt;&lt;/a&gt;    &lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;abort&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_7c36b65c22c0406784d03e9b371463f5-7"&gt;&lt;/a&gt;    &lt;span class="cp"&gt;#pragma BullseyeCoverage on&lt;/span&gt;
&lt;a name="rest_code_7c36b65c22c0406784d03e9b371463f5-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;So that the condition won't be set as uncovered.&lt;/p&gt;
&lt;p&gt;As for the coverage, it's pretty straightforward. For example:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_84cda93813b24dfda766f0cd48134cc1-1"&gt;&lt;/a&gt;covmerge -c -ffinal.cov sse.cov avx.cov
&lt;/pre&gt;&lt;p&gt;and it's really fast. Unfortunately, the merging is only done at the function
level, not at the statement or at the condition level. This is a bit
disappointing, especially from a commercial tool. Nevertheless, it works well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;To conclude, Bullseye seems to be a pretty good tool. It has more features than
standard gcov coverage and all features are well integrated together. I have
only covered the features I was interested in, there are plenty of other things
you can look at on the &lt;a class="reference external" href="http://www.bullseye.com/"&gt;official website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, if you don't need the extra features such as the visualizer (or use
something like Sonar for this), or the merge or code excluding, it's probably
not worth paying the price for it. In my case, since the merge is not better
than my C++ tool (both do almost the same and my tool does some basic line
coverage merging as well) and I don't need the visualizer, I won't pay the price
for it. Moreover, they don't have student or open source licensing, therefore,
I'll continue with my complicated toolchain :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>coverage</category><category>test</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2016/09/short-review-of-bullseye-coverage.html</guid><pubDate>Fri, 16 Sep 2016 11:25:44 GMT</pubDate></item><item><title>Expression Templates Library (ETL) 1.0</title><link>http://baptiste-wicht.com/posts/2016/09/expression-templates-library-etl-10.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've just released the first official version of my Expression Templates Library
(ETL for short): The version 1.0.&lt;/p&gt;
&lt;p&gt;Until now, I was using a simple rolling release model, but I think it's now time
to switch to some basic versioning. The project is now at a stable state.&lt;/p&gt;
&lt;p&gt;ETL 1.0 has the following main features:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Smart Expression Templates&lt;/li&gt;
&lt;li&gt;Matrix and vector (runtime-sized and compile-time-sized)&lt;/li&gt;
&lt;li&gt;Simple element-wise operations&lt;/li&gt;
&lt;li&gt;Reductions (sum, mean, max, ...)&lt;/li&gt;
&lt;li&gt;Unary operations (sigmoid, log, exp, abs, ...)&lt;/li&gt;
&lt;li&gt;Matrix multiplication&lt;/li&gt;
&lt;li&gt;Convolution (1D and 2D and higher variations)&lt;/li&gt;
&lt;li&gt;Max Pooling&lt;/li&gt;
&lt;li&gt;Fast Fourrier Transform&lt;/li&gt;
&lt;li&gt;Use of SSE/AVX to speed up operations&lt;/li&gt;
&lt;li&gt;Use of BLAS/MKL/CUBLAS/CUFFT/CUDNN libraries to speed up operations&lt;/li&gt;
&lt;li&gt;Symmetric matrix adapter (experimental)&lt;/li&gt;
&lt;li&gt;Sparse matrix (experimental)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="examples"&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;Here is an example of expressions in ETL:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_eefb5fb2a00d41d28de8e918c6478f88-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;fast_matrix&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_eefb5fb2a00d41d28de8e918c6478f88-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;fast_matrix&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;4.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;4.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_eefb5fb2a00d41d28de8e918c6478f88-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;fast_matrix&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_eefb5fb2a00d41d28de8e918c6478f88-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_eefb5fb2a00d41d28de8e918c6478f88-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;fast_matrix&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;2.111&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Or another I'm using in my neural networks library:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_2387a68aae8147bf9e82f75cf1fbdba9-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;In that case, the vector-matrix multiplication will be executed using a BLAS
kernel (if ETL is configured correclty) and the assignment, the sigmoid and the
addition will be automatically vectorized to use either AVX or SSE depending
on the machine.&lt;/p&gt;
&lt;p&gt;Or with a convolutional layer and a ReLU activation function:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_8a16b5922d6b4073884f725c1d6845db-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NH1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NH2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_4d_valid_flipped&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NC&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NV1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NV2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v_a&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_8a16b5922d6b4073884f725c1d6845db-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b_rep&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;h_a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This will automatically be computed either with NVIDIA CUDNN (if available) or
with optimized SSE/AVX kernels.&lt;/p&gt;
&lt;p&gt;For more information, you can take a look at the &lt;a class="reference external" href="https://github.com/wichtounet/etl/wiki"&gt;Reference&lt;/a&gt; on the wiki.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="next-version"&gt;
&lt;h2&gt;Next version&lt;/h2&gt;
&lt;p&gt;For the next version, I'll focus on several things:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Improve matrix-matrix multiplication kernels when BLAS is not available. There
is a lot of room for improvement here&lt;/li&gt;
&lt;li&gt;Complete support for symmetric matrices (currently experimental)&lt;/li&gt;
&lt;li&gt;Maybe some new adapters such as Hermitian matrices&lt;/li&gt;
&lt;li&gt;GPU improvements for some operations that can be done entirely on GPU&lt;/li&gt;
&lt;li&gt;New convolution performanceimprovements&lt;/li&gt;
&lt;li&gt;Perhaps more complete parallel support for some implementations&lt;/li&gt;
&lt;li&gt;Drop some compiler support to use full C++14 support&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="download-etl"&gt;
&lt;h2&gt;Download ETL&lt;/h2&gt;
&lt;p&gt;You can download ETL &lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;on Github&lt;/a&gt;. If you
only interested in the 1.0 version, you can look at the
&lt;a class="reference external" href="https://github.com/wichtounet/etl/releases"&gt;Releases pages&lt;/a&gt; or clone the tag
1.0. There are several branches:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;master&lt;/em&gt; Is the eternal development branch, may not always be stable&lt;/li&gt;
&lt;li&gt;&lt;em&gt;stable&lt;/em&gt; Is a branch always pointing to the last tag, no development here&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the future release, there always will tags pointing to the corresponding
commits. I'm not following the git flow way, I'd rather try to have a more
linear history with one eternal development branch, rather than an useless
develop branch or a load of other branches for releases.&lt;/p&gt;
&lt;p&gt;Don't hesitate to comment this post if you have any comment on this library or
any question. You can also open an Issue on Github if you have a problem using
this library or propose a Pull Request if you have any contribution you'd like
to make to the library.&lt;/p&gt;
&lt;p&gt;Hope this may be useful to some of you :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>Compilers</category><category>etl</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2016/09/expression-templates-library-etl-10.html</guid><pubDate>Fri, 02 Sep 2016 14:12:38 GMT</pubDate></item><item><title>Update: Thor, Thesis and Publications</title><link>http://baptiste-wicht.com/posts/2016/08/update-thor-thesis-and-publications.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Since it's been a real while since the last post I've written here, I wanted to
write a short status update.&lt;/p&gt;
&lt;p&gt;I had to serve one month in the army, which does not help at all for
productivity :P Since the update to Boost Spirit X3, I haven't worked on my
eddic compiler again, but I've switched back to my operating system project:
thor. I'm having a lot of fun with it again and it's in much better state than
before.&lt;/p&gt;
&lt;p&gt;We also have been very productive on the publication side, with four new
publications this year in various conferences. I'll update the blog when the
proceedings are published. I'll be going to ICANN 2016 and ANNPR 2016 next week
and probably to ICFHR in October. And of course, I'll go back to Meeting C++ in
November :) As for my thesis, it's finally going great, I've started writing
regularly and it's taking form!&lt;/p&gt;
&lt;div class="section" id="thor"&gt;
&lt;h2&gt;Thor&lt;/h2&gt;
&lt;p&gt;My project Thor Operating System now has much more features than before:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;64bit operating system&lt;/li&gt;
&lt;li&gt;Preemptive Multiprocessing&lt;/li&gt;
&lt;li&gt;Keyboard / Mouse driver&lt;/li&gt;
&lt;li&gt;Full ACPI support with ACPICA&lt;/li&gt;
&lt;li&gt;Read/Write ATA driver&lt;/li&gt;
&lt;li&gt;FAT32 file system support&lt;/li&gt;
&lt;li&gt;HPET/RTC/PIT drivers&lt;/li&gt;
&lt;li&gt;Basic PCI support&lt;/li&gt;
&lt;li&gt;Multi stage booting with FAT32&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since last time, I've fixed tons of bug in the system. Although there are still
some culprit, it's much more stable than before. They were a lot of bugs in the
scheduler with loads of race conditions. I hope I've working through most of
them now.&lt;/p&gt;
&lt;p&gt;I'm currently working on the network stack. I'm able to receive and send packets
using the Realtek 8139 card. I have working support for Ethernet, IP and ARP.
I'm currently working on adding ICMP support. I've come to realize that the
hardest part is not to develop the code here but to find a way to test it.
Network in Qemu is a huge pain in the ass to configure. And then, you need tools
to generate some packets or at least answer to packets send by the virtual
machine, and it's really bad... Nevertheless, it's pretty fun overall :)&lt;/p&gt;
&lt;p&gt;Aside from this, I'm also working on a window manager. I'll try to post an
update on this.&lt;/p&gt;
&lt;p&gt;You can take a look at the &lt;a class="reference external" href="https://github.com/wichtounet/thor-os"&gt;thor sources&lt;/a&gt; if you're interested.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="future"&gt;
&lt;h2&gt;Future&lt;/h2&gt;
&lt;p&gt;For the time being, I'll focus my effort on the thor project. I also have some
development to do on my home automation system: &lt;a class="reference external" href="https://github.com/wichtounet/asgard-server"&gt;asgard-server&lt;/a&gt; that I plan to finalize and deploy in a useful way this weekend in my apartment. You can also expect some updates on my deep learning library where I've started work to make it more user-friendly (kind of). I'm also still waiting on the first stable version of doctest for a new comparison with Catch.&lt;/p&gt;
&lt;p&gt;I really want to try to publish again some more posts on the blog. I'll
especially try to publish some more updates about Thor.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>deep learning</category><category>osdev</category><category>publications</category><category>thor</category><guid>http://baptiste-wicht.com/posts/2016/08/update-thor-thesis-and-publications.html</guid><pubDate>Tue, 23 Aug 2016 05:40:13 GMT</pubDate></item><item><title>eddic 1.2.4: New Boost Spirit X3 parser and minor cleanups</title><link>http://baptiste-wicht.com/posts/2016/06/eddic-124-new-boost-spirit-x3-parser-and-minor-cleanups.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;After almost 2 years, the new version of eddic (the compiler of the EDDI
programming language) is out! eddic 1.2.4&lt;/p&gt;
&lt;p&gt;I haven't worked a lot on this project in the last years, I have been busy with
my Ph.D. related projects (ETL and DLL), my operating systems, cpm, ... I've
mostly worked on the parser to test the new version of Boost Spirit: X3. This
will be described on the next section, with the other changes in the later
section.&lt;/p&gt;
&lt;div class="section" id="new-boost-spirit-x3"&gt;
&lt;h2&gt;New Boost Spirit X3&lt;/h2&gt;
&lt;p&gt;Boost Spirit X3 is a completely revamped version of Boost Spirit X3. It's aimed
at performance, both at compile-time and at runtime and uses recent features of
modern C++. It's not compatible with Boost Spirit Qi, so you'll most likely
have to rewrite a lot of stuff, in the parser, in the Abstract Syntax Tree
(AST) and in the AST passes as well.&lt;/p&gt;
&lt;p&gt;For reference, I'm using the Boost 1.59 version.&lt;/p&gt;
&lt;div class="section" id="pros"&gt;
&lt;h3&gt;Pros&lt;/h3&gt;
&lt;p&gt;Let's start with the pros.&lt;/p&gt;
&lt;p&gt;First, the runtime performance is definitely better. Parsing all my eddi test
cases and samples, &lt;em&gt;takes 42% less time than with the previous parser&lt;/em&gt;. It is
important to know that the old parser was very optimized, with moves instead of
copies and with a static lexer. You can take a look at &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2013/06/improving-eddic-boost-spirit-parser-performances.html"&gt;this post&lt;/a&gt;
to see what was necessary to optimize the old Qi. I think it's a good result
since the new grammar does not use a lexer (x3 does not support it) and does
not need these optimizations. This improvement really was my objective. I'll
try to push it farther in the future.&lt;/p&gt;
&lt;p&gt;Compile-time performance is also much better. It takes 3 times less time to
compile the new parser (1 minute to around 20 seconds). Moreover, the new
parser is now in only one file, rather than being it necessary to split it all
over the place for compile-time performance. Even though it's not really
important for me, it's still good to have :)&lt;/p&gt;
&lt;p&gt;Especially due to the performance point, I've been able to remove some code,
the lexer, the generated static lexer and the special pointers optimizations of
the AST.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cons"&gt;
&lt;h3&gt;Cons&lt;/h3&gt;
&lt;p&gt;Unfortunately, there are some disadvantages of using the new Spirit X3.&lt;/p&gt;
&lt;p&gt;First, the AST needs to be changed. For good parsing performance, you need to
use x3::variant and x3::forward_ast. This is a major pain in the ass since
x3::variant is much less practical to use than boost::variant. Almost
everything is explicit, meaning uglier code than before, in my opinion.
Moreover, you need to work around x3::forward_ast for boost::get, whereas
boost::recursive_wrapper was working better in that matter. I've had to create
my own wrapper around boost::get in order to be able to use the new tree. In my
opinion, this is clearly a regression.&lt;/p&gt;
&lt;p&gt;Secondly, although X3 was also meant to remove the need to use some hacks in
the grammar, I ended up having more hacks than before. For instance, many AST
node have a fake field in order to make X3 happy. I've still had to use the
horrible eps hack at one place. I've had to create a few more rules in order to
fix type deduction that is working differently than before (worse for me). And
for some reasons, I had to replace some expectations from the grammar to make it
parse correctly. This is a really important regression in my opinion, since it
may make the parsing slower and will make the error message less nice.&lt;/p&gt;
&lt;p&gt;The previous error handling system allowed me to track the file from which an
AST node was parsed from. Although the new error handler is a lot nicer than
the old system, it does not have this feature, so I had to work around this by
using new annotation nodes and a new global handler. Overall, it's probably a
bit worse than before, but makes for lighter AST nodes.&lt;/p&gt;
&lt;p&gt;Finally, for some reason, I haven't been able to use the debug option of the
library (lots of compile time errors). That complicated a bit the debugging of
the parser.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="spirit-x3-or-spirit-qi"&gt;
&lt;h3&gt;Spirit X3 or Spirit Qi ?&lt;/h3&gt;
&lt;p&gt;Overall, I have to say I'm a bit disappointed by Spirit X3. Even though it's
faster at runtime and faster to compile, I was really expecting less issues
with it. What I really did not like was all the changes I had to make because
of x3::variant and x3::forward_ast. Overall, I really don't think it was worth
the trouble porting my parser to Spirit X3.&lt;/p&gt;
&lt;p&gt;If you have a new project, I would still consider using Boost Spirit X3.&lt;/p&gt;
&lt;p&gt;If you have an existing parser, I would probably not advice porting it to X3.
Unless you really have issues with parsing performances (and especially if you
have not already optimized QI parser), it's probably not worth the trouble and
all the time necessary for all the changes.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="other-changes"&gt;
&lt;h2&gt;Other changes&lt;/h2&gt;
&lt;p&gt;The other changes are much more minor. First of all, I've gotten rid of CMake.
This project has really made me hate CMake. I have actually gotten rid of it on
all my projects. I'm now using plain Makefiles and having a much better time
with them. I've also replaced boost Program Options with cxxopts. It's a much
more modern approach for program options parsing. Moreover, it's much more
lightweight and it's header only. Only advantages. There also have been lots of
changes to code (still not very good quality though).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="future"&gt;
&lt;h2&gt;Future&lt;/h2&gt;
&lt;p&gt;eddic was my first real project in C++ and this can be seen in the code and the
organization. The quality of the code is really bad now that I read it again.
Some things are actually terrible :P It's probably normal since I was a
beginner in C++ at the time.&lt;/p&gt;
&lt;p&gt;For the future version of the compiler, I want to clean the code a lot more and
focus on the EDDI language adding new features. Moreover, I'll also get rid of
Boost Test Framework by using Catch (or doctest if it is ready).&lt;/p&gt;
&lt;p&gt;As for now, I'm not sure on which project I'm going to focus. Either I'll
continue working on the compiler or I'll start working again on my operating
system (thor-os) in which I was working on process concurrency (without too
much success :P). I'll probably post next updates on this post in the coming
months.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="download"&gt;
&lt;h2&gt;Download&lt;/h2&gt;
&lt;p&gt;You can find the EDDI Compiler sources on the &lt;a class="reference external" href="https://github.com/wichtounet/eddic"&gt;Github repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The version is available in the &lt;em&gt;v1.2.4&lt;/em&gt; tag available in the GitHub
repository, in the releases pages or directly in the &amp;lt;em&amp;gt;master&amp;lt;/em&amp;gt; branch.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>Boost</category><category>C++</category><category>C++14</category><category>eddic</category><guid>http://baptiste-wicht.com/posts/2016/06/eddic-124-new-boost-spirit-x3-parser-and-minor-cleanups.html</guid><pubDate>Sun, 26 Jun 2016 20:35:04 GMT</pubDate></item><item><title>Speed up compilation by 13% by simplifying Catch unit tests</title><link>http://baptiste-wicht.com/posts/2016/05/speedup-compilation-by-13-by-simplifying-unit-test-with-catch.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;In the previous two days, I've working on improving compilation time of my
project Expression Templates Library (ETL). I have been able to reduce the
compilation time of the complete test suite from 794 seconds to 764 seconds
(using only one thread). Trying to get further, I started checking what was
taking the most time in a test case when I saw that the REQUIRE calls of &lt;strong&gt;the
test library were taking a large portion of the compilation time!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I have been &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2014/07/catch-powerful-yet-simple-cpp-test-framework.html"&gt;using Catch as my test framework&lt;/a&gt;
for more than two years and it's really been great overall. It is a great tool,
header-only, fully-featured, XML reporting for Sonar, ... It really has
everything I need from a test framework.&lt;/p&gt;
&lt;p&gt;Contrary to some popular test frameworks that provides ASSERT_EQUALS,
ASSERT_GREATER and all fashion of assert macros, Catch only provides one
version: REQUIRE. For instance:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_d9072c85786c469dbe5512f8b4e97b9e-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_d9072c85786c469dbe5512f8b4e97b9e-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;5.5&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_d9072c85786c469dbe5512f8b4e97b9e-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mf"&gt;22.01f&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The left and right part are detected with some smart template and operator
overloading techniques and this makes for very nice test output in case of
errors, for instance:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_9771fd94878e4ec7a7c36583ceb81f3a-1"&gt;&lt;/a&gt;test/src/dyn_matrix.cpp:16: FAILED:
&lt;a name="rest_code_9771fd94878e4ec7a7c36583ceb81f3a-2"&gt;&lt;/a&gt;  REQUIRE( test_matrix.rows() == 2UL )
&lt;a name="rest_code_9771fd94878e4ec7a7c36583ceb81f3a-3"&gt;&lt;/a&gt;with expansion:
&lt;a name="rest_code_9771fd94878e4ec7a7c36583ceb81f3a-4"&gt;&lt;/a&gt;  3 == 2
&lt;/pre&gt;&lt;p&gt;I think this is pretty nice and the tests are really clear. However, &lt;em&gt;it comes
with a cost&lt;/em&gt; and I underestimated this at first.&lt;/p&gt;
&lt;p&gt;To overcome this, I create two new macros (and few other variations)
REQUIRE_EQUALS and REQUIRE_DIRECT that simply bypass Catch deduction of the
expression:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-1"&gt;&lt;/a&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;evaluate_result_direct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ResultBuilder&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setResultType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setLhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="s"&gt;"true"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"false"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setOp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;""&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;endExpression&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-6"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-8"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-9"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;evaluate_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ResultBuilder&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt; &lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setResultType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setLhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setRhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-13"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setOp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"=="&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-14"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;endExpression&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-15"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-17"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define REQUIRE_DIRECT(value) \&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-18"&gt;&lt;/a&gt;&lt;span class="cp"&gt;    evaluate_result_direct(Catch::ResultBuilder( "REQUIRE", CATCH_INTERNAL_LINEINFO, #value, Catch::ResultDisposition::Normal ), value);&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-19"&gt;&lt;/a&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-20"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define REQUIRE_EQUALS(lhs, rhs) \&lt;/span&gt;
&lt;a name="rest_code_b696725ab60e4ca8940ec4a5e92c5695-21"&gt;&lt;/a&gt;&lt;span class="cp"&gt;    evaluate_result(Catch::ResultBuilder( "REQUIRE", CATCH_INTERNAL_LINEINFO, #lhs " == " #rhs, Catch::ResultDisposition::Normal ), lhs, rhs);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;There is really nothing too special about it, I simply followed the macros and
functions in Catch source code until I found out what to bypass.&lt;/p&gt;
&lt;p&gt;And now, we use them directly:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_10d4b4184f3642748ffbe032e0d982bd-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE_DIRECT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;am_i_true&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_10d4b4184f3642748ffbe032e0d982bd-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE_EQUALS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is a bit less nice and it requires to know a few more macros, I admit, but
it turns out to be much faster (and who really cares about the beauty of test
code anyway...). Indeed, the total compilation time of the tests went from 764
seconds to 664 seconds!  This is &lt;strong&gt;a 13% reduction of the compilation time&lt;/strong&gt;!
I really am impressed of the overhead of this technique. I cannot justify this
slowdown just for a bit nicer test code. Finally, the output in case of error
remains exactly the same as before.&lt;/p&gt;
&lt;p&gt;This proves that sometimes the bottlenecks are not where we expect them :)&lt;/p&gt;
&lt;p&gt;If you are interested, you can find the adapted code on &lt;a class="reference external" href="https://github.com/wichtounet/etl/blob/master/test/include/fast_catch.hpp"&gt;Github&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>Tests</category><guid>http://baptiste-wicht.com/posts/2016/05/speedup-compilation-by-13-by-simplifying-unit-test-with-catch.html</guid><pubDate>Wed, 25 May 2016 10:35:16 GMT</pubDate></item><item><title>Use templight and Templar to debug C++ templates</title><link>http://baptiste-wicht.com/posts/2016/02/use-templight-and-templar-to-debug-cpp-templates.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;C++ has some very good tools to debug, profile and analyze source files and executables. This all works well for standard runtime program. But, when you are using templates, you sometimes want these tools to act at compile-time. And at this point the support is much more scarce.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/mikael-s-persson/templight"&gt;templight&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/schulmar/Templar"&gt;Templar&lt;/a&gt; and two tools that are trying to fix this issue.&lt;/p&gt;
&lt;p&gt;From the templight site:&lt;/p&gt;
&lt;blockquote&gt;
Templight is a Clang-based tool to profile the time and memory consumption of template instantiations and to perform interactive debugging sessions to gain introspection into the template instantiation process.&lt;/blockquote&gt;
&lt;p&gt;and Templar is a visualization tool for the traces generated by templight.&lt;/p&gt;
&lt;div class="section" id="installation"&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;Unfortunately, the templight installation is not user-friendly at all. You need to clone the complete LLVM/Clang tree and add templight inside it before compiling the complete clang toolchain. But that is the case for all clang-based tools... You also need to patch clang but that may not be necessary in the future. The complete instructions are available &lt;a class="reference external" href="https://github.com/mikael-s-persson/templight#getting-and-compiling-templight"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The installation of Templar is much more convenient:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_c618ee8d49ac412e90744573f871648d-1"&gt;&lt;/a&gt;git clone https://github.com/schulmar/Templar.git
&lt;a name="rest_code_c618ee8d49ac412e90744573f871648d-2"&gt;&lt;/a&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; Templar
&lt;a name="rest_code_c618ee8d49ac412e90744573f871648d-3"&gt;&lt;/a&gt;git checkout feature/templight2
&lt;a name="rest_code_c618ee8d49ac412e90744573f871648d-4"&gt;&lt;/a&gt;qmake .
&lt;a name="rest_code_c618ee8d49ac412e90744573f871648d-5"&gt;&lt;/a&gt;make
&lt;a name="rest_code_c618ee8d49ac412e90744573f871648d-6"&gt;&lt;/a&gt;sudo make install
&lt;/pre&gt;&lt;p&gt;The branch feature/templight2 has much more features than the master and should support both Qt4 and Qt5, but I have only tested it on Qt4.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example"&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Let's use the class Fibonacci function as an example:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-6"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-8"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-11"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-13"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-14"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-15"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-16"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-18"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-19"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Fibonacci&amp;lt;5&amp;gt;:"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_122d2ec066934ca78f8d142e77895cf6-20"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Nothing fancy here, we're simply printing the fifth Fibonacci number on the console.&lt;/p&gt;
&lt;p&gt;You can compile it with templight++:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_35d6d890071f4f89a83bce7f47053f1e-1"&gt;&lt;/a&gt;templight++ -Xtemplight -profiler -Xtemplight -memory -Xtemplight -ignore-system -std&lt;span class="o"&gt;=&lt;/span&gt;c++14 main.cpp
&lt;/pre&gt;&lt;p&gt;All the templight options starts with -Xtemplight and then you can use any clang++ options. This will generate a &lt;em&gt;a.memory.trace.pbf&lt;/em&gt; file in the current directory. You can then run Templar. use File &amp;gt; Open Trace to open the trace file. This should open a window of this sort:&lt;/p&gt;
&lt;img alt="/images/templar.png" class="align-center" src="http://baptiste-wicht.com/images/templar.png"&gt;
&lt;p&gt;The top-left panel contains the source code of the application, automatically
refreshed whenever you move in the template tree. In the top right, there is
the template instantiation graph. In the bottom left, you'll see a list of list
of files to be able to filter them and in the bottom right, you'll see the list
of templates events. You can sort the list of template events by duration which
is really convenient. You can then select Fibonacci&amp;lt;5&amp;gt; by double clicking it in
the list (once sorted, it should be near the top). This should give you a tree
looking something like that:&lt;/p&gt;
&lt;img alt="/images/templar_tree.png" class="align-center" src="http://baptiste-wicht.com/images/templar_tree.png"&gt;
&lt;p&gt;The edgy nodes are template instantiations and the round nodes are template
memoization. We can directly see that each instantiation was only done once. I
think this graph view is really helpful if you need to debug computation done
at compile time. You can see that that not all nodes are displayed, this is
because there is a limit on the displayed depth. Simply click on Fibonacci&amp;lt;3&amp;gt;
and the remaining nodes will be shown.&lt;/p&gt;
&lt;p&gt;I have already used this tool to find the most time-consuming templates in ETL
an DLL. This is a great tool to indicate where you should focus on improving
the template compile-time. I have also been able to find some unnecessary
instantiations that could be avoided (either with SFINAE or with refactorings).&lt;/p&gt;
&lt;p&gt;templight also contains a fully-fledged debugger for template programs, but I haven't tested it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, I would say that templight and Templar are really helping with
template debugging and profiling. There is a real lack of tools in this domain
and I hope to see more tools of this kind in the future. I hope this will help
you develop template-heavy programs or template metaprograms.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>templates</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2016/02/use-templight-and-templar-to-debug-cpp-templates.html</guid><pubDate>Mon, 08 Feb 2016 07:11:18 GMT</pubDate></item><item><title>Improve DLL and ETL Compile Time further</title><link>http://baptiste-wicht.com/posts/2016/01/improve-dll-and-etl-compile-time-further.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;For a while, the compilation time of my matrix/vector computation library (ETL), based on Expression Templates has become more and more problematic. I've already worked on this problem &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2015/06/how-i-improved-a-bit-compile-time-of-etl.html"&gt;here&lt;/a&gt; and &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2015/06/improve-etl-compile-time-with-precompiled-headers.html"&gt;there&lt;/a&gt;, using some general techniques (pragmas, precompiled headers, header removals and so on). On this post, I'll talk about two major improvements I have been able to do directly in the code.&lt;/p&gt;
&lt;div class="section" id="use-of-static-if"&gt;
&lt;h2&gt;Use of static_if&lt;/h2&gt;
&lt;p&gt;Remember &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2015/07/simulate-static_if-with-c11c14.html"&gt;static_if&lt;/a&gt; ? I was able to use it to really reduce the compile time of DLL.&lt;/p&gt;
&lt;p&gt;I wrote a script to time each test case of the DLL project to find the test cases that took the longest to compile. Once I found the best candidate, I isolated the functions that took the longest to compile. It was quite tedious and I did it by hand, primarily by commenting parts of the code and going deeper and deeper in the code. I was quite suprised to find that a single function call (template function of course ;) ) was responsible for 60% of the compilation time of my candidate test case. The function was instantiating a whole bunch of expression templates (to compute the free energy of several models). The function itself was not really optimizable, but what was really interesting is that this function was only used in some very rare cases and that these cases were known at compile-time :) This was a perfect case to use a static_if. And once the call was inside the static_if, the test case was indeed about 60% faster. &lt;strong&gt;This reduced the overall compilation time of DLL by about 30%&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;This could also of course also have been achieved by using two functions, one with the call, one empty and selected by SFINAE (Substitution Failure Is Not An Error). I prefer the statif_if version since this really shows the intent and hides SFINAE behind nicer syntax.&lt;/p&gt;
&lt;p&gt;I was also able to use static_if at other places in the DLL code to avoid instantiating some templates, but the improvements were much less dramatic (about 1% of the total compilation time). I was very lucky to find a single function that accounted for so much compile time. After some more tests, I concluded that much of the compilation time of DLL was spent compiling the Expression Templates from my ETL library so I decided to delve into ETL code directly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="removal-of-std-async"&gt;
&lt;h2&gt;Removal of std::async&lt;/h2&gt;
&lt;p&gt;The second improvement was very surprising. I was working on improving the compilation of ETL and found out that the sum and average reductions of matrices were dramatically slow, about an order of magnitude slower than standard operations on matrices. In parallel (but the two facts are linked), I also found out another weird fact when splitting a file into 10 parts (the file was comprised of 10 test cases). Compiling the 10 parts separarely (and sequentially, not multiple threads) was about 40% faster than compiling the complete file. There was no swapping so it was not a memory issue. This is not expected. Generally, it is faster to compile a big file than to compile its parts separately. The advantage of smaller files is that you can compile them in parallel and that incremental builds are faster (only compile a small part).&lt;/p&gt;
&lt;p&gt;By elimination, I found out that most of the time was spent inside the function that was dispatching in parallel the work for accumulating the sum of a matrix. Here is the function:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;Functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;AccFunctor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-2"&gt;&lt;/a&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;dispatch_1d_acc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Functor&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AccFunctor&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-6"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-7"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-9"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-10"&gt;&lt;/a&gt;            &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-11"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-15"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="nl"&gt;fut&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-16"&gt;&lt;/a&gt;            &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fut&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-17"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-18"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-19"&gt;&lt;/a&gt;        &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-20"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_791c204967e04c238376b81fd0e6c75d-21"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;There isn't anything really fancy about this function. This takes one functor that will be done in parallel and one function for accumulation.  It dispatches all the work in batch and then accumulates the results. I tried several things to optimize the compilation time of this function, but nothing worked. The line that was consuming all the time was the std::async line. This function was using std::async because the thread pool that I'm generally using does not support returning values from parallel functors. I decided to use a workaround and use my thread pool and I came out with this version:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;Functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;AccFunctor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-2"&gt;&lt;/a&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;dispatch_1d_acc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Functor&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AccFunctor&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-5"&gt;&lt;/a&gt;        &lt;span class="n"&gt;cpp&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;default_thread_pool&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-7"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-8"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-10"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;sub_functor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-11"&gt;&lt;/a&gt;            &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-12"&gt;&lt;/a&gt;        &lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-13"&gt;&lt;/a&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-14"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-15"&gt;&lt;/a&gt;            &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;do_task&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-16"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-19"&gt;&lt;/a&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-20"&gt;&lt;/a&gt;        &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wait&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-21"&gt;&lt;/a&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-22"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="nl"&gt;fut&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-23"&gt;&lt;/a&gt;            &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fut&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-24"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-25"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-26"&gt;&lt;/a&gt;        &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-27"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_296c3fc46a0c417c929fbee362b4ab9b-28"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I simply preallocate space for all the threads and create a new functor calling the input functor and saving its result inside the vector. It is less nice, but it works well. And it compiles MUCH faster. This &lt;strong&gt;reduced the compilation time&lt;/strong&gt; of my biggest test case &lt;strong&gt;by a factor of 8&lt;/strong&gt; (from 344 seconds to 44 seconds). This is really crazy. It also fixed the problem where splitting the test case was faster than big file (it is now twice faster to compile the big files than compiling all the small files separately). &lt;strong&gt;This reduced the total compilation time of dll by about 400%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As of now, I still have no idea why this makes such a big difference. I have looked at the std::async code, but I haven't found a valid reason for this slowdown. If someone has any idea, I'd be very glad to discuss in the comments below.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="improving-the-template-instantiation-tree"&gt;
&lt;h2&gt;Improving the template instantiation tree&lt;/h2&gt;
&lt;p&gt;I recently discovered the templight tool that is a profiler for templates (pretty cool). After some time, I was able to build it and use it on ETL. For now, I haven't been able to reduce compile time a lot, but I have been able to reduce the template instantiation tree a lot seeing that some instantiations were completely useless and I optimized the code to remove them.&lt;/p&gt;
&lt;p&gt;I won't be go into much details here because I plan to write a post on this subject in the coming days.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, I would say that it is pretty hard to improve the compile time of complex C++ programs once you have gone through all the standard methods. However, I was very happy to found that &lt;strong&gt;two optimizations in the source code reduced the overall compilation of DLL by almost 500%&lt;/strong&gt;. I will continue working on this, but for now, the compilation time is much more reasonable.&lt;/p&gt;
&lt;p&gt;I hope the two main facts in this article were interesting. If you have similar experience, comments or ideas for further improvements, I'd be glad to discuss them with you in the comments :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>Compilers</category><category>dll</category><category>etl</category><category>gcc</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2016/01/improve-dll-and-etl-compile-time-further.html</guid><pubDate>Fri, 29 Jan 2016 16:02:34 GMT</pubDate></item><item><title>Simulate static_if with C++11/C++14</title><link>http://baptiste-wicht.com/posts/2015/07/simulate-static_if-with-c11c14.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;If you are doing a lot of template metaprogramming and other template magic stuff, you are likely to miss a &lt;code&gt;static_if&lt;/code&gt; in the language. Unfortunately, it didn't make the cut for C++11 and it seems unlikely that it will make it in C++17.&lt;/p&gt;
&lt;div class="section" id="static-if"&gt;
&lt;h2&gt;static_if&lt;/h2&gt;
&lt;p&gt;As its name indicates, &lt;code&gt;static_if&lt;/code&gt; is an if statement but that is done at compile-time. At first, it could seem that the main point is performance, but that is not the case. With recent compilers, if you have an if statement with a compile-time constant, it will never be executed at runtime and only the correct branch will be included in the final executable code. However, even if the compiler knows that a branch will never be executed, it still has to ensure that this branch compiles. This is not the case with &lt;code&gt;static_if&lt;/code&gt;. With &lt;code&gt;static_if&lt;/code&gt;, only the valid branch is compiled, the other can contains invalid code. The most common reason to use a &lt;code&gt;static_if&lt;/code&gt; is inside a template where you perform a test on a template argument and execute code based on this test. &lt;code&gt;static_if&lt;/code&gt; has another advantage on standard if. Since only one branch is instantiated, it may save quite a lot of compile-time.&lt;/p&gt;
&lt;p&gt;Let's say we have to write a template function that, if the template argument is a string, removes the last character of the string argument, otherwise decrement the argument (I know, stupid example, but simple). With &lt;code&gt;static_if&lt;/code&gt;, you can write it like this:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_82549665ec97469da559df4db5c1d105-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_82549665ec97469da559df4db5c1d105-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_82549665ec97469da559df4db5c1d105-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;static_if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_82549665ec97469da559df4db5c1d105-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop_back&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_82549665ec97469da559df4db5c1d105-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_82549665ec97469da559df4db5c1d105-6"&gt;&lt;/a&gt;        &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_82549665ec97469da559df4db5c1d105-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_82549665ec97469da559df4db5c1d105-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I think it is quite elegant.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-problem"&gt;
&lt;h2&gt;The problem&lt;/h2&gt;
&lt;p&gt;Some may think, that we could do the same with C++ standard if statement:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_60dc3782fabe472b89c994c1bc5c0591-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_60dc3782fabe472b89c994c1bc5c0591-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_60dc3782fabe472b89c994c1bc5c0591-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_60dc3782fabe472b89c994c1bc5c0591-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop_back&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_60dc3782fabe472b89c994c1bc5c0591-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_60dc3782fabe472b89c994c1bc5c0591-6"&gt;&lt;/a&gt;        &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_60dc3782fabe472b89c994c1bc5c0591-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_60dc3782fabe472b89c994c1bc5c0591-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;However, this won't work. This template cannot be instantiated for &lt;code&gt;std::string&lt;/code&gt; since it doesn't have an operator -- and it cannot be instantiated for int since it doesn't have a &lt;code&gt;pop_back()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;There are two solutions in plain C++: specialization and SFINAE. Let's start with specialization:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_9112315ec22c4afe89dc11ce8b30cd00-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_9112315ec22c4afe89dc11ce8b30cd00-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_9112315ec22c4afe89dc11ce8b30cd00-3"&gt;&lt;/a&gt;    &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_9112315ec22c4afe89dc11ce8b30cd00-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_9112315ec22c4afe89dc11ce8b30cd00-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_9112315ec22c4afe89dc11ce8b30cd00-6"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_9112315ec22c4afe89dc11ce8b30cd00-7"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_9112315ec22c4afe89dc11ce8b30cd00-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop_back&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_9112315ec22c4afe89dc11ce8b30cd00-9"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;We do a specialization for &lt;code&gt;std::string&lt;/code&gt; case so that in the general case it uses -- and in the &lt;code&gt;std::string&lt;/code&gt; case, it uses &lt;code&gt;pop_back()&lt;/code&gt;. And the SFINAE version:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_ccc563850acb47dfb095dedc5ad23b5e-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;enable_if_t&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;!&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ccc563850acb47dfb095dedc5ad23b5e-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_ccc563850acb47dfb095dedc5ad23b5e-3"&gt;&lt;/a&gt;    &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_ccc563850acb47dfb095dedc5ad23b5e-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_ccc563850acb47dfb095dedc5ad23b5e-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_ccc563850acb47dfb095dedc5ad23b5e-6"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;enable_if_t&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ccc563850acb47dfb095dedc5ad23b5e-7"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_ccc563850acb47dfb095dedc5ad23b5e-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop_back&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_ccc563850acb47dfb095dedc5ad23b5e-9"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The first function is enabled when the type is not a &lt;code&gt;std::string&lt;/code&gt; and the second function is enabled when the type is a &lt;code&gt;std::string&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Both solutions needs two functions to make it work. In this particular case, specialization is easier since the condition states exactly one type. If the condition was more complex for instance testing that a constant inside the type is equals to some value, we could only do it with SFINAE.&lt;/p&gt;
&lt;p&gt;Even if both solutions work, both solutions are more complicated than the static_if version and both solutions are creating more functions than what should be necessary.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="one-solution"&gt;
&lt;h2&gt;One solution&lt;/h2&gt;
&lt;p&gt;There is one way to emulate a kind of &lt;code&gt;static_if&lt;/code&gt; with C++14 generic lambdas. It is kind of using anonymous template function to emulate what we did with the previous solutions but does it behind the scene. Here the code I'm using for this emulation:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;namespace&lt;/span&gt; &lt;span class="n"&gt;static_if_detail&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;identity&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-4"&gt;&lt;/a&gt;    &lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="k"&gt;operator&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-6"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-10"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;Cond&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-11"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;statement&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-13"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-14"&gt;&lt;/a&gt;        &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;identity&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-15"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-17"&gt;&lt;/a&gt;    &lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-18"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;else_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="p"&gt;){}&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-19"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-20"&gt;&lt;/a&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-21"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-22"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;statement&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-23"&gt;&lt;/a&gt;    &lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-24"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="p"&gt;){}&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-25"&gt;&lt;/a&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-26"&gt;&lt;/a&gt;    &lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-27"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;else_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-28"&gt;&lt;/a&gt;        &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;identity&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-29"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-30"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-31"&gt;&lt;/a&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-32"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="c1"&gt;//end of namespace static_if_detail&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-33"&gt;&lt;/a&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-34"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;Cond&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-35"&gt;&lt;/a&gt;&lt;span class="n"&gt;static_if_detail&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;statement&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Cond&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;static_if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-36"&gt;&lt;/a&gt;    &lt;span class="n"&gt;static_if_detail&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;statement&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Cond&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;if_&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-37"&gt;&lt;/a&gt;    &lt;span class="n"&gt;if_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-38"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;if_&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_9c6671fdfb2546aea154cc0ac08a8dce-39"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Note: I got the idea (and most of the code) from the &lt;a class="reference external" href="http://lists.boost.org/Archives/boost/2014/08/216607.php"&gt;Boost Mailing List&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The condition is passed a non-type template parameter and the code for the branch is a passed a generic lambda functor. The &lt;code&gt;static_if&lt;/code&gt; function returns a statement structure. We could avoid returning a struct and directly execute, or not, the functor based on the condition, but using a structure allows for the &lt;code&gt;else_&lt;/code&gt; part which may be practical. The structure &lt;code&gt;statement&lt;/code&gt; is specialized on the condition. If the condition is true, the right part will execute the functor while the false part will not execute anything. The specialization when the condition is false willl do the contrary. A special point here is the use of the identity function. The function is passed to the lambda. The user can then use this function to make non-dependent type dependent. This is necessary if we want to call functions on non-dependent types and these functions may not exist. For instance, you may want to call a function on &lt;code&gt;this&lt;/code&gt;, which is not a dependent type.&lt;/p&gt;
&lt;p&gt;Here is how the code will look using this solution:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_524821a4927b4df28fcd07b40f8628ca-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_524821a4927b4df28fcd07b40f8628ca-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_524821a4927b4df28fcd07b40f8628ca-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;static_if&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_524821a4927b4df28fcd07b40f8628ca-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;pop_back&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_524821a4927b4df28fcd07b40f8628ca-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}).&lt;/span&gt;&lt;span class="n"&gt;else_&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_524821a4927b4df28fcd07b40f8628ca-6"&gt;&lt;/a&gt;        &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_524821a4927b4df28fcd07b40f8628ca-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_524821a4927b4df28fcd07b40f8628ca-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;It is not as elegant as the "real" &lt;code&gt;static_if&lt;/code&gt; version, but it is closer than the other solutions.&lt;/p&gt;
&lt;p&gt;If you don't use the lazy identity function (f), it still works on g++, but not on clang for some reasons.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We saw that there are some solutions to emulate &lt;code&gt;static_if&lt;/code&gt; in C++ that you may use to make the code easier to read. I'm personally using this trick on branches with few lines of code and when I don't have to use the identity function too much, otherwise it is cleaner to use standard SFINAE functions to do the job. When you only have a if and no else, this trick is even better because that is where it saves the more code.&lt;/p&gt;
&lt;p&gt;I hope this can be useful to some of you ;)&lt;/p&gt;
&lt;p&gt;You can find my implementation &lt;a class="reference external" href="https://github.com/wichtounet/cpp_utils/blob/master/static_if.hpp"&gt;on Github&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>C++14</category><guid>http://baptiste-wicht.com/posts/2015/07/simulate-static_if-with-c11c14.html</guid><pubDate>Sun, 12 Jul 2015 13:23:34 GMT</pubDate></item></channel></rss>