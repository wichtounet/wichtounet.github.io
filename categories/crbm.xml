<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog blog("Baptiste Wicht"); (Posts about crbm)</title><link>https://baptiste-wicht.com/</link><description></description><atom:link href="https://baptiste-wicht.com/categories/crbm.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sun, 10 Sep 2023 05:33:37 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>My thesis is available: Deep Learning Feature Extraction for Image Processing</title><link>https://baptiste-wicht.com/posts/2018/01/my-thesis-is-available-deep-learning-feature-extraction-for-image-processing.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;p&gt;I'm happy to say that I've finally put my thesis online and updated
my &lt;a class="reference external" href="https://baptiste-wicht.com/stories/publications.html"&gt;Publications&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;I should have done that earlier but it slipped my mind, so there it is!&lt;/p&gt;
&lt;p&gt;My thesis (Deep Learning Feature Extraction for Image Processing) is now
available to download.  Here is the abstract of the thesis:&lt;/p&gt;
&lt;p&gt;In this thesis, we propose to use methodologies that automatically learn how to
extract relevant features from images. We are especially interested in
evaluating how these features compare against handcrafted features. More
precisely, we are interested in the unsupervised training that is used for the
Restricted Boltzmann Machine (RBM) and Convolutional RBM (CRBM) models. These
models relaunched the Deep Learning interest of the last decade. During the time
of this thesis, the auto-encoders approach, especially Convolutional
Auto-Encoders (CAE) have been used more and more. Therefore, one objective of
this thesis is also to compare the CRBM approach with the CAE approach.&lt;/p&gt;
&lt;p&gt;The scope of this work is defined by several machine learning tasks. The first
one, handwritten digit recognition, is analysed to see how much the unsupervised
pretraining technique introduced with the Deep Belief Network (DBN) model
improves the training of neural networks. The second, detection and recognition
of Sudoku in images, is evaluating the efficiency of DBN and Convolutional DBN
(CDBN) models for classification of images of poor quality. Finally, features
are learned fully unsupervised from images for a keyword spotting task and are
compared against well-known handcrafted features. Moreover, the thesis was also
oriented around a software engineering axis. Indeed, a complete machine learning
framework was developed during this thesis to explore possible optimizations and
possible algorithms in order to train the tested models as fast as possible.&lt;/p&gt;
&lt;p&gt;If you are interested, you can:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://www.researchgate.net/publication/322505397_Deep_Learning_feature_Extraction_for_Image_Processing"&gt;Read it on ResearchGate&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/publication_store/phd_thesis.pdf"&gt;Directly download the PDF&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I hope this will interest a few of you! As always, if you have any question,
don't hesitate to let me a comment ;)&lt;/p&gt;
&lt;p&gt;As for the current projects, I'm still currently working on the next version of
budgetwarrior, but I don't have any expected release date. It will depend on
much time I'm able to put to the project.&lt;/p&gt;</description><category>crbm</category><category>dll</category><category>Machine Learning</category><category>Personal</category><category>projects</category><category>publications</category><category>rbm</category><guid>https://baptiste-wicht.com/posts/2018/01/my-thesis-is-available-deep-learning-feature-extraction-for-image-processing.html</guid><pubDate>Mon, 15 Jan 2018 14:11:57 GMT</pubDate></item><item><title>Publication: CPU Performance Optimizations for RBM and CRBM</title><link>https://baptiste-wicht.com/posts/2017/02/publication-cpu-performance-optimizations-rbm-crbm.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;p&gt;Recently, we have published a paper about performance optimizations that may
interest you.&lt;/p&gt;
&lt;p&gt;The paper is &lt;a class="reference external" href="https://www.researchgate.net/publication/307908790_On_CPU_Performance_Optimization_of_Restricted_Boltzmann_Machine_and_Convolutional_RBM"&gt;On CPU Performance Optimizations for Restricted Boltzmann Machine and Convolutional RBM&lt;/a&gt;, published in the Proceedings of the Artificial Neural Networks and Pattern Recognition workshop (ANNPR-2016). I've presented this paper in Germany, at Ulm.&lt;/p&gt;
&lt;p&gt;Although most of the performance research going on is focused on GPU, there are
still of research laboratories that are only equipped with CPU and it remains
important to be as fast as possible on CPU. Moreover, this is something
I really like.&lt;/p&gt;
&lt;p&gt;For this publication, I have tried to make my Restricted Boltzmann Machine (RBM)
and Convolutional RBM (CRBM) implementations in my DLL library as fast as
possible.&lt;/p&gt;
&lt;p&gt;The first part of the article is about Restricted Boltzmann Machine (RBM) which
are a form of dense Artificial Neural Network (ANN). Their training is very
similar to that of the ANN with Gradient Descent. Four different network
configurations are being tested.&lt;/p&gt;
&lt;p&gt;First, mini-batch training is shown to be much faster than online training, even
when online training is performed in parallel. Once mini-batch training is used,
BLAS operations are used in order to get as much performance as possible on the
different operations, mainly the Matrix Matrix Multiplication with the use of
the GEMM operation from the Intel Math Kernel Library (MKL). Moreover, the
parallel version of the MKL is also used to get even more performance. When all
these optimizations are performed, speedups of 11 to 30 are obtained compared to
the online training, depending on the network configurations. This final version
is able  to perform one epoch of Contrastive Divergence in 4 to 15 seconds
depending on the network, for 60000 images.&lt;/p&gt;
&lt;p&gt;The second part of the article is about Convolutional Restricted Boltzmann
Machine (CRBM). This is almost the equivalent of a Convolutional Neural Network
(CNN). Again four different networks are evaluated.&lt;/p&gt;
&lt;p&gt;The main problem with CRBM is that there are no standard implementations of the
convolution operation that is really fast. Therefore, it is not possible to
simply use a BLAS library to make the computation as fast as possible. The first
optimization that was tried is to vectorize the convolutions. With this, the
speedups have been between 1.1 and 1.9 times faster. I'm not really satisfied
with these results since in fact per convolution the speedups are much better.
Moreover, I have since been able to obtain better speedups but the deadline was
too short to include them in this paper. I'll try to talk about these
improvements in more details on this blog. What is more interesting to to
parallellize the different convolutions since they are mostly independent. This
can bring a speedup of the amount of cores available on the machine. Since
convolutions are extremely memory hungry, virtual cores with Hyper Threading
generally does not help. An interesting optimization is to use a Matrix
Multiplication to compute several valid convolutions at once.  This can give an
additional speedup between 1.6 and 2.2 compared to the vectorized version. While
it is possible to use the FFT to reduce the full convolution as well, in our
experiment the images were not big enough for this to be interesting. The final
speedups are about 10 times faster with these optimizations.&lt;/p&gt;
&lt;p&gt;We have obtained pretty good and I'm happy we have been published. However, I'm
not very satisfied with these results since I've been able to get even faster
since this and when compared with other frameworks, DLL is actually quite
competitive. I'll try to publish something new in the future.&lt;/p&gt;
&lt;p&gt;If you want more information, you can have a look at the paper. If you want to
look at the code, you can have a look at my projects:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;Expression Templates Library (ETL)&lt;/a&gt;: For
the Matrix Multiplication and Convolutions&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;Deep Learning Library (DLL)&lt;/a&gt;: For the RBM
and CRBM implementations&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Don't hesitate to ask any questions if you want more information :)&lt;/p&gt;</description><category>C++</category><category>CPU</category><category>crbm</category><category>dbn</category><category>Deep Learning</category><category>dll</category><category>etl</category><category>Intel</category><category>Performances</category><category>publications</category><category>rbm</category><category>thesis</category><guid>https://baptiste-wicht.com/posts/2017/02/publication-cpu-performance-optimizations-rbm-crbm.html</guid><pubDate>Tue, 07 Feb 2017 16:33:33 GMT</pubDate></item></channel></rss>