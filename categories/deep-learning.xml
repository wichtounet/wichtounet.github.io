<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Blog blog("Baptiste Wicht"); (Posts about Deep Learning)</title><link>http://baptiste-wicht.com/</link><description></description><atom:link href="http://baptiste-wicht.com/categories/deep-learning.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sun, 26 Nov 2017 15:45:14 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Initial support for Long Short Term Memory (LSTM) in DLL</title><link>http://baptiste-wicht.com/posts/2017/11/initial-support-for-long-short-term-memory-lstm-in-dll.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I'm really happy to announce that I just merged support for&lt;/p&gt;
&lt;p&gt;Long Short Term Memory
(LSTM) cells into my Deep Learning Library (DLL) machine learning framework. Two
weeks ago, &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/11/initial-support-for-recurrent-neural-network-rnn-in-dll.html"&gt;I already merged suport for Recurrent Neural network (RNN)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It's nothing fancy yet, but forward propagation of LSTM and basic
Backpropagation Through Time (BPTT) are now supported. It was not really
complicated to implemenet the forward pass but the backward pass is much
complicated for an LSTM than for a RNN. It took me quite a long time to figure
out all the gradients formulas and the documentation on that is quite scarce.&lt;/p&gt;
&lt;p&gt;For now, still only existing classification loss is supported for RNN and LSTM.
As I said last time, I still plan to add support for sequence-to-sequence loss
in order to be able to train models able to generate characters. However, I don't
know when I'll be able to work on that. Now that I've got the code for LSTM,
I should be able to implement a GRU cell and NAS cell quite easily I believe.&lt;/p&gt;
&lt;p&gt;For example, here is a simple LSTM used on MNIST for classification:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/neural/dense_layer.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/neural/lstm_layer.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/neural/recurrent_last_layer.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-4"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/network.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-5"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/datasets.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-7"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="cm"&gt;/*argc*/&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;char&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="cm"&gt;/*argv*/&lt;/span&gt; &lt;span class="p"&gt;[])&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-8"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Load the dataset&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_mnist_dataset_nc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;scale_pre&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{});&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-11"&gt;&lt;/a&gt;    &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;time_steps&lt;/span&gt;      &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;sequence_length&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-15"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Build the network&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-17"&gt;&lt;/a&gt;    &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_network_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;network_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-19"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;lstm_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;time_steps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sequence_length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;last_only&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-20"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;recurrent_last_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;time_steps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-21"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-22"&gt;&lt;/a&gt;        &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-23"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ADAM&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;      &lt;span class="c1"&gt;// Adam&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-24"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;                       &lt;span class="c1"&gt;// The mini-batch size&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-25"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-26"&gt;&lt;/a&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-27"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-28"&gt;&lt;/a&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-29"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Display the network and dataset&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-30"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-31"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-32"&gt;&lt;/a&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-33"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Train the network for performance sake&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-34"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-35"&gt;&lt;/a&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-36"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Test the network on test set&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-37"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-38"&gt;&lt;/a&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-39"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_46428948f03c4ce3974867842b7c3854-40"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The network is quite similar to the one used previously with an RNN, just
replace rnn with lstm and that's it. It starts with LSTM layer, followed by
a layer extracting the last time step and finally a dense layer with a softmax
function. The network is trained with Adam for 50 epochs. You can change the
activation function , the initializer for the weights and the biases and number
of steps for BPTT truncation.&lt;/p&gt;
&lt;p&gt;Here is the result I got on my last run:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_ae889957e735441095395a7bd801573f-1"&gt;&lt;/a&gt;------------------------------------------------------------
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-2"&gt;&lt;/a&gt;| Index | Layer                | Parameters | Output Shape |
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-3"&gt;&lt;/a&gt;------------------------------------------------------------
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-4"&gt;&lt;/a&gt;| 0     | LSTM (TANH) (dyn)    |      51200 | [Bx28x100]   |
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-5"&gt;&lt;/a&gt;| 1     | RNN(last)            |          0 | [Bx100]      |
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-6"&gt;&lt;/a&gt;| 2     | Dense(SOFTMAX) (dyn) |       1000 | [Bx10]       |
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-7"&gt;&lt;/a&gt;------------------------------------------------------------
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-8"&gt;&lt;/a&gt;              Total Parameters:      52200
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-10"&gt;&lt;/a&gt;--------------------------------------------
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-11"&gt;&lt;/a&gt;| mnist | Size  | Batches | Augmented Size |
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-12"&gt;&lt;/a&gt;--------------------------------------------
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-13"&gt;&lt;/a&gt;| train | 60000 | 600     | 60000          |
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-14"&gt;&lt;/a&gt;| test  | 10000 | 100     | 10000          |
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-15"&gt;&lt;/a&gt;--------------------------------------------
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-17"&gt;&lt;/a&gt;Network with 3 layers
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-18"&gt;&lt;/a&gt;    LSTM(dyn): 28x28 -&amp;gt; TANH -&amp;gt; 28x100
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-19"&gt;&lt;/a&gt;    RNN(last): 28x100 -&amp;gt; 100
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-20"&gt;&lt;/a&gt;    Dense(dyn): 100 -&amp;gt; SOFTMAX -&amp;gt; 10
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-21"&gt;&lt;/a&gt;Total parameters: 52200
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-22"&gt;&lt;/a&gt;Dataset
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-23"&gt;&lt;/a&gt;Training: In-Memory Data Generator
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-24"&gt;&lt;/a&gt;              Size: 60000
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-25"&gt;&lt;/a&gt;           Batches: 600
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-26"&gt;&lt;/a&gt;Testing: In-Memory Data Generator
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-27"&gt;&lt;/a&gt;              Size: 10000
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-28"&gt;&lt;/a&gt;           Batches: 100
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-29"&gt;&lt;/a&gt;
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-30"&gt;&lt;/a&gt;Train the network with "Stochastic Gradient Descent"
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-31"&gt;&lt;/a&gt;    Updater: ADAM
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-32"&gt;&lt;/a&gt;       Loss: CATEGORICAL_CROSS_ENTROPY
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-33"&gt;&lt;/a&gt; Early Stop: Goal(error)
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-34"&gt;&lt;/a&gt;
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-35"&gt;&lt;/a&gt;With parameters:
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-36"&gt;&lt;/a&gt;          epochs=50
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-37"&gt;&lt;/a&gt;      batch_size=100
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-38"&gt;&lt;/a&gt;   learning_rate=0.001
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-39"&gt;&lt;/a&gt;           beta1=0.9
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-40"&gt;&lt;/a&gt;           beta2=0.999
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-41"&gt;&lt;/a&gt;
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-42"&gt;&lt;/a&gt;epoch   0/50 batch  600/ 600 - error: 0.07943 loss: 0.28504 time 20910ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-43"&gt;&lt;/a&gt;epoch   1/50 batch  600/ 600 - error: 0.06683 loss: 0.24021 time 20889ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-44"&gt;&lt;/a&gt;epoch   2/50 batch  600/ 600 - error: 0.04828 loss: 0.18233 time 21061ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-45"&gt;&lt;/a&gt;epoch   3/50 batch  600/ 600 - error: 0.04407 loss: 0.16665 time 20839ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-46"&gt;&lt;/a&gt;epoch   4/50 batch  600/ 600 - error: 0.03515 loss: 0.13290 time 22108ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-47"&gt;&lt;/a&gt;epoch   5/50 batch  600/ 600 - error: 0.03207 loss: 0.12019 time 21393ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-48"&gt;&lt;/a&gt;epoch   6/50 batch  600/ 600 - error: 0.02973 loss: 0.11239 time 28199ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-49"&gt;&lt;/a&gt;epoch   7/50 batch  600/ 600 - error: 0.02653 loss: 0.10455 time 37039ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-50"&gt;&lt;/a&gt;epoch   8/50 batch  600/ 600 - error: 0.02482 loss: 0.09657 time 23127ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-51"&gt;&lt;/a&gt;epoch   9/50 batch  600/ 600 - error: 0.02177 loss: 0.08422 time 41766ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-52"&gt;&lt;/a&gt;epoch  10/50 batch  600/ 600 - error: 0.02453 loss: 0.09382 time 29765ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-53"&gt;&lt;/a&gt;epoch  11/50 batch  600/ 600 - error: 0.02575 loss: 0.09796 time 21449ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-54"&gt;&lt;/a&gt;epoch  12/50 batch  600/ 600 - error: 0.02107 loss: 0.07833 time 42056ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-55"&gt;&lt;/a&gt;epoch  13/50 batch  600/ 600 - error: 0.01877 loss: 0.07171 time 24673ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-56"&gt;&lt;/a&gt;epoch  14/50 batch  600/ 600 - error: 0.02095 loss: 0.08481 time 20878ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-57"&gt;&lt;/a&gt;epoch  15/50 batch  600/ 600 - error: 0.02040 loss: 0.07578 time 41515ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-58"&gt;&lt;/a&gt;epoch  16/50 batch  600/ 600 - error: 0.01580 loss: 0.06083 time 25705ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-59"&gt;&lt;/a&gt;epoch  17/50 batch  600/ 600 - error: 0.01945 loss: 0.07046 time 20903ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-60"&gt;&lt;/a&gt;epoch  18/50 batch  600/ 600 - error: 0.01728 loss: 0.06683 time 41828ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-61"&gt;&lt;/a&gt;epoch  19/50 batch  600/ 600 - error: 0.01577 loss: 0.05947 time 27810ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-62"&gt;&lt;/a&gt;epoch  20/50 batch  600/ 600 - error: 0.01528 loss: 0.05883 time 21477ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-63"&gt;&lt;/a&gt;epoch  21/50 batch  600/ 600 - error: 0.01345 loss: 0.05127 time 44718ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-64"&gt;&lt;/a&gt;epoch  22/50 batch  600/ 600 - error: 0.01410 loss: 0.05357 time 25174ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-65"&gt;&lt;/a&gt;epoch  23/50 batch  600/ 600 - error: 0.01268 loss: 0.04765 time 23827ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-66"&gt;&lt;/a&gt;epoch  24/50 batch  600/ 600 - error: 0.01342 loss: 0.05004 time 47232ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-67"&gt;&lt;/a&gt;epoch  25/50 batch  600/ 600 - error: 0.01730 loss: 0.06872 time 22532ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-68"&gt;&lt;/a&gt;epoch  26/50 batch  600/ 600 - error: 0.01337 loss: 0.05016 time 30114ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-69"&gt;&lt;/a&gt;epoch  27/50 batch  600/ 600 - error: 0.01842 loss: 0.07049 time 40136ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-70"&gt;&lt;/a&gt;epoch  28/50 batch  600/ 600 - error: 0.01262 loss: 0.04639 time 21793ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-71"&gt;&lt;/a&gt;epoch  29/50 batch  600/ 600 - error: 0.01403 loss: 0.05292 time 34096ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-72"&gt;&lt;/a&gt;epoch  30/50 batch  600/ 600 - error: 0.01185 loss: 0.04456 time 35420ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-73"&gt;&lt;/a&gt;epoch  31/50 batch  600/ 600 - error: 0.01098 loss: 0.04180 time 20909ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-74"&gt;&lt;/a&gt;epoch  32/50 batch  600/ 600 - error: 0.01337 loss: 0.04687 time 30113ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-75"&gt;&lt;/a&gt;epoch  33/50 batch  600/ 600 - error: 0.01415 loss: 0.05292 time 37393ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-76"&gt;&lt;/a&gt;epoch  34/50 batch  600/ 600 - error: 0.00982 loss: 0.03615 time 20962ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-77"&gt;&lt;/a&gt;epoch  35/50 batch  600/ 600 - error: 0.01178 loss: 0.04830 time 29305ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-78"&gt;&lt;/a&gt;epoch  36/50 batch  600/ 600 - error: 0.00882 loss: 0.03408 time 38293ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-79"&gt;&lt;/a&gt;epoch  37/50 batch  600/ 600 - error: 0.01148 loss: 0.04341 time 20841ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-80"&gt;&lt;/a&gt;epoch  38/50 batch  600/ 600 - error: 0.00960 loss: 0.03701 time 29204ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-81"&gt;&lt;/a&gt;epoch  39/50 batch  600/ 600 - error: 0.00850 loss: 0.03094 time 39802ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-82"&gt;&lt;/a&gt;epoch  40/50 batch  600/ 600 - error: 0.01473 loss: 0.05136 time 20831ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-83"&gt;&lt;/a&gt;epoch  41/50 batch  600/ 600 - error: 0.01007 loss: 0.03579 time 29856ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-84"&gt;&lt;/a&gt;epoch  42/50 batch  600/ 600 - error: 0.00943 loss: 0.03370 time 38200ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-85"&gt;&lt;/a&gt;epoch  43/50 batch  600/ 600 - error: 0.01205 loss: 0.04409 time 21162ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-86"&gt;&lt;/a&gt;epoch  44/50 batch  600/ 600 - error: 0.00980 loss: 0.03674 time 32279ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-87"&gt;&lt;/a&gt;epoch  45/50 batch  600/ 600 - error: 0.01068 loss: 0.04133 time 38448ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-88"&gt;&lt;/a&gt;epoch  46/50 batch  600/ 600 - error: 0.00913 loss: 0.03478 time 20797ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-89"&gt;&lt;/a&gt;epoch  47/50 batch  600/ 600 - error: 0.00985 loss: 0.03759 time 28885ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-90"&gt;&lt;/a&gt;epoch  48/50 batch  600/ 600 - error: 0.00912 loss: 0.03295 time 41120ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-91"&gt;&lt;/a&gt;epoch  49/50 batch  600/ 600 - error: 0.00930 loss: 0.03438 time 21282ms
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-92"&gt;&lt;/a&gt;Restore the best (error) weights from epoch 39
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-93"&gt;&lt;/a&gt;Training took 1460s
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-94"&gt;&lt;/a&gt;
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-95"&gt;&lt;/a&gt;Evaluation Results
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-96"&gt;&lt;/a&gt;   error: 0.02440
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-97"&gt;&lt;/a&gt;    loss: 0.11315
&lt;a name="rest_code_ae889957e735441095395a7bd801573f-98"&gt;&lt;/a&gt;evaluation took 1000ms
&lt;/pre&gt;&lt;p&gt;Again, nothing fancy yet, but this example has not been optimized for
performance nor for accuracy.&lt;/p&gt;
&lt;p&gt;I also made a few changes to the RNN layer. I added support for biases and
improved the code as well for performance and readability.&lt;/p&gt;
&lt;p&gt;All this support is now in the &lt;strong&gt;master&lt;/strong&gt; branch of the DLL project if you want
to check it out. You can also check out the example online:
&lt;a class="reference external" href="https://github.com/wichtounet/dll/blob/master/examples/src/mnist_lstm.cpp"&gt;mnist_lstm.cpp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can access the project &lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;on Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Currently I'm working on the GPU performance again. The performance of some is
still not as good as I want it to be, especially complex operation like used in
Adam and Nadam. Currently, there are many calls to GPU BLAS libraries and
I want to try to extract some more optimized patterns. Once it's done, I'll post
more on that later on the blog.&lt;/p&gt;&lt;/div&gt;</description><category>Deep Learning</category><category>dll</category><category>Machine Learning</category><category>projects</category><category>rnn</category><guid>http://baptiste-wicht.com/posts/2017/11/initial-support-for-long-short-term-memory-lstm-in-dll.html</guid><pubDate>Fri, 24 Nov 2017 14:16:37 GMT</pubDate></item><item><title>Initial support for Recurrent Neural Network (RNN) in DLL</title><link>http://baptiste-wicht.com/posts/2017/11/initial-support-for-recurrent-neural-network-rnn-in-dll.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I'm happy to announce that I just merged support for Recurrent Neural Networks
(RNNs) into my Deep Learning Library (DLL) machine learning framework.&lt;/p&gt;
&lt;p&gt;It's nothing fancy yet, but forward propagation of RNN and basic Backpropagation
Through Time (BPTT) are now supported. For now, only existing classification
loss is supported for RNN. I plan to add support for sequence-to-sequence loss
in order to be able to train models able to generate characters, but I don't
know when I'll be able to work on that. I also plan to add support for other
types of cells such as LSTM and GRU (maybe NAS) in the future.&lt;/p&gt;
&lt;p&gt;For example, here is a simple RNN used on MNIST:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/neural/dense_layer.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/neural/recurrent_layer.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/neural/recurrent_last_layer.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-4"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/network.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-5"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/datasets.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-7"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="cm"&gt;/*argc*/&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;char&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="cm"&gt;/*argv*/&lt;/span&gt; &lt;span class="p"&gt;[])&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-8"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Load the dataset&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_mnist_dataset_nc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;scale_pre&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{});&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-11"&gt;&lt;/a&gt;    &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;time_steps&lt;/span&gt;      &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;sequence_length&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-15"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Build the network&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-17"&gt;&lt;/a&gt;    &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_network_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;network_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-19"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;recurrent_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;time_steps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sequence_length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;last_only&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-20"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;recurrent_last_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;time_steps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-21"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-22"&gt;&lt;/a&gt;        &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-23"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ADAM&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;      &lt;span class="c1"&gt;// Adam&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-24"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;                       &lt;span class="c1"&gt;// The mini-batch size&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-25"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-26"&gt;&lt;/a&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-27"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-28"&gt;&lt;/a&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-29"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Display the network and dataset&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-30"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-31"&gt;&lt;/a&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-32"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Train the network for performance sake&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-33"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-34"&gt;&lt;/a&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-35"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Test the network on test set&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-36"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-37"&gt;&lt;/a&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-38"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_26d4164c41c74b52a7e4b27bfbd42b79-39"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The network starts with recurrent layer, followed by a layer that extracts only
the last layer and finally a dense layer with a softmax function. The recurrent
layer has support to change the activation function, change the initializer for
the two weights matrices of the RNN and the number of steps for BPTT truncation.&lt;/p&gt;
&lt;p&gt;Here is a possible result:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-1"&gt;&lt;/a&gt;Network with 3 layers
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-2"&gt;&lt;/a&gt;    RNN(dyn): 28x28 -&amp;gt; TANH -&amp;gt; 28x100
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-3"&gt;&lt;/a&gt;    RNN(last): 28x100 -&amp;gt; 100
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-4"&gt;&lt;/a&gt;    Dense(dyn): 100 -&amp;gt; SOFTMAX -&amp;gt; 10
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-5"&gt;&lt;/a&gt;Total parameters: 13800
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-6"&gt;&lt;/a&gt;Train the network with "Stochastic Gradient Descent"
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-7"&gt;&lt;/a&gt;    Updater: ADAM
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-8"&gt;&lt;/a&gt;       Loss: CATEGORICAL_CROSS_ENTROPY
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-9"&gt;&lt;/a&gt; Early Stop: Goal(error)
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-11"&gt;&lt;/a&gt;With parameters:
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-12"&gt;&lt;/a&gt;          epochs=50
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-13"&gt;&lt;/a&gt;      batch_size=100
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-14"&gt;&lt;/a&gt;   learning_rate=0.001
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-15"&gt;&lt;/a&gt;           beta1=0.9
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-16"&gt;&lt;/a&gt;           beta2=0.999
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-18"&gt;&lt;/a&gt;Epoch   0/50 - Classification error: 0.11635 Loss: 0.39999 Time 4717ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-19"&gt;&lt;/a&gt;Epoch   1/50 - Classification error: 0.11303 Loss: 0.36994 Time 4702ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-20"&gt;&lt;/a&gt;Epoch   2/50 - Classification error: 0.06732 Loss: 0.23469 Time 4702ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-21"&gt;&lt;/a&gt;Epoch   3/50 - Classification error: 0.04865 Loss: 0.17091 Time 4696ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-22"&gt;&lt;/a&gt;Epoch   4/50 - Classification error: 0.05957 Loss: 0.20437 Time 4706ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-23"&gt;&lt;/a&gt;Epoch   5/50 - Classification error: 0.05022 Loss: 0.16888 Time 4696ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-24"&gt;&lt;/a&gt;Epoch   6/50 - Classification error: 0.03912 Loss: 0.13743 Time 4698ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-25"&gt;&lt;/a&gt;Epoch   7/50 - Classification error: 0.04097 Loss: 0.14509 Time 4706ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-26"&gt;&lt;/a&gt;Epoch   8/50 - Classification error: 0.03938 Loss: 0.13397 Time 4694ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-27"&gt;&lt;/a&gt;Epoch   9/50 - Classification error: 0.03525 Loss: 0.12284 Time 4706ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-28"&gt;&lt;/a&gt;Epoch  10/50 - Classification error: 0.03927 Loss: 0.13770 Time 4694ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-29"&gt;&lt;/a&gt;Epoch  11/50 - Classification error: 0.03315 Loss: 0.11315 Time 4711ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-30"&gt;&lt;/a&gt;Epoch  12/50 - Classification error: 0.05037 Loss: 0.17123 Time 4711ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-31"&gt;&lt;/a&gt;Epoch  13/50 - Classification error: 0.02927 Loss: 0.10042 Time 4780ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-32"&gt;&lt;/a&gt;Epoch  14/50 - Classification error: 0.03322 Loss: 0.11027 Time 4746ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-33"&gt;&lt;/a&gt;Epoch  15/50 - Classification error: 0.03397 Loss: 0.11585 Time 4684ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-34"&gt;&lt;/a&gt;Epoch  16/50 - Classification error: 0.02938 Loss: 0.09984 Time 4708ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-35"&gt;&lt;/a&gt;Epoch  17/50 - Classification error: 0.03262 Loss: 0.11152 Time 4690ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-36"&gt;&lt;/a&gt;Epoch  18/50 - Classification error: 0.02872 Loss: 0.09753 Time 4672ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-37"&gt;&lt;/a&gt;Epoch  19/50 - Classification error: 0.02548 Loss: 0.08605 Time 4691ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-38"&gt;&lt;/a&gt;Epoch  20/50 - Classification error: 0.02245 Loss: 0.07797 Time 4693ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-39"&gt;&lt;/a&gt;Epoch  21/50 - Classification error: 0.02705 Loss: 0.08984 Time 4684ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-40"&gt;&lt;/a&gt;Epoch  22/50 - Classification error: 0.02422 Loss: 0.08164 Time 4688ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-41"&gt;&lt;/a&gt;Epoch  23/50 - Classification error: 0.02645 Loss: 0.08804 Time 4690ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-42"&gt;&lt;/a&gt;Epoch  24/50 - Classification error: 0.02927 Loss: 0.09739 Time 4715ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-43"&gt;&lt;/a&gt;Epoch  25/50 - Classification error: 0.02578 Loss: 0.08669 Time 4702ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-44"&gt;&lt;/a&gt;Epoch  26/50 - Classification error: 0.02785 Loss: 0.09368 Time 4700ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-45"&gt;&lt;/a&gt;Epoch  27/50 - Classification error: 0.02472 Loss: 0.08237 Time 4695ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-46"&gt;&lt;/a&gt;Epoch  28/50 - Classification error: 0.02125 Loss: 0.07324 Time 4690ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-47"&gt;&lt;/a&gt;Epoch  29/50 - Classification error: 0.01977 Loss: 0.06635 Time 4688ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-48"&gt;&lt;/a&gt;Epoch  30/50 - Classification error: 0.03635 Loss: 0.12140 Time 4689ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-49"&gt;&lt;/a&gt;Epoch  31/50 - Classification error: 0.02862 Loss: 0.09704 Time 4698ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-50"&gt;&lt;/a&gt;Epoch  32/50 - Classification error: 0.02463 Loss: 0.08158 Time 4686ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-51"&gt;&lt;/a&gt;Epoch  33/50 - Classification error: 0.02565 Loss: 0.08771 Time 4697ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-52"&gt;&lt;/a&gt;Epoch  34/50 - Classification error: 0.02278 Loss: 0.07634 Time 4718ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-53"&gt;&lt;/a&gt;Epoch  35/50 - Classification error: 0.02105 Loss: 0.07075 Time 4697ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-54"&gt;&lt;/a&gt;Epoch  36/50 - Classification error: 0.02770 Loss: 0.09358 Time 4711ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-55"&gt;&lt;/a&gt;Epoch  37/50 - Classification error: 0.02627 Loss: 0.08805 Time 4742ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-56"&gt;&lt;/a&gt;Epoch  38/50 - Classification error: 0.02282 Loss: 0.07712 Time 4708ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-57"&gt;&lt;/a&gt;Epoch  39/50 - Classification error: 0.02305 Loss: 0.07661 Time 4697ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-58"&gt;&lt;/a&gt;Epoch  40/50 - Classification error: 0.02243 Loss: 0.07773 Time 4700ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-59"&gt;&lt;/a&gt;Epoch  41/50 - Classification error: 0.02467 Loss: 0.08234 Time 4712ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-60"&gt;&lt;/a&gt;Epoch  42/50 - Classification error: 0.01808 Loss: 0.06186 Time 4691ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-61"&gt;&lt;/a&gt;Epoch  43/50 - Classification error: 0.02388 Loss: 0.07917 Time 4681ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-62"&gt;&lt;/a&gt;Epoch  44/50 - Classification error: 0.02162 Loss: 0.07508 Time 4699ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-63"&gt;&lt;/a&gt;Epoch  45/50 - Classification error: 0.01877 Loss: 0.06289 Time 4735ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-64"&gt;&lt;/a&gt;Epoch  46/50 - Classification error: 0.02263 Loss: 0.07969 Time 4764ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-65"&gt;&lt;/a&gt;Epoch  47/50 - Classification error: 0.02100 Loss: 0.07207 Time 4684ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-66"&gt;&lt;/a&gt;Epoch  48/50 - Classification error: 0.02425 Loss: 0.08076 Time 4752ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-67"&gt;&lt;/a&gt;Epoch  49/50 - Classification error: 0.02328 Loss: 0.07803 Time 4718ms
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-68"&gt;&lt;/a&gt;Restore the best (error) weights from epoch 42
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-69"&gt;&lt;/a&gt;Training took 235s
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-70"&gt;&lt;/a&gt;Evaluation Results
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-71"&gt;&lt;/a&gt;   error: 0.03000
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-72"&gt;&lt;/a&gt;    loss: 0.12260
&lt;a name="rest_code_5f651d989a014ec39f1859d00ab7ec25-73"&gt;&lt;/a&gt;evaluation took 245ms
&lt;/pre&gt;&lt;p&gt;Nothing fancy, but this example is not necessarily optimized.&lt;/p&gt;
&lt;p&gt;All this support is now in the &lt;strong&gt;master&lt;/strong&gt; branch of the DLL project if you want
to check it out. You can also check out the example online:
&lt;a class="reference external" href="https://github.com/wichtounet/dll/blob/master/examples/src/mnist_rnn.cpp"&gt;mnist_rnn.cpp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can access the project &lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;on Github&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>Deep Learning</category><category>dll</category><category>Machine Learning</category><category>projects</category><category>rnn</category><guid>http://baptiste-wicht.com/posts/2017/11/initial-support-for-recurrent-neural-network-rnn-in-dll.html</guid><pubDate>Sun, 12 Nov 2017 14:22:44 GMT</pubDate></item><item><title>DLL New Features: Embeddings and Merge layers</title><link>http://baptiste-wicht.com/posts/2017/10/dll-new-features-embeddings-and-merge-layers.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've just finished integrating new features into DLL, my deep learning library.
I've added support for an embeddings layer, a group layer and a merge layer.
This is not yet released, but available in the master branch.&lt;/p&gt;
&lt;p&gt;Embeddings are used more and more these days to learn dense representation of
characters or word. An embedding layer in a neural network transform labels into
a vector. It's generally used as the first layer of the network. The embedding
are learned as part of the network.&lt;/p&gt;
&lt;p&gt;The merge layer allows to create branches in the network. The input is passed to
each sub layer and then the output of each layer is concatenated to form the
output of the merged layers. This can be very useful to use different
convolutional filter sizes.&lt;/p&gt;
&lt;p&gt;The group layer is a simple utility to group layers together. This is mostly to
use with merge layers to form several branches.&lt;/p&gt;
&lt;p&gt;I've put together a new example to use these features on text classification.
The dataset is totally synthetic for now, but this can easily be reproduced with
a normal text classification dataset. This kind of model is called a Character
Convolutional Neural Network.&lt;/p&gt;
&lt;p&gt;Here is the code for example:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// The length of the embedding vector&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;    &lt;span class="c1"&gt;// The word (or sequence) length&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;embedding_network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_network_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;network_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-6"&gt;&lt;/a&gt;        &lt;span class="c1"&gt;// The embedding layer&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;embedding_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-9"&gt;&lt;/a&gt;        &lt;span class="c1"&gt;// The convolutional layers&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-10"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;merge_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-11"&gt;&lt;/a&gt;            &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-12"&gt;&lt;/a&gt;            &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;group_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-13"&gt;&lt;/a&gt;                  &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-14"&gt;&lt;/a&gt;                &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_2d_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-15"&gt;&lt;/a&gt;            &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-16"&gt;&lt;/a&gt;            &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;group_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-17"&gt;&lt;/a&gt;                  &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-18"&gt;&lt;/a&gt;                &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_2d_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-19"&gt;&lt;/a&gt;            &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-20"&gt;&lt;/a&gt;            &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;group_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-21"&gt;&lt;/a&gt;                  &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-22"&gt;&lt;/a&gt;                &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_2d_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-23"&gt;&lt;/a&gt;            &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-24"&gt;&lt;/a&gt;        &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-25"&gt;&lt;/a&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-26"&gt;&lt;/a&gt;        &lt;span class="c1"&gt;// The final softmax layer&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-27"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;48&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-28"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-29"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;NADAM&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;     &lt;span class="c1"&gt;// Nesterov Adam (NADAM)&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-30"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;                        &lt;span class="c1"&gt;// The mini-batch size&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-31"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;                               &lt;span class="c1"&gt;// Shuffle before each epoch&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-32"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-33"&gt;&lt;/a&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-34"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;embedding_network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-35"&gt;&lt;/a&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-36"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Display the network and dataset&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-37"&gt;&lt;/a&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-38"&gt;&lt;/a&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-39"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Train the network for performance sake&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-40"&gt;&lt;/a&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-41"&gt;&lt;/a&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-42"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Test the network on train set&lt;/span&gt;
&lt;a name="rest_code_4e19dc80ab4f4427a57cfcae077907cd-43"&gt;&lt;/a&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The network starts with an embedding layer. The embedding is then passed to
three convolutional layers with different filter sizes, each followed by
a pooling layer. The outputs of the three layers are merged at the end of the
merge layer. Finally, a softmax layer is used for classification.&lt;/p&gt;
&lt;p&gt;This kind of model can be very powerful and is used regularly. These new
features make for a much larger variety of models that can be build with the DLL
library.&lt;/p&gt;
&lt;p&gt;The full code with the dataset generation can be found online:
&lt;a class="reference external" href="https://github.com/wichtounet/dll/blob/master/examples/src/char_cnn.cpp"&gt;char_cnn.cpp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The next feature I want to focus on is recurrent neural networks. I'll probably
try a single RNN layer first and then upgrade to multi-layers and LSTM and maybe
GRU.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>Deep Learning</category><category>dll</category><category>Machine Learning</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/10/dll-new-features-embeddings-and-merge-layers.html</guid><pubDate>Tue, 17 Oct 2017 17:50:40 GMT</pubDate></item><item><title>Update on Deep Learning Library (DLL): Dropout, Batch Normalization, Adaptive Learning Rates, ...</title><link>http://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;It's been a while since I've posted something on this, especially since I had
one month vacation. This year I've been able to integrate a great number of
changes into my Deep Learning Library (DLL) project. It has seen a lot of
refactorings and a lot of new features making it look like a real neural network
library now. In this post, I'll try to outline the last new features and changes
of the library.&lt;/p&gt;
&lt;p&gt;For those that don't know, DLL is a library for neural network training, written
in C++ and for C++. You can train Fully-Connected Neural Networks and
Convolutional Neural Networks. The focus of the framework is on speed and easy
use in C++.&lt;/p&gt;
&lt;p&gt;As for my ETL project and again thanks to my thesis supervisor, the project now
has a logo:&lt;/p&gt;
&lt;img alt="DLL Logo" class="align-center" src="http://baptiste-wicht.com/images/dll_logo.png"&gt;
&lt;div class="section" id="adaptive-learning-rates"&gt;
&lt;h2&gt;Adaptive Learning Rates&lt;/h2&gt;
&lt;p&gt;Before, the framework only supported simple SGD and Momentum updates for the
different parameters of the network. Moreover, it was not very well extendable.
Therefore, I reviewed the system to be able to configure an optimizer for each
network to train. Once that was done, the first thing I did was to add support
for Nesterov Accelerated Gradients (NAG) as a third optimizer. After this,
I realized it was then easy to integrate support for more advanced optimizers
including support for adaptive learning rates. This means that the learning rate
will be adapted for each parameter depending on what the network is learning.
Some of the optimizers even don't need any learning rate. So far, I've
implemented support for the following optimizers: Adagrad, RMSProp, Adam (with
and without bias correction), Adamax (Adam with infinite norm), Nadam (Adam with
Nesterov momentum) and Adadelta (no more learning rate). The user can now choose
the optimizer of its choice, for instance NADAM, as a parameter of the network:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_65db57f56b284740a67712a401e9481c-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Use a Nadam optimizer&lt;/span&gt;
&lt;a name="rest_code_65db57f56b284740a67712a401e9481c-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;NADAM&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Another improvement in the same domain is that the learning rate can also be
decayed over time automatically by the optimizer.&lt;/p&gt;
&lt;p&gt;If you want more information on the different optimizers, you can have a look at
this very good article:
&lt;a class="reference external" href="http://ruder.io/optimizing-gradient-descent/"&gt;An overview of gradient descent optimization algorithms&lt;/a&gt;
from Sebastian Ruder.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="better-loss-support"&gt;
&lt;h2&gt;Better loss support&lt;/h2&gt;
&lt;p&gt;Before, DLL was automatically using Categorical Cross Entropy Loss, but it was
not possible to change it and it was not even possible to see the loss over
time. Now, the current value of the loss is displayed after each epoch of
training and the loss used for training is now configurable. So far, only three
different losses are supported, but it it not difficult to add new loss to the
system. The three losses supported are: Categorical Cross Entropy Loss, Binary
Cross Entropy Loss and Mean Squared Error Loss.&lt;/p&gt;
&lt;p&gt;Again, each network can specify the loss to use:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_eed8aa1dc706466faf7dee11d0de9bae-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Use a Binary Cross Entropy Loss&lt;/span&gt;
&lt;a name="rest_code_eed8aa1dc706466faf7dee11d0de9bae-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;BINARY_CROSS_ENTROPY&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="dropout"&gt;
&lt;h2&gt;Dropout&lt;/h2&gt;
&lt;p&gt;Dropout is a relatively new technique for neural network training. This is
especially made to reduce overfitting since a large number of sub networks will
be trained and it should prevent co-adaptation between different neurons. This
technique is relatively simple. Indeed, it simply randomly sets to zero some of
the input neurons of layers. At each batch, a new mask will be used and this
should lead to a large number of sub networks being trained.&lt;/p&gt;
&lt;p&gt;Here is example of a MLP with Dropout (p=0.5):&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_703a1118472644508799a32ff7656dd5-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_dbn_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_703a1118472644508799a32ff7656dd5-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_703a1118472644508799a32ff7656dd5-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_703a1118472644508799a32ff7656dd5-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dropout_layer_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_703a1118472644508799a32ff7656dd5-5"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_703a1118472644508799a32ff7656dd5-6"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dropout_layer_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_703a1118472644508799a32ff7656dd5-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;SOFTMAX&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_703a1118472644508799a32ff7656dd5-8"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;MOMENTUM&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;     &lt;span class="c1"&gt;// Momentum&lt;/span&gt;
&lt;a name="rest_code_703a1118472644508799a32ff7656dd5-9"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;                          &lt;span class="c1"&gt;// The mini-batch size&lt;/span&gt;
&lt;a name="rest_code_703a1118472644508799a32ff7656dd5-10"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;                                  &lt;span class="c1"&gt;// Shuffle before each epoch&lt;/span&gt;
&lt;a name="rest_code_703a1118472644508799a32ff7656dd5-11"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="batch-normalization"&gt;
&lt;h2&gt;Batch Normalization&lt;/h2&gt;
&lt;p&gt;Batch Normalization is another new technique for training neural networks. This
technique will ensure that each of the layer will receive inputs that look
kind of similar. This is a very large advantage since then you reduce the
different in impact of hyper parameters on different layers. Google reported
much faster training with this technique by getting rid of Dropout and by
increasing the learning rate of training.&lt;/p&gt;
&lt;p&gt;Here is an example of using Batch Normalization in a CNN:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_dbn_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_normalization_layer_4d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-5"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_layer_2d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-6"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_normalization_layer_4d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-8"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_layer_2d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-9"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-10"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_normalization_layer_2d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-11"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;SOFTMAX&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-12"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ADADELTA&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;     &lt;span class="c1"&gt;// Adadelta&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-13"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;                          &lt;span class="c1"&gt;// The mini-batch size&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-14"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;                                  &lt;span class="c1"&gt;// Shuffle the dataset before each epoch&lt;/span&gt;
&lt;a name="rest_code_45d673e19c2943269bc348585e90f5d4-15"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;You may notice that the layer is set as 4D so should only be used after
convolutional layer (or after the input). If you want to use it after
fully-connected layers, you can use the 2D version that works the same way.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="better-dataset-support"&gt;
&lt;h2&gt;Better dataset support&lt;/h2&gt;
&lt;p&gt;At the beginning, I designed DLL so that the user could directly pass data for
training in the form of STL Containers such as the std::vector. This is good in
some cases, but in some cases, the user does not know how to read the data , or
does not want to be bothered with it. Therefore, several data sets reader are
now available. Moreover, the entire system has been reworked to use generators
for data. A generator is simply a concept that has some data to produce. The
advantage of this new system is data augmentation is now supported every where
and much more efficiently than before. It is now possible to perform random
cropping and mirroring of images for instance. Moreover, the data augmentation
can be done in a secondary thread so as to be sure that there is always enough
data available for the training.&lt;/p&gt;
&lt;p&gt;The library now has a powerful dataset reader for both MNIST and CIFAR-10 and
the reader for ImageNet is almost ready. The project has already been used and
tested with these three datasets now. Moreover, the support for directly passing
STL containers has been maintained. In this case, a generator is simply created
around the data provided in the container and the generator is then passed to
the system for training.&lt;/p&gt;
&lt;p&gt;Here for instance is how to read MNIST data and scale (divide) all pixel values
by 255:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_f598f16bb01146508775fb27d0387a8a-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Load the dataset&lt;/span&gt;
&lt;a name="rest_code_f598f16bb01146508775fb27d0387a8a-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_mnist_dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;scale_pre&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{});&lt;/span&gt;
&lt;a name="rest_code_f598f16bb01146508775fb27d0387a8a-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_f598f16bb01146508775fb27d0387a8a-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_f598f16bb01146508775fb27d0387a8a-5"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Train the network&lt;/span&gt;
&lt;a name="rest_code_f598f16bb01146508775fb27d0387a8a-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_f598f16bb01146508775fb27d0387a8a-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_f598f16bb01146508775fb27d0387a8a-8"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Test the network&lt;/span&gt;
&lt;a name="rest_code_f598f16bb01146508775fb27d0387a8a-9"&gt;&lt;/a&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="much-faster-performance"&gt;
&lt;h2&gt;Much faster performance&lt;/h2&gt;
&lt;p&gt;I've spent quite a lot of time improving the performance of the framework. I've
focused on every part of training in order to make training of neural networks
as fast as possible. I've also made a comparison of the framework against
several popular machine learning framework (Caffe, TensorFlow, Keras, Torch and
DeepLearning4J). For instance, here are the results on a small CNN experiment on
MNIST with all the different frameworks in CPU mode and in GPU mode:&lt;/p&gt;
&lt;img alt="DLL Comparison Against other frameworks" class="align-center" src="http://baptiste-wicht.com/images/dll_comparison.png"&gt;
&lt;p&gt;As you can see, DLL is by far the fastest framework on CPU. On GPU, there is
still some work to be done, but this is already ongoing (although a lot of work
remains). This is confirmed on each of the four experiments performed on MNIST,
CIFAR-10 and ImageNet, although the margin is smaller for larger networks (still
about 40% faster than TensorFlow and Keras which are the fastest framework after
DLL on CPU on my tests).&lt;/p&gt;
&lt;p&gt;Overall, DLL is between 2 and 4 times faster than before and is always the
fastest framework for neural network training when training is performed on CPU.&lt;/p&gt;
&lt;p&gt;I proposed a talk about these optimizations and performance for Meeting C++ this
year, but it has unfortunately not been accepted. We also have submitted
a publication about the framework to a conference later this year.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="examples"&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;The project now has a few examples (available &lt;a class="reference external" href="https://github.com/wichtounet/dll/tree/master/examples/src"&gt;here&lt;/a&gt;), well-designed and I try to update them with the latest updates of the framework.&lt;/p&gt;
&lt;p&gt;For instance, here is the CNN example for MNIST (without includes):&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-1"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="cm"&gt;/*argc*/&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;char&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="cm"&gt;/*argv*/&lt;/span&gt; &lt;span class="p"&gt;[])&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-2"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Load the dataset&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_mnist_dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;scale_pre&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{});&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-5"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Build the network&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_dbn_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-8"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-9"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-10"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_layer_2d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-11"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-12"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_layer_2d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-13"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-14"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;SOFTMAX&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-15"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;MOMENTUM&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;     &lt;span class="c1"&gt;// Momentum&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-16"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;                          &lt;span class="c1"&gt;// The mini-batch size&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-17"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;                                  &lt;span class="c1"&gt;// Shuffle the dataset before each epoch&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-18"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-19"&gt;&lt;/a&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-20"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-21"&gt;&lt;/a&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-22"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-23"&gt;&lt;/a&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-24"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Display the network and dataset&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-25"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-26"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-27"&gt;&lt;/a&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-28"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Train the network&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-29"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-30"&gt;&lt;/a&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-31"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Test the network on test set&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-32"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-33"&gt;&lt;/a&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-34"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_496fb137201d43c1bac7de525ee30dbb-35"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="reproducible-results"&gt;
&lt;h2&gt;Reproducible results&lt;/h2&gt;
&lt;p&gt;And last, but maybe not least, I've finally united all the random number
generation code. This means that DLL can now set a global seed and that two
training of the same network and data with the same seed will now produce
exactly the same result.&lt;/p&gt;
&lt;p&gt;The usage is extremely simple:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_79bacddd9d50416e95ba75ceaeda311f-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;set_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;After all these changes, I truly feel that the library is now in a much better
state and could be useful in several projects. I hope that this will be useful
to some more people. Moreover, as you can see by the performance results, the
framework is now extremely efficient at training neural networks on CPU.&lt;/p&gt;
&lt;p&gt;If you want more information, you can consult the
&lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;dll Github Repository&lt;/a&gt;. You can also add
a comment to this post. If you find any problem on the project or have specific
question or request, don't hesitate to open an issue on Github.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>Deep Learning</category><category>dll</category><category>etl</category><category>Machine Learning</category><category>publications</category><category>thesis</category><guid>http://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html</guid><pubDate>Sun, 16 Jul 2017 13:41:51 GMT</pubDate></item><item><title>Speed up TensorFlow inference by compiling it from source</title><link>http://baptiste-wicht.com/posts/2017/05/speed-up-tensorflow-inference-compiling-from-source.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;The most simple way to install TensorFlow is to work in a virtual Python
environment and simply to use either the TensorFlow official packages in pip or
use one of the official wheels for distributions.  There is one big problem with
that technique and it's the fact that the binaries are precompiled so that they
fit as many hardware configuration as possible. This is normal from Google since
generating precompiled binaries for all the possible combinations of processor
capabilities would be a nightmare. This is not a problem for GPU
since the CUDA Libraries will take care of the difference from one graphics card
to another. But it is a problem with CPU performance. Indeed, different
processors have different capabilities. For instance, the vectorization
capabilities are different from processor to processor (SSE, AVX, AVX2,
AVX-512F, FMA, ...). All those options can make a significant difference in the
performance of the programs. Although most of the machine learning training
occurs on GPU most of the time, the inference is mostly done on the CPU.
Therefore, it probably remains important to be as fast as possible on CPU.&lt;/p&gt;
&lt;p&gt;So if you care about performance on CPU, you should install TensorFlow from
sources directly yourself. This will allow compilation of the TensorFlow sources
with -march=native which will enable all the hardware capabilities of machine on
which you are compiling the library.&lt;/p&gt;
&lt;p&gt;Depending on your problem, this may give you some nice speedup. In my case, on
a very small Recurrent Neural Network, it made inference about 20% faster.  On
a larger problem and depending on your processor, you may gain much more than
that. If you are training on CPU, this may make a very large difference in total
time.&lt;/p&gt;
&lt;p&gt;Installing TensorFlow is sometimes a bit cumbersome. You'll likely have to
compile Bazel from sources as well and depending on your processor, it may take
a long time to finish. Nevertheless, I have successfully compiled TensorFlow
from sources on several machines now without too many problems. Just pay close
attention to the options you are setting while configuring TensorFlow, for
instance CUDA configuration if you want GPU support.&lt;/p&gt;
&lt;p&gt;I hope this little trick will help you gain some time :)&lt;/p&gt;
&lt;p&gt;Here is the &lt;a class="reference external" href="https://www.tensorflow.org/install/install_sources"&gt;link to compile TensorFlow from source&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>CPU</category><category>Deep Learning</category><category>GPU</category><category>Intel</category><category>Machine Learning</category><category>Performance</category><category>tensorflow</category><guid>http://baptiste-wicht.com/posts/2017/05/speed-up-tensorflow-inference-compiling-from-source.html</guid><pubDate>Wed, 10 May 2017 12:18:33 GMT</pubDate></item><item><title>Publications: Deep Learning Features for Handwritten Keyword Spotting</title><link>http://baptiste-wicht.com/posts/2017/04/publications-deep-learning-features-handwritten-keyword-spotting.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;After my previous post about my publication on CPU performance optimization,
I wanted to talk a bit about two publications on Handwritten Keyword Spotting,
in which we extract features with Convolutional RBM RBM&lt;/p&gt;
&lt;p&gt;We published two different papers:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.researchgate.net/publication/306081095_Keyword_Spotting_with_Convolutional_Deep_Belief_Networks_and_Dynamic_Time_Warping"&gt;Keyword Spotting With Convolutional Deep Belief Networks and Dynamic Time Warping&lt;/a&gt;, in the Proceedings of the International Conference on Artificial Neural Networks (ICANN-2016), Barcelona, Spain&lt;/li&gt;
&lt;li&gt;Mixed Handwritten and printed digit recognition in Sudoku With Convolutional Deep Belief Network (Link will come), in the Proceedings of the International Conference on Pattern Recognition (ICPR-2016), Cancun, Mexico&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second paper is mostly a large extension of the first one, so I'll focus on
the complete version.&lt;/p&gt;
&lt;p&gt;On a side note, I also co-authored a third paper:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.researchgate.net/publication/312486359_Inkball_Models_as_Features_for_Handwriting_Recognition"&gt;Inkball Models as Features for Handwriting Recognition&lt;/a&gt;, in the Proceedings of the International Conference on Frontiers of Handwriting Recognition (ICFHR-2016), Shenzen, China&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We mostly used our existing system to generate features for a comparison between
different set of features for handwritten keyword spotting. It was my first time
in China and I enjoyed the stay a lot. I also had the chance to meet my
girlfriend in Shenzen, all the more reason to mention this publication :)&lt;/p&gt;
&lt;p&gt;Back on the main subject. The idea behind these publications is to
a Convolutional Deep Belief Network (CDBN) to extract features from the images
and then pass these features to either a Dynamic Time Warping (DTW) algorithm or
an Hidden Markov Model (HMM). The following image describe the overall system:&lt;/p&gt;
&lt;img alt="Keyword Spotting System" class="align-center" src="http://baptiste-wicht.com/images/kws_system.png"&gt;
&lt;p&gt;The features are extracted from preprocessed normalized binary images. Using
a sliding window, moving from left to right, one pixel at a time, the features
are extracted on each window. The feature extractor is a Convolutional Deep
Belief Network, trained fully unsupervised. The features are then normalized so
that each feature group sum to one and then each has zero-mean and
unit-variance. The network used for feature extraction is depicted in the
following image:&lt;/p&gt;
&lt;img alt="Convolutional Deep Belief Network features" class="align-center" src="http://baptiste-wicht.com/images/kws_network.png"&gt;
&lt;p&gt;Two Convolutional Restricted Boltzmann Machines (CRBMs) are used, each followed
by a max pooling layer.&lt;/p&gt;
&lt;p&gt;Once the features are extracted, they can be passed to the classifier for
keyword spotting scoring. We tested our features with two different approaches
for word scoring. The first one is a template matching strategy, Dynamic Time
Warping (DTW), is a very simple measure of distance between two sequences of
different length. The two sequences are warped non-linearly to minimize the
distance between each pair of features. A template from the training set is
compared to the word image being evaluated. This works pretty well for simple
data sets but fails when the writing styles of the test set are not known in the
training set. The second classifier is more powerful and trained, a Hidden
Markov Model (HMM). Character models are trained using the entire training set.
From these character models, a keyword model as well as an unconstrained model
(the filler model) are constructed. The probability of these two models is
computed using Viterbi and the final score is computed using log-odds scoring of
these two models using the filler model as a form of normalization.&lt;/p&gt;
&lt;p&gt;This technique was evaluated on three datasets (George Washington (GW), Parzival
(PAR) and IAM offline database (IAM)). Our features were compared with three
reference feature sets, one heuristic and two local feature sets.&lt;/p&gt;
&lt;p&gt;The results for DTW:&lt;/p&gt;
&lt;img alt="Keyword Spotting Results with Dynamic Time Warping" class="align-center" src="http://baptiste-wicht.com/images/kws_results_dtw.png"&gt;
&lt;p&gt;Overall, our features exhibit better performance than the other reference.
Except for the Mean Average Precision on the PAR data set. The very low
performance on PAR with DTW is explained by the fact mentioned earlier that it
has poor generalization to unknown writing styles.&lt;/p&gt;
&lt;p&gt;The results for HMM:&lt;/p&gt;
&lt;img alt="Keyword Spotting Results with Hidden Markov Model" class="align-center" src="http://baptiste-wicht.com/images/kws_results_hmm.png"&gt;
&lt;p&gt;With HMM, our features are always better than the other feature sets. However,
the margin of improvement is smaller than when using DTW.&lt;/p&gt;
&lt;p&gt;Overall, the proposed system proved quite powerful and was able to outperform
the three tested feature sets on three datasets for keyword spotting.&lt;/p&gt;
&lt;p&gt;You can find the &lt;a class="reference external" href="https://github.com/wichtounet/word_spotting"&gt;C++ implementation on Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As for my thesis, I have finished the writings about a month ago and it is now
in the hands on my supervisor.&lt;/p&gt;
&lt;p&gt;If you want to have a look, the
&lt;a class="reference external" href="http://baptiste-wicht.com/stories/publications.html"&gt;list of my publications&lt;/a&gt;
is available on this website.&lt;/p&gt;
&lt;p&gt;If you want more details on this project, don't hesitate to ask here or on
Github, or read the papers :)&lt;/p&gt;
&lt;p&gt;I hope the next post about my publications will be about the finalization of my
thesis :)&lt;/p&gt;&lt;/div&gt;</description><category>Deep Learning</category><category>dll</category><category>publications</category><category>thesis</category><guid>http://baptiste-wicht.com/posts/2017/04/publications-deep-learning-features-handwritten-keyword-spotting.html</guid><pubDate>Fri, 21 Apr 2017 18:29:39 GMT</pubDate></item><item><title>Publication: CPU Performance Optimizations for RBM and CRBM</title><link>http://baptiste-wicht.com/posts/2017/02/publication-cpu-performance-optimizations-rbm-crbm.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Recently, we have published a paper about performance optimizations that may
interest you.&lt;/p&gt;
&lt;p&gt;The paper is &lt;a class="reference external" href="https://www.researchgate.net/publication/307908790_On_CPU_Performance_Optimization_of_Restricted_Boltzmann_Machine_and_Convolutional_RBM"&gt;On CPU Performance Optimizations for Restricted Boltzmann Machine and Convolutional RBM&lt;/a&gt;, published in the Proceedings of the Artificial Neural Networks and Pattern Recognition workshop (ANNPR-2016). I've presented this paper in Germany, at Ulm.&lt;/p&gt;
&lt;p&gt;Although most of the performance research going on is focused on GPU, there are
still of research laboratories that are only equipped with CPU and it remains
important to be as fast as possible on CPU. Moreover, this is something
I really like.&lt;/p&gt;
&lt;p&gt;For this publication, I have tried to make my Restricted Boltzmann Machine (RBM)
and Convolutional RBM (CRBM) implementations in my DLL library as fast as
possible.&lt;/p&gt;
&lt;p&gt;The first part of the article is about Restricted Boltzmann Machine (RBM) which
are a form of dense Artificial Neural Network (ANN). Their training is very
similar to that of the ANN with Gradient Descent. Four different network
configurations are being tested.&lt;/p&gt;
&lt;p&gt;First, mini-batch training is shown to be much faster than online training, even
when online training is performed in parallel. Once mini-batch training is used,
BLAS operations are used in order to get as much performance as possible on the
different operations, mainly the Matrix Matrix Multiplication with the use of
the GEMM operation from the Intel Math Kernel Library (MKL). Moreover, the
parallel version of the MKL is also used to get even more performance. When all
these optimizations are performed, speedups of 11 to 30 are obtained compared to
the online training, depending on the network configurations. This final version
is able  to perform one epoch of Contrastive Divergence in 4 to 15 seconds
depending on the network, for 60000 images.&lt;/p&gt;
&lt;p&gt;The second part of the article is about Convolutional Restricted Boltzmann
Machine (CRBM). This is almost the equivalent of a Convolutional Neural Network
(CNN). Again four different networks are evaluated.&lt;/p&gt;
&lt;p&gt;The main problem with CRBM is that there are no standard implementations of the
convolution operation that is really fast. Therefore, it is not possible to
simply use a BLAS library to make the computation as fast as possible. The first
optimization that was tried is to vectorize the convolutions. With this, the
speedups have been between 1.1 and 1.9 times faster. I'm not really satisfied
with these results since in fact per convolution the speedups are much better.
Moreover, I have since been able to obtain better speedups but the deadline was
too short to include them in this paper. I'll try to talk about these
improvements in more details on this blog. What is more interesting to to
parallellize the different convolutions since they are mostly independent. This
can bring a speedup of the amount of cores available on the machine. Since
convolutions are extremely memory hungry, virtual cores with Hyper Threading
generally does not help. An interesting optimization is to use a Matrix
Multiplication to compute several valid convolutions at once.  This can give an
additional speedup between 1.6 and 2.2 compared to the vectorized version. While
it is possible to use the FFT to reduce the full convolution as well, in our
experiment the images were not big enough for this to be interesting. The final
speedups are about 10 times faster with these optimizations.&lt;/p&gt;
&lt;p&gt;We have obtained pretty good and I'm happy we have been published. However, I'm
not very satisfied with these results since I've been able to get even faster
since this and when compared with other frameworks, DLL is actually quite
competitive. I'll try to publish something new in the future.&lt;/p&gt;
&lt;p&gt;If you want more information, you can have a look at the paper. If you want to
look at the code, you can have a look at my projects:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;Expression Templates Library (ETL)&lt;/a&gt;: For
the Matrix Multiplication and Convolutions&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;Deep Learning Library (DLL)&lt;/a&gt;: For the RBM
and CRBM implementations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Don't hesitate to ask any questions if you want more information :)&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>CPU</category><category>crbm</category><category>dbn</category><category>Deep Learning</category><category>dll</category><category>etl</category><category>Intel</category><category>Performances</category><category>publications</category><category>rbm</category><category>thesis</category><guid>http://baptiste-wicht.com/posts/2017/02/publication-cpu-performance-optimizations-rbm-crbm.html</guid><pubDate>Tue, 07 Feb 2017 16:33:33 GMT</pubDate></item><item><title>Publications - Sudoku Recognition with Deep Belief Network</title><link>http://baptiste-wicht.com/posts/2017/01/publications-sudoku-recognition-with-deep-belief-network.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I recently realized that I never talked about my publications on this website...
I thought it was time to start. I'll start to write a few posts about my earlier
publications and then I'll try to write something for the new ones not too late.&lt;/p&gt;
&lt;p&gt;For the story, I'm currently a PHD student at the University of Fribourg, in
Switzerland. My PHD is about the use of Deep Learning technologies to
automatically extract features from images. I have developed my Deep Learning
Library (DLL) project for this thesis. We have published a few articles on the
various projects that we tackled during the thesis. I'll try to go in order.&lt;/p&gt;
&lt;p&gt;At the beginning of the thesis, I used Restricted Boltzmann Machine and Deep
Belief Network to perform digit recognition on images of Sudoku taken with
a phone camera. We published two papers on this subject.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.researchgate.net/publication/282303748_Camera-based_Sudoku_recognition_with_deep_belief_network"&gt;Camera-based Sudoku Recognition with Deep Belief Network&lt;/a&gt;, in the Proceedings of the International Conference on Soft Computing and Pattern Recognition (SOCPAR-2014)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.researchgate.net/publication/307545305_Mixed_handwritten_and_printed_digit_recognition_in_Sudoku_with_Convolutional_Deep_Belief_Network"&gt;Mixed Handwritten and printed digit recognition in Sudoku With Convolutional Deep Belief Network&lt;/a&gt;, in the Proceedings of the International Conference on Document Analysis and Recognition (ICDAR-2015)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Sudoku grid and digits are detected using standard image processing
techniques:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;The image is first converted to grayscale, then a median blur is applied to
remove noise and the image is binarized using Adapteive Thresholding&lt;/li&gt;
&lt;li&gt;The edges are detected using the Canny algorithm. From these, the lines are
detected using a Progressive Probabilistic Hough Transform&lt;/li&gt;
&lt;li&gt;Using a connected component analysis, the segments of lines are clustered
together to detect the Sudoku Grid&lt;/li&gt;
&lt;li&gt;The cells are then detected inside the grid using the inner lines and contour
detection is used to isolate the digits.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here is one of the original images from our dataset:&lt;/p&gt;
&lt;img alt="Original image from our dataset" class="align-center" src="http://baptiste-wicht.com/images/image124.jpg"&gt;
&lt;p&gt;Here are the detected characters from the previous image:&lt;/p&gt;
&lt;img alt="Detected digits from our application" class="align-center" src="http://baptiste-wicht.com/images/image124_char.jpg"&gt;
&lt;p&gt;Once all the digits have been found they are passed to a Deep Belief Network for
recognition. A Deep Belief Network is composed of several Restricted Boltzmann
Machines (RBM) that are stacked. The network is pretrained, by training each
RBM, in turn, with Contrastive Divergence. This algorithm basically trains each
RBM as an auto-encoder and learns a good feature representation of the inputs.
Once all the layers have been trained, the network can then be trained as
a regular neural network with Stochastic Gradient Descent.&lt;/p&gt;
&lt;p&gt;In the second paper, the images of Sudoku are containing both computer printed
and handwritten digits (the grid is already filled). The other difference is
that the second system used a Convolutional DBN instead of DBN. The difference
being that each layer is a Convolutional RBM. Such a model will learn a set of
small filters that will be applied to each position of the image.&lt;/p&gt;
&lt;p&gt;On the second version of the dataset, we have been able to achieve 99.14% of
recognition of the digits or 92.5% of fully-recognized grid  with the
Convolutional Network.&lt;/p&gt;
&lt;p&gt;You can find the &lt;a class="reference external" href="https://github.com/wichtounet/sudoku_recognizer"&gt;C++ implementation on Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want to have a look, I've updated the
&lt;a class="reference external" href="http://baptiste-wicht.com/stories/publications.html"&gt;list of my publications&lt;/a&gt;
on this website.&lt;/p&gt;
&lt;p&gt;If you want more details on this project, don't hesitate to ask here or on
Github, or read the paper :)
The next post about my publications will probably be about CPU performances!&lt;/p&gt;&lt;/div&gt;</description><category>Deep Learning</category><category>dll</category><category>Personal</category><category>publications</category><category>thesis</category><guid>http://baptiste-wicht.com/posts/2017/01/publications-sudoku-recognition-with-deep-belief-network.html</guid><pubDate>Fri, 20 Jan 2017 07:56:46 GMT</pubDate></item><item><title>Update: Thor, Thesis and Publications</title><link>http://baptiste-wicht.com/posts/2016/08/update-thor-thesis-and-publications.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Since it's been a real while since the last post I've written here, I wanted to
write a short status update.&lt;/p&gt;
&lt;p&gt;I had to serve one month in the army, which does not help at all for
productivity :P Since the update to Boost Spirit X3, I haven't worked on my
eddic compiler again, but I've switched back to my operating system project:
thor. I'm having a lot of fun with it again and it's in much better state than
before.&lt;/p&gt;
&lt;p&gt;We also have been very productive on the publication side, with four new
publications this year in various conferences. I'll update the blog when the
proceedings are published. I'll be going to ICANN 2016 and ANNPR 2016 next week
and probably to ICFHR in October. And of course, I'll go back to Meeting C++ in
November :) As for my thesis, it's finally going great, I've started writing
regularly and it's taking form!&lt;/p&gt;
&lt;div class="section" id="thor"&gt;
&lt;h2&gt;Thor&lt;/h2&gt;
&lt;p&gt;My project Thor Operating System now has much more features than before:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;64bit operating system&lt;/li&gt;
&lt;li&gt;Preemptive Multiprocessing&lt;/li&gt;
&lt;li&gt;Keyboard / Mouse driver&lt;/li&gt;
&lt;li&gt;Full ACPI support with ACPICA&lt;/li&gt;
&lt;li&gt;Read/Write ATA driver&lt;/li&gt;
&lt;li&gt;FAT32 file system support&lt;/li&gt;
&lt;li&gt;HPET/RTC/PIT drivers&lt;/li&gt;
&lt;li&gt;Basic PCI support&lt;/li&gt;
&lt;li&gt;Multi stage booting with FAT32&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since last time, I've fixed tons of bug in the system. Although there are still
some culprit, it's much more stable than before. They were a lot of bugs in the
scheduler with loads of race conditions. I hope I've working through most of
them now.&lt;/p&gt;
&lt;p&gt;I'm currently working on the network stack. I'm able to receive and send packets
using the Realtek 8139 card. I have working support for Ethernet, IP and ARP.
I'm currently working on adding ICMP support. I've come to realize that the
hardest part is not to develop the code here but to find a way to test it.
Network in Qemu is a huge pain in the ass to configure. And then, you need tools
to generate some packets or at least answer to packets send by the virtual
machine, and it's really bad... Nevertheless, it's pretty fun overall :)&lt;/p&gt;
&lt;p&gt;Aside from this, I'm also working on a window manager. I'll try to post an
update on this.&lt;/p&gt;
&lt;p&gt;You can take a look at the &lt;a class="reference external" href="https://github.com/wichtounet/thor-os"&gt;thor sources&lt;/a&gt; if you're interested.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="future"&gt;
&lt;h2&gt;Future&lt;/h2&gt;
&lt;p&gt;For the time being, I'll focus my effort on the thor project. I also have some
development to do on my home automation system: &lt;a class="reference external" href="https://github.com/wichtounet/asgard-server"&gt;asgard-server&lt;/a&gt; that I plan to finalize and deploy in a useful way this weekend in my apartment. You can also expect some updates on my deep learning library where I've started work to make it more user-friendly (kind of). I'm also still waiting on the first stable version of doctest for a new comparison with Catch.&lt;/p&gt;
&lt;p&gt;I really want to try to publish again some more posts on the blog. I'll
especially try to publish some more updates about Thor.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>Deep Learning</category><category>osdev</category><category>publications</category><category>thesis</category><category>thor</category><guid>http://baptiste-wicht.com/posts/2016/08/update-thor-thesis-and-publications.html</guid><pubDate>Tue, 23 Aug 2016 05:40:13 GMT</pubDate></item><item><title>Simplify Deep Learning Library usage on Linux and Windows!</title><link>http://baptiste-wicht.com/posts/2016/04/simplify-deep-learning-library-usage-on-linux-and-windows.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;No, I'm not dead ;) I've been very busy with my Ph.D (and playing Path of Exile,
let's be honest...) and haven't had time to write something here in a long time.&lt;/p&gt;
&lt;p&gt;Until now, there was too way to use my
&lt;a class="reference external" href="https://github.com/wichtounet/dll/"&gt;Deep Learning Library (DLL)&lt;/a&gt; project:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Write a C++ program that uses the library&lt;/li&gt;
&lt;li&gt;Install DLL and write a configuration file to define your network and the problem to solve&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first version gives you all the features of the tool and allows you to build
exactly what you need. The second version is a bit more limited, but does not
require any C++ knowledge. However, it still does require a recent C++ compiler
and build system.&lt;/p&gt;
&lt;p&gt;Due to the high C++ requirements that are not met by Visual Studio and the fact
that I don't work on Windows, this platform is not supported by the tool. Until
now!&lt;/p&gt;
&lt;p&gt;I've added a third option to use DLL in the form of a Docker image to make the
second option even easier and allow the use of DLL on Windows. All you need is
Docker, which is available on Linux, Mac and Windows. This is still limited to
the second option in that you need to write a configuration describing the
network, but you need to build DLL and don't need to install all its
dependencies.&lt;/p&gt;
&lt;div class="section" id="usage"&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;To install the image, you can simply use &lt;cite&gt;docker pull&lt;/cite&gt;:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_824288bf323f4bf1b3264fa979b3edf4-1"&gt;&lt;/a&gt;docker pull wichtounet/docker-dll
&lt;/pre&gt;&lt;p&gt;Then, to run it, you have to create a folder containing a &lt;cite&gt;dll.conf&lt;/cite&gt; file and
mount in the container at &lt;cite&gt;/dll/data/&lt;/cite&gt;. There are some examples in the
&lt;a class="reference external" href="https://github.com/wichtounet/docker-dll/"&gt;image repository&lt;/a&gt;.  For instance,
on Linux from the cloned repository:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_8fa3e8139423411b8b522ee57f1eafe6-1"&gt;&lt;/a&gt;docker run -v &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;pwd&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/rbm_mnist/:/dll/data/ wichtounet/docker-dll
&lt;/pre&gt;&lt;p&gt;or on Windows:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_e07104b49dc74b1d9be1daf155b5121c-1"&gt;&lt;/a&gt;docker run -v /c/Users/Baptiste/rbm_mnist/:/dll/data wichtounet/docker-dll
&lt;/pre&gt;&lt;p&gt;This will automatically run the actions specified in the configuration file and
train your network.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I would really have thought this would be harder, but it turned out that Docker
is a very good solution to deploy multiplatform demo tools :)&lt;/p&gt;
&lt;p&gt;As of now, there is only support for mnist data format in the tool in this
form, but I plan to add basic CSV support as well in the near future.&lt;/p&gt;
&lt;p&gt;I hope that this will help people willing to try the library with a simpler
usage.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>Deep Learning</category><category>dll</category><category>Linux</category><category>Machine Learning</category><category>projects</category><category>Windows</category><guid>http://baptiste-wicht.com/posts/2016/04/simplify-deep-learning-library-usage-on-linux-and-windows.html</guid><pubDate>Fri, 29 Apr 2016 10:48:18 GMT</pubDate></item><item><title>Short introduction to deep learning</title><link>http://baptiste-wicht.com/posts/2014/09/short-introduction-to-deep-learning.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;At my school, I gave a short presentation about Deep Learning and the
implementation I made in C++.&lt;/p&gt;
&lt;p&gt;It is nothing fancy, but it could be interesting to someone.&lt;/p&gt;
&lt;div style="text-align:center;"&gt;&lt;iframe src="//www.slideshare.net/slideshow/embed_code/39024941" width="476" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;p&gt;Don't hesitate if you have any comments or questions about the presentation ;)&lt;/p&gt;
&lt;p&gt;The implementation is here: &lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;https://github.com/wichtounet/dll&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>dbn</category><category>Deep Learning</category><category>dll</category><category>rbm</category><guid>http://baptiste-wicht.com/posts/2014/09/short-introduction-to-deep-learning.html</guid><pubDate>Fri, 12 Sep 2014 18:41:58 GMT</pubDate></item></channel></rss>