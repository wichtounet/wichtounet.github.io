<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Tutorials and short posts about programming, C++, Java, Assembly, Operating Systems Development, Compilers, ...">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Blog blog("Baptiste Wicht"); (old posts, page 31) | Blog blog("Baptiste Wicht");</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="http://baptiste-wicht.com/index-31.html">
<link rel="prev" href="index.html" type="text/html">
<link rel="next" href="index-30.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><link href="favicon.ico" rel="icon" type="image/x-icon">
<link rel="publisher" href="https://plus.google.com/+BaptisteWicht">
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-2175227-7', 'auto');
  var metas = document.getElementsByTagName('meta'), tagsList = [];
  for (var i=0; i<metas.length; i++) {
    if (metas[i].getAttribute('property') == 'article:tag') {
      tagsList.push( metas[i].getAttribute('content'));
    }
  }
  ga('set', 'dimension1', tagsList.join('|'));
  ga('send', 'pageview');
</script>
</head>
<body>

<!-- Menubar -->

<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<div class="container-fluid">
<!-- This keeps the margins nice -->
    <div class="row">
        <div class="col-sm-3 col-lg-2">
            <nav class="navbar navbar-inverse navbar-fixed-side"><div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="http://baptiste-wicht.com/">
                        <span id="blog-title">Blog blog("Baptiste Wicht");</span>
                    </a>
                </div>
<!-- /.navbar-header -->

                <div class="collapse navbar-collapse navbar-ex1-collapse">
                    <ul class="nav navbar-nav">
<li>
<a href="stories/about.html">About</a>
                </li>
<li>
<a href="stories/publications.html">Publications</a>
                </li>
<li>
<a href="stories/projects.html">Projects</a>
                </li>
<li>
<a href="categories/index.html">Tags</a>
                </li>
<li>
<a href="archive.html">Archives</a>
                </li>
<li>
<a href="http://feeds.feedburner.com/BaptisteWicht">RSS</a>


                            </li>
<li class="navbar-content">
                                <h3>Tags</h3>
                            </li>
                            <li class="navbar-empty">
                                <div id="tag_cloud_left_container" style="line-height: 18px !important;"></div>
                            </li>
                            <li class="navbar-block">


                        <li class="wicht-navbar-right">
                            <a target="_blank" title="Follow @wichtounet on Twitter" href="https://twitter.com/wichtounet">
                                <img src="assets/img/twitter.png" alt="Follow @wichtounet on Twitter"></a>
                        </li>

                        <li class="wicht-navbar-right">
                            <a target="_blank" title="Follow +BaptisteWicht on Google+" href="https://plus.google.com/+BaptisteWicht">
                                <img src="assets/img/google_plus.png" alt="Follow +BaptisteWicht on Google+"></a>
                        </li>


                    </ul>
</div>
<!-- /.navbar-collapse -->
            </nav>
</div> <!-- col -->
        <div class="col-sm-9 col-lg-10">
            <div id="content"></div>
            
        <article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/06/jenkins-tip-send-notifications-fixed-builds-declarative-pipeline.html" class="u-url">Jenkins Tip: Send notifications on fixed builds in declarative pipeline</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-06-07T09:52:57+02:00">2017-06-07 09:52</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>In <a class="reference external" href="https://baptiste-wicht.com/posts/2017/06/jenkins-declarative-pipeline-and-awesome-github-integration.html">my previous post</a>, I presented a few news about Jenkins and about the fact that I switched to declarative pipelines and Github Organization support for my projects.</p>
<p>The main issue I had with this system is that I lost the ability to get
notifications on build that recover. Normally, I would get an email indicating
that build X was back to normal, but I haven't found a way to solve that for
declarative pipeline.</p>
<p>By following a few posts on StackOverflow, I now have the solution and it is the
same problem that was already present in scripted pipelines. Namely, the status
of the current build is not set early enough for the notification.  Basically,
you have to set the notification yourself. Here is what a declarative pipeline
looks like:</p>
<pre class="code groovy"><a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-1"></a><span class="n">pipeline</span> <span class="o">{</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-2"></a>    <span class="n">agent</span> <span class="n">any</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-3"></a>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-4"></a>    <span class="n">stages</span> <span class="o">{</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-5"></a>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-6"></a>        <span class="c1">// Normal Stages</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-7"></a>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-8"></a>        <span class="n">stage</span> <span class="o">(</span><span class="s1">'success'</span><span class="o">){</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-9"></a>            <span class="n">steps</span> <span class="o">{</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-10"></a>                <span class="n">script</span> <span class="o">{</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-11"></a>                    <span class="n">currentBuild</span><span class="o">.</span><span class="na">result</span> <span class="o">=</span> <span class="s1">'SUCCESS'</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-12"></a>                <span class="o">}</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-13"></a>            <span class="o">}</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-14"></a>        <span class="o">}</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-15"></a>    <span class="o">}</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-16"></a>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-17"></a>    <span class="n">post</span> <span class="o">{</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-18"></a>        <span class="n">failure</span> <span class="o">{</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-19"></a>            <span class="n">script</span> <span class="o">{</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-20"></a>                <span class="n">currentBuild</span><span class="o">.</span><span class="na">result</span> <span class="o">=</span> <span class="s1">'FAILURE'</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-21"></a>            <span class="o">}</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-22"></a>        <span class="o">}</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-23"></a>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-24"></a>        <span class="n">always</span> <span class="o">{</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-25"></a>            <span class="n">step</span><span class="o">([</span><span class="n">$class</span><span class="o">:</span> <span class="s1">'Mailer'</span><span class="o">,</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-26"></a>                <span class="nl">notifyEveryUnstableBuild:</span> <span class="kc">true</span><span class="o">,</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-27"></a>                <span class="nl">recipients:</span> <span class="s2">"baptiste.wicht@gmail.com"</span><span class="o">,</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-28"></a>                <span class="nl">sendToIndividuals:</span> <span class="kc">true</span><span class="o">])</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-29"></a>        <span class="o">}</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-30"></a>    <span class="o">}</span>
<a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-31"></a><span class="o">}</span>
</pre>
<p>There are two important things here. First, a new stage (success) is added that
simply set the result of the current build to SUCCESS once it is done. It must
be the last stage on the pipeline. This could also be added as the last step of
the last stage instead of adding a new stage, but I think it's clearer like
this. The second thing is the failure block in which the result of the current
build is set to FAILURE. With these two things, the Mailer plugin now sends
notification when a build has been fixed.</p>
<p>I hope that will help some of you. I personally think that it should be much
easier than that. All this boilerplate is polluting the pipeline that should be
kept more maintainable, but for now it seems, it's the nicest way to achieve
that, short of handling all conditions in the post block and sending mail
directly there, but that would result in even more boilerplate code.</p>
</div>
        </div>
            
        
    <a href="posts/2017/06/jenkins-tip-send-notifications-fixed-builds-declarative-pipeline.html#disqus_thread" data-disqus-identifier="cache/posts/2017/06/jenkins-tip-send-notifications-fixed-builds-declarative-pipeline.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/06/jenkins-declarative-pipeline-and-awesome-github-integration.html" class="u-url">Jenkins Declarative Pipeline and Awesome Github Integration</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-06-04T20:52:10+02:00">2017-06-04 20:52</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>This post is about some news about Jenkins and how I've updated my Jenkins
usage. This may be a bit of an enthusiastic post ;)</p>
<p>At the beginning of Jenkins, the best way to define the commands to be executed
for your builds was simply to write the commands in the Jenkins interface. This
worked quite well. Later on, Jenkins introduced the notion of Pipeline. Instead
of a single set of commands to be executed, the build was defined in
multi-stages pipeline of commands. This is defined as a Groovy script. One big
advantage of this is that all the code for creating the build is inside the
repository. This has the advantage that each build is reproducible. This enabled
to define complex pipelines of commands for your builds. Moreover, this also
allows to have a clean view of which steps are failing and which steps are
taking how much of the time of the build. For instance, here it's a view of the
pipeline steps for my DLL project:</p>
<img alt="Jenkins stage view for DLL pipeline." class="align-center" src="images/jenkins_pipeline_dll.png"><p>I think that's pretty cool :)</p>
<p>They recently added a new feature, the declarative pipelines. Instead of
scripting the Pipeline in Groovy, the new system uses its own syntax, completely
declarative, to put blocks together and add ways of doing actions at specific
points and setting environment and so on. I think the new syntax is much nicer
than the Groovy scripted Pipeline way, so I started converting my scripts. I'll
give an example in a few paragraphs. But first, I'd like to talk about Github
integration. Before, every time I created a new project, I add to add it to
Jenkins by creating a new project, updating the link to the Github project and
a few things in order to add it. This is not so bad but what if you want to
build on several branches and keep track of the status of the branches and maybe
of the Pull Requests as well. All of this is now very simple. You can now
declare the Github organizations (and users) you are of building projects from
and the projects inside the organization will be automatically detected as long
as they have a Jenkinsfile inside. That means that you'll never have to create
a project yourself or handle branches. Indeed, all the created projects can now
handle multiples. For instance, here is the status of the two current branches
of my dll project:</p>
<img alt="Jenkins branches for DLL Github project" class="align-center" src="images/jenkins_github_branches.png"><p>It's maybe not the best example since one branch is failing and the other is
unstable, but you can see that you can track the builds for each branch in
a nice way.</p>
<p>A very good feature of this integration is that Jenkins will now automatically
marks commits on your Github with the status of your builds at no cost! For
instance, here is the status on my ETL project after I configured on Jenkins and
made the first builds:</p>
<img alt="Jenkins marking commits in Github" class="align-center" src="images/jenkins_github_integration_marks.png"><p>Pretty cool I think :)</p>
<p>Another nice thing in Jenkins is the Blue Ocean interface. This is an
alternative interface, especially well-suited for multi-branch projects and
pipelines. It looks much more modern and I think it's quite good. Here are a few
views of it:</p>
<p>The Activity view for the last events of the project:</p>
<img alt="Jenkins Blue Ocean Activity view for DLL project" class="align-center" src="images/jenkins_blue_ocean_dll_activity.png"><p>The Branches view for the status of each branch:</p>
<img alt="Jenkins Blue Ocean Branches view for DLL project" class="align-center" src="images/jenkins_blue_ocean_dll_branches.png"><p>The view of the status of a build:</p>
<img alt="Jenkins Blue Ocean view of a build for DLL project" class="align-center" src="images/jenkins_blue_ocean_dll_build.png"><p>The status of the tests for a given build:</p>
<img alt="Jenkins Blue Ocean view of a build tests for DLL project" class="align-center" src="images/jenkins_blue_ocean_dll_build_tests.png"><p>It's likely that it won't appeal to everyone, but I think it's pretty nice.</p>
<p>If we get back to the declarative Pipeline, here is the declarative pipeline for
my Expression Templates Library (ETL) project:</p>
<pre class="code groovy"><a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-1"></a><span class="n">pipeline</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-2"></a>    <span class="n">agent</span> <span class="n">any</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-3"></a>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-4"></a>    <span class="n">environment</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-5"></a>       <span class="n">CXX</span> <span class="o">=</span> <span class="s2">"g++-4.9.4"</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-6"></a>       <span class="n">LD</span> <span class="o">=</span> <span class="s2">"g++-4.9.4"</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-7"></a>       <span class="n">ETL_MKL</span> <span class="o">=</span> <span class="s1">'true'</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-8"></a>    <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-9"></a>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-10"></a>    <span class="n">stages</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-11"></a>        <span class="n">stage</span> <span class="o">(</span><span class="s1">'git'</span><span class="o">){</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-12"></a>            <span class="n">steps</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-13"></a>                <span class="n">checkout</span><span class="o">([</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-14"></a>                    <span class="n">$class</span><span class="o">:</span> <span class="s1">'GitSCM'</span><span class="o">,</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-15"></a>                    <span class="nl">branches:</span> <span class="n">scm</span><span class="o">.</span><span class="na">branches</span><span class="o">,</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-16"></a>                    <span class="nl">doGenerateSubmoduleConfigurations:</span> <span class="kc">false</span><span class="o">,</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-17"></a>                    <span class="nl">extensions:</span> <span class="n">scm</span><span class="o">.</span><span class="na">extensions</span> <span class="o">+</span> <span class="o">[[</span><span class="n">$class</span><span class="o">:</span> <span class="s1">'SubmoduleOption'</span><span class="o">,</span> <span class="nl">disableSubmodules:</span> <span class="kc">false</span><span class="o">,</span> <span class="nl">recursiveSubmodules:</span> <span class="kc">true</span><span class="o">,</span> <span class="nl">reference:</span> <span class="s1">''</span><span class="o">,</span> <span class="nl">trackingSubmodules:</span> <span class="kc">false</span><span class="o">]],</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-18"></a>                    <span class="nl">submoduleCfg:</span> <span class="o">[],</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-19"></a>                    <span class="nl">userRemoteConfigs:</span> <span class="n">scm</span><span class="o">.</span><span class="na">userRemoteConfigs</span><span class="o">])</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-20"></a>            <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-21"></a>        <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-22"></a>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-23"></a>        <span class="n">stage</span> <span class="o">(</span><span class="s1">'pre-analysis'</span><span class="o">)</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-24"></a>            <span class="n">steps</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-25"></a>                <span class="n">sh</span> <span class="s1">'cppcheck --xml-version=2 -j3 --enable=all --std=c++11 `git ls-files "*.hpp" "*.cpp"` 2&gt; cppcheck_report.xml'</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-26"></a>                <span class="n">sh</span> <span class="s1">'sloccount --duplicates --wide --details include/etl test workbench &gt; sloccount.sc'</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-27"></a>                <span class="n">sh</span> <span class="s1">'cccc include/etl/*.hpp test/*.cpp workbench/*.cpp || true'</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-28"></a>            <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-29"></a>        <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-30"></a>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-31"></a>        <span class="n">stage</span> <span class="o">(</span><span class="s1">'build'</span><span class="o">){</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-32"></a>            <span class="n">steps</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-33"></a>                <span class="n">sh</span> <span class="s1">'make clean'</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-34"></a>                <span class="n">sh</span> <span class="s1">'make -j6 release'</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-35"></a>            <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-36"></a>        <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-37"></a>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-38"></a>        <span class="n">stage</span> <span class="o">(</span><span class="s1">'test'</span><span class="o">){</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-39"></a>            <span class="n">steps</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-40"></a>                <span class="n">sh</span> <span class="s1">'ETL_THREADS=-j6 ETL_GPP=g++-4.9.4 LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/opt/intel/mkl/lib/intel64:/opt/intel/lib/intel64\" ./scripts/test_runner.sh'</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-41"></a>                <span class="n">archive</span> <span class="s1">'catch_report.xml'</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-42"></a>                <span class="n">junit</span> <span class="s1">'catch_report.xml'</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-43"></a>            <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-44"></a>        <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-45"></a>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-46"></a>        <span class="n">stage</span> <span class="o">(</span><span class="s1">'sonar-master'</span><span class="o">){</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-47"></a>            <span class="n">when</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-48"></a>                <span class="n">branch</span> <span class="s1">'master'</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-49"></a>            <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-50"></a>            <span class="n">steps</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-51"></a>                <span class="n">sh</span> <span class="s2">"/opt/sonar-runner/bin/sonar-runner"</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-52"></a>            <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-53"></a>        <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-54"></a>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-55"></a>        <span class="n">stage</span> <span class="o">(</span><span class="s1">'sonar-branch'</span><span class="o">){</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-56"></a>            <span class="n">when</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-57"></a>                <span class="n">not</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-58"></a>                    <span class="n">branch</span> <span class="s1">'master'</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-59"></a>                <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-60"></a>            <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-61"></a>            <span class="n">steps</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-62"></a>                <span class="n">sh</span> <span class="s2">"/opt/sonar-runner/bin/sonar-runner -Dsonar.branch=${env.BRANCH_NAME}"</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-63"></a>            <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-64"></a>        <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-65"></a>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-66"></a>        <span class="n">stage</span> <span class="o">(</span><span class="s1">'bench'</span><span class="o">){</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-67"></a>            <span class="n">steps</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-68"></a>                <span class="n">build</span> <span class="nl">job:</span> <span class="s1">'etl - benchmark'</span><span class="o">,</span> <span class="nl">wait:</span> <span class="kc">false</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-69"></a>            <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-70"></a>        <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-71"></a>    <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-72"></a>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-73"></a>    <span class="n">post</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-74"></a>        <span class="n">always</span> <span class="o">{</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-75"></a>            <span class="n">step</span><span class="o">([</span><span class="n">$class</span><span class="o">:</span> <span class="s1">'Mailer'</span><span class="o">,</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-76"></a>                <span class="nl">notifyEveryUnstableBuild:</span> <span class="kc">true</span><span class="o">,</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-77"></a>                <span class="nl">recipients:</span> <span class="s2">"baptiste.wicht@gmail.com"</span><span class="o">,</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-78"></a>                <span class="nl">sendToIndividuals:</span> <span class="kc">true</span><span class="o">])</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-79"></a>        <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-80"></a>    <span class="o">}</span>
<a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-81"></a><span class="o">}</span>
</pre>
<p>There is nothing really fancy about, it's probably average. Moreover, since I'm
not an expert on pipelines and I've just discovered declarative pipelines, it
may not be optimal, but it works. As you'll see there are some problems
I haven't been able to fix.</p>
<p>The first part declares the environment variables for the build. Then, the
multiple build stages are listed. The first stage checkout the code from the
SCM. This ugly piece of code is here to allow to checkout the submodules. It is
the only solution I have found so far. It's very ugly but it works. The second
steps is simply some basic static analysis. The next step is the classical build
step. Then, the tests are run. In that case, I'm using a script because the
tests are compiled with several different sets of options and it was much easier
to put that in a script that in the Pipeline. Moreover, that also means I can
run them standalone. The variables in the line to run the script is another
problem I haven't been able to fix so far. If I declare these variables in an
environment block, they are not passed to the script for some reason, so I had
to use this ugly line. The next two blocks are for Sonar analysis. If you start
with Sonar, you can simply the second block that passed the branch information
to Sonar. Unfortunately, Sonar is very limited in terms of Git branches. Each
branch is considered as another totally different project. That means the false
positives defined in the master branch will not be used in the second branch.
Therefore, I kept a clean master and several different projects for the other
branches. Once Sonar improves this branch handling stuff, if they ever do, I'll
be able to get rid of one of these conditional stages. The last stage is simple
running the benchmark job. Finally, the post block is using the Mailer plugin to
send failed builds information. Again, there is a problem here since this does
not send "Back to normal" information as it used to do before. I've asked this
question on StackOverflow, but haven't received an answer so far. I'll post
a better solution on this blog once I have one. If any of you have some
solutions to these problems, don't hesitate to post in the comments below or to
contact me on Github.</p>
<p>Here it is. I really think Jenkins is getting even greater now with all this
cool stuff and I advice you to try it out!</p>
</div>
        </div>
            
        
    <a href="posts/2017/06/jenkins-declarative-pipeline-and-awesome-github-integration.html#disqus_thread" data-disqus-identifier="cache/posts/2017/06/jenkins-declarative-pipeline-and-awesome-github-integration.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html" class="u-url">C++ Containers Benchmark: vector/list/deque and plf::colony</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-05-21T12:46:23+02:00">2017-05-21 12:46</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>Already more than three years ago, I've written a <a class="reference external" href="https://baptiste-wicht.com/posts/2012/12/cpp-benchmark-vector-list-deque.html">benchmark of some of the STL containers</a>,
namely the vector, the list and the deque. Since this article was very popular,
I decided to improve the benchmarks and collect again all the results. There are
now more benchmarks and some problems have been fixed in the benchmark code.
Moreover, I have also added a new container, the plf::colony. Therefore, there
are four containers tested:</p>
<ul class="simple">
<li>The std::vector: This is a dynamically-resized array of elements. All the
elements are contiguous in memory. If an element is inserted or removed it at
a position other than the end, the following elements will be moved to fill
the gap or to open a gap. Elements can be accessed at random position in
constant time. The array is resized so that it can several more elements, not
resized at each insert operation. This means that insertion at the end of the
container is done in amortized constant time.</li>
<li>The std::deque: The deque is a container that offer constant time insertion
both at the front and at the back of the collection. In current c++ libraries,
it is implementation as a collection of dynamically allocated fixed-size
array. Not all elements are contiguous, but depending on the size of the data
type, this still has good data locality. Access to a random element is also
done in constant time, but with more overhead than the vector. For insertions
and removal at random positions, the elements are shifted either to the front
or to the back meaning that it is generally faster than the vector, by twice
in average.</li>
<li>The std::list: This is a doubly-linked list. It supports constant time
insertions at any position of the collection. However, it does not support
constant time random access. The elements are obviously not contiguous, since
they are all allocated in nodes. For small elements, this collection has
a very big memory overhead.</li>
<li>The plf::colony: This container is a non-standard container which is
unordered, it means that the insertion order will not necessarily be
preserved. It provides strong iterators guarantee, pointers to non-erased
element are not invalidated by insertion or erasure. It is especially tailored
for high-insertion/erasure workloads. Moreover, it is also specially optimized
for non-scalar types, namely structs and classes with relatively large data
size (greater than 128 bits on the official documentation). Its implementation
is more complicated than the other containers. It is also implemented as
a list of memory blocks, but they are of increasingly large sizes. When
elements are erased, there position is not removed, but marked as erased so
that it can be reused for fast insertion later on. This container uses the
same conventions as the standard containers and was proposed for inclusion to
the standard library, which is the main reason why it's included in this
benchmark. If you want more information, you can consult the
<a class="reference external" href="http://plflib.org/colony.htm">official website</a>.</li>
</ul>
<p>In the text and results, the namespaces will be omitted. Note that I have only
included sequence containers in my test. These are the most common containers in
practices and these also the containers I'm the most familiar with. I could have
included multiset in this benchmark, but the interface and purpose being
different, I didn't want the benchmark to be confusing.</p>
<p>All the examples are compiled with g++-4.9.4 (-std=c++11 -march=native -O2) and
run on a Gentoo Linux machine with an Intel Core i7-4770 at 3.4GHz.</p>
<p>For each graph, the vertical axis represent the amount of time necessary to
perform the operations, so the lower values are the better. The horizontal axis
is always the number of elements of the collection. For some graph, the
logarithmic scale could be clearer, a button is available after each graph to
change the vertical scale to a logarithmic scale.</p>
<p>The tests are done with several different data types. The trivial data types are
varying in size, they hold an array of longs and the size of the array varies to
change the size of the data type. The non-trivial data type is composed of
a string (just long enough to avoid SSO (Small String Optimization) (even though
I'm using GCC)). The non-trivial data types comes in a second version with
noexcept move operations.  Not all results are presented for each data types if
there are not significant differences between in order to keep this article
relatively short (it's already probably too long :P).</p>
<p class="more"><a href="posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html">Read more…</a></p>
</div>
        </div>
            
        
    <a href="posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html#disqus_thread" data-disqus-identifier="cache/posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/05/speed-up-tensorflow-inference-compiling-from-source.html" class="u-url">Speed up TensorFlow inference by compiling it from source</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-05-10T14:18:33+02:00">2017-05-10 14:18</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>The most simple way to install TensorFlow is to work in a virtual Python
environment and simply to use either the TensorFlow official packages in pip or
use one of the official wheels for distributions.  There is one big problem with
that technique and it's the fact that the binaries are precompiled so that they
fit as many hardware configuration as possible. This is normal from Google since
generating precompiled binaries for all the possible combinations of processor
capabilities would be a nightmare. This is not a problem for GPU
since the CUDA Libraries will take care of the difference from one graphics card
to another. But it is a problem with CPU performance. Indeed, different
processors have different capabilities. For instance, the vectorization
capabilities are different from processor to processor (SSE, AVX, AVX2,
AVX-512F, FMA, ...). All those options can make a significant difference in the
performance of the programs. Although most of the machine learning training
occurs on GPU most of the time, the inference is mostly done on the CPU.
Therefore, it probably remains important to be as fast as possible on CPU.</p>
<p>So if you care about performance on CPU, you should install TensorFlow from
sources directly yourself. This will allow compilation of the TensorFlow sources
with -march=native which will enable all the hardware capabilities of machine on
which you are compiling the library.</p>
<p>Depending on your problem, this may give you some nice speedup. In my case, on
a very small Recurrent Neural Network, it made inference about 20% faster.  On
a larger problem and depending on your processor, you may gain much more than
that. If you are training on CPU, this may make a very large difference in total
time.</p>
<p>Installing TensorFlow is sometimes a bit cumbersome. You'll likely have to
compile Bazel from sources as well and depending on your processor, it may take
a long time to finish. Nevertheless, I have successfully compiled TensorFlow
from sources on several machines now without too many problems. Just pay close
attention to the options you are setting while configuring TensorFlow, for
instance CUDA configuration if you want GPU support.</p>
<p>I hope this little trick will help you gain some time :)</p>
<p>Here is the <a class="reference external" href="https://www.tensorflow.org/install/install_sources">link to compile TensorFlow from source</a>.</p>
</div>
        </div>
            
        
    <a href="posts/2017/05/speed-up-tensorflow-inference-compiling-from-source.html#disqus_thread" data-disqus-identifier="cache/posts/2017/05/speed-up-tensorflow-inference-compiling-from-source.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/05/update-on-expression-templates-library-etl.html" class="u-url">Update on Expression Templates Library (ETL)</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-05-06T21:31:48+02:00">2017-05-06 21:31</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>It's been a while since I've <a class="reference external" href="https://baptiste-wicht.com/posts/2016/09/expression-templates-library-etl-10.html">released the version 1.0 of ETL</a>. There is some work to do before I release the next version, but I wanted to give you a quick update on what has been going on for ETL in the last months. There has been a lot of changes in the library and the next version will be a major update when I'm done with some refactorings and improvements.</p>
<p>Thanks to my thesis supervisor, the project now has a logo:</p>
<img alt="ETL Logo" class="align-center" src="images/logo.png"><p>There are quite a few new features, although probably nothing really major. The
support for square root has been improved with cubic root and inverse root.
Vectors can now be transformed using floor and ceil. Cross product of vector has
been implemented as well. Batched outer product and batched bias averaging (for
machine learning) are now supported. Reductions have also been improved with
absolute sum and mean (asum/asum) support and min_index and max_index. argmax
can now be used to get the max index in each sub dimensions. Matrix can now be
decomposed into their Q/R decomposition rather than only their PALU
decomposition. The matrices can now be sliced by getting only a sub part of the
matrix. The pooling operators  have also been improved with stride and padding
support. Matrices and vectors can also be shuffled. Moreover, a few adapters
are now available for hermitian matrices, symmetric matrices and lower and upper
matrices. So far the support for these adapters is not huge, but they are
guaranteed to validate their constraints.</p>
<p>Several operations have been optimized for speed. All the pooling and upsample
operators are now parallelized and the most used kernel (2x2 pooling) is now
more optimized. 4D convolution kernels (for machine learning) have been greatly
improved. There are now very specialized vectorized kernels for classic kernel
configurations (for instance 3x3 or 5x5) and the selection of implementations is
now smarter than before. The support of padding now much better than before for
small amount of padding. Moreover, for small kernels the full convolution can
now be evaluated using the valid convolution kernels directly with some padding,
for much faster overall performance. Matrix-matrix multiplication with
transposed matrices is now much faster when using BLAS kernels. Indeed, the
transposition is not performed but handled inside the kernels. Moreover, the
performance of the transposition itself is also much faster. Finally, accesses
to 3D and 4D matrices is now much faster than before.</p>
<p>The parallelization feature of ETL has been completely reworked. Before, there
was a thread pool for each algorithm that was parallelized. Now, there is
a global thread engine with one thread pool. Since parallelization is not nested
in ETL, this improves performance slightly by greatly diminishing the number of
threads that are created throughout an application.</p>
<p>Vectorization has also been greatly improved in ETL. Integer operations are now
automatically vectorized on processors that support this. The automatic
vectorizer now is able to use non-temporal stores for very large operations.
A non-temporal store bypasses the cache, thus gaining some time. Since very
large matrices do not fit in cache, this is a net gain. Moreover, the alignment
detection in the automatic vectorizer has also been improved. Support for
Fused-Multiply-Add (FMA) operations has also been integrated in the algorithms
that can make use of it. The matrix-matrix multiplications and vector-matrix
multiplications now have optimized vectorized kernels. They also have versions
for column-major matrices now. The old egblas version of the gemm, based on BLIS
kernels, has been removed since it was only supporting double-precision and was
not faster than the new vectorized algorithm. I plan to reintegrate a version of
the GEMM based on BLIS in the future but with more optimizations and support for
all precisions and integers. The sum and the dot product now also have
specialized vectorized implementations. The min and max operations are now
automatically-vectorized.</p>
<p>The GPU has also been almost completely reworked. Now, operations can be chained
without any copies between GPU and CPU. Several new operations have also been
added with support to GPU. Moreover, to complement operations that are not
available in any of the supported NVIDIA libraries, I've created a simple
library that can be used to add a few more GPU operations. Nevertheless a lot of
operations are still missing and only algorithms are available not expressions
(such as c = a + b * 1.0) that are entirely computed on CPU. I have plans to
improve that further, but probably not before the version 1.2.</p>
<p>There also have been a lot of refactorings in the code of the library. A lot of
expressions now have less overhead and are specialized for performance.
Moreover, temporary expressions are currently being reworked in order to be more
simple and maintainable and easier to optimize in the future.</p>
<p>Finally, there also was quite a few bug fixes. Most of them have been found by
the use of the library in the Deep Learning Library (DLL) project.</p>
</div>
        </div>
            
        
    <a href="posts/2017/05/update-on-expression-templates-library-etl.html#disqus_thread" data-disqus-identifier="cache/posts/2017/05/update-on-expression-templates-library-etl.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/04/home-automation-power-meter-integration-zwave-domoticz.html" class="u-url">Home Automation: Power Meter and Integration of Zwave into Domoticz</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-04-23T16:43:13+02:00">2017-04-23 16:43</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>I've improved a bit my home automation installation. It's been a while since
the last upgrade, but unfortunately I cannot afford as many upgrades as I would
like :P</p>
<p>For a long time I wanted to monitor the power consumption of a few of my
appliances in my apartment. Especially my Linux servers so that I could try to
improve the consumption and reduce my bill on the long run. Unfortunately, there
are very few options for power meter in Switzerland due to the special type of
plug we have. The only option I found is a Zwave power plug. For a while,
I waited to see if I could find other options because Zwave devices are quite
expensive and I would have rather preferred to stay with simpler and cheaper
RF-433 appliances. Since I didn't find anything, I ordered a ZWave USB
controller from  Aeon Labs (the generation 5). I also ordered two Aeon Labs
Swiss Smart Plug with power meter.</p>
<p>Here is an image of the Aeon Labs key:</p>
<img alt="Aeon Labs ZWave USB Key" class="align-center" src="images/zwave_usb.jpg"><p>And of the power meter in usage:</p>
<img alt="ZWave power meter" class="align-center" src="images/power_meter.jpg"><p>Integration of ZWave into Domoticz was extremely easy. I just plugged the USB
key, restarted Domoticz (seems necessary for it to pick the new tty) and added
new hardware "OpenZWave USB" with the correct serial port. From there, there are
two main ways to add new devices. The first is to remove the USB key and use the
synchronization button on both the key and the device close to each other. The
other way is to use the "Include Node" option on Domoticz and then press the
synchronization button on the device to detect the new device. I used the second
option since it seemed simpler and it worked perfectly. I did that for my two
plugs and it worked fine. Directly after this, 5 new devices were added for each
of the plug. One for the voltage, one for the current , two for the  usage (I
don't know why there is two, but they are both reporting the same value) and one
for the switch on/off. I was a bit afraid that only the On/Off part of the smart
plug would work on Domoticz, but I had absolutely no problem.</p>
<p>Here is for instance the power usage of last 24 hours on my television system:</p>
<img alt="Power usage on television system" class="align-center" src="images/domoticz_power_usage.png"><p>For now, I haven't integrated this information on any rule, but I plan to
monitor this information in the coming weeks and try to improve my consumption,
especially for my servers. I also plan to purchase more of these plugs once my
home automation budget can be upgraded.</p>
<p>On another note, I also purchased a Chacon wall remote switch working in RF-433.
Although it is quite cheap, I'm very disappointed by the quality of this switch.
I add to straighten myself the pins that are attached to the battery because
there was no contact. After that, it worked correctly and it is able to work
with the RFLink module.</p>
<p>I have to say that I'm quite satisfied with ZWave devices with this experience.
Even though I still feel it is way too expensive, it is high quality and have
a good finishing. I'll probably purchase more ZWave devices in the future. I'm
especially interested in The Aeotec 6 in 1 sensor for temperature humidity,
motion, light, UV and vibration. This would allow me to have much information in
each room with only one sensor in place of several sensors in each room like
I currently have.</p>
<p>I still have a few Milight Bulbs and LEDS to install with a secondary Milight
bridge that I will install in the coming week, but I probably won't do a post
about this.</p>
</div>
        </div>
            
        
    <a href="posts/2017/04/home-automation-power-meter-integration-zwave-domoticz.html#disqus_thread" data-disqus-identifier="cache/posts/2017/04/home-automation-power-meter-integration-zwave-domoticz.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/04/publications-deep-learning-features-handwritten-keyword-spotting.html" class="u-url">Publications: Deep Learning Features for Handwritten Keyword Spotting</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-04-21T20:29:39+02:00">2017-04-21 20:29</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>After my previous post about my publication on CPU performance optimization,
I wanted to talk a bit about two publications on Handwritten Keyword Spotting,
in which we extract features with Convolutional RBM RBM</p>
<p>We published two different papers:</p>
<ul class="simple">
<li>
<a class="reference external" href="https://www.researchgate.net/publication/306081095_Keyword_Spotting_with_Convolutional_Deep_Belief_Networks_and_Dynamic_Time_Warping">Keyword Spotting With Convolutional Deep Belief Networks and Dynamic Time Warping</a>, in the Proceedings of the International Conference on Artificial Neural Networks (ICANN-2016), Barcelona, Spain</li>
<li>Mixed Handwritten and printed digit recognition in Sudoku With Convolutional Deep Belief Network (Link will come), in the Proceedings of the International Conference on Pattern Recognition (ICPR-2016), Cancun, Mexico</li>
</ul>
<p>The second paper is mostly a large extension of the first one, so I'll focus on
the complete version.</p>
<p>On a side note, I also co-authored a third paper:</p>
<ul class="simple">
<li>
<a class="reference external" href="https://www.researchgate.net/publication/312486359_Inkball_Models_as_Features_for_Handwriting_Recognition">Inkball Models as Features for Handwriting Recognition</a>, in the Proceedings of the International Conference on Frontiers of Handwriting Recognition (ICFHR-2016), Shenzen, China</li>
</ul>
<p>We mostly used our existing system to generate features for a comparison between
different set of features for handwritten keyword spotting. It was my first time
in China and I enjoyed the stay a lot. I also had the chance to meet my
girlfriend in Shenzen, all the more reason to mention this publication :)</p>
<p>Back on the main subject. The idea behind these publications is to
a Convolutional Deep Belief Network (CDBN) to extract features from the images
and then pass these features to either a Dynamic Time Warping (DTW) algorithm or
an Hidden Markov Model (HMM). The following image describe the overall system:</p>
<img alt="Keyword Spotting System" class="align-center" src="images/kws_system.png"><p>The features are extracted from preprocessed normalized binary images. Using
a sliding window, moving from left to right, one pixel at a time, the features
are extracted on each window. The feature extractor is a Convolutional Deep
Belief Network, trained fully unsupervised. The features are then normalized so
that each feature group sum to one and then each has zero-mean and
unit-variance. The network used for feature extraction is depicted in the
following image:</p>
<img alt="Convolutional Deep Belief Network features" class="align-center" src="images/kws_network.png"><p>Two Convolutional Restricted Boltzmann Machines (CRBMs) are used, each followed
by a max pooling layer.</p>
<p>Once the features are extracted, they can be passed to the classifier for
keyword spotting scoring. We tested our features with two different approaches
for word scoring. The first one is a template matching strategy, Dynamic Time
Warping (DTW), is a very simple measure of distance between two sequences of
different length. The two sequences are warped non-linearly to minimize the
distance between each pair of features. A template from the training set is
compared to the word image being evaluated. This works pretty well for simple
data sets but fails when the writing styles of the test set are not known in the
training set. The second classifier is more powerful and trained, a Hidden
Markov Model (HMM). Character models are trained using the entire training set.
From these character models, a keyword model as well as an unconstrained model
(the filler model) are constructed. The probability of these two models is
computed using Viterbi and the final score is computed using log-odds scoring of
these two models using the filler model as a form of normalization.</p>
<p>This technique was evaluated on three datasets (George Washington (GW), Parzival
(PAR) and IAM offline database (IAM)). Our features were compared with three
reference feature sets, one heuristic and two local feature sets.</p>
<p>The results for DTW:</p>
<img alt="Keyword Spotting Results with Dynamic Time Warping" class="align-center" src="images/kws_results_dtw.png"><p>Overall, our features exhibit better performance than the other reference.
Except for the Mean Average Precision on the PAR data set. The very low
performance on PAR with DTW is explained by the fact mentioned earlier that it
has poor generalization to unknown writing styles.</p>
<p>The results for HMM:</p>
<img alt="Keyword Spotting Results with Hidden Markov Model" class="align-center" src="images/kws_results_hmm.png"><p>With HMM, our features are always better than the other feature sets. However,
the margin of improvement is smaller than when using DTW.</p>
<p>Overall, the proposed system proved quite powerful and was able to outperform
the three tested feature sets on three datasets for keyword spotting.</p>
<p>You can find the <a class="reference external" href="https://github.com/wichtounet/word_spotting">C++ implementation on Github</a>.</p>
<p>As for my thesis, I have finished the writings about a month ago and it is now
in the hands on my supervisor.</p>
<p>If you want to have a look, the
<a class="reference external" href="http://baptiste-wicht.com/stories/publications.html">list of my publications</a>
is available on this website.</p>
<p>If you want more details on this project, don't hesitate to ask here or on
Github, or read the papers :)</p>
<p>I hope the next post about my publications will be about the finalization of my
thesis :)</p>
</div>
        </div>
            
        
    <a href="posts/2017/04/publications-deep-learning-features-handwritten-keyword-spotting.html#disqus_thread" data-disqus-identifier="cache/posts/2017/04/publications-deep-learning-features-handwritten-keyword-spotting.html">Comments</a>


        </article><article class="postbox h-entry post-text"><h1 class="p-name">
<a href="posts/2017/03/partial-type-erasing-deep-learning-library-dll-improve-compilation-time.html" class="u-url">Partial type erasing in Deep Learning Library (DLL) to improve compilation time</a>
        <br><small>  
             Posted: <time class="published dt-published" datetime="2017-03-15T07:43:44+01:00">2017-03-15 07:43</time></small>
</h1>
        <hr>
<div class="p-summary">
        <div>
<p>In a previous post, I compared the <a class="reference external" href="https://baptiste-wicht.com/posts/2017/03/disappointing-zapcc-performance-on-deep-learning-library-dll.html">compilation time on my Deep Learning Library (DLL) project with different compilers</a>. I realized that the compilation times were quickly going unreasonable for this library, especially for compiling the unit cases which clearly hurts the development of the library. Indeed, you want to be able to run the unit tests reasonably quickly after you integrated new changes.</p>
<div class="section" id="reduce-the-compilation-time">
<h2>Reduce the compilation time</h2>
<p>The first thing I did was to split the compilation in three executables: one for
the unit tests, one for the various performance tests and one for the various other
miscellaneous tests. With this, it is much faster to compile only the unit test
cases.</p>
<p>But this can be improved significantly more. In DLL a network is a variadic
template containing the list of layers, in order. In DLL, there are two main
different ways of declaring a neural networks. In the first version, the fast
version, the layers directly know their sizes:</p>
<pre class="code cpp"><a name="rest_code_f16feeb38686463faeb78f276a61043d-1"></a><span class="k">using</span> <span class="n">network_t</span> <span class="o">=</span>
<a name="rest_code_f16feeb38686463faeb78f276a61043d-2"></a>    <span class="n">dll</span><span class="o">::</span><span class="n">dbn_desc</span><span class="o">&lt;</span>
<a name="rest_code_f16feeb38686463faeb78f276a61043d-3"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dbn_layers</span><span class="o">&lt;</span>
<a name="rest_code_f16feeb38686463faeb78f276a61043d-4"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">rbm_desc</span><span class="o">&lt;</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_f16feeb38686463faeb78f276a61043d-5"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">rbm_desc</span><span class="o">&lt;</span><span class="mi">500</span>    <span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_f16feeb38686463faeb78f276a61043d-6"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">rbm_desc</span><span class="o">&lt;</span><span class="mi">400</span>    <span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">hidden</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">unit_type</span><span class="o">::</span><span class="n">SOFTMAX</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="o">&gt;</span><span class="p">,</span>
<a name="rest_code_f16feeb38686463faeb78f276a61043d-7"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">trainer</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">sgd_trainer</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;&gt;::</span><span class="n">dbn_t</span><span class="p">;</span>
<a name="rest_code_f16feeb38686463faeb78f276a61043d-8"></a>
<a name="rest_code_f16feeb38686463faeb78f276a61043d-9"></a><span class="k">auto</span> <span class="n">network</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">network_t</span><span class="o">&gt;</span><span class="p">();</span>
<a name="rest_code_f16feeb38686463faeb78f276a61043d-10"></a><span class="n">network</span><span class="o">-&gt;</span><span class="n">pretrain</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">training_images</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
<a name="rest_code_f16feeb38686463faeb78f276a61043d-11"></a><span class="n">network</span><span class="o">-&gt;</span><span class="n">fine_tune</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">training_images</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">training_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
</pre>
<p>In my opinion, this is the best way to use DLL. This is the fastest and the
clearest. Moreover, the dimensions of the network can be validated at compile
time, which is always better than at runtime. However, the dimensions of the
network cannot be changed at runtime.  For this, there is a different version,
the dynamic version:</p>
<pre class="code cpp"><a name="rest_code_7dba155d64d04952aa1bff84e2570a48-1"></a><span class="k">using</span> <span class="n">network_t</span> <span class="o">=</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-2"></a>    <span class="n">dll</span><span class="o">::</span><span class="n">dbn_desc</span><span class="o">&lt;</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-3"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dbn_layers</span><span class="o">&lt;</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-4"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">dyn_rbm_desc</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-5"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">dyn_rbm_desc</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-6"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">dyn_rbm_desc</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">hidden</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">unit_type</span><span class="o">::</span><span class="n">SOFTMAX</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="o">&gt;</span><span class="p">,</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-7"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">trainer</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">sgd_trainer</span><span class="o">&gt;&gt;::</span><span class="n">dbn_t</span><span class="p">;</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-8"></a>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-9"></a><span class="k">auto</span> <span class="n">network</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">network_t</span><span class="o">&gt;</span><span class="p">();</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-10"></a>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-11"></a><span class="n">network</span><span class="o">-&gt;</span><span class="k">template</span> <span class="n">layer_get</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">().</span><span class="n">init_layer</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">500</span><span class="p">);</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-12"></a><span class="n">network</span><span class="o">-&gt;</span><span class="k">template</span> <span class="n">layer_get</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">().</span><span class="n">init_layer</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">400</span><span class="p">);</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-13"></a><span class="n">network</span><span class="o">-&gt;</span><span class="k">template</span> <span class="n">layer_get</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">().</span><span class="n">init_layer</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-14"></a><span class="n">network</span><span class="o">-&gt;</span><span class="k">template</span> <span class="n">layer_get</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">().</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">;</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-15"></a><span class="n">network</span><span class="o">-&gt;</span><span class="k">template</span> <span class="n">layer_get</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">().</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">;</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-16"></a><span class="n">network</span><span class="o">-&gt;</span><span class="k">template</span> <span class="n">layer_get</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">().</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">;</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-17"></a>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-18"></a><span class="n">network</span><span class="o">-&gt;</span><span class="n">pretrain</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">training_images</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
<a name="rest_code_7dba155d64d04952aa1bff84e2570a48-19"></a><span class="n">network</span><span class="o">-&gt;</span><span class="n">fine_tune</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">training_images</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">training_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
</pre>
<p>This is a bit more verbose, but the configuration can be changed at runtime with
this system. Moreover, this is also faster to compile. On the other hand, there
is some performance slowdown.</p>
<p>There is also a third version that is a hybrid of the first version:</p>
<pre class="code cpp"><a name="rest_code_c95085d4c8b34af3ace718c536a89849-1"></a><span class="k">using</span> <span class="n">network_t</span> <span class="o">=</span>
<a name="rest_code_c95085d4c8b34af3ace718c536a89849-2"></a>    <span class="n">dll</span><span class="o">::</span><span class="n">dyn_dbn_desc</span><span class="o">&lt;</span>
<a name="rest_code_c95085d4c8b34af3ace718c536a89849-3"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dbn_layers</span><span class="o">&lt;</span>
<a name="rest_code_c95085d4c8b34af3ace718c536a89849-4"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">rbm_desc</span><span class="o">&lt;</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_c95085d4c8b34af3ace718c536a89849-5"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">rbm_desc</span><span class="o">&lt;</span><span class="mi">500</span>    <span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_c95085d4c8b34af3ace718c536a89849-6"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">rbm_desc</span><span class="o">&lt;</span><span class="mi">400</span>    <span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="n">dll</span><span class="o">::</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">hidden</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">unit_type</span><span class="o">::</span><span class="n">SOFTMAX</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="o">&gt;</span><span class="p">,</span>
<a name="rest_code_c95085d4c8b34af3ace718c536a89849-7"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">trainer</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">sgd_trainer</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">64</span><span class="o">&gt;&gt;::</span><span class="n">dbn_t</span><span class="p">;</span>
<a name="rest_code_c95085d4c8b34af3ace718c536a89849-8"></a>
<a name="rest_code_c95085d4c8b34af3ace718c536a89849-9"></a><span class="k">auto</span> <span class="n">network</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">network_t</span><span class="o">&gt;</span><span class="p">();</span>
<a name="rest_code_c95085d4c8b34af3ace718c536a89849-10"></a><span class="n">network</span><span class="o">-&gt;</span><span class="n">pretrain</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">training_images</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
<a name="rest_code_c95085d4c8b34af3ace718c536a89849-11"></a><span class="n">network</span><span class="o">-&gt;</span><span class="n">fine_tune</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">training_images</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">training_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">);</span>
</pre>
<p>Only one line was changed compared to the first version, <code>dbn_desc</code>
becomes <code>dyn_dbn_desc</code>. What this changes is that all the layers are
automatically transformed into their dynamic versions and all the parameters are
propagated at runtime. This is a form a type erasing since the sizes will not be
propagated at compilation time. But this is simple since the types are simply
transformed from one type to another directly. Behind the scene, it's the
dynamic version using the front-end of the fast version. This is almost as fast
to compile as the dynamic version, but the code is much better. It executes the
same as the dynamic version.</p>
<p>If we compare the compilation time of the three versions when compiling a single
network and 5 different networks with different architectures, we get the
following results (with clang):</p>
<table border="1" class="docutils">
<colgroup>
<col width="52%">
<col width="48%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Model</th>
<th class="head">Time [s]</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>1 Fast</td>
<td>30</td>
</tr>
<tr>
<td>1 Dynamic</td>
<td>16.6</td>
</tr>
<tr>
<td>1 Hybrid</td>
<td>16.6</td>
</tr>
<tr>
<td>5 Fast</td>
<td>114</td>
</tr>
<tr>
<td>5 Dynamic</td>
<td>16.6</td>
</tr>
<tr>
<td>5 Hybrid</td>
<td>21.9</td>
</tr>
</tbody>
</table>
<p>Even with one single network, the compilation time is reduced by 44%. When five
different networks are compilation, time is reduced by 85%. This can be
explained easily. Indeed, for the hybrid and dynamic versions, the layers will
have the same type and therefore a lot of template instantiations will only be
done once instead of five times. This makes a lot of difference since almost
everything is template inside the library.</p>
<p>Unfortunately, this also has an impact on the runtime of the network:</p>
<table border="1" class="docutils">
<colgroup>
<col width="26%">
<col width="41%">
<col width="32%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Model</th>
<th class="head">Pretrain [s]</th>
<th class="head">Train [s]</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>Fast</td>
<td>195</td>
<td>114</td>
</tr>
<tr>
<td>Dynamic</td>
<td>203</td>
<td>123</td>
</tr>
<tr>
<td>Hybrid</td>
<td>204</td>
<td>122</td>
</tr>
</tbody>
</table>
<p>On average, for dense models, the slowdown is between 4% and 8%. For
convolutional models, it is between 10% and 25%. I will definitely work on
trying to make the dynamic and especially the hybrid version faster in the
future, most on the work should be on the matrix library (ETL) that is used.</p>
<p>Since for test cases, a 20% increase in runtime is not really a problem, tests
being fast already, I decided to add an option to DLL so that everything can be
compiled by default in hybrid model. By using a compilation flag, all the
<code>dbn_desc</code> are becoming <code>dyn_dbn_desc</code> and therefore each used
network is becoming a hybrid network. Without a single change in the code, the
compilation time of the entire library can be significantly improved, as seen in
the next section.  This can also be used in user code to improve compilation
time during debugging and experiments and can be turned off for the final
training.</p>
<p>On my Continuous Integration system, I will build the system in both
configurations. This is not really an issue, since my personal machine at home
is more powerful than what I have available here.</p>
</div>
<div class="section" id="results">
<h2>Results</h2>
<p>On a first experiment, I measured the difference before and after this change on
the three executables of the library, with gcc:</p>
<table border="1" class="docutils">
<colgroup>
<col width="23%">
<col width="26%">
<col width="26%">
<col width="26%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Model</th>
<th class="head">Unit [s]</th>
<th class="head">Perf [s]</th>
<th class="head">Misc [s]</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>Before</td>
<td>1029</td>
<td>192</td>
<td>937</td>
</tr>
<tr>
<td>After</td>
<td>617</td>
<td>143</td>
<td>619</td>
</tr>
<tr>
<td>Speedup</td>
<td>40.03%</td>
<td>25.52%</td>
<td>33.93%</td>
</tr>
</tbody>
</table>
<p>It is clear that the speedups are very significant! The compilation is between
25% and 40% faster with the new option. Overall, this is a speedup of 36%!
I also noticed that the compilation takes significantly less memory than before.
Therefore, I decided to rerun the compiler benchmark on the library. In the
previous experiment, zapcc was taking so much memory that it was impossible to
use more than one thread. Let's see how it is faring now. The time to compile
the full unit tests is computed for each compiler. Let's start in debug mode:</p>
<table border="1" class="docutils">
<colgroup>
<col width="23%">
<col width="19%">
<col width="19%">
<col width="19%">
<col width="19%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Debug</th>
<th class="head">-j1</th>
<th class="head">-j2</th>
<th class="head">-j3</th>
<th class="head">-j4</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>clang-3.9</td>
<td>527</td>
<td>268</td>
<td>182</td>
<td>150</td>
</tr>
<tr>
<td>gcc-4.9.3</td>
<td>591</td>
<td>303</td>
<td>211</td>
<td>176</td>
</tr>
<tr>
<td>gcc-5.3.0</td>
<td>588</td>
<td>302</td>
<td>209</td>
<td>175</td>
</tr>
<tr>
<td>zapcc-1.0</td>
<td><strong>375</strong></td>
<td><strong>187</strong></td>
<td><strong>126</strong></td>
<td><strong>121</strong></td>
</tr>
</tbody>
</table>
<p>This time, zapcc is able to scale to four threads without problems. Moreover, it
is always the fastest compiler, by a significant margin, in this configuration.
It is followed by clang and then by gcc for which both versions are about the
same speed.</p>
<p>If we compile again in release mode:</p>
<table border="1" class="docutils">
<colgroup>
<col width="24%">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="16%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Release</th>
<th class="head">-j1</th>
<th class="head">-j2</th>
<th class="head">-j3</th>
<th class="head">-j4</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>clang-3.9</td>
<td>1201</td>
<td>615</td>
<td>421</td>
<td>356</td>
</tr>
<tr>
<td>gcc-4.9.3</td>
<td>1041</td>
<td>541</td>
<td>385</td>
<td>321</td>
</tr>
<tr>
<td>gcc-5.3.0</td>
<td>1114</td>
<td>579</td>
<td>412</td>
<td>348</td>
</tr>
<tr>
<td>zapcc-1.0</td>
<td><strong>897</strong></td>
<td><strong>457</strong></td>
<td><strong>306</strong></td>
<td><em>306</em></td>
</tr>
</tbody>
</table>
<p>The difference in compilation time is very large, it's twice slower to compile
with all optimizations enabled. It also takes significantly more memory. Indeed,
zapcc was not able to compile with 4 threads. Nevertheless, even the results
with three threads are better than the other compilers using four threads. zapcc
is clearly the winner again on this test, followed by gcc4-9 which is faster
than gcc-5.3 which is itself faster than clang. It seems that while clang is
better at frontend than gcc, it is slower for optimizations. Note that this may
also be an indication that clang performs more optimizations than gcc and may
not be slower.</p>
</div>
<div class="section" id="conclusion">
<h2>Conclusion</h2>
<p>By using some form of type erasing to simplify the templates types at compile
time, I was able to reduce the overall compilation time of my Deep Learning
Library (DLL) by 36%. Moreover, this can be done by switching a simple
compilation flag. This also very significantly reduce the memory used during the
compilation, allowing zapcc to to compile with up to three threads, compared
with only one before. This makes zapcc the fastest compiler again on this
benchmark. Overall, this will make debugging much easier on this library and
will save me a lot of time.</p>
<p>In the future, I plan to try to improve compilation time even more. I have a few
ideas, especially in ETL that should significantly improve the compilation time
but that will require a lot of time to implement, so that will likely have to
wait a while. In the coming days, I plan to work on the performance of DLL,
especially for stochastic gradient descent.</p>
<p>If you want more information on DLL, you can check out the
<a class="reference external" href="https://github.com/wichtounet/dll">dll Github repository</a>.</p>
</div>
</div>
        </div>
            
        
    <a href="posts/2017/03/partial-type-erasing-deep-learning-library-dll-improve-compilation-time.html#disqus_thread" data-disqus-identifier="cache/posts/2017/03/partial-type-erasing-deep-learning-library-dll-improve-compilation-time.html">Comments</a>


        </article><nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-30.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="blogwichtounet";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div> <!-- col -->
    </div>
<!-- row  -->
</div>
<!-- container-fluid -->

<!-- End of Menubar -->

<!-- Footer -->

<footer>
    Contents © 2017         <a href="mailto:baptistewicht@gmail.com">Baptiste Wicht</a> - Powered by         <a href="http://getnikola.com" rel="nofollow">Nikola</a>         - License: 
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="padding-left:5px;border-width:0" src="assets/img/cc.png"></a>
        <ul class="footer_inline_ul"></ul></footer><!-- Late loading stuff  --><script src="assets/js/all-nocdn.js"></script><script type="text/javascript">
      $(document).ready(function() {
        $.getJSON("/assets/js/tx3_tag_cloud.json", function(data){
            var items = [];
            $.each(data, function(key, val){
                var count = val[0];
                var url = val[1];
                var posts = val[2];

                if(count > 9){
                    items.push("<li data-weight='" + count + "'><a href='" + url + "'>" + key + "</a></li>");
                }
            });

            $("<ul/>", {
                "id": "tag_cloud_left",
                html: items.join("")
            }).appendTo("#tag_cloud_left_container");

            $("#tag_cloud_left").tx3TagCloud({
                multiplier: 0.8 // default multiplier is "1"
            });
        });
      });
    </script><!-- Google platform JS -->
</body>
</html>
