<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog blog("Baptiste Wicht");</title><link>https://baptiste-wicht.com/</link><description>Tutorials and short posts about programming, C++, Java, Assembly, Operating Systems Development, Compilers, ...</description><atom:link rel="self" href="https://baptiste-wicht.com/rss.xml" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Fri, 16 Mar 2018 08:05:14 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>I got rid of Vivaldi browser for Google Chrome</title><link>https://baptiste-wicht.com/posts/2018/03/i-got-rid-of-vivaldi-browser-for-google-chrome.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;About a year ago, &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/01/vivaldi-vimium-finally-no-more-firefox.html"&gt;I switched from Firefox to Vivaldi&lt;/a&gt;. This week, I decided to get rid of Vivaldi and replaced with Google Chrome. In this post, I'm going to outline the reasons why I got rid of it.&lt;/p&gt;
&lt;p&gt;Before, I switched to Vivaldi because Firefox was dropping support for XUL/XPCOM
extensions and I was using Pentadactyl. In fact, Pentadactyl was the only reason
I was using Firefox. It was slow and bloated and a bit unstable, but the
extension was making it worth. Since they are dropping support for such
extensions, I did not want to use Firefox anymore. So I switched to Vivaldi with
Vimium. It's not as great as Firefox plus Pentadactyl. But it's a more
customizable version of Google Chrome on which it's based.&lt;/p&gt;
&lt;p&gt;But, in that year or so of using Vivaldi, I have had many issues. Some of them
were not too bad and there was some workarounds. But they continued to pile up
and they did not fix any of them so now, I decided it's too much.&lt;/p&gt;
&lt;p&gt;Since the beginning, it always has been slow. It's not really bad, but still
noticeable compared to Chrome. Especially opening Vivaldi is pretty bad. This is
something I can live with, but they should really do something to make it
faster.&lt;/p&gt;
&lt;p&gt;The thing that I had the most issues with is multimedia. For instance Youtube
(but all the other platforms have the same issues).&lt;/p&gt;
&lt;p&gt;The first problem with media is to get a video in fullscreen. Most of the time,
when I press the fullscreen button on Youtube, it grays out the screen and
I have to press ESC. If I do that around five to ten times, it finally goes
fullscreen. It may be because of my multi-monitor setup but Google Chrome has no
issues whatsoever with that. It's pretty painful to do, but again I could live
for since I don't use full screen a lot.&lt;/p&gt;
&lt;p&gt;A second problem I had with media is they were running too fast. I'm not
kidding, really too fast, not too slow. The media was running about twice too
fast, you could see the seconds going fast on Youtube. I never seen this issue
in any other tool, but it was happening at every start of Vivaldi. The fix was
to restart Vivaldi every time this happened and the video played normally.&lt;/p&gt;
&lt;p&gt;Another problem I had from the beginning is to make all HTML5 videos work. You
have to download the binary plugins from Chrome to let Vivaldi play all HTML5
videos. It's not a big deal, but the problem is that they are overwritten after
each update of Vivaldi. So you have to do it all the time.&lt;/p&gt;
&lt;p&gt;A new media issue I had on the last update of Vivaldi is with Flash. At the
beginning it was working even if it was outdated. I just had to confirm to run
it with a warning. But, since the last update, I only had the warning that it
was outdated. But I could not confirm to use it, the option was not here
anymore. And it was still happening after I updated Flash... The only option to
run Flash was to use a private navigation window...&lt;/p&gt;
&lt;p&gt;And finally, I had another big issue with the last version of Vivaldi as well.
The browser keeps crashing on my work computer. It can stay up a few minutes and
then crash. The complete interface is not updated. I can still press the tabs
and I can see the title of the window change, but the interface does not update.
Again, it may come from my special window manager (I use awesome), but it's the
only application not working...&lt;/p&gt;
&lt;p&gt;With all these issues and especially the last two new problems, I decided it was
time to cut the losses. So I reinstalled Google Chrome, transferred my plugins
and everything worked like a charm. I still use Vimium to use vim bindings so my
usage of the browser does not change. Of course, I don't have the customization
that I had with Vivaldi. I would really really like to get rid of the address
bar in the browser. I would also like to significantly reduce the size of the
tab bar. But I prefer to live without these improvements than with so many bugs.
I think Vivaldi is a good idea, but with a terrible implementation.&lt;/p&gt;
&lt;p&gt;I also considered qutebrowser as an alternative. But for now it's still missing
many features that I don't want to get rid of. So I will stay with Google Chrome
for the time being.&lt;/p&gt;
&lt;p&gt;What about you ? Do you have any experience with Vivaldi ?&lt;/p&gt;&lt;/div&gt;</description><category>Gentoo</category><category>Google</category><category>Personal</category><category>Web</category><guid>https://baptiste-wicht.com/posts/2018/03/i-got-rid-of-vivaldi-browser-for-google-chrome.html</guid><pubDate>Fri, 16 Mar 2018 07:31:45 GMT</pubDate></item><item><title>Decrease DLL neural network compilation time with C++17</title><link>https://baptiste-wicht.com/posts/2018/02/decrease-dll-neural-network-compilation-time-with-c%2B%2B17.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Just last week, &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2018/02/c%2B%2B17-migration-of-expression-templates-library-etl.html"&gt;I've migrated my Expression Templates Library (ETL) library to C++17&lt;/a&gt;,
it is now also done in my Deep Learning Library (DLL) library. In ETL, this
resulted in a &lt;em&gt;much nicer code overall&lt;/em&gt;, but no real improvement in compilation
time.&lt;/p&gt;
&lt;p&gt;The objective of the migration of DLL was two-fold. First, I also wanted to
simplify some code, especially with &lt;code&gt;if constexpr&lt;/code&gt;. But I also especially
wanted to try to reduce the compilation time. In the past,
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/09/how-i-made-deep-learning-library-38-faster-to-compile-optimization-and-cpp17-if-constexpr.html"&gt;I've already tried a few changes with C++17&lt;/a&gt;, with good results on the compilation of the entire test suite.
While this is very good, this is not very representative of users of the library.
Indeed, normally you'll have only one network in your source file not several.
The new changes will especially help in the case of many networks, but less in
the case of a single network per source file.&lt;/p&gt;
&lt;p&gt;This time, I decided to test the compilation on the examples. I've tested the
eight official examples from the DLL library:&lt;/p&gt;
&lt;ol class="arabic simple" start="0"&gt;
&lt;li&gt;mnist_dbn: A fully-connected Deep Belief Network (DBN) on the MNIST data set
with three layers&lt;/li&gt;
&lt;li&gt;char_cnn: A special CNN with embeddings and merge and group layers for text
recognition&lt;/li&gt;
&lt;li&gt;imagenet_cnn: A 12 layers Convolutional Neural Network (CNN) for Imagenet&lt;/li&gt;
&lt;li&gt;mnist_ae: A simple two-layers auto-encoder for MNIST&lt;/li&gt;
&lt;li&gt;mnist_cnn: A simple 6 layers CNN for MNIST&lt;/li&gt;
&lt;li&gt;mnist_deep_ae: A deep auto-encoder for MNIST, only fully-connected&lt;/li&gt;
&lt;li&gt;mnist_lstm: A Recurrent Neural Network (RNN) with Long Short Term Memory
(LSTM) cells&lt;/li&gt;
&lt;li&gt;mnist_mlp: A simple fully-connected network for MNIST, with dropout&lt;/li&gt;
&lt;li&gt;mnist_rnn: A simple RNN with simple cells for MNIST&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is really representative of what users can do with the library and I think
it's a much better for compilation time.&lt;/p&gt;
&lt;p&gt;For reference, you can find &lt;a class="reference external" href="https://github.com/wichtounet/dll/tree/master/examples/src"&gt;the source code of all the examples online&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="results"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;Let's start with the results. I've tested this at different stages of the
migration with clang 5 and GCC 7.2. I tested the following steps:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;The original C++14 version&lt;/li&gt;
&lt;li&gt;Simply compiling in c++17 mode (-std=c++17)&lt;/li&gt;
&lt;li&gt;Using the C++17 version of the ETL library&lt;/li&gt;
&lt;li&gt;Upgrading DLL to C++17 (without ETL)&lt;/li&gt;
&lt;li&gt;ETL and DLL in C++17 versions&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I've compiled each example independently in release_debug mode. Here are the
results for G++ 7.2:&lt;/p&gt;
&lt;table border="1" class="docutils align-center"&gt;
&lt;colgroup&gt;
&lt;col width="21%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Example&lt;/th&gt;
&lt;th class="head"&gt;0&lt;/th&gt;
&lt;th class="head"&gt;1&lt;/th&gt;
&lt;th class="head"&gt;2&lt;/th&gt;
&lt;th class="head"&gt;3&lt;/th&gt;
&lt;th class="head"&gt;4&lt;/th&gt;
&lt;th class="head"&gt;5&lt;/th&gt;
&lt;th class="head"&gt;6&lt;/th&gt;
&lt;th class="head"&gt;7&lt;/th&gt;
&lt;th class="head"&gt;8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;C++14&lt;/td&gt;
&lt;td&gt;37.818&lt;/td&gt;
&lt;td&gt;32.944&lt;/td&gt;
&lt;td&gt;33.511&lt;/td&gt;
&lt;td&gt;15.403&lt;/td&gt;
&lt;td&gt;29.998&lt;/td&gt;
&lt;td&gt;16.911&lt;/td&gt;
&lt;td&gt;24.745&lt;/td&gt;
&lt;td&gt;18.974&lt;/td&gt;
&lt;td&gt;19.006&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;-std=c++17&lt;/td&gt;
&lt;td&gt;38.358&lt;/td&gt;
&lt;td&gt;32.409&lt;/td&gt;
&lt;td&gt;32.707&lt;/td&gt;
&lt;td&gt;15.810&lt;/td&gt;
&lt;td&gt;30.042&lt;/td&gt;
&lt;td&gt;16.896&lt;/td&gt;
&lt;td&gt;24.635&lt;/td&gt;
&lt;td&gt;19.134&lt;/td&gt;
&lt;td&gt;19.027&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;ETL C++17&lt;/td&gt;
&lt;td&gt;36.045&lt;/td&gt;
&lt;td&gt;31.000&lt;/td&gt;
&lt;td&gt;30.942&lt;/td&gt;
&lt;td&gt;15.322&lt;/td&gt;
&lt;td&gt;28.840&lt;/td&gt;
&lt;td&gt;16.747&lt;/td&gt;
&lt;td&gt;24.151&lt;/td&gt;
&lt;td&gt;18.208&lt;/td&gt;
&lt;td&gt;18.939&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;DLL C++17&lt;/td&gt;
&lt;td&gt;35.251&lt;/td&gt;
&lt;td&gt;32.577&lt;/td&gt;
&lt;td&gt;32.854&lt;/td&gt;
&lt;td&gt;15.653&lt;/td&gt;
&lt;td&gt;29.758&lt;/td&gt;
&lt;td&gt;16.851&lt;/td&gt;
&lt;td&gt;24.606&lt;/td&gt;
&lt;td&gt;19.098&lt;/td&gt;
&lt;td&gt;19.146&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Final C++17&lt;/td&gt;
&lt;td&gt;32.289&lt;/td&gt;
&lt;td&gt;31.133&lt;/td&gt;
&lt;td&gt;30.939&lt;/td&gt;
&lt;td&gt;15.232&lt;/td&gt;
&lt;td&gt;28.753&lt;/td&gt;
&lt;td&gt;16.526&lt;/td&gt;
&lt;td&gt;24.326&lt;/td&gt;
&lt;td&gt;18.116&lt;/td&gt;
&lt;td&gt;17.819&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Final Improvement&lt;/td&gt;
&lt;td&gt;14.62%&lt;/td&gt;
&lt;td&gt;5.49%&lt;/td&gt;
&lt;td&gt;7.67%&lt;/td&gt;
&lt;td&gt;1.11%&lt;/td&gt;
&lt;td&gt;4.15%&lt;/td&gt;
&lt;td&gt;2.27%&lt;/td&gt;
&lt;td&gt;1.69%&lt;/td&gt;
&lt;td&gt;4.52%&lt;/td&gt;
&lt;td&gt;6.24%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The difference by just enabling c++17 is not significant. On the other hand,
some significant gain can be obtained by using the C++17 version of ETL,
especially for the DBN version and for the CNN versions. Except for the DBN
case, the migration of DLL to C++17 did not bring any significant advantage.
When everything is combined, the gains are more important :) In the best case,
the example is 14.6% faster to compile.&lt;/p&gt;
&lt;p&gt;Let's see if it's the same with clang++ 5.0:&lt;/p&gt;
&lt;table border="1" class="docutils align-center"&gt;
&lt;colgroup&gt;
&lt;col width="21%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Example&lt;/th&gt;
&lt;th class="head"&gt;0&lt;/th&gt;
&lt;th class="head"&gt;1&lt;/th&gt;
&lt;th class="head"&gt;2&lt;/th&gt;
&lt;th class="head"&gt;3&lt;/th&gt;
&lt;th class="head"&gt;4&lt;/th&gt;
&lt;th class="head"&gt;5&lt;/th&gt;
&lt;th class="head"&gt;6&lt;/th&gt;
&lt;th class="head"&gt;7&lt;/th&gt;
&lt;th class="head"&gt;8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;C++14&lt;/td&gt;
&lt;td&gt;40.690&lt;/td&gt;
&lt;td&gt;34.753&lt;/td&gt;
&lt;td&gt;35.488&lt;/td&gt;
&lt;td&gt;16.146&lt;/td&gt;
&lt;td&gt;31.926&lt;/td&gt;
&lt;td&gt;17.708&lt;/td&gt;
&lt;td&gt;29.806&lt;/td&gt;
&lt;td&gt;19.207&lt;/td&gt;
&lt;td&gt;20.858&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;-std=c++17&lt;/td&gt;
&lt;td&gt;40.502&lt;/td&gt;
&lt;td&gt;34.664&lt;/td&gt;
&lt;td&gt;34.990&lt;/td&gt;
&lt;td&gt;16.027&lt;/td&gt;
&lt;td&gt;31.510&lt;/td&gt;
&lt;td&gt;17.630&lt;/td&gt;
&lt;td&gt;29.465&lt;/td&gt;
&lt;td&gt;19.161&lt;/td&gt;
&lt;td&gt;20.860&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;ETL C++17&lt;/td&gt;
&lt;td&gt;37.386&lt;/td&gt;
&lt;td&gt;33.008&lt;/td&gt;
&lt;td&gt;33.896&lt;/td&gt;
&lt;td&gt;15.519&lt;/td&gt;
&lt;td&gt;30.269&lt;/td&gt;
&lt;td&gt;16.995&lt;/td&gt;
&lt;td&gt;28.897&lt;/td&gt;
&lt;td&gt;18.383&lt;/td&gt;
&lt;td&gt;19.809&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;DLL C++17&lt;/td&gt;
&lt;td&gt;37.252&lt;/td&gt;
&lt;td&gt;34.592&lt;/td&gt;
&lt;td&gt;35.250&lt;/td&gt;
&lt;td&gt;16.131&lt;/td&gt;
&lt;td&gt;31.782&lt;/td&gt;
&lt;td&gt;17.606&lt;/td&gt;
&lt;td&gt;29.595&lt;/td&gt;
&lt;td&gt;19.126&lt;/td&gt;
&lt;td&gt;20.782&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Final C++17&lt;/td&gt;
&lt;td&gt;34.470&lt;/td&gt;
&lt;td&gt;33.154&lt;/td&gt;
&lt;td&gt;33.881&lt;/td&gt;
&lt;td&gt;15.415&lt;/td&gt;
&lt;td&gt;30.279&lt;/td&gt;
&lt;td&gt;17.078&lt;/td&gt;
&lt;td&gt;28.808&lt;/td&gt;
&lt;td&gt;18.497&lt;/td&gt;
&lt;td&gt;19.761&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Final Improvement&lt;/td&gt;
&lt;td&gt;15.28%&lt;/td&gt;
&lt;td&gt;4.60%&lt;/td&gt;
&lt;td&gt;4.52%&lt;/td&gt;
&lt;td&gt;4.52%&lt;/td&gt;
&lt;td&gt;5.15%&lt;/td&gt;
&lt;td&gt;3.55%&lt;/td&gt;
&lt;td&gt;3.34%&lt;/td&gt;
&lt;td&gt;3.69%&lt;/td&gt;
&lt;td&gt;5.25%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;First of all, as I have seen time after time, clang is still slower than GCC.
It's a not a big difference, but still significant. Overall, the gains are a bit
higher on clang than on GCC, but not by much. Interestingly, the migration of
DLL to C++17 is less interesting in terms of compilation time for clang. It
seems even to slow down compilation on some examples. On the other hand, the
migration of ETL is more important than on GCC.&lt;/p&gt;
&lt;p&gt;Overall, every example is faster to compile using both libraries in C++17, but
we don't have spectacular speed-ups. With clang, we have speedups from 3.3% to
15.3%. With GCC, we have speedup  from 1.1% to 14.6%. It's not very high, but
I'm already satisfied with these results.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="c-17-in-dll"&gt;
&lt;h2&gt;C++17 in DLL&lt;/h2&gt;
&lt;p&gt;Overall, the migration of DLL to C++17 was quite similar to that of ETL. You can
take a look at my &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2018/02/c%2B%2B17-migration-of-expression-templates-library-etl.html"&gt;previous article&lt;/a&gt;
if you want more details on C++17 features I've used.&lt;/p&gt;
&lt;p&gt;I've &lt;em&gt;replaced a lot of SFINAE functions&lt;/em&gt; with &lt;code&gt;if constexpr&lt;/code&gt;. I've also
replaced a lot of &lt;code&gt;statif_if&lt;/code&gt; with &lt;code&gt;if constexpr&lt;/code&gt;. There was a large
number of these in DLL's code. I also enabled all the &lt;code&gt;constexpr&lt;/code&gt; that
were commented for this exact time :)&lt;/p&gt;
&lt;p&gt;I was also thinking that I could replace a lot of meta-programming stuff with
&lt;em&gt;fold expressions&lt;/em&gt;. While I was able to replace a few of them, most of them were
harder to replace with fold expressions. Indeed, the variadic pack is often
hidden behind another class and therefore the pack is not directly usable from
the network class or the group and merge layers classes. I didn't want to start
a big refactoring just to use a C++17 feature, the current state of this code is
fine.&lt;/p&gt;
&lt;p&gt;I made some use of structured bindings as well, but again not as much as I was
thinking. In fact, a lot of time, I'm assigning the elements of a pair or tuple
to existing variables not declaring new variables and unfortunately, you can
only use structured bindings with &lt;code&gt;auto&lt;/code&gt; declaration.&lt;/p&gt;
&lt;p&gt;Overall, the &lt;em&gt;code is significantly better now&lt;/em&gt;, but there was less impact than
there was on ETL. It's also a smaller code base, so maybe this is normal and my
expectations were too high ;)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The trunk of DLL is now a C++17 library :) I think this improve the quality of
the code by a nice margin! Even though, there is still some work to be done to
improve the code, especially for the DBN pretraining code, the quality is quite
good now. Moreover, the switch to C++17 made the compilation of neural networks
using the DLL library &lt;em&gt;faster to compile&lt;/em&gt;, from 1.1% in the worst case to 15.3% in
the best case! I don't know when I will release the next version of DLL, but it
will take some time. I'll especially have to polish the RNN support and add
a sequence to sequence loss before I will release the 1.1 version of DLL.&lt;/p&gt;
&lt;p&gt;I'm quite satisfied with C++17 even if I would have liked a bit more features to
play with! I'm already a big fan of &lt;code&gt;if constexpr&lt;/code&gt;, this can make the code
much nicer and fold expressions are much more intuitive than their previous
recursive template counterpart.&lt;/p&gt;
&lt;p&gt;I may also consider migrating some parts of the cpp-utils library, but if I do,
it will only be through the use of conditionals in order not to break the other
projects that are based on the library.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++17</category><category>clang</category><category>Compilers</category><category>Deep Learning</category><category>dll</category><category>etl</category><category>gcc</category><category>Machine Learning</category><category>Performance</category><category>projects</category><guid>https://baptiste-wicht.com/posts/2018/02/decrease-dll-neural-network-compilation-time-with-c%2B%2B17.html</guid><pubDate>Wed, 07 Feb 2018 10:39:02 GMT</pubDate></item><item><title>C++17 Migration of Expression Templates Library (ETL)</title><link>https://baptiste-wicht.com/posts/2018/02/c%2B%2B17-migration-of-expression-templates-library-etl.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've finally decided to migrate my Expression Templates Library (ETL) project to
C++17. I've talking about doing that for a long time and I've released several
releases without doing the change, but the next version will be a C++17 library.
The reason why I didn't want to rush the change was that this means the library
needs a very recent compiler that may not be available to everybody. Indeed,
after this change, the ETL library now needs at least GCC 7.1 or Clang 4.0.&lt;/p&gt;
&lt;p&gt;I've already made some previous experiments in the past. For instance,
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/09/how-i-made-deep-learning-library-38-faster-to-compile-optimization-and-cpp17-if-constexpr.html"&gt;by using if constexpr, I've managed to speed up compilation by 38%&lt;/a&gt; and I've also written an article about &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2015/05/cpp17-fold-expressions.html"&gt;the fold expressions introduced in C++17&lt;/a&gt;. But I haven't migrated a full library yet. This is now done with ETL. In this article, I'll try to give some example of improvements by using C++17.&lt;/p&gt;
&lt;p&gt;This will only cover the C++17 features I'm using in the updated ETL library,
I won't cover all of the new C++17 features.&lt;/p&gt;
&lt;div class="section" id="if-constexpr"&gt;
&lt;h2&gt;if constexpr&lt;/h2&gt;
&lt;p&gt;The most exciting new thing in C++17 for me is the &lt;code&gt;if constexpr&lt;/code&gt;
statement. This is a really really great thing. In essence, it's a normal
&lt;code&gt;if&lt;/code&gt; statement, but with one very important difference. The statement that
is not taken (the &lt;code&gt;else&lt;/code&gt; if the condition is true, or the &lt;code&gt;if
constexpr&lt;/code&gt; if the condition is false) is &lt;em&gt;discarded&lt;/em&gt;. And what is interesting
is what happens to &lt;em&gt;discarded&lt;/em&gt; statements:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;The body of a &lt;em&gt;discarded&lt;/em&gt; statement does not participate in return type
deduction.&lt;/li&gt;
&lt;li&gt;The discarded statement is not instantiated&lt;/li&gt;
&lt;li&gt;The discarded statement can &lt;em&gt;odr-use&lt;/em&gt; a variable that is not defined&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Personally, I'm especially interested by points 1 and 2. Let's start with an
example where point 1 is useful. In ETL, I have a make_temporary function. This
function either forwards an ETL container or creates a temporary container from
an ETL expression. This is based on a compile-time traits. The return type of
the function is the not the same in both cases. What you did in those case
before C++17, is use SFINAE and make two functions:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_e19be39c9a0f4e76b046649052ce19d4-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cpp_enable_iff&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is_dma&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_e19be39c9a0f4e76b046649052ce19d4-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;decltype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;make_temporary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_e19be39c9a0f4e76b046649052ce19d4-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_e19be39c9a0f4e76b046649052ce19d4-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_e19be39c9a0f4e76b046649052ce19d4-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_e19be39c9a0f4e76b046649052ce19d4-6"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cpp_enable_iff&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="n"&gt;is_dma&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_e19be39c9a0f4e76b046649052ce19d4-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;decltype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;make_temporary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_e19be39c9a0f4e76b046649052ce19d4-8"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;force_temporary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_e19be39c9a0f4e76b046649052ce19d4-9"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;One version of the function will forward and the other version will force
a temporary and the return type can be different since these are two different
functions. This is not bad, but still requires two functions where you only want
to write one. However, in C++17, we can do much better using &lt;code&gt;if constexpr&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_deb7989b0056481db65f30523960fdbb-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_deb7989b0056481db65f30523960fdbb-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;decltype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;make_temporary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_deb7989b0056481db65f30523960fdbb-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is_dma&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_deb7989b0056481db65f30523960fdbb-4"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_deb7989b0056481db65f30523960fdbb-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_deb7989b0056481db65f30523960fdbb-6"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;force_temporary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_deb7989b0056481db65f30523960fdbb-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_deb7989b0056481db65f30523960fdbb-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I think this version is really superior to the previous one. We only have one
function and the logic is much clearer!&lt;/p&gt;
&lt;p&gt;Let's now see an advantage of the point 2. In ETL, there are two kinds of
matrices, matrices with compile-time dimensions (fast matrices) and matrices
with runtime dimensions (dynamic matrices). When they are used, for instance for
a matrix-multiplication, I use static assertions for fast matrices and runtime
assertions for dynamic matrices. Here is an example for the validation of the
matrix-matrix multiplication:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cpp_disable_iff&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_fast&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;check&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static_assert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_2d&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"Matrix multiplication needs matrices"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_assert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-5"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;         &lt;span class="c1"&gt;//interior dimensions&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-6"&gt;&lt;/a&gt;            &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;//exterior dimension 1&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-7"&gt;&lt;/a&gt;            &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;//exterior dimension 2&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-8"&gt;&lt;/a&gt;        &lt;span class="s"&gt;"Invalid sizes for multiplication"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-12"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-13"&gt;&lt;/a&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-14"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cpp_enable_iff&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_fast&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-15"&gt;&lt;/a&gt;&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;check&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-16"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static_assert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_2d&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"Matrix multiplication needs matrices"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-17"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static_assert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;         &lt;span class="c1"&gt;//interior dimensions&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-19"&gt;&lt;/a&gt;            &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;//exterior dimension 1&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-20"&gt;&lt;/a&gt;            &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="c1"&gt;//exterior dimension 2&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-21"&gt;&lt;/a&gt;        &lt;span class="s"&gt;"Invalid sizes for multiplication"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-22"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-23"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-24"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_63b242fd690f4cbfa1f31e1b1ceef255-25"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Again, we use SFINAE to distinguish the two different cases. In that case, we
cannot use a normal &lt;code&gt;if&lt;/code&gt; since the value of the dimensions cannot be taken
at compile-time for dynamic matrices, more precisely, some templates cannot be
instantiated for dynamic matrices. As for the cpp_unused, we have to use for the
static version because we don't use them and for the dynamic version because
they won't be used if the assertions are not enabled. Let's use &lt;code&gt;if constexpr&lt;/code&gt; to avoid having two functions:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;check&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static_assert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_2d&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"Matrix multiplication needs matrices"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nf"&gt;constexpr&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_fast&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-6"&gt;&lt;/a&gt;        &lt;span class="k"&gt;static_assert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;         &lt;span class="c1"&gt;//interior dimensions&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-7"&gt;&lt;/a&gt;                          &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;//exterior dimension 1&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-8"&gt;&lt;/a&gt;                          &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="c1"&gt;//exterior dimension 2&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-9"&gt;&lt;/a&gt;                      &lt;span class="s"&gt;"Invalid sizes for multiplication"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-10"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-11"&gt;&lt;/a&gt;        &lt;span class="n"&gt;cpp_assert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;         &lt;span class="c1"&gt;//interior dimensions&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-12"&gt;&lt;/a&gt;                       &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;//exterior dimension 1&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-13"&gt;&lt;/a&gt;                       &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;//exterior dimension 2&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-14"&gt;&lt;/a&gt;                   &lt;span class="s"&gt;"Invalid sizes for multiplication"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-15"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-17"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-18"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-19"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_57749476a32341fea9d783bcbb341c2b-20"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Since the &lt;em&gt;discarded&lt;/em&gt; won't be instantiated, we can now use a single function!
We also avoid some duplications of the first static assertion of the unused
statements. Pretty great, right ? But we can do better with C++17. Indeed, it
added a nice new attribute &lt;code&gt;[[maybe_unused]]&lt;/code&gt;. Let's see what this gives
us:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;check&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="n"&gt;maybe_unused&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;maybe_unused&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;maybe_unused&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static_assert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_2d&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"Matrix multiplication needs matrices"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nf"&gt;constexpr&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_fast&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-6"&gt;&lt;/a&gt;        &lt;span class="k"&gt;static_assert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;         &lt;span class="c1"&gt;//interior dimensions&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-7"&gt;&lt;/a&gt;                          &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;//exterior dimension 1&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-8"&gt;&lt;/a&gt;                          &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="c1"&gt;//exterior dimension 2&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-9"&gt;&lt;/a&gt;                      &lt;span class="s"&gt;"Invalid sizes for multiplication"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-10"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-11"&gt;&lt;/a&gt;        &lt;span class="n"&gt;cpp_assert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;         &lt;span class="c1"&gt;//interior dimensions&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-12"&gt;&lt;/a&gt;                       &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;//exterior dimension 1&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-13"&gt;&lt;/a&gt;                       &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;//exterior dimension 2&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-14"&gt;&lt;/a&gt;                   &lt;span class="s"&gt;"Invalid sizes for multiplication"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-15"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_7013c99e14a4452a8b9034e8bcd11c5b-16"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;No more need for &lt;code&gt;cpp_unused&lt;/code&gt; trick :) This attribute tells the compiler
that a variable or parameter can be sometimes unused and therefore does not lead
to a warning for it. Only one thing that is not great with this attribute is
that it's too long, 16 characters. It almost double the width of my check
function signature. Imagine if you have more parameters, you'll soon have to use
several lines. I wish there was a way to set an attribute for all parameters
together or a shortcut. I'm considering whether to use a short macro to use in
place of it, but haven't yet decided.&lt;/p&gt;
&lt;p&gt;Just a note, if you have &lt;code&gt;else if&lt;/code&gt; statements, you need to set them as
&lt;code&gt;constexpr&lt;/code&gt; as well! This was a bit weird for me, but you can figure it as
if the condition is &lt;code&gt;constexpr&lt;/code&gt;, then the &lt;code&gt;if&lt;/code&gt; (or &lt;code&gt;else if&lt;/code&gt;)
is &lt;code&gt;constexpr&lt;/code&gt; as well.&lt;/p&gt;
&lt;p&gt;Overall, I'm really satisfied with the new &lt;cite&gt;if constexpr&lt;/cite&gt;! This really makes the
code much nicer in many cases, especially if you abuse metaprogramming like
I do.&lt;/p&gt;
&lt;p&gt;You may remember that I've &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2015/07/simulate-static_if-with-c11c14.html"&gt;coded a version of static if in the past with C++14&lt;/a&gt; in the past. This was able to solve point 2, but not point 1 and was much uglier. Now we have a good solution to it. I've replaced two of these in the current code with the new &lt;code&gt;if constexpr&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fold-expressions"&gt;
&lt;h2&gt;Fold expressions&lt;/h2&gt;
&lt;p&gt;For me, fold expressions is the second major feature of C++17. I wont' go into
too much details here, since
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2015/05/cpp17-fold-expressions.html"&gt;I've already talked about fold expression in the past&lt;/a&gt;
. But I'll show two examples of refactorings I've been able to do with this.&lt;/p&gt;
&lt;p&gt;Here was the size() function of a static matrix in ETL before:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_c0427b7191144a248e36f6f69f4530aa-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="nf"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_c0427b7191144a248e36f6f69f4530aa-2"&gt;&lt;/a&gt;   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mul_all&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Dims&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_c0427b7191144a248e36f6f69f4530aa-3"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The Dims parameter pack from the declaration of fast_matrix:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_e286c4cfc04b4ff18e19804a3b126d35-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;ST&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;order&lt;/span&gt; &lt;span class="n"&gt;SO&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;Dims&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_e286c4cfc04b4ff18e19804a3b126d35-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;fast_matrix_impl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;And the mul_all is a simple helper that multiplies each value of the variadic
parameter pack:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_27066a39616d40b38286416bed61b7ec-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;Dims&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_27066a39616d40b38286416bed61b7ec-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;mul_all_impl&lt;/span&gt; &lt;span class="k"&gt;final&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;integral_constant&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mul_all_impl&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Dims&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{};&lt;/span&gt;
&lt;a name="rest_code_27066a39616d40b38286416bed61b7ec-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_27066a39616d40b38286416bed61b7ec-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_27066a39616d40b38286416bed61b7ec-5"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;mul_all_impl&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;final&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;integral_constant&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{};&lt;/span&gt;
&lt;a name="rest_code_27066a39616d40b38286416bed61b7ec-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_27066a39616d40b38286416bed61b7ec-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;Dims&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_27066a39616d40b38286416bed61b7ec-8"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;mul_all&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mul_all_impl&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Dims&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Before C++17, the only way to compute this result at compilation time was to use
template recursion, either with types or with constexpr functions. I think this
is pretty heavy only for doing a multiplication sum. Now, with fold expressions,
we can manipulate the parameter pack directly and rewrite our size function:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_c1c11548a2d84b8e9dd1bcfefcac163b-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="nf"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_c1c11548a2d84b8e9dd1bcfefcac163b-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dims&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;...);&lt;/span&gt;
&lt;a name="rest_code_c1c11548a2d84b8e9dd1bcfefcac163b-3"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is much better! This clearly states that each value of the parameter should
be multiplied together. For instance &lt;code&gt;1,2,3&lt;/code&gt; will become &lt;code&gt;(1*2)*3&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Another place where I was using this was to code a traits that tests if a set of
boolean are all true at compilation-time:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_64eb7b7b40654e8898b4a5bcdd98fca0-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_64eb7b7b40654e8898b4a5bcdd98fca0-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;and_v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_64eb7b7b40654e8898b4a5bcdd98fca0-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;tmp_detail&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;bool_list&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_64eb7b7b40654e8898b4a5bcdd98fca0-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;tmp_detail&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;bool_list&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;...,&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I was using a nice trick here to test if all booleans are true. I don't remember
where I picked it up, but it's quite nice and very fast to compile.&lt;/p&gt;
&lt;p&gt;This was used for instance to test that a set of expressions are all
single-precision floating points:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_e5d2e5f1caa64332ac68095cad38e63e-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_e5d2e5f1caa64332ac68095cad38e63e-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;all_single_precision&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;and_v&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is_single_precision&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;)...&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Now, we can get rid of the and_v traits and use directly the parameter pack
directly:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_12e0324003d34bc7a492b0e3ed2d97c2-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_12e0324003d34bc7a492b0e3ed2d97c2-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;all_single_precision&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is_single_precision&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;...);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I think using fold expressions results in much clearer syntax and better code
and it's a pretty nice feature overall :)&lt;/p&gt;
&lt;p&gt;As a note here, I'd like to mention, that you can also use this syntax to call
a function on each argument that you have, which makes for much nicer syntax as
well and I'll be using that in DLL once I migrate it to C++17.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="miscellaneous"&gt;
&lt;h2&gt;Miscellaneous&lt;/h2&gt;
&lt;p&gt;There are also a few more C++17 features that I've used to improve ETL, but that
have a bit less impact.&lt;/p&gt;
&lt;p&gt;A very nice feature of C++17 is the support for structured bindings. Often you
end up with a function that returns several parts of information in the form of
a pair or a tuple or even a fixed-size array. You can use an object for this,
but if you don't, you end up with code that is not terribly nice:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_c47bbca622114675812cfaf2d3a43bbf-1"&gt;&lt;/a&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_c47bbca622114675812cfaf2d3a43bbf-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_c47bbca622114675812cfaf2d3a43bbf-3"&gt;&lt;/a&gt;&lt;span class="kt"&gt;float&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_c47bbca622114675812cfaf2d3a43bbf-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;tie&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_function&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;It's not terribly bad, but in these cases, you should be be hoping for something
better. With c++17, you can do better:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_2f0576df51ac40c9a8edb3fcb4183b59-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_function&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Now you can directly use auto to deduce the types of the three variables at once
and you can get all the results in the variables at once as well :) I think this
is really nice and can really profit some projects. In ETL, I've almost no use
for this, but I'm going to be using that a bit more in DLL.&lt;/p&gt;
&lt;p&gt;Something really nice to clean up the code in C++17 is the ability to declared
nested namespaces in one line. Before, you have a nested namespace
etl::impl::standard for instance, you would do:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_be78b737dee14e649fd10efecf8fea35-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;namespace&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_be78b737dee14e649fd10efecf8fea35-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;namespace&lt;/span&gt; &lt;span class="n"&gt;impl&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_be78b737dee14e649fd10efecf8fea35-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;namespace&lt;/span&gt; &lt;span class="n"&gt;standard&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_be78b737dee14e649fd10efecf8fea35-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_be78b737dee14e649fd10efecf8fea35-5"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Someting inside etl::impl::standard&lt;/span&gt;
&lt;a name="rest_code_be78b737dee14e649fd10efecf8fea35-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_be78b737dee14e649fd10efecf8fea35-7"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="c1"&gt;// end of namespace standard&lt;/span&gt;
&lt;a name="rest_code_be78b737dee14e649fd10efecf8fea35-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="c1"&gt;// end of namespace impl&lt;/span&gt;
&lt;a name="rest_code_be78b737dee14e649fd10efecf8fea35-9"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="c1"&gt;// end of namespace etl&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;In C++17, you can do:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_5f27ae5e31fa41e4a87b8901e95ee330-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;namespace&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;impl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;standard&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_5f27ae5e31fa41e4a87b8901e95ee330-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_5f27ae5e31fa41e4a87b8901e95ee330-3"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Someting inside etl::impl::standard&lt;/span&gt;
&lt;a name="rest_code_5f27ae5e31fa41e4a87b8901e95ee330-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_5f27ae5e31fa41e4a87b8901e95ee330-5"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="c1"&gt;// end of namespace etl::impl::standard&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I think it's pretty neat :)&lt;/p&gt;
&lt;p&gt;Another very small change is the ability to use the typename keyword in place of
the class keyword when declaring template template parameters. Before, you had
to declare:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_26810ca1e2314ed5ba28a113d969445e-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;now you can also use:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_f1bafc9a22044929b239a10132890620-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;It's just some syntactic sugar, but I think it's quite nice.&lt;/p&gt;
&lt;p&gt;The last improvement that I want to talk about is one that probably very few
know about but it's pretty neat. Since C++11, you can use the &lt;code&gt;alignas(X)&lt;/code&gt;
specifier for types and objects to specify on how many bytes you want to align
these. This is pretty nice if you want to align on the stack. However, this
won't always work for dynamic memory allocation. Imagine this struct:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_b57052a0e8a84a9c8e36b989f6d56b86-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="nf"&gt;alignas&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="n"&gt;test_struct&lt;/span&gt;  &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="kt"&gt;char&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="p"&gt;};&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;If you declare an object of this type on the stack, you have the guarantee that
it will be aligned on 128 bytes. However, if you use &lt;code&gt;new&lt;/code&gt; to allocate it
on the heap, you don't have such guarantee. Indeed, the problem is that 128 is
greater than the maximum default alignment. This is called an over-aligned type.
In such cases, the result will be aligned on the max alignment of your system.
Since C++17, &lt;code&gt;new&lt;/code&gt; supports aligned dynamic memory allocation of
over-aligned types. Therefore, you can use a simple &lt;code&gt;alignas&lt;/code&gt; to allocate
dynamic over-aligned types :) I need this in ETL for matrices that need to be
aligned for vectorized code. Before, I was using a larger array with some
padding in order to find an aligned element inside, but that is not very nice,
now the code is much better.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="compilation-time"&gt;
&lt;h2&gt;Compilation Time&lt;/h2&gt;
&lt;p&gt;I've done a few tests to see how much impact these news features have on
compilation time. Here, I'm doing benchmark on compiling the entire test suite
in different compilation mode, I enabled most compilation options (all GPU and
BLAS options in order to make sure almost all of the library is compiled).&lt;/p&gt;
&lt;p&gt;Since I'm a bit short on time before going to vacation, I've only gathered the
results with g++. Here are the results with G++ 7.2.0&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="28%"&gt;
&lt;col width="16%"&gt;
&lt;col width="21%"&gt;
&lt;col width="35%"&gt;
&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;&lt;/td&gt;
&lt;td&gt;debug&lt;/td&gt;
&lt;td&gt;release&lt;/td&gt;
&lt;td&gt;release_debug&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;C++14&lt;/td&gt;
&lt;td&gt;862s&lt;/td&gt;
&lt;td&gt;1961s&lt;/td&gt;
&lt;td&gt;1718s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;C++17&lt;/td&gt;
&lt;td&gt;892s&lt;/td&gt;
&lt;td&gt;2018s&lt;/td&gt;
&lt;td&gt;1745s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Difference&lt;/td&gt;
&lt;td&gt;+3.4%&lt;/td&gt;
&lt;td&gt;+2.9%&lt;/td&gt;
&lt;td&gt;+1.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Overall, I'm a bit disappointed by these results, it's around 3% slower to
compile the C++17 version than the C++14 version. I was thinking that this would
a least be as fast to compile as before. It seems that currently with G++ 7.2,
&lt;code&gt;if constexpr&lt;/code&gt; are slower to compile than the equivalent SFINAE functions.
I didn't do individual benchmarks of all the features I've migrated, therefore,
it may not be coming from &lt;code&gt;if constexpr&lt;/code&gt;, but since it's the greatest
change by far, it's the more likely candidate. Once I'll have a little more
time, after my vacations, I'll try to see if that is also the case with clang.&lt;/p&gt;
&lt;p&gt;Keep in mind that we are compiling the test suite here. The ETL test suite is
using the manual selection mode of the library in order to be able to test all
the possible implementations of each operation. This makes a considerable
difference in performance. I expect better compilation time when this is used in
automatic selection mode (the default mode). In the default mode, a lot more
code can be disabled with &lt;code&gt;if constexpr&lt;/code&gt;. I will test this next with the
DLL library which I will also migrate to C++17.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This concludes this report on the migration of my ETL library from C++14 to
C++17. Overall, I'm really satisfied with the improvement of the code, it's much
better. I'm a bit disappointed by the slight increase  (around 3%) in
compilation time, but it's not dramatic either. I'm still hoping that once it's
used in DLL, I will see a decrease in compilation, but we'll see that when I'll
be done with the migration of DLL to C++17 which may take some time since I'll
have two weeks vacation in China starting Friday.&lt;/p&gt;
&lt;p&gt;The new version is available only through the &lt;em&gt;master&lt;/em&gt; branch. It will be
released as the 1.3 version probably when I integrate some new features, but in
itself will not be released as new version. You can take a look in the
&lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;Github etl repository&lt;/a&gt; if you are interested.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>C++17</category><category>etl</category><category>projects</category><guid>https://baptiste-wicht.com/posts/2018/02/c%2B%2B17-migration-of-expression-templates-library-etl.html</guid><pubDate>Fri, 02 Feb 2018 13:03:26 GMT</pubDate></item><item><title>budgetwarrior 1.0: Web interface and asset tracking!</title><link>https://baptiste-wicht.com/posts/2018/01/budgetwarrior-10-web-interface-and-asset-tracking.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I'm happy to announce the release of budgetwarrior 1.0. This is a major change
over the previous version.&lt;/p&gt;
&lt;div class="section" id="web-interface"&gt;
&lt;h2&gt;Web Interface&lt;/h2&gt;
&lt;p&gt;Until now, budgetwarrior could only be used in command line. This is fine for
me, but not for every body. Since I wanted to share my budget with my
girlfriend, I needed something less nerdy ;)&lt;/p&gt;
&lt;p&gt;Therefore, I added support for &lt;em&gt;a web interface for budgetwarrior&lt;/em&gt;. Every feature
of the console application is now available in the web version. Moreover, since
the web version offers &lt;em&gt;slightly better&lt;/em&gt; graphical capabilities, I added a few
more graphs and somewhat more information at some places. I'm not nearly an
expert in web interface, but I think I managed to get something not too bad
together. There are still some things to improve that I'll go through in the
future but so far the web interface is pretty satisfying and it is &lt;strong&gt;mobile friendly&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;The web server is coded in C++ (who would have guessed...) and is embedded in
the application, you need to use the command &lt;strong&gt;server&lt;/strong&gt; to use it:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
budget server
&lt;/pre&gt;
&lt;p&gt;and the server will be launched (by default at localhost:8080). You can
configure the port with &lt;code&gt;server_port=X&lt;/code&gt; in the configuration file and the
listen address with &lt;code&gt;server_listen=X&lt;/code&gt;. You can access your server at
&lt;a class="reference external" href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here is what this will display:&lt;/p&gt;
&lt;img alt="Web interface index" src="https://baptiste-wicht.com/images/budgetwarrior_web_index.png"&gt;
&lt;p&gt;Note: All the data is randomized&lt;/p&gt;
&lt;p&gt;The main page shows your assets, the current net worth, your monthly cash-flow
and the state of your objectives.&lt;/p&gt;
&lt;p&gt;The menu will give you access to all the features of the application. You can
add expenses and earnings, see reports, manage your assets and your objectives
and so on. Basically, you can do everything you did in the application, but you
have access to more visualization tools than you would on the console. For
instance, you can access your fortune over time:&lt;/p&gt;
&lt;img alt="Web interface fortune graph" src="https://baptiste-wicht.com/images/budgetwarrior_web_fortune.png"&gt;
&lt;p&gt;or see how your portfolio does in terms of currency:&lt;/p&gt;
&lt;img alt="Web interface portofolio currency breakdown" src="https://baptiste-wicht.com/images/budgetwarrior_web_portfolio_currency.png"&gt;
&lt;p&gt;Normally, unless I forgot something (in which case, I'll fix it), everything
should be doable from the web interface. This is simply easier people that are
not as nerdy as me for console ;)&lt;/p&gt;
&lt;p&gt;The management is still the same, the server will write to the same file the
base application uses. Therefore, you cannot use the server and the command line
application on the same machine at the same time. Nevertheless, if the server is
not running, you can still use the command line application. This could be
useful if you want to use the web visualization while still using the command
line tool for managing the budget.&lt;/p&gt;
&lt;p&gt;The default user and password is admin:1234, but you of course change it using
web_password and web_user in the configuration. You can also disable the
security if you are sure of yourself by setting &lt;code&gt;server_secure=true&lt;/code&gt; in
the configuration. The server currently does not support&lt;/p&gt;
&lt;p&gt;Currently, it does not protect against concurrent modifications of the same
data. It is very unlikely to happen with only a few people using the
applications, but I plan to improve that in the future.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="server-mode"&gt;
&lt;h2&gt;Server mode&lt;/h2&gt;
&lt;p&gt;Although it's not possible to use both the server and the command line
application at the same time, it's possible to use the command line application
in server mode. In this case, instead of reading and writing the data from the
hard disk, the application will send requests to the server to read and write
the data it needs. With this, you can use both the server and the command line
application at the same time!&lt;/p&gt;
&lt;p&gt;While running, the server exposes a simple API that can be used to get
all the information about the budget data and that can also be used to add new
expenses, earnings and so on directly to the server data. The API is also
protected by authentication.&lt;/p&gt;
&lt;p&gt;Currently, the server does not support HTTPS. However, you can run it behind
a proxy such as nginx which is running in HTTPS. This is what I'm doing. The
server mode supports SSL from the client to the server, you just have to set
&lt;code&gt;server_sll=true&lt;/code&gt; in the configuration.&lt;/p&gt;
&lt;p&gt;This is the mode I'm currently using and will continue using. With this, I can
quickly do some modifications using the command line and if I want to see
advanced visualization, I just have to open my browser and everything is
updated. Moreover, in the future, other people involved with my budget will be
able to access the web interface. This also solves the synchronization problem
in a much better way than before.&lt;/p&gt;
&lt;p&gt;Just as it was the case with the server, this is not made to be used in parallel
by different users. This should be perfectly fine for a small household.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="assets-tracking"&gt;
&lt;h2&gt;Assets Tracking&lt;/h2&gt;
&lt;p&gt;Already a few months ago, I've added &lt;cite&gt;the feature to track assets &amp;lt;https://baptiste-wicht.com/posts/2017/10/budgetwarrior-track-assets-portfolio-savings-rates-auto-completion.html&amp;gt; `_ into budgetwarrior. You can define the list of the assets you possess. The tool will then help you track the value of your assets. You can set your desired distribution of bonds, cash and stocks and the tool will help you see if you need to rebalance your assets. This will let you compute your net worth, with :code:`budget asset value&lt;/cite&gt;:&lt;/p&gt;
&lt;img alt="View of your assets" src="https://baptiste-wicht.com/images/budgetwarrior_asset_values.png"&gt;
&lt;p&gt;Moreover, you can also set a few of your assets as your portfolio assets. These
assets have a desired distribution and are handled different. These are the
assets you directly manage yourself, your investment portfolio. You can then
track their value and see if they need rebalancing. For instance, here is
a randomized rebalancing of your portfolio, with &lt;code&gt;budget asset rebalance&lt;/code&gt;:&lt;/p&gt;
&lt;img alt="View of the needed rebalance" src="https://baptiste-wicht.com/images/budgetwarrior_rebalance.png"&gt;
&lt;p&gt;All these features are now also available on the web version as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="better-console-usability"&gt;
&lt;h2&gt;Better console usability&lt;/h2&gt;
&lt;p&gt;A few months ago, I added some &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/10/budgetwarrior-track-assets-portfolio-savings-rates-auto-completion.html"&gt;quality-of-life improvements to the console appplication&lt;/a&gt;. You can now cycle through the list of possible values for accounts for instance in the console! This is down with the UP and DOWN keys. Now, I also added auto-completion with TAB key. You can write Ins&amp;lt;TAB&amp;gt; and it will complete to Insurances if you have an Insurances account in your budget. This makes it much faster to enter new expenses or to update asset values.&lt;/p&gt;
&lt;div class="section" id="installation"&gt;
&lt;h3&gt;Installation&lt;/h3&gt;
&lt;p&gt;If you are on Gentoo, you can install it using layman:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
layman -a wichtounet
emerge -a budgetwarrior
&lt;/pre&gt;
&lt;p&gt;If you are on Arch Linux, you can use this &lt;cite&gt;AUR repository
&amp;lt;https://github.com/StreakyCobra/aur&amp;gt;&lt;/cite&gt; (wait a few day for the new version to be
updated)_&lt;/p&gt;
&lt;p&gt;For other systems, you'll have to install from sources:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
git clone --recursive git://github.com/wichtounet/budgetwarrior.git
git checkout 1.0
cd budgetwarrior
make
sudo make install
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Overall, even though I'm not a fan of web development, it was quite fun to add
all these features to budgetwarrior and made it much better I think. This is
a very significant change to the project since it almost doubled in number of
source lines of code, but I think it's a change that was needed.&lt;/p&gt;
&lt;p&gt;I think these changes really make budgetwarrior more useful to a wider group of
people and I'm pretty to have finally come around and implemented them. I still
have a few things I plan to improve in the near future. First, I want to make
the website a bit faster, there are many scripts and stylesheets that are being
loaded and make the site a bit bloated. I'll also enable gzip compression of the
website to speed up things. I will also ensure that the server can handle
requests concurrently without any problem of the data (should be simple since we
don't need high performance). I may also add a new module to budgetwarrior to
track your progress towards retirement if this is something you are interested
in, but I haven't decided in what form exactly. Finally, I will also try to
optimize the requests that are being done between the server and the client when
run in server mode. Indeed, it currently downloads almost all the data from the
server which is far from optimal.&lt;/p&gt;
&lt;p&gt;If you are interested by the sources, you can download them on Github:
&lt;a class="reference external" href="https://github.com/wichtounet/budgetwarrior"&gt;budgetwarrior&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you have a suggestion or you found a bug, please post an issue on Github.&lt;/p&gt;
&lt;p&gt;If you have any comment, don't hesitate to contact me, either by letting a
comment on this post or by email.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>budgetwarrior</category><category>C++</category><category>projects</category><category>Release</category><guid>https://baptiste-wicht.com/posts/2018/01/budgetwarrior-10-web-interface-and-asset-tracking.html</guid><pubDate>Fri, 26 Jan 2018 11:52:31 GMT</pubDate></item><item><title>My thesis is available: Deep Learning Feature Extraction for Image Processing</title><link>https://baptiste-wicht.com/posts/2018/01/my-thesis-is-available-deep-learning-feature-extraction-for-image-processing.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I'm happy to say that I've finally put my thesis online and updated
my &lt;a class="reference external" href="https://baptiste-wicht.com/stories/publications.html"&gt;Publications&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;I should have done that earlier but it slipped my mind, so there it is!&lt;/p&gt;
&lt;p&gt;My thesis (Deep Learning Feature Extraction for Image Processing) is now
available to download.  Here is the abstract of the thesis:&lt;/p&gt;
&lt;p&gt;In this thesis, we propose to use methodologies that automatically learn how to
extract relevant features from images. We are especially interested in
evaluating how these features compare against handcrafted features. More
precisely, we are interested in the unsupervised training that is used for the
Restricted Boltzmann Machine (RBM) and Convolutional RBM (CRBM) models. These
models relaunched the Deep Learning interest of the last decade. During the time
of this thesis, the auto-encoders approach, especially Convolutional
Auto-Encoders (CAE) have been used more and more. Therefore, one objective of
this thesis is also to compare the CRBM approach with the CAE approach.&lt;/p&gt;
&lt;p&gt;The scope of this work is defined by several machine learning tasks. The first
one, handwritten digit recognition, is analysed to see how much the unsupervised
pretraining technique introduced with the Deep Belief Network (DBN) model
improves the training of neural networks. The second, detection and recognition
of Sudoku in images, is evaluating the efficiency of DBN and Convolutional DBN
(CDBN) models for classification of images of poor quality. Finally, features
are learned fully unsupervised from images for a keyword spotting task and are
compared against well-known handcrafted features. Moreover, the thesis was also
oriented around a software engineering axis. Indeed, a complete machine learning
framework was developed during this thesis to explore possible optimizations and
possible algorithms in order to train the tested models as fast as possible.&lt;/p&gt;
&lt;p&gt;If you are interested, you can:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.researchgate.net/publication/322505397_Deep_Learning_feature_Extraction_for_Image_Processing"&gt;Read it on ResearchGate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/publication_store/phd_thesis.pdf"&gt;Directly download the PDF&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I hope this will interest a few of you! As always, if you have any question,
don't hesitate to let me a comment ;)&lt;/p&gt;
&lt;p&gt;As for the current projects, I'm still currently working on the next version of
budgetwarrior, but I don't have any expected release date. It will depend on
much time I'm able to put to the project.&lt;/p&gt;&lt;/div&gt;</description><category>crbm</category><category>dll</category><category>Machine Learning</category><category>Personal</category><category>projects</category><category>publications</category><category>rbm</category><guid>https://baptiste-wicht.com/posts/2018/01/my-thesis-is-available-deep-learning-feature-extraction-for-image-processing.html</guid><pubDate>Mon, 15 Jan 2018 14:11:57 GMT</pubDate></item><item><title>Expression Templates Library 1.2.1: Faster GPU and new features</title><link>https://baptiste-wicht.com/posts/2018/01/expression-templates-library-121-faster-gpu-and-new-features.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Happy new year to all my dear readers!&lt;/p&gt;
&lt;p&gt;It has been a while since I've posted on this blog. I've had to serve three
weeks in the army and then I had two weeks vacation. I've been actively working
on budgetwarrior with a brand new web interface! More on that later ;)&lt;/p&gt;
&lt;p&gt;Today, I'm happy to release the version 1.2.1 of my Expression Templates Library
(ETL) project. This is a minor version but with significantly better GPU support
and a few new features and bug fixes so I decided to release it now.&lt;/p&gt;
&lt;div class="section" id="faster-gpu-support"&gt;
&lt;h2&gt;Faster GPU support&lt;/h2&gt;
&lt;p&gt;Last year, I &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/11/advanced-gpu-patterns-optimization-in-etl.html"&gt;implemented the support for the detection of advanced GPU patterns in ETL&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This will significantly reduce the number of CUDA kernel calls that are being
launched. For instance, each of the following expressions will be evaluated
using a single GPU kernel:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_66ab9ef64de04b35ba3cc37a3de9e6b0-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;a name="rest_code_66ab9ef64de04b35ba3cc37a3de9e6b0-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;a name="rest_code_66ab9ef64de04b35ba3cc37a3de9e6b0-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;1.2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;a name="rest_code_66ab9ef64de04b35ba3cc37a3de9e6b0-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;a name="rest_code_66ab9ef64de04b35ba3cc37a3de9e6b0-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This makes some operation significantly faster.&lt;/p&gt;
&lt;p&gt;Moreover, I've reduced a lot the numbers of device synchronization in the
library. Especially, I've removed almost all synchronization from the
etl-gpu-blas sub library. This means that synchronization is mostly only done
when data needs to go back to the CPU. For machine learning, this means at the
end of the epoch to compute the final error. This makes a HUGE difference in
time, I didn't realize before that I was doing way too much synchronization.&lt;/p&gt;
&lt;p&gt;With these two changes, I've been able to attain &lt;em&gt;state of the art training performance on GPU&lt;/em&gt; with my Deep Learning Library (DLL) project!&lt;/p&gt;
&lt;p&gt;Moreover, I've now added for random number generations on the GPU and for
shuffle operations as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="new-features"&gt;
&lt;h2&gt;New Features&lt;/h2&gt;
&lt;p&gt;I've also added a few new features recently. They were especially added to
support new features in DLL.&lt;/p&gt;
&lt;p&gt;Matrices and vectors can now be normalized in order to have zero-mean and
unit-variance distribution. You can also merge matrices together. For now, there
is no GPU support, so this will use CPU anyway. I plan to fix that later.&lt;/p&gt;
&lt;p&gt;In addition to bias_batch_mean that I added before, I also added bias_batch_var
now with the variance in place of the mean. This is mainly used for Batch
Normalization in machine learning, but it may have some other usages. The GPU
support has been added as well directly.&lt;/p&gt;
&lt;p&gt;And the last feature is the support for embedding and the gradients of
embedding. Again this is totally related to machine learning, but can be very
useful as well. I haven't add the time to develop the GPU version so far, but
this will come as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="performance"&gt;
&lt;h2&gt;Performance&lt;/h2&gt;
&lt;p&gt;Nothing fancy on the CPU performance side, I only added vectorization for
hyperbolic versions. This makes &lt;em&gt;tanh much faster on CPU&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="bug-fixes"&gt;
&lt;h2&gt;Bug Fixes&lt;/h2&gt;
&lt;p&gt;I fixed quite a few bugs in this version, which is one of the main reason
I released it:&lt;/p&gt;
&lt;p&gt;1. When using large fast_matrix and aliasing was detected, there was a big chance of stack overflow occurring. This is now fixed by using a dynamic temporary.
1. Some assignables such sub_view did not perform any detection for aliasing. This is now fixed and aliasing is detected everywhere.
1. fast_dyn_matrix can now be correctly used with &lt;em&gt;bool&lt;/em&gt;
1. The use of iterators was not always ensuring correct CPU/GPU consistency. This is now correctly handled.
1. The 4D convolution in GPU were not using the correct flipping
1. Fix small compilation bug with sub_matrix and GPU&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="what-s-next"&gt;
&lt;h2&gt;What's next ?&lt;/h2&gt;
&lt;p&gt;I don't really know what will be in the next release. This should be the release
1.3. One possible idea would be to improve and review the support for sparse
matrix which is more than  poor as of now. But I'm not really motivated to
work on that :P Moreover, I'm now &lt;em&gt;actively&lt;/em&gt; working on the next release of
budgetwarrior which will probably still come this month.&lt;/p&gt;
&lt;p&gt;I'm also still hesitating in switching to C++17 for the library to make it
faster to compile. And also to clean some parts of the code. I would be able to
remove quite some SFINAE with the new &lt;em&gt;if constexpr&lt;/em&gt;, but I'm afraid this will
make the library to difficult to use since it would need at least GCC 7 or clang
3.9.&lt;/p&gt;
&lt;div class="section" id="download-etl"&gt;
&lt;h3&gt;Download ETL&lt;/h3&gt;
&lt;p&gt;You can download ETL &lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;on Github&lt;/a&gt;. If you
only interested in the 1.2.1 version, you can look at the
&lt;a class="reference external" href="https://github.com/wichtounet/etl/releases"&gt;Releases pages&lt;/a&gt; or clone the tag
1.2.1. There are several branches:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;master&lt;/em&gt; Is the eternal development branch, may not always be stable&lt;/li&gt;
&lt;li&gt;&lt;em&gt;stable&lt;/em&gt; Is a branch always pointing to the last tag, no development here&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the future release, there always will tags pointing to the corresponding
commits. You can also have access to previous releases on Github or via the
release tags.&lt;/p&gt;
&lt;p&gt;The documentation is still a bit sparse. There are a few examples and the Wiki,
but there still is work to be done. If you have questions on how to use or
configure the library, please don't hesitate.&lt;/p&gt;
&lt;p&gt;Don't hesitate to comment this post if you have any comment on this library or
any question. You can also open an Issue on Github if you have a problem using
this library or propose a Pull Request if you have any contribution you'd like
to make to the library.&lt;/p&gt;
&lt;p&gt;Hope this may be useful to some of you :)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>C++17</category><category>etl</category><category>GPU</category><category>Performance</category><category>projects</category><category>Releases</category><guid>https://baptiste-wicht.com/posts/2018/01/expression-templates-library-121-faster-gpu-and-new-features.html</guid><pubDate>Tue, 09 Jan 2018 10:06:15 GMT</pubDate></item><item><title>Advanced GPU Patterns Optimization in ETL</title><link>https://baptiste-wicht.com/posts/2017/11/advanced-gpu-patterns-optimization-in-etl.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;The GPU performance of my Expression Templates Library (ETL) is pretty good when
most of the time is spent inside expensive operations such as Matrix-Matrix
Multiplication or convolutions. However, when most of the time is spent in
linear kernels, performance is not great because this will invoke a lot of CUDA
kernels. Indeed, the way it is done is that each sub expressions compute its
result in a temporary GPU vector (or matrix) and these temporaries are passed
through the expressions. For instance, this expression:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_a99ed031ffd44e11b09e8a44d32c4594-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;1.2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;will be executed on the GPU as something like this:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_831916d29c9b4b1e88e88e7332522bab-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
&lt;a name="rest_code_831916d29c9b4b1e88e88e7332522bab-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;t2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;a name="rest_code_831916d29c9b4b1e88e88e7332522bab-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;t2&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;that will results in three GPU kernels being invoked. In the CPU case, the
complete expression will be executed as one CPU kernel, that is constructed with
Expression Templates. Unfortunately, a CUDA kernel cannot be constructed in the
same way since the CUDA compiler does not support general template
metaprogramming. That's why I've implemented by using small kernels for each
expression.&lt;/p&gt;
&lt;p&gt;Fortunately, we can do better with a bit more meta-programming. Indeed, there
are some patterns that are repeated a lot and that easily be implemented in CUDA
kernels. I've started detecting a few of these patterns and for each of them
a single CUDA kernel is executed. For instance, each of the following
expressions can be executed with a single kernel:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_d0cc7f6b59504849b807d01b232e6447-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;a name="rest_code_d0cc7f6b59504849b807d01b232e6447-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;a name="rest_code_d0cc7f6b59504849b807d01b232e6447-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;1.2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;a name="rest_code_d0cc7f6b59504849b807d01b232e6447-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
&lt;a name="rest_code_d0cc7f6b59504849b807d01b232e6447-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This results in significantly performance improvement for these expressions!&lt;/p&gt;
&lt;p&gt;I have tested these new improvements in my Deep Learning Library (DLL) project
(not yet merged) and it resulted in &lt;strong&gt;25% faster momentum computation&lt;/strong&gt; and
&lt;strong&gt;17% faster Nesterov Adam (NADAM)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I'm going to continue to investigate which kernels need to be made faster for
DLL and try to improve the overall performance. Currently, the GPU performance
of DLL is very good for large convolutional networks, but could be improved for
small fully-connected networks. Indeed, in that case, quite some time is spent
outside the matrix-matrix multiplication and inside serial expressions for which
GPU could be improved. Once I'm done with my optimizations, I'll probably post
again on the blog with the latest results.&lt;/p&gt;
&lt;p&gt;All these new optimizations are now in the &lt;strong&gt;master&lt;/strong&gt; branch of the ETL
project if you want to check it out. You can access the project
&lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;on Github&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>dll</category><category>etl</category><category>GPU</category><category>Optimization</category><category>Performance</category><category>projects</category><guid>https://baptiste-wicht.com/posts/2017/11/advanced-gpu-patterns-optimization-in-etl.html</guid><pubDate>Sun, 26 Nov 2017 14:44:29 GMT</pubDate></item><item><title>Initial support for Long Short Term Memory (LSTM) in DLL</title><link>https://baptiste-wicht.com/posts/2017/11/initial-support-for-long-short-term-memory-lstm-in-dll.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I'm really happy to announce that I just merged support for&lt;/p&gt;
&lt;p&gt;Long Short Term Memory
(LSTM) cells into my Deep Learning Library (DLL) machine learning framework. Two
weeks ago, &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/11/initial-support-for-recurrent-neural-network-rnn-in-dll.html"&gt;I already merged suport for Recurrent Neural network (RNN)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It's nothing fancy yet, but forward propagation of LSTM and basic
Backpropagation Through Time (BPTT) are now supported. It was not really
complicated to implemenet the forward pass but the backward pass is much
complicated for an LSTM than for a RNN. It took me quite a long time to figure
out all the gradients formulas and the documentation on that is quite scarce.&lt;/p&gt;
&lt;p&gt;For now, still only existing classification loss is supported for RNN and LSTM.
As I said last time, I still plan to add support for sequence-to-sequence loss
in order to be able to train models able to generate characters. However, I don't
know when I'll be able to work on that. Now that I've got the code for LSTM,
I should be able to implement a GRU cell and NAS cell quite easily I believe.&lt;/p&gt;
&lt;p&gt;For example, here is a simple LSTM used on MNIST for classification:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/neural/dense_layer.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/neural/lstm_layer.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/neural/recurrent_last_layer.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-4"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/network.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-5"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/datasets.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-7"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="cm"&gt;/*argc*/&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;char&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="cm"&gt;/*argv*/&lt;/span&gt; &lt;span class="p"&gt;[])&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-8"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Load the dataset&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_mnist_dataset_nc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;scale_pre&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{});&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-11"&gt;&lt;/a&gt;    &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;time_steps&lt;/span&gt;      &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;sequence_length&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-15"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Build the network&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-17"&gt;&lt;/a&gt;    &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_network_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;network_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-19"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;lstm_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;time_steps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sequence_length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;last_only&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-20"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;recurrent_last_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;time_steps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-21"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-22"&gt;&lt;/a&gt;        &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-23"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ADAM&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;      &lt;span class="c1"&gt;// Adam&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-24"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;                       &lt;span class="c1"&gt;// The mini-batch size&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-25"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-26"&gt;&lt;/a&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-27"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-28"&gt;&lt;/a&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-29"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Display the network and dataset&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-30"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-31"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-32"&gt;&lt;/a&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-33"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Train the network for performance sake&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-34"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-35"&gt;&lt;/a&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-36"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Test the network on test set&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-37"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-38"&gt;&lt;/a&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-39"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_60b6b02a793f42abadd2c8272fe908e4-40"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The network is quite similar to the one used previously with an RNN, just
replace rnn with lstm and that's it. It starts with LSTM layer, followed by
a layer extracting the last time step and finally a dense layer with a softmax
function. The network is trained with Adam for 50 epochs. You can change the
activation function , the initializer for the weights and the biases and number
of steps for BPTT truncation.&lt;/p&gt;
&lt;p&gt;Here is the result I got on my last run:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-1"&gt;&lt;/a&gt;------------------------------------------------------------
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-2"&gt;&lt;/a&gt;| Index | Layer                | Parameters | Output Shape |
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-3"&gt;&lt;/a&gt;------------------------------------------------------------
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-4"&gt;&lt;/a&gt;| 0     | LSTM (TANH) (dyn)    |      51200 | [Bx28x100]   |
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-5"&gt;&lt;/a&gt;| 1     | RNN(last)            |          0 | [Bx100]      |
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-6"&gt;&lt;/a&gt;| 2     | Dense(SOFTMAX) (dyn) |       1000 | [Bx10]       |
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-7"&gt;&lt;/a&gt;------------------------------------------------------------
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-8"&gt;&lt;/a&gt;              Total Parameters:      52200
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-10"&gt;&lt;/a&gt;--------------------------------------------
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-11"&gt;&lt;/a&gt;| mnist | Size  | Batches | Augmented Size |
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-12"&gt;&lt;/a&gt;--------------------------------------------
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-13"&gt;&lt;/a&gt;| train | 60000 | 600     | 60000          |
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-14"&gt;&lt;/a&gt;| test  | 10000 | 100     | 10000          |
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-15"&gt;&lt;/a&gt;--------------------------------------------
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-17"&gt;&lt;/a&gt;Network with 3 layers
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-18"&gt;&lt;/a&gt;    LSTM(dyn): 28x28 -&amp;gt; TANH -&amp;gt; 28x100
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-19"&gt;&lt;/a&gt;    RNN(last): 28x100 -&amp;gt; 100
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-20"&gt;&lt;/a&gt;    Dense(dyn): 100 -&amp;gt; SOFTMAX -&amp;gt; 10
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-21"&gt;&lt;/a&gt;Total parameters: 52200
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-22"&gt;&lt;/a&gt;Dataset
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-23"&gt;&lt;/a&gt;Training: In-Memory Data Generator
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-24"&gt;&lt;/a&gt;              Size: 60000
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-25"&gt;&lt;/a&gt;           Batches: 600
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-26"&gt;&lt;/a&gt;Testing: In-Memory Data Generator
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-27"&gt;&lt;/a&gt;              Size: 10000
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-28"&gt;&lt;/a&gt;           Batches: 100
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-29"&gt;&lt;/a&gt;
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-30"&gt;&lt;/a&gt;Train the network with "Stochastic Gradient Descent"
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-31"&gt;&lt;/a&gt;    Updater: ADAM
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-32"&gt;&lt;/a&gt;       Loss: CATEGORICAL_CROSS_ENTROPY
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-33"&gt;&lt;/a&gt; Early Stop: Goal(error)
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-34"&gt;&lt;/a&gt;
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-35"&gt;&lt;/a&gt;With parameters:
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-36"&gt;&lt;/a&gt;          epochs=50
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-37"&gt;&lt;/a&gt;      batch_size=100
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-38"&gt;&lt;/a&gt;   learning_rate=0.001
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-39"&gt;&lt;/a&gt;           beta1=0.9
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-40"&gt;&lt;/a&gt;           beta2=0.999
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-41"&gt;&lt;/a&gt;
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-42"&gt;&lt;/a&gt;epoch   0/50 batch  600/ 600 - error: 0.07943 loss: 0.28504 time 20910ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-43"&gt;&lt;/a&gt;epoch   1/50 batch  600/ 600 - error: 0.06683 loss: 0.24021 time 20889ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-44"&gt;&lt;/a&gt;epoch   2/50 batch  600/ 600 - error: 0.04828 loss: 0.18233 time 21061ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-45"&gt;&lt;/a&gt;epoch   3/50 batch  600/ 600 - error: 0.04407 loss: 0.16665 time 20839ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-46"&gt;&lt;/a&gt;epoch   4/50 batch  600/ 600 - error: 0.03515 loss: 0.13290 time 22108ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-47"&gt;&lt;/a&gt;epoch   5/50 batch  600/ 600 - error: 0.03207 loss: 0.12019 time 21393ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-48"&gt;&lt;/a&gt;epoch   6/50 batch  600/ 600 - error: 0.02973 loss: 0.11239 time 28199ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-49"&gt;&lt;/a&gt;epoch   7/50 batch  600/ 600 - error: 0.02653 loss: 0.10455 time 37039ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-50"&gt;&lt;/a&gt;epoch   8/50 batch  600/ 600 - error: 0.02482 loss: 0.09657 time 23127ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-51"&gt;&lt;/a&gt;epoch   9/50 batch  600/ 600 - error: 0.02177 loss: 0.08422 time 41766ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-52"&gt;&lt;/a&gt;epoch  10/50 batch  600/ 600 - error: 0.02453 loss: 0.09382 time 29765ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-53"&gt;&lt;/a&gt;epoch  11/50 batch  600/ 600 - error: 0.02575 loss: 0.09796 time 21449ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-54"&gt;&lt;/a&gt;epoch  12/50 batch  600/ 600 - error: 0.02107 loss: 0.07833 time 42056ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-55"&gt;&lt;/a&gt;epoch  13/50 batch  600/ 600 - error: 0.01877 loss: 0.07171 time 24673ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-56"&gt;&lt;/a&gt;epoch  14/50 batch  600/ 600 - error: 0.02095 loss: 0.08481 time 20878ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-57"&gt;&lt;/a&gt;epoch  15/50 batch  600/ 600 - error: 0.02040 loss: 0.07578 time 41515ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-58"&gt;&lt;/a&gt;epoch  16/50 batch  600/ 600 - error: 0.01580 loss: 0.06083 time 25705ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-59"&gt;&lt;/a&gt;epoch  17/50 batch  600/ 600 - error: 0.01945 loss: 0.07046 time 20903ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-60"&gt;&lt;/a&gt;epoch  18/50 batch  600/ 600 - error: 0.01728 loss: 0.06683 time 41828ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-61"&gt;&lt;/a&gt;epoch  19/50 batch  600/ 600 - error: 0.01577 loss: 0.05947 time 27810ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-62"&gt;&lt;/a&gt;epoch  20/50 batch  600/ 600 - error: 0.01528 loss: 0.05883 time 21477ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-63"&gt;&lt;/a&gt;epoch  21/50 batch  600/ 600 - error: 0.01345 loss: 0.05127 time 44718ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-64"&gt;&lt;/a&gt;epoch  22/50 batch  600/ 600 - error: 0.01410 loss: 0.05357 time 25174ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-65"&gt;&lt;/a&gt;epoch  23/50 batch  600/ 600 - error: 0.01268 loss: 0.04765 time 23827ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-66"&gt;&lt;/a&gt;epoch  24/50 batch  600/ 600 - error: 0.01342 loss: 0.05004 time 47232ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-67"&gt;&lt;/a&gt;epoch  25/50 batch  600/ 600 - error: 0.01730 loss: 0.06872 time 22532ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-68"&gt;&lt;/a&gt;epoch  26/50 batch  600/ 600 - error: 0.01337 loss: 0.05016 time 30114ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-69"&gt;&lt;/a&gt;epoch  27/50 batch  600/ 600 - error: 0.01842 loss: 0.07049 time 40136ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-70"&gt;&lt;/a&gt;epoch  28/50 batch  600/ 600 - error: 0.01262 loss: 0.04639 time 21793ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-71"&gt;&lt;/a&gt;epoch  29/50 batch  600/ 600 - error: 0.01403 loss: 0.05292 time 34096ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-72"&gt;&lt;/a&gt;epoch  30/50 batch  600/ 600 - error: 0.01185 loss: 0.04456 time 35420ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-73"&gt;&lt;/a&gt;epoch  31/50 batch  600/ 600 - error: 0.01098 loss: 0.04180 time 20909ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-74"&gt;&lt;/a&gt;epoch  32/50 batch  600/ 600 - error: 0.01337 loss: 0.04687 time 30113ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-75"&gt;&lt;/a&gt;epoch  33/50 batch  600/ 600 - error: 0.01415 loss: 0.05292 time 37393ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-76"&gt;&lt;/a&gt;epoch  34/50 batch  600/ 600 - error: 0.00982 loss: 0.03615 time 20962ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-77"&gt;&lt;/a&gt;epoch  35/50 batch  600/ 600 - error: 0.01178 loss: 0.04830 time 29305ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-78"&gt;&lt;/a&gt;epoch  36/50 batch  600/ 600 - error: 0.00882 loss: 0.03408 time 38293ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-79"&gt;&lt;/a&gt;epoch  37/50 batch  600/ 600 - error: 0.01148 loss: 0.04341 time 20841ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-80"&gt;&lt;/a&gt;epoch  38/50 batch  600/ 600 - error: 0.00960 loss: 0.03701 time 29204ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-81"&gt;&lt;/a&gt;epoch  39/50 batch  600/ 600 - error: 0.00850 loss: 0.03094 time 39802ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-82"&gt;&lt;/a&gt;epoch  40/50 batch  600/ 600 - error: 0.01473 loss: 0.05136 time 20831ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-83"&gt;&lt;/a&gt;epoch  41/50 batch  600/ 600 - error: 0.01007 loss: 0.03579 time 29856ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-84"&gt;&lt;/a&gt;epoch  42/50 batch  600/ 600 - error: 0.00943 loss: 0.03370 time 38200ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-85"&gt;&lt;/a&gt;epoch  43/50 batch  600/ 600 - error: 0.01205 loss: 0.04409 time 21162ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-86"&gt;&lt;/a&gt;epoch  44/50 batch  600/ 600 - error: 0.00980 loss: 0.03674 time 32279ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-87"&gt;&lt;/a&gt;epoch  45/50 batch  600/ 600 - error: 0.01068 loss: 0.04133 time 38448ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-88"&gt;&lt;/a&gt;epoch  46/50 batch  600/ 600 - error: 0.00913 loss: 0.03478 time 20797ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-89"&gt;&lt;/a&gt;epoch  47/50 batch  600/ 600 - error: 0.00985 loss: 0.03759 time 28885ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-90"&gt;&lt;/a&gt;epoch  48/50 batch  600/ 600 - error: 0.00912 loss: 0.03295 time 41120ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-91"&gt;&lt;/a&gt;epoch  49/50 batch  600/ 600 - error: 0.00930 loss: 0.03438 time 21282ms
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-92"&gt;&lt;/a&gt;Restore the best (error) weights from epoch 39
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-93"&gt;&lt;/a&gt;Training took 1460s
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-94"&gt;&lt;/a&gt;
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-95"&gt;&lt;/a&gt;Evaluation Results
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-96"&gt;&lt;/a&gt;   error: 0.02440
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-97"&gt;&lt;/a&gt;    loss: 0.11315
&lt;a name="rest_code_c730a39179444fe4a2bf5677ddb7f200-98"&gt;&lt;/a&gt;evaluation took 1000ms
&lt;/pre&gt;&lt;p&gt;Again, nothing fancy yet, but this example has not been optimized for
performance nor for accuracy.&lt;/p&gt;
&lt;p&gt;I also made a few changes to the RNN layer. I added support for biases and
improved the code as well for performance and readability.&lt;/p&gt;
&lt;p&gt;All this support is now in the &lt;strong&gt;master&lt;/strong&gt; branch of the DLL project if you want
to check it out. You can also check out the example online:
&lt;a class="reference external" href="https://github.com/wichtounet/dll/blob/master/examples/src/mnist_lstm.cpp"&gt;mnist_lstm.cpp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can access the project &lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;on Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Currently I'm working on the GPU performance again. The performance of some is
still not as good as I want it to be, especially complex operation like used in
Adam and Nadam. Currently, there are many calls to GPU BLAS libraries and
I want to try to extract some more optimized patterns. Once it's done, I'll post
more on that later on the blog.&lt;/p&gt;&lt;/div&gt;</description><category>Deep Learning</category><category>dll</category><category>Machine Learning</category><category>projects</category><category>rnn</category><guid>https://baptiste-wicht.com/posts/2017/11/initial-support-for-long-short-term-memory-lstm-in-dll.html</guid><pubDate>Fri, 24 Nov 2017 14:16:37 GMT</pubDate></item><item><title>DLL: Pretty printing and live output</title><link>https://baptiste-wicht.com/posts/2017/11/dll-pretty-printing-and-live-output.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've improved a lot the display of my Deep Learning Library (DLL). I know this
is generally not the most important point in a machine learning framework, but
the first impression being important. Therefore, I decided it was time to get
a nicer output in the console for training networks.&lt;/p&gt;
&lt;p&gt;A network or a dataset can be displayed using the &lt;code&gt;display()&lt;/code&gt; function.
I've added a &lt;code&gt;display_pretty()&lt;/code&gt; function to them to display it more
nicely. I've also added the &lt;code&gt;dll::dump_timers_nice()&lt;/code&gt; function to do the
same for &lt;code&gt;dll::dump_timers()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I've also improved the display for the results of the batches during training.
Now, the display is updated every 100ms and it also displays the current
estimated time until the end of the epoch. With that, the user should have
a much better idea on what's going on during training, especially when training
networks when the epochs are taking a long time to complete.&lt;/p&gt;
&lt;p&gt;Here is a full output of the training of fully-connected network on MNIST
(&lt;cite&gt;mnist_mlp.cpp &amp;lt;https://github.com/wichtounet/dll/blob/master/examples/src/mnist_mlp.cpp&amp;gt;&lt;/cite&gt;):&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-1"&gt;&lt;/a&gt; ------------------------------------------------------------
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-2"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt; Index &lt;span class="p"&gt;|&lt;/span&gt; Layer                &lt;span class="p"&gt;|&lt;/span&gt; Parameters &lt;span class="p"&gt;|&lt;/span&gt; Output Shape &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-3"&gt;&lt;/a&gt; ------------------------------------------------------------
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-4"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;     &lt;span class="p"&gt;|&lt;/span&gt; Dense&lt;span class="o"&gt;(&lt;/span&gt;SIGMOID&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;dyn&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt;     &lt;span class="m"&gt;392000&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;Bx500&lt;span class="o"&gt;]&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-5"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;     &lt;span class="p"&gt;|&lt;/span&gt; Dropout&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.50&lt;span class="o"&gt;)(&lt;/span&gt;dyn&lt;span class="o"&gt;)&lt;/span&gt;   &lt;span class="p"&gt;|&lt;/span&gt;          &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;Bx500&lt;span class="o"&gt;]&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-6"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;     &lt;span class="p"&gt;|&lt;/span&gt; Dense&lt;span class="o"&gt;(&lt;/span&gt;SIGMOID&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;dyn&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt;     &lt;span class="m"&gt;125000&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;Bx250&lt;span class="o"&gt;]&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-7"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;     &lt;span class="p"&gt;|&lt;/span&gt; Dropout&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.50&lt;span class="o"&gt;)(&lt;/span&gt;dyn&lt;span class="o"&gt;)&lt;/span&gt;   &lt;span class="p"&gt;|&lt;/span&gt;          &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;Bx250&lt;span class="o"&gt;]&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-8"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;     &lt;span class="p"&gt;|&lt;/span&gt; Dense&lt;span class="o"&gt;(&lt;/span&gt;SOFTMAX&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;dyn&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt;       &lt;span class="m"&gt;2500&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;Bx10&lt;span class="o"&gt;]&lt;/span&gt;       &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-9"&gt;&lt;/a&gt; ------------------------------------------------------------
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-10"&gt;&lt;/a&gt;                Total Parameters:     &lt;span class="m"&gt;519500&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-12"&gt;&lt;/a&gt; --------------------------------------------
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-13"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt; mnist &lt;span class="p"&gt;|&lt;/span&gt; Size  &lt;span class="p"&gt;|&lt;/span&gt; Batches &lt;span class="p"&gt;|&lt;/span&gt; Augmented Size &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-14"&gt;&lt;/a&gt; --------------------------------------------
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-15"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt; train &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;60000&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;600&lt;/span&gt;     &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;60000&lt;/span&gt;          &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-16"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nb"&gt;test&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;10000&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;100&lt;/span&gt;     &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;10000&lt;/span&gt;          &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-17"&gt;&lt;/a&gt; --------------------------------------------
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-18"&gt;&lt;/a&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-19"&gt;&lt;/a&gt;Train the network with &lt;span class="s2"&gt;"Stochastic Gradient Descent"&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-20"&gt;&lt;/a&gt;    Updater: NADAM
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-21"&gt;&lt;/a&gt;       Loss: CATEGORICAL_CROSS_ENTROPY
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-22"&gt;&lt;/a&gt; Early Stop: Goal&lt;span class="o"&gt;(&lt;/span&gt;error&lt;span class="o"&gt;)&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-23"&gt;&lt;/a&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-24"&gt;&lt;/a&gt;With parameters:
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-25"&gt;&lt;/a&gt;          &lt;span class="nv"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;50&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-26"&gt;&lt;/a&gt;      &lt;span class="nv"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-27"&gt;&lt;/a&gt;   &lt;span class="nv"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.002
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-28"&gt;&lt;/a&gt;           &lt;span class="nv"&gt;beta1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.9
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-29"&gt;&lt;/a&gt;           &lt;span class="nv"&gt;beta2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.999
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-30"&gt;&lt;/a&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-31"&gt;&lt;/a&gt;epoch   &lt;span class="m"&gt;0&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.04623 loss: &lt;span class="m"&gt;0&lt;/span&gt;.15097 &lt;span class="nb"&gt;time&lt;/span&gt; 3230ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-32"&gt;&lt;/a&gt;epoch   &lt;span class="m"&gt;1&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.03013 loss: &lt;span class="m"&gt;0&lt;/span&gt;.09947 &lt;span class="nb"&gt;time&lt;/span&gt; 3188ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-33"&gt;&lt;/a&gt;epoch   &lt;span class="m"&gt;2&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.02048 loss: &lt;span class="m"&gt;0&lt;/span&gt;.06565 &lt;span class="nb"&gt;time&lt;/span&gt; 3102ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-34"&gt;&lt;/a&gt;epoch   &lt;span class="m"&gt;3&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.01593 loss: &lt;span class="m"&gt;0&lt;/span&gt;.05258 &lt;span class="nb"&gt;time&lt;/span&gt; 3189ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-35"&gt;&lt;/a&gt;epoch   &lt;span class="m"&gt;4&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.01422 loss: &lt;span class="m"&gt;0&lt;/span&gt;.04623 &lt;span class="nb"&gt;time&lt;/span&gt; 3160ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-36"&gt;&lt;/a&gt;epoch   &lt;span class="m"&gt;5&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.01112 loss: &lt;span class="m"&gt;0&lt;/span&gt;.03660 &lt;span class="nb"&gt;time&lt;/span&gt; 3131ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-37"&gt;&lt;/a&gt;epoch   &lt;span class="m"&gt;6&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.01078 loss: &lt;span class="m"&gt;0&lt;/span&gt;.03546 &lt;span class="nb"&gt;time&lt;/span&gt; 3200ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-38"&gt;&lt;/a&gt;epoch   &lt;span class="m"&gt;7&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.01003 loss: &lt;span class="m"&gt;0&lt;/span&gt;.03184 &lt;span class="nb"&gt;time&lt;/span&gt; 3246ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-39"&gt;&lt;/a&gt;epoch   &lt;span class="m"&gt;8&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00778 loss: &lt;span class="m"&gt;0&lt;/span&gt;.02550 &lt;span class="nb"&gt;time&lt;/span&gt; 3222ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-40"&gt;&lt;/a&gt;epoch   &lt;span class="m"&gt;9&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00782 loss: &lt;span class="m"&gt;0&lt;/span&gt;.02505 &lt;span class="nb"&gt;time&lt;/span&gt; 3119ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-41"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;10&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00578 loss: &lt;span class="m"&gt;0&lt;/span&gt;.02056 &lt;span class="nb"&gt;time&lt;/span&gt; 3284ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-42"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;11&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00618 loss: &lt;span class="m"&gt;0&lt;/span&gt;.02045 &lt;span class="nb"&gt;time&lt;/span&gt; 3220ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-43"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;12&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00538 loss: &lt;span class="m"&gt;0&lt;/span&gt;.01775 &lt;span class="nb"&gt;time&lt;/span&gt; 3444ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-44"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;13&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00563 loss: &lt;span class="m"&gt;0&lt;/span&gt;.01803 &lt;span class="nb"&gt;time&lt;/span&gt; 3304ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-45"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;14&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00458 loss: &lt;span class="m"&gt;0&lt;/span&gt;.01598 &lt;span class="nb"&gt;time&lt;/span&gt; 3577ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-46"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;15&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00437 loss: &lt;span class="m"&gt;0&lt;/span&gt;.01436 &lt;span class="nb"&gt;time&lt;/span&gt; 3228ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-47"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;16&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00360 loss: &lt;span class="m"&gt;0&lt;/span&gt;.01214 &lt;span class="nb"&gt;time&lt;/span&gt; 3180ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-48"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;17&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00405 loss: &lt;span class="m"&gt;0&lt;/span&gt;.01309 &lt;span class="nb"&gt;time&lt;/span&gt; 3090ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-49"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;18&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00408 loss: &lt;span class="m"&gt;0&lt;/span&gt;.01346 &lt;span class="nb"&gt;time&lt;/span&gt; 3045ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-50"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;19&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00337 loss: &lt;span class="m"&gt;0&lt;/span&gt;.01153 &lt;span class="nb"&gt;time&lt;/span&gt; 3071ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-51"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;20&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00297 loss: &lt;span class="m"&gt;0&lt;/span&gt;.01021 &lt;span class="nb"&gt;time&lt;/span&gt; 3131ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-52"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;21&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00318 loss: &lt;span class="m"&gt;0&lt;/span&gt;.01103 &lt;span class="nb"&gt;time&lt;/span&gt; 3076ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-53"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;22&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00277 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00909 &lt;span class="nb"&gt;time&lt;/span&gt; 3090ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-54"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;23&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00242 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00818 &lt;span class="nb"&gt;time&lt;/span&gt; 3163ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-55"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;24&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00267 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00913 &lt;span class="nb"&gt;time&lt;/span&gt; 3229ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-56"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;25&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00295 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00947 &lt;span class="nb"&gt;time&lt;/span&gt; 3156ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-57"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;26&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00252 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00809 &lt;span class="nb"&gt;time&lt;/span&gt; 3066ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-58"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;27&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00227 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00773 &lt;span class="nb"&gt;time&lt;/span&gt; 3156ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-59"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;28&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00203 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00728 &lt;span class="nb"&gt;time&lt;/span&gt; 3158ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-60"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;29&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00240 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00753 &lt;span class="nb"&gt;time&lt;/span&gt; 3114ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-61"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;30&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00263 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00864 &lt;span class="nb"&gt;time&lt;/span&gt; 3099ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-62"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;31&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00210 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00675 &lt;span class="nb"&gt;time&lt;/span&gt; 3096ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-63"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;32&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00163 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00628 &lt;span class="nb"&gt;time&lt;/span&gt; 3120ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-64"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;33&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00182 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00611 &lt;span class="nb"&gt;time&lt;/span&gt; 3045ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-65"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;34&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00125 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00468 &lt;span class="nb"&gt;time&lt;/span&gt; 3140ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-66"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;35&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00183 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00598 &lt;span class="nb"&gt;time&lt;/span&gt; 3093ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-67"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;36&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00232 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00711 &lt;span class="nb"&gt;time&lt;/span&gt; 3068ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-68"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;37&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00170 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00571 &lt;span class="nb"&gt;time&lt;/span&gt; 3057ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-69"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;38&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00162 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00530 &lt;span class="nb"&gt;time&lt;/span&gt; 3115ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-70"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;39&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00155 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00513 &lt;span class="nb"&gt;time&lt;/span&gt; 3226ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-71"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;40&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00150 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00501 &lt;span class="nb"&gt;time&lt;/span&gt; 2987ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-72"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;41&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00122 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00425 &lt;span class="nb"&gt;time&lt;/span&gt; 3117ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-73"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;42&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00108 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00383 &lt;span class="nb"&gt;time&lt;/span&gt; 3102ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-74"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;43&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00165 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00533 &lt;span class="nb"&gt;time&lt;/span&gt; 2977ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-75"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;44&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00142 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00469 &lt;span class="nb"&gt;time&lt;/span&gt; 3009ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-76"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;45&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00098 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00356 &lt;span class="nb"&gt;time&lt;/span&gt; 3055ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-77"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;46&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00127 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00409 &lt;span class="nb"&gt;time&lt;/span&gt; 3076ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-78"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;47&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00132 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00438 &lt;span class="nb"&gt;time&lt;/span&gt; 3068ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-79"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;48&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00130 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00459 &lt;span class="nb"&gt;time&lt;/span&gt; 3045ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-80"&gt;&lt;/a&gt;epoch  &lt;span class="m"&gt;49&lt;/span&gt;/50 batch  &lt;span class="m"&gt;600&lt;/span&gt;/ &lt;span class="m"&gt;600&lt;/span&gt; - error: &lt;span class="m"&gt;0&lt;/span&gt;.00107 loss: &lt;span class="m"&gt;0&lt;/span&gt;.00365 &lt;span class="nb"&gt;time&lt;/span&gt; 3103ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-81"&gt;&lt;/a&gt;Restore the best &lt;span class="o"&gt;(&lt;/span&gt;error&lt;span class="o"&gt;)&lt;/span&gt; weights from epoch &lt;span class="m"&gt;45&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-82"&gt;&lt;/a&gt;Training took 160s
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-83"&gt;&lt;/a&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-84"&gt;&lt;/a&gt;Evaluation Results
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-85"&gt;&lt;/a&gt;   error: &lt;span class="m"&gt;0&lt;/span&gt;.01740
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-86"&gt;&lt;/a&gt;    loss: &lt;span class="m"&gt;0&lt;/span&gt;.07861
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-87"&gt;&lt;/a&gt;evaluation took 67ms
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-88"&gt;&lt;/a&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-89"&gt;&lt;/a&gt; -----------------------------------------------------------------------------
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-90"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt; %        &lt;span class="p"&gt;|&lt;/span&gt; Timer                         &lt;span class="p"&gt;|&lt;/span&gt; Count  &lt;span class="p"&gt;|&lt;/span&gt; Total     &lt;span class="p"&gt;|&lt;/span&gt; Average   &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-91"&gt;&lt;/a&gt; -----------------------------------------------------------------------------
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-92"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;100&lt;/span&gt;.000% &lt;span class="p"&gt;|&lt;/span&gt; net:train:ft                  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;160&lt;/span&gt;.183s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;160&lt;/span&gt;.183s  &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-93"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;100&lt;/span&gt;.000% &lt;span class="p"&gt;|&lt;/span&gt; net:trainer:train             &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;      &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;160&lt;/span&gt;.183s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;160&lt;/span&gt;.183s  &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-94"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="m"&gt;99&lt;/span&gt;.997% &lt;span class="p"&gt;|&lt;/span&gt; net:trainer:train:epoch       &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;50&lt;/span&gt;     &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;160&lt;/span&gt;.178s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;.20356s  &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-95"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="m"&gt;84&lt;/span&gt;.422% &lt;span class="p"&gt;|&lt;/span&gt; net:trainer:train:epoch:batch &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;30000&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;135&lt;/span&gt;.229s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;.50764ms &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-96"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="m"&gt;84&lt;/span&gt;.261% &lt;span class="p"&gt;|&lt;/span&gt; sgd::train_batch              &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;30000&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;134&lt;/span&gt;.971s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;.49904ms &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-97"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="m"&gt;44&lt;/span&gt;.404% &lt;span class="p"&gt;|&lt;/span&gt; sgd::grad                     &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;30000&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;71&lt;/span&gt;.1271s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;.3709ms  &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-98"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="m"&gt;35&lt;/span&gt;.453% &lt;span class="p"&gt;|&lt;/span&gt; sgd::forward                  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;30000&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;56&lt;/span&gt;.7893s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;.89298ms &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-99"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="m"&gt;32&lt;/span&gt;.245% &lt;span class="p"&gt;|&lt;/span&gt; sgd::update_weights           &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;90000&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;51&lt;/span&gt;.6505s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;573&lt;/span&gt;.894us &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-100"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="m"&gt;32&lt;/span&gt;.226% &lt;span class="p"&gt;|&lt;/span&gt; sgd::apply_grad:nadam         &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;180000&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;51&lt;/span&gt;.6211s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;286&lt;/span&gt;.783us &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-101"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="m"&gt;28&lt;/span&gt;.399% &lt;span class="p"&gt;|&lt;/span&gt; dense:dyn:forward             &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;180300&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;45&lt;/span&gt;.4903s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;252&lt;/span&gt;.303us &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-102"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="m"&gt;17&lt;/span&gt;.642% &lt;span class="p"&gt;|&lt;/span&gt; dropout:train:forward         &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;60000&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;28&lt;/span&gt;.2595s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;470&lt;/span&gt;.99us  &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-103"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="m"&gt;13&lt;/span&gt;.707% &lt;span class="p"&gt;|&lt;/span&gt; net:trainer:train:epoch:error &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;50&lt;/span&gt;     &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;21&lt;/span&gt;.957s   &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;439&lt;/span&gt;.14ms  &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-104"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;  &lt;span class="m"&gt;12&lt;/span&gt;.148% &lt;span class="p"&gt;|&lt;/span&gt; dense:dyn:gradients           &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;90000&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;19&lt;/span&gt;.4587s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;216&lt;/span&gt;.207us &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-105"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;   &lt;span class="m"&gt;4&lt;/span&gt;.299% &lt;span class="p"&gt;|&lt;/span&gt; sgd::backward                 &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;30000&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;.88546s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;229&lt;/span&gt;.515us &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-106"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;   &lt;span class="m"&gt;3&lt;/span&gt;.301% &lt;span class="p"&gt;|&lt;/span&gt; dense:dyn:backward            &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;60000&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;5&lt;/span&gt;.28729s  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;88&lt;/span&gt;.121us  &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-107"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;   &lt;span class="m"&gt;0&lt;/span&gt;.560% &lt;span class="p"&gt;|&lt;/span&gt; dense:dyn:errors              &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;60000&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;896&lt;/span&gt;.471ms &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;14&lt;/span&gt;.941us  &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-108"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;   &lt;span class="m"&gt;0&lt;/span&gt;.407% &lt;span class="p"&gt;|&lt;/span&gt; dropout:backward              &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;60000&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;651&lt;/span&gt;.523ms &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;.858us  &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-109"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;   &lt;span class="m"&gt;0&lt;/span&gt;.339% &lt;span class="p"&gt;|&lt;/span&gt; dropout:test:forward          &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;60000&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;542&lt;/span&gt;.799ms &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;9&lt;/span&gt;.046us   &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-110"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;   &lt;span class="m"&gt;0&lt;/span&gt;.161% &lt;span class="p"&gt;|&lt;/span&gt; net:compute_loss:CCE          &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;60100&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;257&lt;/span&gt;.915ms &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;.291us   &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-111"&gt;&lt;/a&gt; &lt;span class="p"&gt;|&lt;/span&gt;   &lt;span class="m"&gt;0&lt;/span&gt;.099% &lt;span class="p"&gt;|&lt;/span&gt; sgd::error                    &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;30000&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;158&lt;/span&gt;.33ms  &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;5&lt;/span&gt;.277us   &lt;span class="p"&gt;|&lt;/span&gt;
&lt;a name="rest_code_b51e57d06d8041c2a48dc1f9096eacda-112"&gt;&lt;/a&gt; -----------------------------------------------------------------------------
&lt;/pre&gt;&lt;p&gt;I hope this will make the output of the machine learning framework more useful.&lt;/p&gt;
&lt;p&gt;All this support is now in the &lt;strong&gt;master&lt;/strong&gt; branch of the DLL project if you want
to check it out. You can also check out the example online:
&lt;a class="reference external" href="https://github.com/wichtounet/dll/blob/master/examples/src/mnist_mlp.cpp"&gt;mnist_mlp.cpp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can access the project &lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;on Github&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>dll</category><category>Machine Learning</category><category>projects</category><guid>https://baptiste-wicht.com/posts/2017/11/dll-pretty-printing-and-live-output.html</guid><pubDate>Sun, 19 Nov 2017 14:15:57 GMT</pubDate></item><item><title>Inventor on four new research patents</title><link>https://baptiste-wicht.com/posts/2017/11/inventor-on-four-new-research-patents.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;During the first years of my thesis I worked on CTI research project with the
American company Verisign, which has also an office near my school. A CTI
research project is a project that is partially funded by the Commission on
Innovation and Technology (CTI) where a school and a company work together.
I was quite lucky to work on this project with the awesome people at Verisign
Fribourg. After the success of the project, Verisign filled several patents
regarding various points of the projects.&lt;/p&gt;
&lt;p&gt;I'm quite happy now that these four patents are now approved and published. They
They have been approved by both the United States Patent and Trademark Office
(USPTO) and European Patent Office (EPO). The parents have been cl= aimed by
Verisign, I'm only one of the inventor, I got no claim on the patent. But it's
still a great thing.&lt;/p&gt;
&lt;p&gt;Here are the names of the four patents:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Systems and methods for automatic phonetization of domain names&lt;/li&gt;
&lt;li&gt;Construction of phonetic representation of a string of characters&lt;/li&gt;
&lt;li&gt;Method for writing a foreign language in a pseudo language phonetically resembling native language of the speaker&lt;/li&gt;
&lt;li&gt;Construction of a phonetic representation of a generated string of characters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can take a look at them on USPTO or EPO or on Google Patents, but the way
a patent is written make it relatively hard to follow, it's more on a lawyer
level or maybe I'm simply not used to patents anymore.&lt;/p&gt;
&lt;p&gt;All these patents come from the research done during the CTI project with
Verisign. In this project, name suggestions were generated from the phonetic
sound of the name. The idea being to generate names that sounds the same as
another input (airmix could become rmix or rmics). We are using various
technologies to make this work: IG-Tree, Viterbi and HMM. And since we used
a model with an encoder and a decoder, we can also mix languages. For instance,
write something in French the way a English work would work (for instance school
could become scoule).&lt;/p&gt;
&lt;p&gt;These patents concludes a very interesting and successful project. I'm now
working on yet another CTI research project with Verisign and it will surely be
as successful as the first one.&lt;/p&gt;&lt;/div&gt;</description><category>Machine Learning</category><category>patents</category><category>projects</category><category>publications</category><guid>https://baptiste-wicht.com/posts/2017/11/inventor-on-four-new-research-patents.html</guid><pubDate>Fri, 17 Nov 2017 13:50:33 GMT</pubDate></item><item><title>Initial support for Recurrent Neural Network (RNN) in DLL</title><link>https://baptiste-wicht.com/posts/2017/11/initial-support-for-recurrent-neural-network-rnn-in-dll.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I'm happy to announce that I just merged support for Recurrent Neural Networks
(RNNs) into my Deep Learning Library (DLL) machine learning framework.&lt;/p&gt;
&lt;p&gt;It's nothing fancy yet, but forward propagation of RNN and basic Backpropagation
Through Time (BPTT) are now supported. For now, only existing classification
loss is supported for RNN. I plan to add support for sequence-to-sequence loss
in order to be able to train models able to generate characters, but I don't
know when I'll be able to work on that. I also plan to add support for other
types of cells such as LSTM and GRU (maybe NAS) in the future.&lt;/p&gt;
&lt;p&gt;For example, here is a simple RNN used on MNIST:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/neural/dense_layer.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/neural/recurrent_layer.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/neural/recurrent_last_layer.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-4"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/network.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-5"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"dll/datasets.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-7"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="cm"&gt;/*argc*/&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;char&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="cm"&gt;/*argv*/&lt;/span&gt; &lt;span class="p"&gt;[])&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-8"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Load the dataset&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_mnist_dataset_nc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;scale_pre&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{});&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-11"&gt;&lt;/a&gt;    &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;time_steps&lt;/span&gt;      &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;sequence_length&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-15"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Build the network&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-17"&gt;&lt;/a&gt;    &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_network_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;network_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-19"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;recurrent_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;time_steps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sequence_length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;last_only&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-20"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;recurrent_last_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;time_steps&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-21"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;hidden_units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-22"&gt;&lt;/a&gt;        &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-23"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ADAM&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;      &lt;span class="c1"&gt;// Adam&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-24"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;                       &lt;span class="c1"&gt;// The mini-batch size&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-25"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-26"&gt;&lt;/a&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-27"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-28"&gt;&lt;/a&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-29"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Display the network and dataset&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-30"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-31"&gt;&lt;/a&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-32"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Train the network for performance sake&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-33"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-34"&gt;&lt;/a&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-35"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Test the network on test set&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-36"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-37"&gt;&lt;/a&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-38"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_c379b388739c47b98637c86808867c5a-39"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The network starts with recurrent layer, followed by a layer that extracts only
the last layer and finally a dense layer with a softmax function. The recurrent
layer has support to change the activation function, change the initializer for
the two weights matrices of the RNN and the number of steps for BPTT truncation.&lt;/p&gt;
&lt;p&gt;Here is a possible result:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-1"&gt;&lt;/a&gt;Network with 3 layers
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-2"&gt;&lt;/a&gt;    RNN(dyn): 28x28 -&amp;gt; TANH -&amp;gt; 28x100
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-3"&gt;&lt;/a&gt;    RNN(last): 28x100 -&amp;gt; 100
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-4"&gt;&lt;/a&gt;    Dense(dyn): 100 -&amp;gt; SOFTMAX -&amp;gt; 10
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-5"&gt;&lt;/a&gt;Total parameters: 13800
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-6"&gt;&lt;/a&gt;Train the network with "Stochastic Gradient Descent"
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-7"&gt;&lt;/a&gt;    Updater: ADAM
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-8"&gt;&lt;/a&gt;       Loss: CATEGORICAL_CROSS_ENTROPY
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-9"&gt;&lt;/a&gt; Early Stop: Goal(error)
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-11"&gt;&lt;/a&gt;With parameters:
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-12"&gt;&lt;/a&gt;          epochs=50
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-13"&gt;&lt;/a&gt;      batch_size=100
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-14"&gt;&lt;/a&gt;   learning_rate=0.001
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-15"&gt;&lt;/a&gt;           beta1=0.9
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-16"&gt;&lt;/a&gt;           beta2=0.999
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-18"&gt;&lt;/a&gt;Epoch   0/50 - Classification error: 0.11635 Loss: 0.39999 Time 4717ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-19"&gt;&lt;/a&gt;Epoch   1/50 - Classification error: 0.11303 Loss: 0.36994 Time 4702ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-20"&gt;&lt;/a&gt;Epoch   2/50 - Classification error: 0.06732 Loss: 0.23469 Time 4702ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-21"&gt;&lt;/a&gt;Epoch   3/50 - Classification error: 0.04865 Loss: 0.17091 Time 4696ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-22"&gt;&lt;/a&gt;Epoch   4/50 - Classification error: 0.05957 Loss: 0.20437 Time 4706ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-23"&gt;&lt;/a&gt;Epoch   5/50 - Classification error: 0.05022 Loss: 0.16888 Time 4696ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-24"&gt;&lt;/a&gt;Epoch   6/50 - Classification error: 0.03912 Loss: 0.13743 Time 4698ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-25"&gt;&lt;/a&gt;Epoch   7/50 - Classification error: 0.04097 Loss: 0.14509 Time 4706ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-26"&gt;&lt;/a&gt;Epoch   8/50 - Classification error: 0.03938 Loss: 0.13397 Time 4694ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-27"&gt;&lt;/a&gt;Epoch   9/50 - Classification error: 0.03525 Loss: 0.12284 Time 4706ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-28"&gt;&lt;/a&gt;Epoch  10/50 - Classification error: 0.03927 Loss: 0.13770 Time 4694ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-29"&gt;&lt;/a&gt;Epoch  11/50 - Classification error: 0.03315 Loss: 0.11315 Time 4711ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-30"&gt;&lt;/a&gt;Epoch  12/50 - Classification error: 0.05037 Loss: 0.17123 Time 4711ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-31"&gt;&lt;/a&gt;Epoch  13/50 - Classification error: 0.02927 Loss: 0.10042 Time 4780ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-32"&gt;&lt;/a&gt;Epoch  14/50 - Classification error: 0.03322 Loss: 0.11027 Time 4746ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-33"&gt;&lt;/a&gt;Epoch  15/50 - Classification error: 0.03397 Loss: 0.11585 Time 4684ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-34"&gt;&lt;/a&gt;Epoch  16/50 - Classification error: 0.02938 Loss: 0.09984 Time 4708ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-35"&gt;&lt;/a&gt;Epoch  17/50 - Classification error: 0.03262 Loss: 0.11152 Time 4690ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-36"&gt;&lt;/a&gt;Epoch  18/50 - Classification error: 0.02872 Loss: 0.09753 Time 4672ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-37"&gt;&lt;/a&gt;Epoch  19/50 - Classification error: 0.02548 Loss: 0.08605 Time 4691ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-38"&gt;&lt;/a&gt;Epoch  20/50 - Classification error: 0.02245 Loss: 0.07797 Time 4693ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-39"&gt;&lt;/a&gt;Epoch  21/50 - Classification error: 0.02705 Loss: 0.08984 Time 4684ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-40"&gt;&lt;/a&gt;Epoch  22/50 - Classification error: 0.02422 Loss: 0.08164 Time 4688ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-41"&gt;&lt;/a&gt;Epoch  23/50 - Classification error: 0.02645 Loss: 0.08804 Time 4690ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-42"&gt;&lt;/a&gt;Epoch  24/50 - Classification error: 0.02927 Loss: 0.09739 Time 4715ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-43"&gt;&lt;/a&gt;Epoch  25/50 - Classification error: 0.02578 Loss: 0.08669 Time 4702ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-44"&gt;&lt;/a&gt;Epoch  26/50 - Classification error: 0.02785 Loss: 0.09368 Time 4700ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-45"&gt;&lt;/a&gt;Epoch  27/50 - Classification error: 0.02472 Loss: 0.08237 Time 4695ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-46"&gt;&lt;/a&gt;Epoch  28/50 - Classification error: 0.02125 Loss: 0.07324 Time 4690ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-47"&gt;&lt;/a&gt;Epoch  29/50 - Classification error: 0.01977 Loss: 0.06635 Time 4688ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-48"&gt;&lt;/a&gt;Epoch  30/50 - Classification error: 0.03635 Loss: 0.12140 Time 4689ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-49"&gt;&lt;/a&gt;Epoch  31/50 - Classification error: 0.02862 Loss: 0.09704 Time 4698ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-50"&gt;&lt;/a&gt;Epoch  32/50 - Classification error: 0.02463 Loss: 0.08158 Time 4686ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-51"&gt;&lt;/a&gt;Epoch  33/50 - Classification error: 0.02565 Loss: 0.08771 Time 4697ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-52"&gt;&lt;/a&gt;Epoch  34/50 - Classification error: 0.02278 Loss: 0.07634 Time 4718ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-53"&gt;&lt;/a&gt;Epoch  35/50 - Classification error: 0.02105 Loss: 0.07075 Time 4697ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-54"&gt;&lt;/a&gt;Epoch  36/50 - Classification error: 0.02770 Loss: 0.09358 Time 4711ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-55"&gt;&lt;/a&gt;Epoch  37/50 - Classification error: 0.02627 Loss: 0.08805 Time 4742ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-56"&gt;&lt;/a&gt;Epoch  38/50 - Classification error: 0.02282 Loss: 0.07712 Time 4708ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-57"&gt;&lt;/a&gt;Epoch  39/50 - Classification error: 0.02305 Loss: 0.07661 Time 4697ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-58"&gt;&lt;/a&gt;Epoch  40/50 - Classification error: 0.02243 Loss: 0.07773 Time 4700ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-59"&gt;&lt;/a&gt;Epoch  41/50 - Classification error: 0.02467 Loss: 0.08234 Time 4712ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-60"&gt;&lt;/a&gt;Epoch  42/50 - Classification error: 0.01808 Loss: 0.06186 Time 4691ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-61"&gt;&lt;/a&gt;Epoch  43/50 - Classification error: 0.02388 Loss: 0.07917 Time 4681ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-62"&gt;&lt;/a&gt;Epoch  44/50 - Classification error: 0.02162 Loss: 0.07508 Time 4699ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-63"&gt;&lt;/a&gt;Epoch  45/50 - Classification error: 0.01877 Loss: 0.06289 Time 4735ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-64"&gt;&lt;/a&gt;Epoch  46/50 - Classification error: 0.02263 Loss: 0.07969 Time 4764ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-65"&gt;&lt;/a&gt;Epoch  47/50 - Classification error: 0.02100 Loss: 0.07207 Time 4684ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-66"&gt;&lt;/a&gt;Epoch  48/50 - Classification error: 0.02425 Loss: 0.08076 Time 4752ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-67"&gt;&lt;/a&gt;Epoch  49/50 - Classification error: 0.02328 Loss: 0.07803 Time 4718ms
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-68"&gt;&lt;/a&gt;Restore the best (error) weights from epoch 42
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-69"&gt;&lt;/a&gt;Training took 235s
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-70"&gt;&lt;/a&gt;Evaluation Results
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-71"&gt;&lt;/a&gt;   error: 0.03000
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-72"&gt;&lt;/a&gt;    loss: 0.12260
&lt;a name="rest_code_c49fc5f5eebe4e90bc3c32b5a2901c9f-73"&gt;&lt;/a&gt;evaluation took 245ms
&lt;/pre&gt;&lt;p&gt;Nothing fancy, but this example is not necessarily optimized.&lt;/p&gt;
&lt;p&gt;All this support is now in the &lt;strong&gt;master&lt;/strong&gt; branch of the DLL project if you want
to check it out. You can also check out the example online:
&lt;a class="reference external" href="https://github.com/wichtounet/dll/blob/master/examples/src/mnist_rnn.cpp"&gt;mnist_rnn.cpp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can access the project &lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;on Github&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>Deep Learning</category><category>dll</category><category>Machine Learning</category><category>projects</category><category>rnn</category><guid>https://baptiste-wicht.com/posts/2017/11/initial-support-for-recurrent-neural-network-rnn-in-dll.html</guid><pubDate>Sun, 12 Nov 2017 14:22:44 GMT</pubDate></item><item><title>DLL New Features: Embeddings and Merge layers</title><link>https://baptiste-wicht.com/posts/2017/10/dll-new-features-embeddings-and-merge-layers.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've just finished integrating new features into DLL, my deep learning library.
I've added support for an embeddings layer, a group layer and a merge layer.
This is not yet released, but available in the master branch.&lt;/p&gt;
&lt;p&gt;Embeddings are used more and more these days to learn dense representation of
characters or word. An embedding layer in a neural network transform labels into
a vector. It's generally used as the first layer of the network. The embedding
are learned as part of the network.&lt;/p&gt;
&lt;p&gt;The merge layer allows to create branches in the network. The input is passed to
each sub layer and then the output of each layer is concatenated to form the
output of the merged layers. This can be very useful to use different
convolutional filter sizes.&lt;/p&gt;
&lt;p&gt;The group layer is a simple utility to group layers together. This is mostly to
use with merge layers to form several branches.&lt;/p&gt;
&lt;p&gt;I've put together a new example to use these features on text classification.
The dataset is totally synthetic for now, but this can easily be reproduced with
a normal text classification dataset. This kind of model is called a Character
Convolutional Neural Network.&lt;/p&gt;
&lt;p&gt;Here is the code for example:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// The length of the embedding vector&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;    &lt;span class="c1"&gt;// The word (or sequence) length&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;embedding_network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_network_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;network_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-6"&gt;&lt;/a&gt;        &lt;span class="c1"&gt;// The embedding layer&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;embedding_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-9"&gt;&lt;/a&gt;        &lt;span class="c1"&gt;// The convolutional layers&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-10"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;merge_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-11"&gt;&lt;/a&gt;            &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-12"&gt;&lt;/a&gt;            &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;group_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-13"&gt;&lt;/a&gt;                  &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-14"&gt;&lt;/a&gt;                &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_2d_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-15"&gt;&lt;/a&gt;            &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-16"&gt;&lt;/a&gt;            &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;group_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-17"&gt;&lt;/a&gt;                  &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-18"&gt;&lt;/a&gt;                &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_2d_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-19"&gt;&lt;/a&gt;            &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-20"&gt;&lt;/a&gt;            &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;group_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-21"&gt;&lt;/a&gt;                  &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-22"&gt;&lt;/a&gt;                &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_2d_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-23"&gt;&lt;/a&gt;            &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-24"&gt;&lt;/a&gt;        &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-25"&gt;&lt;/a&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-26"&gt;&lt;/a&gt;        &lt;span class="c1"&gt;// The final softmax layer&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-27"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;48&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-28"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-29"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;NADAM&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;     &lt;span class="c1"&gt;// Nesterov Adam (NADAM)&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-30"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;                        &lt;span class="c1"&gt;// The mini-batch size&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-31"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;                               &lt;span class="c1"&gt;// Shuffle before each epoch&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-32"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-33"&gt;&lt;/a&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-34"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;embedding_network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-35"&gt;&lt;/a&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-36"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Display the network and dataset&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-37"&gt;&lt;/a&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-38"&gt;&lt;/a&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-39"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Train the network for performance sake&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-40"&gt;&lt;/a&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-41"&gt;&lt;/a&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-42"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Test the network on train set&lt;/span&gt;
&lt;a name="rest_code_1e9b403327ae41bf965ca5eeecda3b39-43"&gt;&lt;/a&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The network starts with an embedding layer. The embedding is then passed to
three convolutional layers with different filter sizes, each followed by
a pooling layer. The outputs of the three layers are merged at the end of the
merge layer. Finally, a softmax layer is used for classification.&lt;/p&gt;
&lt;p&gt;This kind of model can be very powerful and is used regularly. These new
features make for a much larger variety of models that can be build with the DLL
library.&lt;/p&gt;
&lt;p&gt;The full code with the dataset generation can be found online:
&lt;a class="reference external" href="https://github.com/wichtounet/dll/blob/master/examples/src/char_cnn.cpp"&gt;char_cnn.cpp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The next feature I want to focus on is recurrent neural networks. I'll probably
try a single RNN layer first and then upgrade to multi-layers and LSTM and maybe
GRU.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>Deep Learning</category><category>dll</category><category>Machine Learning</category><category>projects</category><guid>https://baptiste-wicht.com/posts/2017/10/dll-new-features-embeddings-and-merge-layers.html</guid><pubDate>Tue, 17 Oct 2017 17:50:40 GMT</pubDate></item><item><title>I successfully defended my Ph.D.</title><link>https://baptiste-wicht.com/posts/2017/10/i-successfully-defended-my-phd.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I'm happy to announce that I've successfully defended my thesis "Deep Learning
Features for Image Processing". After four years, I've defended it officially in
front of the thesis committed last Friday and then again two days ago I've
successfully publicly defended in front of my friends, family and colleagues.&lt;/p&gt;
&lt;p&gt;I'm now a "Doctor of Philosophy in Computer Science :)&lt;/p&gt;
&lt;p&gt;I will update my thesis with the last comments in November and send the final
version to the university. At which point, I'll publish it on this website as
well.&lt;/p&gt;&lt;/div&gt;</description><category>Machine Learning</category><category>Personal</category><category>thesis</category><guid>https://baptiste-wicht.com/posts/2017/10/i-successfully-defended-my-phd.html</guid><pubDate>Sun, 15 Oct 2017 15:16:29 GMT</pubDate></item><item><title>Budgetwarrior: Track assets and portfolio, savings rates and auto-completion</title><link>https://baptiste-wicht.com/posts/2017/10/budgetwarrior-track-assets-portfolio-savings-rates-auto-completion.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;This last month, I've been reading quite a few blogs about personal finance and
I've decided to integrate more features into budgetwarrior. This post is about
three new features that I've integrated. It's not yet a new release, so if you
want to test this version, you'll have to compile it from the &lt;em&gt;master&lt;/em&gt; branch on
Git.&lt;/p&gt;
&lt;p&gt;As it was last time, the values on my screenshots have all been randomized.&lt;/p&gt;
&lt;p&gt;If you have several assets with different distributions, I believe it is a great
value to have them all shown at the same time. Especially if you want to change
the distribution of your portfolio or if you plan big changes in it.&lt;/p&gt;
&lt;div class="section" id="track-assets"&gt;
&lt;h2&gt;Track assets&lt;/h2&gt;
&lt;p&gt;The first feature I've added is a feature to precisely track each of your assets
independently. And you can also track the allocation of your portfolio in terms
of stocks, bonds and cash. The tool also lets you set the desired distribution
of your assets and will compute the difference that you should make in order to
comply to your desired distribution.&lt;/p&gt;
&lt;p&gt;First, you need to define all your asset classes (your accounts, funds, and
stocks, ...) and their distribution with &lt;code&gt;budget asset add&lt;/code&gt;. It also
supports to set a currency. The default currency is now CHF, but you can set it
in the configuration file, for instance &lt;code&gt;default_currency=USD&lt;/code&gt;. You can
see your assets using &lt;code&gt;budget asset&lt;/code&gt;:&lt;/p&gt;
&lt;img alt="View of your assets" src="https://baptiste-wicht.com/images/budgetwarrior_assets.png"&gt;
&lt;p&gt;You can then set the value of your assets using &lt;code&gt;budget asset value add&lt;/code&gt;.
The system will save all the values of your assets. For now, only the last value
is used in the application to display. In the future, I plan to add new reports
for evolution of the portfolio over time. You can see your current net worth
with the &lt;code&gt;budget asset value&lt;/code&gt;:&lt;/p&gt;
&lt;img alt="View of your portfolio" src="https://baptiste-wicht.com/images/budgetwarrior_asset_values.png"&gt;
&lt;p&gt;The different currencies will all be converted to the default currency.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="savings-rate"&gt;
&lt;h2&gt;Savings rate&lt;/h2&gt;
&lt;p&gt;The second change I did is to compute the savings rate of each month and year.
The savings rate is simply the portion of your income that you are able to save
each month. The savings rate for a year is simple the average of the savings
rate of each month.&lt;/p&gt;
&lt;p&gt;The savings rate of the month can be seen with &lt;code&gt;budget overview month&lt;/code&gt;:&lt;/p&gt;
&lt;img alt="Savings rate of the month" src="https://baptiste-wicht.com/images/budgetwarrior_savings_rate.png"&gt;
&lt;p&gt;The saving rates of each month can also be seen in the overview of the year with
&lt;code&gt;budget overview year&lt;/code&gt;:&lt;/p&gt;
&lt;img alt="Savings rate of the year" src="https://baptiste-wicht.com/images/budgetwarrior_savings_rate_year.png"&gt;
&lt;p&gt;This shows the savings rate of each month, the average of the year and the
average of the current year up to the current month.&lt;/p&gt;
&lt;p&gt;The savings rate is a very important metric of your budget. In my case, it's
currently way too low and made me realize I really need to save more. Any
savings rate below 10% is too low. There are no rule as too much it should be,
but I'd like to augment mine to at least 20% next year.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="auto-completion"&gt;
&lt;h2&gt;Auto-completion&lt;/h2&gt;
&lt;p&gt;The last feature is mostly some quality-of-life improvement. Some of the inputs
in the console can now be completed. It's not really auto-completion per se, but
you can cycle through the list of possible values using the UP and DOWN.&lt;/p&gt;
&lt;p&gt;This makes it much easier to set some values such as asset names (in
&lt;code&gt;budget asset value add&lt;/code&gt; for instance), account names and objective types
and sources. I'm trying to make the input of values easier.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I don't know exactly what else will be integrated in this feature, but I may
already improve some visualization for asset values. If I learn something new
about personal finance that I may integrate in the tool, I'll do it as well.&lt;/p&gt;
&lt;p&gt;If you are interested by the sources or want to install this version,
you can download them on Github:
&lt;a class="reference external" href="https://github.com/wichtounet/budgetwarrior"&gt;budgetwarrior&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The new features are in the &lt;em&gt;master&lt;/em&gt; branch.&lt;/p&gt;
&lt;p&gt;If you have a suggestion for a new features or you found a bug, please post an
issue on Github, I'd be glad to help you.&lt;/p&gt;
&lt;p&gt;If you have any comment, don't hesitate to contact me, either by letting a
comment on this post or by email.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>budgetwarrior</category><category>C++</category><category>Linux</category><guid>https://baptiste-wicht.com/posts/2017/10/budgetwarrior-track-assets-portfolio-savings-rates-auto-completion.html</guid><pubDate>Thu, 12 Oct 2017 17:40:14 GMT</pubDate></item><item><title>Deep Learning Library 1.0 - Fast Neural Network Library</title><link>https://baptiste-wicht.com/posts/2017/10/deep-learning-library-10-fast-neural-network-library.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;img alt="DLL Logo" class="align-center" src="https://baptiste-wicht.com/images/dll_logo.png"&gt;
&lt;p&gt;I'm very happy to announce the release of the first version of Deep Learning
Library (DLL) 1.0. DLL is a neural network library with a focus on speed and
ease of use.&lt;/p&gt;
&lt;p&gt;I started working on this library about 4 years ago for my Ph.D. thesis.
I needed a good library to train and use Restricted Boltzmann Machines (RBMs)
and at this time there was no good support for it. Therefore, I decided to write
my own. It now has very complete support for the RBM and the Convolutional RBM
(CRBM) models. Stacks of RBMs (or Deep Belief Networks (DBNs)) can be pretrained
using Contrastive Divergence and then either fine-tuned with mini-batch gradient
descent or Conjugate Gradient or used as a feature extractor. Over the years,
the library has been extended to handle Artificial Neural Networks (ANNs) and
Convolutional Neural Networks (CNNs). The network is also able to train regular
auto-encoders. Several advanced layers such as Dropout or Batch Normalization
are also available as well as adaptive learning rates techniques such as
Adadelta and Adam. The library also has integrated support for a few datasets:
MNIST, CIFAR-10 and ImageNet.&lt;/p&gt;
&lt;p&gt;This library can be used using a C++ interface. The library is fully
header-only. It requires a C++14 compiler, which means a minimum of clang 3.9 or
GCC 6.3.&lt;/p&gt;
&lt;p&gt;In this post, I'm going to present a few examples on using the library and give
some information about the performance of the library and the roadmap for the
project.&lt;/p&gt;
&lt;p class="more"&gt;&lt;a href="https://baptiste-wicht.com/posts/2017/10/deep-learning-library-10-fast-neural-network-library.html"&gt;Read more&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>dll</category><category>etl</category><category>GPU</category><category>Machine Learning</category><category>Performances</category><category>Releases</category><guid>https://baptiste-wicht.com/posts/2017/10/deep-learning-library-10-fast-neural-network-library.html</guid><pubDate>Sat, 07 Oct 2017 13:42:16 GMT</pubDate></item><item><title>Expression Templates Library (ETL) 1.2 - Complete GPU support</title><link>https://baptiste-wicht.com/posts/2017/10/expression-templates-library-etl-1-2-complete-gpu-support.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;img alt="ETL Logo" class="align-center" src="https://baptiste-wicht.com/images/logo.png"&gt;
&lt;p&gt;I'm happy to announce the version 1.2 of my Expression Templates Library (ETL):
ETL 1.2, two months after &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/08/expression-templates-library-etl-11.html"&gt;I released the version 1.1&lt;/a&gt;.
This version features much better GPU Support, a few new features and a lot of
changes in the internal code.&lt;/p&gt;
&lt;div class="section" id="gpu-support"&gt;
&lt;h2&gt;GPU Support&lt;/h2&gt;
&lt;p&gt;Before, only algorithms such as 4D convolution or matrix-matrix multiplication
were computed in the GPU and lots of operations were causing copies between CPU
and GPU version. Now, the support for basic operations has also been completed
and therefore, expressions like this:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_648b30670a9b4d62a2cfd34ec1c43f51-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Can be computed entirely on GPU.&lt;/p&gt;
&lt;p&gt;Each matrix and vector containers have a secondary GPU memory space.  During the
execution, the status of both memory spaces is being managed and when necessary,
copies are made between two spaces. In the best case, there should only be
initial copies to the GPU and then everything should be done on the GPU. I've
also considered using Unified Memory in place of this system, but this is
a problem for fast matrix and I'd rather not have two different systems.&lt;/p&gt;
&lt;p&gt;If you have an expression such as &lt;code&gt;c = a + b * 2&lt;/code&gt;, it can be entirely computed
on GPU, however, it will be computed in two GPU operations such as:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_c2f2a145ff214fc4b3ce87589c60d879-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;a name="rest_code_c2f2a145ff214fc4b3ce87589c60d879-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;t1&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is not perfect in terms of performance but this will be done without any
copies between CPU and GPU memory. I plan to improve this system with a bit more
complex operations to avoid too many GPU operations, but there will always be
more operations than in CPU where this can easily be done in one go.&lt;/p&gt;
&lt;p&gt;There are a few expressions that are not computable on the GPU, such as random
generations. A few transformations are also not fully compatible with GPU.
Moreover, if you access an element with operators &lt;code&gt;[]&lt;/code&gt; or &lt;code&gt;()&lt;/code&gt;, this
will invalidate the GPU memory and force an update to the CPU memory.&lt;/p&gt;
&lt;p&gt;GPU operations are not implemented directly in ETL, there are coming from
various libraries. ETL is using NVIDIA CUDNN, CUFFT and CUDNN for most
algorithms. Moreover, for other operations, I've implemented a libraries with
simple GPU operations: ETL-GPU-BLAS (EGBLAS). You can have a look at
&lt;a class="reference external" href="https://github.com/wichtounet/etl-gpu-blas"&gt;egblas&lt;/a&gt; if you are interested.&lt;/p&gt;
&lt;p&gt;My Deep Learning Library (DLL) project is based on ETL and its performances are
mostly dependent on ETL's performances. Now that ETL fully supports GPU, the
GPU performance of DLL is much improved. You may remember a few weeks ago
I posted &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/08/dll-blazing-fast-neural-network-library.html"&gt;very high CPU performance of DLL&lt;/a&gt;.
Now, I've run again the tests to see the GPU performance with DLL. Here is the
performance for training a small CNN on the MNIST data set:&lt;/p&gt;
&lt;img alt="Performances for training a Convolutional Neural Network on MNIST" class="align-center" src="https://baptiste-wicht.com/images/etl_12_dll_gpu_mnist.png"&gt;
&lt;p&gt;As you can see, the performances on GPU are now excellent. DLL's performances
are on par with Tensorflow and Keras!&lt;/p&gt;
&lt;p&gt;The next results are for training a much larger CNN on ImageNet, with the time
necessary to train a single batch:&lt;/p&gt;
&lt;img alt="Performances for training a Convolutional Neural Network on Imagenet" class="align-center" src="https://baptiste-wicht.com/images/etl_12_dll_gpu_imagenet.png"&gt;
&lt;p&gt;Again, using the new version of ETL inside DLL has led to excellent performance.
The framework is again on par with TensorFlow and Keras and faster than all the
other frameworks. The large difference between DLL and Tensorflow and Keras is
due to the inefficiency of reading the dataset in the two frameworks, so the
performance of the three framework themselves are about the same.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="other-changes"&gt;
&lt;h2&gt;Other Changes&lt;/h2&gt;
&lt;p&gt;The library also has a few other new features. Logarithms of base 2 and base 10
are now supported in complement to the base e that was already available before.
Categorical Cross Entropy (CCE) computation is also available now, the CCE loss
and error can be computed for one or many samples. Convolutions have also been
improved in that you can use mixed types in both the image and the kernel and
different storage order as well. Nevertheless, the most optimized version
remains the version with the same storage order and the same data type.&lt;/p&gt;
&lt;p&gt;I've also made a major change in the way implementations are selected for each
operation. The tests and the benchmark are using a system to force the selection
of an algorithm. This system is now disabled by default. This makes the
compilation much faster by default. Since it's not necessary in most cases, this
will help regular use cases of the library by compiling much faster.&lt;/p&gt;
&lt;p&gt;Overall, the support for complex numbers has been improved in ETL. There are
more routines that are supported and &lt;code&gt;etl::complex&lt;/code&gt; is better supported
throughout the code. I'll still work on this in the future to make it totally
complete.&lt;/p&gt;
&lt;p&gt;The internal code also has a few new changes. First, all traits have been
rewritten to use variable templates instead of struct traits. This makes the
code much nicer in my opinion. Moreover, I've started experimenting with C++17
&lt;code&gt;if constexpr&lt;/code&gt;. Most of the if conditions that can be transformed to if
constexpr have been annotated with comments that I can quickly enable or disable
so that I can test the impact of C++17, especially on compilation time.&lt;/p&gt;
&lt;p&gt;Finally, a few bugs have been fixed. ETL is now working better with parallel
BLAS library. There should not be issues with double parallelization in ETL and
BLAS. There was a slight bug in the Column-Major matrix-matrix multiplication
kernel. Binary operations with different types in the left and right hand sides
was also problematic with vectorization. The last bug was about GPU status in
case ETL containers were moved.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="what-s-next"&gt;
&lt;h2&gt;What's next ?&lt;/h2&gt;
&lt;p&gt;I don't yet know exactly on which features I'm going to focus for the next
version of ETL. I plan to focus a bit more in the near future on Deep Learning
Library (DLL) for which I should release the version 1.0 soon. I also plan to
start support for Recurrent Neural Networks on it, so that will take me quite
some time.&lt;/p&gt;
&lt;p&gt;Nevertheless, I'm still planning to consider the switch to C++17, since it is
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/09/how-i-made-deep-learning-library-38-faster-to-compile-optimization-and-cpp17-if-constexpr.html"&gt;a bit faster to compile ETL with if constexpr&lt;/a&gt;. The next version of ETL will also probably have GPU-support for
integers, at least in the cases that depend on the etl-gpu-blas library, which
is the standard operators. I also plan to improve the support for complex
numbers, especially in terms of performance and tests. Hopefully, I will have also time (and motivation)
to start working on  the sparse capabilities of ETL. It really needs much more
unit tests and the performance should be improved as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="download-etl"&gt;
&lt;h2&gt;Download ETL&lt;/h2&gt;
&lt;p&gt;You can download ETL &lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;on Github&lt;/a&gt;. If you
only interested in the 1.2 version, you can look at the
&lt;a class="reference external" href="https://github.com/wichtounet/etl/releases"&gt;Releases pages&lt;/a&gt; or clone the tag
1.2. There are several branches:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;master&lt;/em&gt; Is the eternal development branch, may not always be stable&lt;/li&gt;
&lt;li&gt;&lt;em&gt;stable&lt;/em&gt; Is a branch always pointing to the last tag, no development here&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the future release, there always will tags pointing to the corresponding
commits. You can also have access to previous releases on Github or via the
release tags.&lt;/p&gt;
&lt;p&gt;The documentation is still a bit sparse. There are a few examples and the Wiki,
but there still is work to be done. If you have questions on how to use or
configure the library, please don't hesitate.&lt;/p&gt;
&lt;p&gt;Don't hesitate to comment this post if you have any comment on this library or
any question. You can also open an Issue on Github if you have a problem using
this library or propose a Pull Request if you have any contribution you'd like
to make to the library.&lt;/p&gt;
&lt;p&gt;Hope this may be useful to some of you :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>C++17</category><category>Compilers</category><category>dll</category><category>etl</category><category>GPU</category><category>Performance</category><category>projects</category><category>Release</category><guid>https://baptiste-wicht.com/posts/2017/10/expression-templates-library-etl-1-2-complete-gpu-support.html</guid><pubDate>Mon, 02 Oct 2017 08:49:02 GMT</pubDate></item><item><title>C++11 Performance tip: Update on when to use std::pow ?</title><link>https://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-update-when-to-use-std-pow.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;A few days ago, I published a post comparing the
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-when-to-use-std-pow.html"&gt;performance of std::pow against direct multiplications&lt;/a&gt;. When not compiling with -ffast-math, direct multiplication was significantly faster than &lt;code&gt;std::pow&lt;/code&gt;, around two orders of magnitude faster when comparing &lt;code&gt;x * x * x&lt;/code&gt; and &lt;code&gt;code:std::pow(x, 3)&lt;/code&gt;.
One comment that I've got was to test for which &lt;code&gt;n&lt;/code&gt; is
&lt;code&gt;code:std::pow(x, n)&lt;/code&gt; becoming faster than multiplying in a loop. Since
std::pow is using a special algorithm to perform the computation rather than be
simply loop-based multiplications, there may be a point after which it's more interesting to use the
algorithm rather than a loop. So I decided to do the tests. You can also find
the result in the original article, which I've updated.&lt;/p&gt;
&lt;p&gt;First, our pow function:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_556b5342f0ca4cbc9718057599efee9a-1"&gt;&lt;/a&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="nf"&gt;my_pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_556b5342f0ca4cbc9718057599efee9a-2"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_556b5342f0ca4cbc9718057599efee9a-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_556b5342f0ca4cbc9718057599efee9a-4"&gt;&lt;/a&gt;    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_556b5342f0ca4cbc9718057599efee9a-5"&gt;&lt;/a&gt;        &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_556b5342f0ca4cbc9718057599efee9a-6"&gt;&lt;/a&gt;        &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_556b5342f0ca4cbc9718057599efee9a-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_556b5342f0ca4cbc9718057599efee9a-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_556b5342f0ca4cbc9718057599efee9a-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_556b5342f0ca4cbc9718057599efee9a-10"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;And now, let's see the performance. I've compiled my benchmark with GCC 4.9.3
and running on my old Sandy Bridge processor. Here are the results for 1000
calls to each functions:&lt;/p&gt;
&lt;div id="graph_std_pow_my_pow_1" style="width: 700px; height: 400px;"&gt;&lt;/div&gt;&lt;p&gt;We can see that between &lt;code&gt;n=100&lt;/code&gt; and &lt;code&gt;n=110&lt;/code&gt;, &lt;code&gt;std::pow(x, n)&lt;/code&gt;
starts to be faster than &lt;code&gt;my_pow(x, n)&lt;/code&gt;. At this point, you should only
use &lt;code&gt;std::pow(x, n)&lt;/code&gt;.  Interestingly too, the time for &lt;code&gt;std::pow(x,
n)&lt;/code&gt; is decreasing. Let's see how is the performance with higher range of
&lt;code&gt;n&lt;/code&gt;:&lt;/p&gt;
&lt;div id="graph_std_pow_my_pow_2" style="width: 700px; height: 400px;"&gt;&lt;/div&gt;&lt;p&gt;We can see that the pow function time still remains stable while our loop-based
pow function still increases linearly. At &lt;code&gt;n=1000&lt;/code&gt;, &lt;code&gt;std::pow&lt;/code&gt; is
one order of magnitude faster than &lt;code&gt;my_pow&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Overall, if you do not care much about extreme accuracy, you may consider using
you own pow function for small-ish (integer) &lt;code&gt;n&lt;/code&gt; values. After
&lt;code&gt;n=100&lt;/code&gt;, it becomes more interesting to use &lt;code&gt;std::pow&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you want more results on the subject, you take a look at the
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-when-to-use-std-pow.html"&gt;original article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are interested in the code of this benchmark, it's available online:
&lt;a class="reference external" href="https://github.com/wichtounet/articles/blob/master/src/bench_pow_my_pow.cpp"&gt;bench_pow_my_pow.cpp&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript" src="https://www.google.com/jsapi"&gt;&lt;/script&gt;
&lt;script type="text/javascript"&gt;google.load('visualization', '1.0', {'packages':['corechart']});&lt;/script&gt;
&lt;script type="text/javascript"&gt;
function draw_graph_pow_my_pow_1(){
var data = google.visualization.arrayToDataTable([
['n', 'my_pow(x, n)', 'std::pow(x, n)'],
['10',   2,     127],
['20',   17,     123],
['30',   26,     127],
['40',   36,     123],
['50',   43,     123],
['60',   55,     123],
['70',   72,     123],
['80',   85,     123],
['90',   102,    126],
['100',  114,    125],
['110',  131,    115],
['120',  144,    111],
['130',  165,    111],
['140',  173,    108],
['150',  189,    107],
['160',  202,    112],
['170',  219,    106],
['180',  232,    105],
['190',  249,    108],
['200',  261,    105],
]);
var graph = new google.visualization.LineChart(document.getElementById('graph_std_pow_my_pow_1'));
var options = {curveType: "function",title: "std::pow(x, 2) (float)",animation: {duration:1200, easing:"in"},width: 700, height: 400,hAxis: {title:"Number of elements", slantedText:true},vAxis: {viewWindow: {min:0}, title:"us"}};
graph.draw(data, options);
}
function draw_graph_pow_my_pow_2(){
var data = google.visualization.arrayToDataTable([
['n', 'my_pow(x, n)', 'std::pow(x, n)'],
['100',  114,    125],
['200',  261,    105],
['300',  410,    104],
['400',  558,    104],
['500',  708,    104],
['600',  855,    104],
['700',  1002,   104],
['800',  1148,   104],
['900',  1300,   104],
['1000', 1442,   104],
]);
var graph = new google.visualization.LineChart(document.getElementById('graph_std_pow_my_pow_2'));
var options = {curveType: "function",title: "std::pow(x, 2) (float)",animation: {duration:1200, easing:"in"},width: 700, height: 400,hAxis: {title:"Number of elements", slantedText:true},vAxis: {viewWindow: {min:0}, title:"us"}};
graph.draw(data, options);
}
function draw_all(){
draw_graph_pow_my_pow_1();
draw_graph_pow_my_pow_2();
}
google.setOnLoadCallback(draw_all);
&lt;/script&gt;&lt;/div&gt;</description><category>Benchmark</category><category>C++</category><category>C++11</category><category>Performances</category><category>Tip</category><guid>https://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-update-when-to-use-std-pow.html</guid><pubDate>Fri, 22 Sep 2017 09:21:07 GMT</pubDate></item><item><title>How I made my Deep Learning Library 38% faster to compile (Optimization and C++17 if constexpr)</title><link>https://baptiste-wicht.com/posts/2017/09/how-i-made-deep-learning-library-38-faster-to-compile-optimization-and-cpp17-if-constexpr.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;My Deep Learning Library (DLL) project is a C++ library for training and using
artificial neural networks (you can take a look at
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html"&gt;this post about DLL&lt;/a&gt;
if you want more information).&lt;/p&gt;
&lt;p&gt;While I made a lot of effort to make it as fast as possible to train and run
neural networks, the compilation time has been steadily going up and is becoming
quite annoying. This library is heavily templated and all the matrix operations
are done using my Expression Templates Library (ETL) which is more than
template-heavy itself.&lt;/p&gt;
&lt;p&gt;In this post, I'll present two techniques with which I've been able to reduce
the total compilation of the DLL unit tests by up to 38%.&lt;/p&gt;
&lt;p class="more"&gt;&lt;a href="https://baptiste-wicht.com/posts/2017/09/how-i-made-deep-learning-library-38-faster-to-compile-optimization-and-cpp17-if-constexpr.html"&gt;Read more&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>C++17</category><category>clang</category><category>Compilers</category><category>dll</category><category>etl</category><category>gcc</category><category>Performance</category><category>projects</category><guid>https://baptiste-wicht.com/posts/2017/09/how-i-made-deep-learning-library-38-faster-to-compile-optimization-and-cpp17-if-constexpr.html</guid><pubDate>Thu, 21 Sep 2017 17:44:34 GMT</pubDate></item><item><title>C++11 Performance tip: When to use std::pow ?</title><link>https://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-when-to-use-std-pow.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Update: I've added a new section for larger values of &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Recently, I've been wondering about the performance of &lt;code&gt;std::pow(x, n)&lt;/code&gt;.
I'm talking here about the case when &lt;code&gt;n&lt;/code&gt; is an integer. In the case when
&lt;code&gt;n&lt;/code&gt; is not an integer, I believe, you should always use &lt;code&gt;std::pow&lt;/code&gt;
or use another specialized library.&lt;/p&gt;
&lt;p&gt;In case when n is an integer, you can actually replace it with the direct
equivalent (for instance &lt;code&gt;std::pow(x, 3) = x * x x&lt;/code&gt;). If n is very large,
you'd rather write a loop of course ;) In practice, we generally use powers of
two and three much more often than power of 29, although that could happen. Of
course, it especially make sense to wonder about this if the pow is used inside
a loop. If you only use it once outside a loop, that won't be any difference on
the overall performance.&lt;/p&gt;
&lt;p&gt;Since I'm mostly interested in single precision performance (neural networks are
only about single precision), the first benchmarks will be using &lt;code&gt;float&lt;/code&gt;.&lt;/p&gt;
&lt;p class="more"&gt;&lt;a href="https://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-when-to-use-std-pow.html"&gt;Read more&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Benchmark</category><category>C++</category><category>C++11</category><category>Performances</category><category>Tip</category><guid>https://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-when-to-use-std-pow.html</guid><pubDate>Mon, 18 Sep 2017 05:50:44 GMT</pubDate></item><item><title>budgetwarrior 0.4.2 - Budget summary and improved fortune reports</title><link>https://baptiste-wicht.com/posts/2017/09/budgetwarrior-042-budget-summary-improved-fortune-reports.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Almost three years ago, &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2014/09/budgetwarrior-041-expense-templates-and-year-projection.html"&gt;I published the version 0.4.1 of budgetwarrior&lt;/a&gt;. Since then, I've been using this tool almost every day to manage my personal budget. This is the only tool I use to keep track of my expenses and earnings and it makes a great tool for me. I recently felt that it was missing a few features and added them and polished a few things as well and release a new version with all the new stuff. This new version is probably nothing fancy, but a nice upgrade of the tool.&lt;/p&gt;
&lt;p&gt;Don't pay too much attention to the values in the images since I've randomized
all the data for the purpose of this post (new feature, by the way :P).&lt;/p&gt;
&lt;div class="section" id="new-summary-view"&gt;
&lt;h2&gt;New summary view&lt;/h2&gt;
&lt;p&gt;I've added a new report with &lt;code&gt;budget summary&lt;/code&gt;:&lt;/p&gt;
&lt;img alt="/images/budgetwarrior_042_summary.png" src="https://baptiste-wicht.com/images/budgetwarrior_042_summary.png"&gt;
&lt;p&gt;This view gives concise information about the current state of your accounts. It
also gives information about your yearly and monthly objectives. Finally, it
also gives information about the last two fortune values that you've set.
I think this make a great kind of dashboard to view most of the information. If
your terminal is large enough, the three parts will be shown side by side.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="improved-fortune-report"&gt;
&lt;h2&gt;Improved fortune report&lt;/h2&gt;
&lt;p&gt;I've made a few improvements to the &lt;code&gt;budget fortune&lt;/code&gt; view:&lt;/p&gt;
&lt;img alt="/images/budgetwarrior_042_fortune.png" src="https://baptiste-wicht.com/images/budgetwarrior_042_fortune.png"&gt;
&lt;p&gt;It now display the time between the different fortune values and it compute the
average savings (or avg losses) per day in each interval and in average from the
beginning of the first value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="various-changes"&gt;
&lt;h2&gt;Various changes&lt;/h2&gt;
&lt;p&gt;The balance does not propagate over the years anymore. This should mainly change
the behaviour of &lt;code&gt;budget overview&lt;/code&gt;. I don't think it was very
smart to propagate it all the time. The balance now starts at zero for each
year. If you want the old system, you can use the multi_year_balance=true option
in the .budgetrc configuration file.&lt;/p&gt;
&lt;p&gt;The recurring expenses do not use an internal configuration value. This does not
change anything for the behaviour, but means that if you sync between different
machines, it will avoid a lot of possible conflicts :)&lt;/p&gt;
&lt;p&gt;Fixed a few bugs with inconsistency between the different views and reports.
Another bug that was fixed is that &lt;code&gt;budget report&lt;/code&gt; was not always displaying the
first month of the year correctly, this is now fixed.&lt;/p&gt;
&lt;p&gt;The graphs display in &lt;code&gt;budget report&lt;/code&gt; are now automatically adapted to width of
your terminal. Finally, the &lt;code&gt;budget overview&lt;/code&gt; command also displays more
information about the comparison with the previous month.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="installation"&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;If you are on Gentoo, you can install it using layman:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
layman -a wichtounet
emerge -a budgetwarrior
&lt;/pre&gt;
&lt;p&gt;If you are on Arch Linux, you can use this &lt;a class="reference external" href="https://github.com/StreakyCobra/aur-budgetwarrior"&gt;AUR repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For other systems, you'll have to install from sources:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
git clone --recursive git://github.com/wichtounet/budgetwarrior.git
cd budgetwarrior
make
sudo make install
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;A brief tutorial is available on Github: &lt;a class="reference external" href="https://github.com/wichtounet/budgetwarrior/wiki/Start-tutorial"&gt;Starting guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are interested by the sources, you can download them on Github:
&lt;a class="reference external" href="https://github.com/wichtounet/budgetwarrior"&gt;budgetwarrior&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you have any suggestion for a new feature or an improvement to the tool or
you found a bug, please post an issue on Github, I'd be glad to help you. You
can post a comment directly on this post :)&lt;/p&gt;
&lt;p&gt;If you have any other comment, don't hesitate to contact me, either by letting a
comment on this post or by email.&lt;/p&gt;
&lt;p&gt;I hope that this application can be useful to some of you command-line adepts :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>budgetwarrior</category><category>C++</category><category>Gentoo</category><category>Linux</category><category>Releases</category><guid>https://baptiste-wicht.com/posts/2017/09/budgetwarrior-042-budget-summary-improved-fortune-reports.html</guid><pubDate>Thu, 14 Sep 2017 18:42:39 GMT</pubDate></item><item><title>C++11 Concurrency Tutorial - Part 5: Futures</title><link>https://baptiste-wicht.com/posts/2017/09/cpp11-concurrency-tutorial-futures.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've been recently reminded that a long time ago I was doing a series of
tutorial on C++11 Concurrency. For some reason, I haven't continued these
tutorials.  The next post in the series was supposed to be about Futures, so I'm
finally going to do it :)&lt;/p&gt;
&lt;p&gt;Here are the links to the current posts of the C++11 Concurrency Tutorial:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/03/cpp11-concurrency-part1-start-threads.html"&gt;Part 1: Start Threads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/03/cp11-concurrency-tutorial-part-2-protect-shared-data.html"&gt;Part 2: Protect Shared Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/04/c11-concurrency-tutorial-advanced-locking-and-condition-variables.html"&gt;Part 3: Advanced Locking and condition variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/07/c11-concurrency-tutorial-part-4-atomic-type.html"&gt;Part 4: Atomic Types&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, we are going to talk about futures, more precisely
&lt;code&gt;std::future&amp;lt;T&amp;gt;&lt;/code&gt;. What is a future ? It's a very nice and simple mechanism
to work with asynchronous tasks. It also has the advantage of decoupling you
from the threads themselves, you can do multithreading without using
&lt;code&gt;std::thread&lt;/code&gt;. The future itself is a structure pointing to a result that
will be computed in the future. How to create a future ? The simplest way is to
use &lt;code&gt;std::async&lt;/code&gt; that will create an asynchronous task and return
a &lt;code&gt;std::future&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let's start with the simplest of the examples:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_422dae7efd7042ba8fa06a5953e9166c-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;thread&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_422dae7efd7042ba8fa06a5953e9166c-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;future&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_422dae7efd7042ba8fa06a5953e9166c-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_422dae7efd7042ba8fa06a5953e9166c-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_422dae7efd7042ba8fa06a5953e9166c-5"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_422dae7efd7042ba8fa06a5953e9166c-6"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;future&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_422dae7efd7042ba8fa06a5953e9166c-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm a thread"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_422dae7efd7042ba8fa06a5953e9166c-8"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_422dae7efd7042ba8fa06a5953e9166c-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_422dae7efd7042ba8fa06a5953e9166c-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_422dae7efd7042ba8fa06a5953e9166c-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_422dae7efd7042ba8fa06a5953e9166c-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_422dae7efd7042ba8fa06a5953e9166c-13"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Nothing really special here. &lt;code&gt;std::async&lt;/code&gt; will execute the task that we
give it (here a lambda) and return a &lt;code&gt;std::future&lt;/code&gt;. Once you use the
&lt;code&gt;get()&lt;/code&gt; function on a future, it will wait until the result is available
and return this result to you once it is. The &lt;code&gt;get()&lt;/code&gt; function is then
blocking. Since the lambda, is a void lambda, the returned future is of type
&lt;code&gt;std::future&amp;lt;void&amp;gt;&lt;/code&gt; and &lt;code&gt;get()&lt;/code&gt; returns &lt;code&gt;void&lt;/code&gt; as well. It is
very important to know that you cannot call &lt;code&gt;get&lt;/code&gt; several times on the
same future. Once the result is consumed, you cannot consume it again! If you
want to use the result several times, you need to store it yourself after you
called &lt;code&gt;get()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let's see with something that returns a value and actually takes some time
before returning it:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;thread&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;future&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-4"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;chrono&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-6"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;future&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-8"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-9"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-10"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-12"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Do something else ?&lt;/span&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-13"&gt;&lt;/a&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-14"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-15"&gt;&lt;/a&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-16"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_37b7281f8696463eabd93ac27cbbdc66-17"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This time, the future will be of the time &lt;code&gt;std::future&amp;lt;int&amp;gt;&lt;/code&gt; and thus
&lt;code&gt;get()&lt;/code&gt; will also return an &lt;code&gt;int&lt;/code&gt;. &lt;code&gt;std::async&lt;/code&gt; will again
launch a task in an asynchronous way and &lt;code&gt;future.get()&lt;/code&gt; will wait for the
answer. What is interesting, is that you can do something else before the call
to future.&lt;/p&gt;
&lt;p&gt;But &lt;code&gt;get()&lt;/code&gt; is not the only interesting function in &lt;code&gt;std::future&lt;/code&gt;.
You also have &lt;code&gt;wait()&lt;/code&gt; which is almost the same as &lt;code&gt;get()&lt;/code&gt; but does
not consume the result. For instance, you can wait for several futures and then
consume their result together. But, more interesting are the
&lt;code&gt;wait_for(duration)&lt;/code&gt; and &lt;code&gt;wait_until(timepoint)&lt;/code&gt; functions. The
first one wait for the result at most the given time and then returns and the
second one wait for the result at most until the given time point. I think that
&lt;code&gt;wait_for&lt;/code&gt; is more useful in practices, so let's discuss it further.
Finally, an interesting function is &lt;code&gt;bool valid()&lt;/code&gt;. When you use
&lt;code&gt;get()&lt;/code&gt; on the future, it will consume the result, making &lt;code&gt;valid()
returns :code:`false&lt;/code&gt;. So, if you intend to check multiple times for a future,
you should use &lt;code&gt;valid()&lt;/code&gt; first.&lt;/p&gt;
&lt;p&gt;One possible scenario would be if you have several asynchronous tasks, which is
a common scenario. You can imagine that you want to process the results as fast
as possible, so you want to ask the futures for their result several times. If
no result is available, maybe you want to do something else. Here is a possible
implementation:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;thread&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;future&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-4"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;chrono&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-6"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-8"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-9"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-10"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;f2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-14"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-15"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-17"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;f3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-19"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;666&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-20"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-21"&gt;&lt;/a&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-22"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;timeout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;milliseconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-23"&gt;&lt;/a&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-24"&gt;&lt;/a&gt;    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()){&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-25"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wait_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future_status&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ready&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-26"&gt;&lt;/a&gt;            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Task1 is done! "&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-27"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-28"&gt;&lt;/a&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-29"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wait_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future_status&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ready&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-30"&gt;&lt;/a&gt;            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Task2 is done! "&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-31"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-32"&gt;&lt;/a&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-33"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wait_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future_status&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ready&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-34"&gt;&lt;/a&gt;            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Task3 is done! "&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-35"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-36"&gt;&lt;/a&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-37"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm doing my own work!"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-38"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-39"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm done with my own work!"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-40"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-41"&gt;&lt;/a&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-42"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Everything is done, let's go back to the tutorial"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-43"&gt;&lt;/a&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-44"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cbc548308d174b5b89d4539dff6f41f8-45"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The three tasks are started asynchronously with &lt;code&gt;std::async&lt;/code&gt; and the
resulting &lt;code&gt;std::future&lt;/code&gt; are stored. Then, as long as one of the tasks is
not complete, we query each three task and try to process its result. If no
result is available, we simply do something else. This example is important to
understand, it covers pretty much every concept of the futures.&lt;/p&gt;
&lt;p&gt;One interesting thing that remains is that you can pass parameters to your task
via &lt;code&gt;std::async&lt;/code&gt;. Indeed, all the extra parameters that you pass to
&lt;code&gt;std::async&lt;/code&gt; will be passed to the task itself. Here is an example of
spawning tasks in a loop with different parameters:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;thread&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;future&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-4"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;chrono&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-5"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;vector&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-7"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-11"&gt;&lt;/a&gt;        &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;emplace_back&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-12"&gt;&lt;/a&gt;            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-13"&gt;&lt;/a&gt;            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-14"&gt;&lt;/a&gt;        &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-15"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-17"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Start querying"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-18"&gt;&lt;/a&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-19"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nl"&gt;future&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-20"&gt;&lt;/a&gt;      &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-21"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-22"&gt;&lt;/a&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-23"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b3a21a2f8c7945ce80862d8345a0c1a8-24"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Pretty practical :) All The created &lt;code&gt;std::future&amp;lt;size_t&amp;gt;&lt;/code&gt; are stored in
a &lt;code&gt;std::vector&lt;/code&gt; and then are all queried for their result.&lt;/p&gt;
&lt;p&gt;Overall, I think &lt;code&gt;std::future&lt;/code&gt; and &lt;code&gt;std::async&lt;/code&gt; are great tool that
can simplify your asynchronous code a lot. They allow you to make pretty
advanced stuff while keeping the complexity of the code to a minimum.&lt;/p&gt;
&lt;p&gt;I hope this long-due post is going to be interesting to some of you :)
The code for this post is available &lt;a class="reference external" href="https://github.com/wichtounet/articles/tree/master/src/threads/part5"&gt;on Github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I do not yet know if there will be a next installment in the series. I've
covered pretty much everything that is available in C++11 for concurrency. I may
cover the parallel algorithms of C++17 in a following post. If you have any
suggestion for the next post, don't hesitate to post a comment or contact me
directly by email.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>C++11 Concurrency Tutorial</category><category>Concurrency</category><category>Performances</category><guid>https://baptiste-wicht.com/posts/2017/09/cpp11-concurrency-tutorial-futures.html</guid><pubDate>Tue, 12 Sep 2017 13:05:08 GMT</pubDate></item><item><title>Simplify your type traits with C++14 variable templates</title><link>https://baptiste-wicht.com/posts/2017/08/simplify-your-type-traits-with-c%2B%2B14-variable-templates.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Often if you write templated code, you have to write and use a lot of different
traits. In this article, I'll focus on the traits that are representing values,
typically a boolean value. For instance, std::is_const, std::is_same or
std::is_reference are type traits provided by the STL. They are giving you some
information at compile time for a certain type. If you need to write a type
traits, let's say is_float, here is how you would maybe do it in C++11:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_6d58dce2f006449787ce5500202b2ffc-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6d58dce2f006449787ce5500202b2ffc-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_6d58dce2f006449787ce5500202b2ffc-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_6d58dce2f006449787ce5500202b2ffc-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;or a bit nicer with a template type alias and std::integral constant:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_165c4d2aaa9e46afa0355f7f779aefeb-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_165c4d2aaa9e46afa0355f7f779aefeb-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;integral_constant&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;or since is_same is itself a type traits, you can also directly alias it:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_27ee335f1ce94755b2a8cb125077729d-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_27ee335f1ce94755b2a8cb125077729d-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This makes for some very nice syntax, but we still have a type rather than a value.&lt;/p&gt;
&lt;p&gt;Note that in some cases, you cannot use the using technique since it cannot be
specialized and you often need specialization to write some more advanced
traits.&lt;/p&gt;
&lt;p&gt;And then you would use your traits to do something specific based on that
information. For instance with a very basic example:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_be4b1c667c8845d589435b38c078ded6-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_be4b1c667c8845d589435b38c078ded6-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_be4b1c667c8845d589435b38c078ded6-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is_float&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_be4b1c667c8845d589435b38c078ded6-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm a float"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_be4b1c667c8845d589435b38c078ded6-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_be4b1c667c8845d589435b38c078ded6-6"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm not a float"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_be4b1c667c8845d589435b38c078ded6-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_be4b1c667c8845d589435b38c078ded6-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Really nothing fancy here, but that will be enough as examples.&lt;/p&gt;
&lt;p&gt;Even though all this works pretty, it can be made better on two points. First,
every time you use a traits, you need to use the value member (via ::value).
Secondly, every time you declare a new traits, you have to declare a new type or
a type alias. But all you want is a boolean value.&lt;/p&gt;
&lt;p&gt;C++14 introduced a new feature, variable templates. As their name indicates,
they are variables, parametrized with a type. This allows us to write type
traits without using a type alias or struct, meaning we have a real value
instead of a type. If we rewrite our is_float traits with variable templates, we
have the following:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_e2dc9e5b3c0e46bb96153bb1b38265b8-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_e2dc9e5b3c0e46bb96153bb1b38265b8-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I think it's much nicer, the intent is clearly stated and there is no
unnecessary code. Moreover, it's also nicer to use:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_f443f0ee268d4702a032ca81b4972513-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_f443f0ee268d4702a032ca81b4972513-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_f443f0ee268d4702a032ca81b4972513-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is_float&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_f443f0ee268d4702a032ca81b4972513-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm a float"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_f443f0ee268d4702a032ca81b4972513-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_f443f0ee268d4702a032ca81b4972513-6"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm not a float"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_f443f0ee268d4702a032ca81b4972513-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_f443f0ee268d4702a032ca81b4972513-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;No more ::value everywhere :) I think it's really cool.&lt;/p&gt;
&lt;p&gt;Note that, unlike type alias template, they can be specialized, either fully or
partially, so no more limitation on that side.&lt;/p&gt;
&lt;p&gt;Interestingly, variable templates are used in C++17 to provide helpers for each
type traits with values. For instance, std::is_same will have a std::is_same_v
helper that is a variable template. With that, we can simplify our traits a bit
more:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_4803b9b318eb4f0db9861d2994eac2c0-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4803b9b318eb4f0db9861d2994eac2c0-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same_v&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Personally, I replaced all the type traits inside ETL using variable templates.
If you don't want to do it, you can also introduce helpers like in the C++17 STL
and start using the wrappers when you see fit so that you don't break any code.&lt;/p&gt;
&lt;p&gt;If you want to use this feature, you need a C++14 compiler, such as any version
from GCC5 family or clang 3.6. Although I haven't tested, it should also work on
Microsoft VS2015 Update 2.&lt;/p&gt;
&lt;p&gt;Unfortunately there is a bug in both clang (fixed in clang 3.7) and GCC (fixed
in GCC 6 only) that you may encounter if you start using variable templates in
template classes or variable templates used in another variable templates. If
you plan to use variable templates inside a template, such as something like
this:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;outer_traits&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-4"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;sub_traits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-5"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-8"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;outer_helper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outer_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;sub_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-10"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;outer_helper&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_473ca228c070465cac5b6631812f1c24-14"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;You will encounter a not-helpful at all error message with GCC5 family, such as:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_cfc89df1e9084d609e479d09b89b7101-1"&gt;&lt;/a&gt;test.cpp: In instantiation of constexpr const bool outer_helper&amp;lt;float, float&amp;gt;:
&lt;a name="rest_code_cfc89df1e9084d609e479d09b89b7101-2"&gt;&lt;/a&gt;test.cpp:14:22:   required from here
&lt;a name="rest_code_cfc89df1e9084d609e479d09b89b7101-3"&gt;&lt;/a&gt;test.cpp:11:20: error: template&amp;lt;class X&amp;gt; constexpr const bool outer_traits&amp;lt;float&amp;gt;::sub_traits&amp;lt;X&amp;gt; is not a function template
&lt;a name="rest_code_cfc89df1e9084d609e479d09b89b7101-4"&gt;&lt;/a&gt;     constexpr bool outer_helper = outer_traits&amp;lt;T&amp;gt;::template sub_trait
&lt;a name="rest_code_cfc89df1e9084d609e479d09b89b7101-5"&gt;&lt;/a&gt;                    ^
&lt;a name="rest_code_cfc89df1e9084d609e479d09b89b7101-6"&gt;&lt;/a&gt;test.cpp:11:20: error: sub_traits&amp;lt;X&amp;gt; is not a member of outer_traits&amp;lt;float&amp;gt;
&lt;/pre&gt;&lt;p&gt;It comes from a bug in the handling of variable templates as dependent names. If
you don't come in this cases, you can use GCC5 family directly, otherwise,
you'll have to use GCC6 family only.&lt;/p&gt;
&lt;p&gt;I hope this can help some of you to improve your type traits or at least to
discover the power of the new variable templates. Personally, I've rewritten all
the traits from the ETL library using this new feature and I'm pretty satisfied
with the result. Of course, that means that the compiler support was reduced,
but since I don't have many users, it's not a real issue.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>Compilers</category><category>etl</category><category>projects</category><guid>https://baptiste-wicht.com/posts/2017/08/simplify-your-type-traits-with-c%2B%2B14-variable-templates.html</guid><pubDate>Tue, 22 Aug 2017 12:45:11 GMT</pubDate></item><item><title>How to fix mdadm RAID5 / RAID6 growing stuck at 0K/s ?</title><link>https://baptiste-wicht.com/posts/2017/08/how-to-fix-mdadm-raid5-raid6-growing-stuck-at-0ks.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I just started growing again my RAID6 array from 12 to 13 disks and
I encountered a new issue. The reshape started, but with a speed of 0K/s. After
some searching, I found a very simple solution:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_061141cfee254ee992354f70bcaf017f-1"&gt;&lt;/a&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; max &amp;gt; /sys/block/md0/md/sync_max
&lt;/pre&gt;&lt;p&gt;And the reshape started directly at 50M/s :)&lt;/p&gt;
&lt;p&gt;The solution is the same if you are growing any type of RAID level with parity
(RAID5, RAID6, ...).&lt;/p&gt;
&lt;p&gt;Normally, the issues I have are related to speed not very good. I've written
a post in the post about
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2015/03/how-to-speed-up-raid-5-6-growing-with-mdadm.html"&gt;how to speed up RAID5 / RAID6 growing with mdadm&lt;/a&gt;.
Although RAID5 / RAID6 growing, or another reshape operation, will never be very
fast, you can still speed up the process a lot from a few days to a few hours.
Currently, my reshape is working at 48M/s and I'm looking at around 16 hours of
reshape, but I have 13 disks of 3To, so it's not so bad.&lt;/p&gt;
&lt;p&gt;I hope this very simple tip can be helpful to some of you :)&lt;/p&gt;&lt;/div&gt;</description><category>Gentoo</category><category>Hardware</category><category>Home Server</category><category>Linux</category><guid>https://baptiste-wicht.com/posts/2017/08/how-to-fix-mdadm-raid5-raid6-growing-stuck-at-0ks.html</guid><pubDate>Sat, 12 Aug 2017 10:28:43 GMT</pubDate></item><item><title>DLL: Blazing Fast Neural Network Library</title><link>https://baptiste-wicht.com/posts/2017/08/dll-blazing-fast-neural-network-library.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;A few weeks ago, I talked about all
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html"&gt;the new features of my Deep Learning Library (DLL)&lt;/a&gt;
project. I've mentioned that, on several experiments, DLL was always
significantly faster than some popular deep learning frameworks such as
TensorFlow. I'll now go into more details into this comparison and provide all
the results. So far, the paper we wrote about these results has not been
published, so I'll not provide the paper directly yet.&lt;/p&gt;
&lt;p&gt;For those that may not know, DLL is the project I've been developing to support
my Ph.D. thesis. This is a neural network framework  that supports
Fully-Connected Neural Network (FCNN), Convolutional Neural Network (CNN),
Restricted Boltzmann Machine (RBM), Deep Belief Network (DBN), Convolutional RBM
(CRBM) and Convolutional DBN (CDBN). It also supports a large variety of options
such as Dropout, Batch Normalization and Adaptive Learning Rates. You can read
read the
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html"&gt;previous post&lt;/a&gt;
if you want more information about the new features of the framework. And, as those of
you that read my blog frequently may know, I'm a bit obsessed with performance
optimization, so I've spent a considerable amount of time optimizing
the performance of neural network training, on CPU. Since, at the beginning of my
thesis, I had no access to GPU for training, I've focused on CPU. Although there
is now support for GPU, the gains are not yet important enough.&lt;/p&gt;
&lt;div class="section" id="evaluation"&gt;
&lt;h2&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;To see how fast, or not, the library was, it was compared against five popular
machine learning libraries:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Caffe, installed from sources&lt;/li&gt;
&lt;li&gt;TensorFlow 1.0, from pip&lt;/li&gt;
&lt;li&gt;Keras 2.0, from pip&lt;/li&gt;
&lt;li&gt;Torch, installed from sources&lt;/li&gt;
&lt;li&gt;DeepLearning4J 0.7, from Maven&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I've run four different experiments with all these frameworks and compared the
efficiency of each of them for training the same neural networks with the same
options. In each case, the training or testing error have also been compared to
ensure that each framework is doing roughly the same. I wont present here the
details, but in each experiment DLL showed around the same accuracies as the
other frameworks. I will only focus on the speed results in this article.&lt;/p&gt;
&lt;p&gt;Each experiment is done once with only CPU and once with a GPU. For DLL, I only
report the CPU time in both modes, since it's more stable and more optimized.&lt;/p&gt;
&lt;p&gt;The code for the evaluation is available online on the
&lt;a class="reference external" href="https://github.com/wichtounet/frameworks"&gt;Github repository of the frameworks project&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="mnist-fully-connected-neural-network"&gt;
&lt;h2&gt;MNIST: Fully Connected Neural Network&lt;/h2&gt;
&lt;p&gt;The first experiment is performed on The MNIST data set. It consists of 60'000
grayscale images of size 28x28. The goal is to classify each image of a digit
from 0 to 9. To solve this task, I trained a very small fully-connected neural
network with 500 hidden units in the first layer, 250 in the second and 10 final
hidden units (or output units) for classification. The first two layers are
using the logistic sigmoid activation function and the last layer is using the
softmax activation function. The network is trained for 50 epochs with a
categorical cross entropy loss, with mini-batches of 100 images. Here are
results of this experiment:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Training time performance for the different frameworks on the Fully-Connected Neural Network experiment, on MNIST." src="https://baptiste-wicht.com/images/dll_fcnn.png"&gt;
&lt;p class="caption"&gt;Training time performance for the different frameworks on the Fully-Connected
Neural Network experiment, on MNIST. All the times are in seconds.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In DLL mode, the DLL framework is the clear winner here! It's about 35% faster
than TensorFlow and Keras which are coming at the second place. DLL is more than
four times slower than DLL and the last two frameworks (Caffe and
DeepLearning4J) are five times slower than DLL! Once we add a GPU to the system,
the results are very different. Caffe is now the fastest framework, three times
faster than DLL. DLL is less than two times slower than Keras and TensorFlow.
Interestingly, DLL is still faster than Torch and DeepLearning4J.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="mnist-convolutional-neural-network"&gt;
&lt;h2&gt;MNIST: Convolutional Neural Network&lt;/h2&gt;
&lt;p&gt;Although a Fully-Connected Neural Network is an interesting tool, the trend now
is to use Convolutional Neural Network which have proved very efficient at
solving a lot of problems. The second experiment is also using the same data
set. Again, it's a rather small network. The first layer is a convolutional
layer with 8 5x5 kernels, followed by max pooling layer with 2x2 kernel. They
are followed by one more convolutional layers with 8 5x5 kernels and a 2x2 max
pooling layer. These first four layers are followed by two fully-connected
layers, the first with 150 hidden units and the last one with 10 output units.
The activation functions are the same as for the first network, as is the
training procedure. This takes significantly longer to train than the first
network because of the higher complexity of the convolutional layers compared to
the fully-connected layers even though they have much less weights. The results
are present in the next figure:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Training time performance for the different frameworks on the Convolutional Neural Network experiment, on MNIST." src="https://baptiste-wicht.com/images/dll_cnn.png"&gt;
&lt;p class="caption"&gt;Training time performance for the different frameworks on the Convolutional
Neural Network experiment, on MNIST. All the times are in seconds.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Again, on CPU, DLL is the clear winner, by a lot! It's already 3.6 times faster
than the second frameworks Keras and TensorFlow, more than four times faster
than Caffe and Torch and 8 times faster than DeepLearning4J that is proving very
slow on this experiment. Once a GPU is added, Keras and TensorFlow are about
twice faster than DLL. However, DLL is still faster than the other frameworks
even though they are taking advantage of the GPU.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cifar-10"&gt;
&lt;h2&gt;CIFAR-10&lt;/h2&gt;
&lt;p&gt;The second data set that is tested is the CIFAR-10 data set. It's an object
recognition with 10 classes for classification. The training set is composed of
50'000 colour images for 32x32 pixels. The network that is used for this data
set is similar in architecture than the first network, but has more parameters.
The first convolutional layer now has 12 5x5 kernels and the second
convolutional layer has 24 3x3 kernels. The pooling layers are the same. The
first fully-connected has 64 hidden units and the last one has 10 output units.
The last layer again use a softmax activation function while the other layers
are using Rectifier Linear Units (ReLU). The training is done in the same manner
as for the two first networks. Unfortunately, it was not possible to train
DeepLearning4J on this data set, even though there is official support for this
data set. Since I've had no answer to my question regarding this issue, the
results are simply removed from this experiment. It may not seem so but it's
considerably longer to train this network because of the larger number of input
channels and larger number of convolutional kernels in each layer. Let's get to
the results now:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Training time performance for the different frameworks on the Convolutional Neural Network experiment, on CIFAR-10." src="https://baptiste-wicht.com/images/dll_cifar10.png"&gt;
&lt;p class="caption"&gt;Training time performance for the different frameworks on the Convolutional
Neural Network experiment, on CIFAR-10. All the times are in seconds.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;DLL is still the fastest on CPU, but the margin is less than before. It's about
40% faster than TensorFlow and Keras, twice faster than Torch and 2.6 times
faster than Caffe. Once a GPU is added, DLL is about as fast as Torch but slower
than the other three frameworks. TensorFlow and Keras are about four times
faster than DLL while Caffe is about twice faster than DLL. We can see that
with this larger network, the GPU becomes more interesting and that there is
a smaller margin for improvements compared to the other frameworks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="imagenet"&gt;
&lt;h2&gt;ImageNet&lt;/h2&gt;
&lt;p&gt;The last experiment is made on the ImageNet data set. I used the ILSVRC 2012
subset, that consists "only" of about 1.2 million images for training. I've
resized all the images to 256x256 pixels, this makes for 250 times more colour
values than a MNIST image. This dimension and the number of images makes it
impractical to keep the dataset in memory. The images must be loaded in batch
from the disk. No random cropping or mirroring was performed. The network is
much larger to solve this task. The network starts with 5 pairs of convolutional
layers and max pooling layers. The convolutional layers have 3x3 kernels, 16 for
the first two layers and 32 for the three following one. The five max pooling
layers use 2x2 kernels. Each convolutional layer uses zero-padding so that their
output features are the same dimensions as the input. They are followed by two
fully-connected layer. The first one with 2048 hidden units and the last one
with 1000 output units (one for each class). Except for the last layer, using
softmax, the layers all uses ReLU. The network is trained with mini-batches of
128 images (except for DeepLearning4J and Torch, which can only use 64 images on
the amount of RAM available on my machine). To ease the comparison, I report the
time necessary to train one batch of data (or two for DeepLearning4J and Torch).
The results, presented in logarithmic scale because of DeepLearning4J disastrous
results, are as follows:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Training time performance for the different frameworks on the Convolutional Neural Network experiment, on ImageNet." src="https://baptiste-wicht.com/images/dll_imagenet.png"&gt;
&lt;p class="caption"&gt;Training time performance for the different frameworks on the Convolutional
Neural Network experiment, on ImageNet. The times are the time necessary to
train a batch of 128 images. All the times are in milliseconds.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For this final experiment, DLL is again significantly faster than all the other
frameworks. It's about 40% faster than Keras, twice faster than TensorFlow and
Caffe and more than three times faster than Torch. Although 40% may seem not
that much, don't forget that this kind of training may take days, so it can save
you a lot of time. All the frameworks are much faster than DeepLearning4J. Based
on several posts on the internet, I suspect that this comes from the model of
GPU I have been used (GTX 960), but all the other frameworks seem to handle this
card pretty well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I hope this is not too much of a bragging post :P We can see that my efforts to
make the code as fast as possible have paid :) As was shown in the experiments,
my DLL framework is always the fastest framework when the neural network is
trained on CPU. I'm quite pleased with the results since I've done a lot of work
to optimize the speed as much as possible and since I'm competing with
well-known libraries that have been developed by several persons.  Moreover, the
accuracies of the trained networks is similar to that of the networks trained
with the other frameworks. Even when the other frameworks are using GPU, the
library still remains competitive, although never the fastest.&lt;/p&gt;
&lt;p&gt;In the next step (I've no idea when I'll have the time though), I will want to
focus on GPU speed. This will mostly come from a better support of the GPU in
the ETL library on which DLL is based. I have many ideas to improve it a lot,
but it will take me a lot of time.&lt;/p&gt;
&lt;p&gt;If you want more information on the DLL library, you can have a look at
&lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;its Github repository&lt;/a&gt; and especially at
&lt;a class="reference external" href="https://github.com/wichtounet/dll/tree/master/examples/src"&gt;the few examples&lt;/a&gt;.
You can also have a look at &lt;a class="reference external" href="https://baptiste-wicht.com/categories/dll.html"&gt;my posts about DLL&lt;/a&gt;.
Finally, don't hesitate to comment or contact me through Github issues if you
have comments or problems with this post, the library or anything ;)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>dll</category><category>etl</category><category>GPU</category><category>Machine Learning</category><category>projects</category><guid>https://baptiste-wicht.com/posts/2017/08/dll-blazing-fast-neural-network-library.html</guid><pubDate>Fri, 11 Aug 2017 09:09:14 GMT</pubDate></item><item><title>Compiler benchmark GCC and Clang on C++ library (ETL)</title><link>https://baptiste-wicht.com/posts/2017/08/compiler-benchmark-gcc-clang-cpp-library-etl.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;It's been a while since I've done a benchmark of different compilers on C++
code. Since I've recently
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/08/expression-templates-library-etl-11.html"&gt;released the version 1.1 of my ETL project&lt;/a&gt;
(an optimized matrix/vector computation library with expression templates), I've
decided to use it as the base of my benchmark. It's a C++14 library with a lot
of templates. I'm going to compile the full test suite (124 test cases). This is
done directly on the last release (1.1) code. I'm going to compile once in debug
mode and once in release_debug (release plus debug symbols and assertions) and
record the times for each compiler. The tests were compiled with support for
every option in ETL to account to maximal compilation time. Each compilation was
made using four threads (make -j4). I'm also going to test a few of the
benchmarks to see the difference in runtime performance between the code
generated by each compiler. The benchmark will be compiled in release mode and
its compilation time recorded as well.&lt;/p&gt;
&lt;p&gt;I'm going to test the following compilers:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;GCC-4.9.4&lt;/li&gt;
&lt;li&gt;GCC-5.4.0&lt;/li&gt;
&lt;li&gt;GCC-6.3.0&lt;/li&gt;
&lt;li&gt;GCC-7.1.0&lt;/li&gt;
&lt;li&gt;clang-3.9.1&lt;/li&gt;
&lt;li&gt;clang-4.0.1&lt;/li&gt;
&lt;li&gt;zapcc-1.0 (commercial, based on clang-5.0 trunk)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All have been installed directly using Portage (Gentoo package manager) except
for clang-4.0.1 that has been installed from sources and zapcc since it does not
have a Gentoo package. Since clang package on Gentoo does not support
multislotting, I had to install one version from source and the other from the
package manager. This is also the reason I'm testing less versions of clang,
simply less practical.&lt;/p&gt;
&lt;p&gt;For the purpose of these tests, the exact same options have been used throughout
all the compilers. Normally, I use different options for clang than for GCC
(mainly more aggressive vectorization options on clang). This may not lead to
the best performance for each compiler, but allows for comparison between the
results with defaults optimization level. Here are the main options used:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;In debug mode: -g&lt;/li&gt;
&lt;li&gt;In release_debug mode: -g -O2&lt;/li&gt;
&lt;li&gt;In release mode: -g -O3 -DNDEBUG -fomit-frame-pointer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In each case, a lot of warnings are enabled and the ETL options are the same.&lt;/p&gt;
&lt;p&gt;All the results have been gathered on a Gentoo machine running on Intel Core
i7-2600 (Sandy Bridge...) @3.4GHz with 4 cores and 8 threads, 12Go of RAM and
a SSD. I do my best to isolate as much as possible the benchmark from
perturbations and that my benchmark code is quite sound, it may well be that
some results are not totally accurate. Moreover, some of the benchmarks are
using multithreading, which may add some noise and unpredictability. When I was
not sure about the results, I ran the benchmarks several time to confirm them
and overall I'm confident of the results.&lt;/p&gt;
&lt;div class="section" id="compilation-time"&gt;
&lt;h2&gt;Compilation Time&lt;/h2&gt;
&lt;p&gt;Let's start with the results of the performance of the compilers themselves:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="31%"&gt;
&lt;col width="15%"&gt;
&lt;col width="31%"&gt;
&lt;col width="23%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;Debug&lt;/th&gt;
&lt;th class="head"&gt;Release_Debug&lt;/th&gt;
&lt;th class="head"&gt;Benchmark&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;402s&lt;/td&gt;
&lt;td&gt;616s&lt;/td&gt;
&lt;td&gt;100s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;403s&lt;/td&gt;
&lt;td&gt;642s&lt;/td&gt;
&lt;td&gt;95s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;399s&lt;/td&gt;
&lt;td&gt;683s&lt;/td&gt;
&lt;td&gt;102s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;371s&lt;/td&gt;
&lt;td&gt;650s&lt;/td&gt;
&lt;td&gt;105s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;380s&lt;/td&gt;
&lt;td&gt;807s&lt;/td&gt;
&lt;td&gt;106s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;260s&lt;/td&gt;
&lt;td&gt;718s&lt;/td&gt;
&lt;td&gt;92s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;221s&lt;/td&gt;
&lt;td&gt;649s&lt;/td&gt;
&lt;td&gt;108s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note: For Release_Debug and Benchmark, I only use three threads with zapcc,
because 12Go of RAM is not enough memory for four threads.&lt;/p&gt;
&lt;p&gt;There are some very significant differences between the different compilers.
Overall, clang-4.0.1 is by far the fastest free compiler for Debug mode. When
the tests are compiled with optimizations however, clang is falling behind.
It's quite impressive how clang-4.0.1 manages to be so much faster than
clang-3.9.1 both in debug mode and release mode. Really great work by the clang
team here! With these optimizations, clang-4.0.1 is almost on par with gcc-7.1
in release mode.  For GCC, it seems that the cost of optimization has been going
up quite significantly. However, GCC 7.1 seems to have made optimization faster
and standard compilation much faster as well. If we take into account zapcc,
it's the fastest compiler on debug mode, but it's slower than several gcc
versions on release mode.&lt;/p&gt;
&lt;p&gt;Overall, I'm quite impressed by the performance of clang-4.0.1 which seems
really fast! I'll definitely make more tests with this new version of the
compiler in the near future. It's also good to see that g++-7.1 also did make
the build faster than gcc-6.3. However, the fastest gcc version for optimization
is still gcc-4.9.4 which is already an old branch with low C++ standard support.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="runtime-performance"&gt;
&lt;h2&gt;Runtime Performance&lt;/h2&gt;
&lt;p&gt;Let's now take a look at the quality of the generated code. For some of the
benchmarks, I've included two versions of the algorithm. &lt;em&gt;std&lt;/em&gt; is the most
simple algorithm (the naive one) and &lt;em&gt;vec&lt;/em&gt; is the hand-crafted vectorized and
optimized implementation. All the tests were done on single-precision floating
points.&lt;/p&gt;
&lt;div class="section" id="dot-product"&gt;
&lt;h3&gt;Dot product&lt;/h3&gt;
&lt;p&gt;The first benchmark that is run is to compute the dot product between two
vectors. Let's look first at the naive version:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="13%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="7%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;dot (std)&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;th class="head"&gt;500&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;10000&lt;/th&gt;
&lt;th class="head"&gt;100000&lt;/th&gt;
&lt;th class="head"&gt;1000000&lt;/th&gt;
&lt;th class="head"&gt;2000000&lt;/th&gt;
&lt;th class="head"&gt;3000000&lt;/th&gt;
&lt;th class="head"&gt;4000000&lt;/th&gt;
&lt;th class="head"&gt;5000000&lt;/th&gt;
&lt;th class="head"&gt;10000000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;64.96ns&lt;/td&gt;
&lt;td&gt;97.12ns&lt;/td&gt;
&lt;td&gt;126.07ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;25.91us&lt;/td&gt;
&lt;td&gt;326.49us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.92ms&lt;/td&gt;
&lt;td&gt;2.55ms&lt;/td&gt;
&lt;td&gt;3.22ms&lt;/td&gt;
&lt;td&gt;6.36ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;72.96ns&lt;/td&gt;
&lt;td&gt;101.62ns&lt;/td&gt;
&lt;td&gt;127.89ns&lt;/td&gt;
&lt;td&gt;1.90us&lt;/td&gt;
&lt;td&gt;23.39us&lt;/td&gt;
&lt;td&gt;357.63us&lt;/td&gt;
&lt;td&gt;1.23ms&lt;/td&gt;
&lt;td&gt;1.91ms&lt;/td&gt;
&lt;td&gt;2.57ms&lt;/td&gt;
&lt;td&gt;3.20ms&lt;/td&gt;
&lt;td&gt;6.32ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;73.31ns&lt;/td&gt;
&lt;td&gt;102.88ns&lt;/td&gt;
&lt;td&gt;130.16ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;24.314us&lt;/td&gt;
&lt;td&gt;339.13us&lt;/td&gt;
&lt;td&gt;1.47ms&lt;/td&gt;
&lt;td&gt;2.16ms&lt;/td&gt;
&lt;td&gt;2.95ms&lt;/td&gt;
&lt;td&gt;3.70ms&lt;/td&gt;
&lt;td&gt;6.69ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;70.20ns&lt;/td&gt;
&lt;td&gt;104.09ns&lt;/td&gt;
&lt;td&gt;130.98ns&lt;/td&gt;
&lt;td&gt;1.90us&lt;/td&gt;
&lt;td&gt;23.96us&lt;/td&gt;
&lt;td&gt;281.47us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.93ms&lt;/td&gt;
&lt;td&gt;2.58ms&lt;/td&gt;
&lt;td&gt;3.19ms&lt;/td&gt;
&lt;td&gt;6.33ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;64.69ns&lt;/td&gt;
&lt;td&gt;98.69ns&lt;/td&gt;
&lt;td&gt;128.60ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;23.33us&lt;/td&gt;
&lt;td&gt;272.71us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.91ms&lt;/td&gt;
&lt;td&gt;2.56ms&lt;/td&gt;
&lt;td&gt;3.19ms&lt;/td&gt;
&lt;td&gt;6.37ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;60.31ns&lt;/td&gt;
&lt;td&gt;96.34ns&lt;/td&gt;
&lt;td&gt;128.90ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;22.87us&lt;/td&gt;
&lt;td&gt;270.21us&lt;/td&gt;
&lt;td&gt;1.23ms&lt;/td&gt;
&lt;td&gt;1.91ms&lt;/td&gt;
&lt;td&gt;2.55ms&lt;/td&gt;
&lt;td&gt;3.18ms&lt;/td&gt;
&lt;td&gt;6.35ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;61.14ns&lt;/td&gt;
&lt;td&gt;96.92ns&lt;/td&gt;
&lt;td&gt;125.95ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;23.84us&lt;/td&gt;
&lt;td&gt;285.80us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.92ms&lt;/td&gt;
&lt;td&gt;2.55ms&lt;/td&gt;
&lt;td&gt;3.16ms&lt;/td&gt;
&lt;td&gt;6.34ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The differences are not very significant between the different compilers. The
clang-based compilers seem to be the compilers producing the fastest code.
Interestingly, there seem to have been a big regression in gcc-6.3 for large
containers, but that has been fixed in gcc-7.1.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="13%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="9%"&gt;
&lt;col width="7%"&gt;
&lt;col width="8%"&gt;
&lt;col width="9%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="9%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;dot (vec)&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;th class="head"&gt;500&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;10000&lt;/th&gt;
&lt;th class="head"&gt;100000&lt;/th&gt;
&lt;th class="head"&gt;1000000&lt;/th&gt;
&lt;th class="head"&gt;2000000&lt;/th&gt;
&lt;th class="head"&gt;3000000&lt;/th&gt;
&lt;th class="head"&gt;4000000&lt;/th&gt;
&lt;th class="head"&gt;5000000&lt;/th&gt;
&lt;th class="head"&gt;10000000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;48.34ns&lt;/td&gt;
&lt;td&gt;80.53ns&lt;/td&gt;
&lt;td&gt;114.97ns&lt;/td&gt;
&lt;td&gt;1.72us&lt;/td&gt;
&lt;td&gt;22.79us&lt;/td&gt;
&lt;td&gt;354.20us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.89ms&lt;/td&gt;
&lt;td&gt;2.52ms&lt;/td&gt;
&lt;td&gt;3.19ms&lt;/td&gt;
&lt;td&gt;6.55ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;47.16ns&lt;/td&gt;
&lt;td&gt;77.70ns&lt;/td&gt;
&lt;td&gt;113.66ns&lt;/td&gt;
&lt;td&gt;1.72us&lt;/td&gt;
&lt;td&gt;22.71us&lt;/td&gt;
&lt;td&gt;363.86us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.89ms&lt;/td&gt;
&lt;td&gt;2.52ms&lt;/td&gt;
&lt;td&gt;3.19ms&lt;/td&gt;
&lt;td&gt;6.56ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;46.39ns&lt;/td&gt;
&lt;td&gt;77.67ns&lt;/td&gt;
&lt;td&gt;116.28ns&lt;/td&gt;
&lt;td&gt;1.74us&lt;/td&gt;
&lt;td&gt;23.39us&lt;/td&gt;
&lt;td&gt;452.44us&lt;/td&gt;
&lt;td&gt;1.45ms&lt;/td&gt;
&lt;td&gt;2.26ms&lt;/td&gt;
&lt;td&gt;2.87ms&lt;/td&gt;
&lt;td&gt;3.49ms&lt;/td&gt;
&lt;td&gt;7.52ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;49.70ns&lt;/td&gt;
&lt;td&gt;80.40ns&lt;/td&gt;
&lt;td&gt;115.77ns&lt;/td&gt;
&lt;td&gt;1.71us&lt;/td&gt;
&lt;td&gt;22.46us&lt;/td&gt;
&lt;td&gt;355.16us&lt;/td&gt;
&lt;td&gt;1.21ms&lt;/td&gt;
&lt;td&gt;1.85ms&lt;/td&gt;
&lt;td&gt;2.49ms&lt;/td&gt;
&lt;td&gt;3.14ms&lt;/td&gt;
&lt;td&gt;6.47ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;46.13ns&lt;/td&gt;
&lt;td&gt;78.01ns&lt;/td&gt;
&lt;td&gt;114.70ns&lt;/td&gt;
&lt;td&gt;1.66us&lt;/td&gt;
&lt;td&gt;22.82us&lt;/td&gt;
&lt;td&gt;359.42us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.88ms&lt;/td&gt;
&lt;td&gt;2.53ms&lt;/td&gt;
&lt;td&gt;3.16ms&lt;/td&gt;
&lt;td&gt;6.50ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;45.59ns&lt;/td&gt;
&lt;td&gt;74.90ns&lt;/td&gt;
&lt;td&gt;111.29ns&lt;/td&gt;
&lt;td&gt;1.57us&lt;/td&gt;
&lt;td&gt;22.47us&lt;/td&gt;
&lt;td&gt;351.31us&lt;/td&gt;
&lt;td&gt;1.23ms&lt;/td&gt;
&lt;td&gt;1.85ms&lt;/td&gt;
&lt;td&gt;2.49ms&lt;/td&gt;
&lt;td&gt;3.12ms&lt;/td&gt;
&lt;td&gt;6.45ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;45.11ns&lt;/td&gt;
&lt;td&gt;75.04ns&lt;/td&gt;
&lt;td&gt;111.28ns&lt;/td&gt;
&lt;td&gt;1.59us&lt;/td&gt;
&lt;td&gt;22.46us&lt;/td&gt;
&lt;td&gt;357.32us&lt;/td&gt;
&lt;td&gt;1.25ms&lt;/td&gt;
&lt;td&gt;1.89ms&lt;/td&gt;
&lt;td&gt;2.53ms&lt;/td&gt;
&lt;td&gt;3.15ms&lt;/td&gt;
&lt;td&gt;6.47ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If we look at the optimized version, the differences are even slower. Again, the
clang-based compilers are producing the fastest executables, but are closely
followed by gcc, except for gcc-6.3 in which we can still see the same
regression as before.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="logistic-sigmoid"&gt;
&lt;h3&gt;Logistic Sigmoid&lt;/h3&gt;
&lt;p&gt;The next test is to check the performance of the sigmoid operation. In that
case, the evaluator of the library will try to use parallelization and
vectorization to compute it. Let's see how the different compilers fare:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="22%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="13%"&gt;
&lt;col width="15%"&gt;
&lt;col width="13%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sigmoid&lt;/th&gt;
&lt;th class="head"&gt;10&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;10000&lt;/th&gt;
&lt;th class="head"&gt;100000&lt;/th&gt;
&lt;th class="head"&gt;1000000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;8.16us&lt;/td&gt;
&lt;td&gt;5.23us&lt;/td&gt;
&lt;td&gt;6.33us&lt;/td&gt;
&lt;td&gt;29.56us&lt;/td&gt;
&lt;td&gt;259.72us&lt;/td&gt;
&lt;td&gt;2.78ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;7.07us&lt;/td&gt;
&lt;td&gt;5.08us&lt;/td&gt;
&lt;td&gt;6.39us&lt;/td&gt;
&lt;td&gt;29.44us&lt;/td&gt;
&lt;td&gt;266.27us&lt;/td&gt;
&lt;td&gt;2.96ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;7.13us&lt;/td&gt;
&lt;td&gt;5.32us&lt;/td&gt;
&lt;td&gt;6.45us&lt;/td&gt;
&lt;td&gt;28.99us&lt;/td&gt;
&lt;td&gt;261.81us&lt;/td&gt;
&lt;td&gt;2.86ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;7.03us&lt;/td&gt;
&lt;td&gt;5.09us&lt;/td&gt;
&lt;td&gt;6.24us&lt;/td&gt;
&lt;td&gt;28.61us&lt;/td&gt;
&lt;td&gt;252.78us&lt;/td&gt;
&lt;td&gt;2.71ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;7.30us&lt;/td&gt;
&lt;td&gt;5.25us&lt;/td&gt;
&lt;td&gt;6.57us&lt;/td&gt;
&lt;td&gt;30.24us&lt;/td&gt;
&lt;td&gt;256.75us&lt;/td&gt;
&lt;td&gt;1.99ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;7.47us&lt;/td&gt;
&lt;td&gt;5.14us&lt;/td&gt;
&lt;td&gt;5.77us&lt;/td&gt;
&lt;td&gt;26.03us&lt;/td&gt;
&lt;td&gt;235.87us&lt;/td&gt;
&lt;td&gt;1.81ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;7.51us&lt;/td&gt;
&lt;td&gt;5.26us&lt;/td&gt;
&lt;td&gt;6.48us&lt;/td&gt;
&lt;td&gt;28.86us&lt;/td&gt;
&lt;td&gt;258.31us&lt;/td&gt;
&lt;td&gt;1.95ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Interestingly, we can see that gcc-7.1 is the fastest for small vectors while
clang-4.0 is the best for producing code for larger vectors. However, except for
the biggest vector size, the difference is not really significantly. Apparently,
there is a regression in zapcc (or clang-5.0) since it's slower than clang-4.0
at the same level as clang-3.9.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="y-alpha-x-y-axpy"&gt;
&lt;h3&gt;y = alpha * x + y (axpy)&lt;/h3&gt;
&lt;p&gt;The third benchmark is the well-known axpy (y = alpha * x + y). This is entirely
resolved by expressions templates in the library, no specific algorithm is used.
Let's see the results:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="24%"&gt;
&lt;col width="13%"&gt;
&lt;col width="13%"&gt;
&lt;col width="11%"&gt;
&lt;col width="13%"&gt;
&lt;col width="13%"&gt;
&lt;col width="14%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;saxpy&lt;/th&gt;
&lt;th class="head"&gt;10&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;10000&lt;/th&gt;
&lt;th class="head"&gt;100000&lt;/th&gt;
&lt;th class="head"&gt;1000000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;38.1ns&lt;/td&gt;
&lt;td&gt;61.6ns&lt;/td&gt;
&lt;td&gt;374ns&lt;/td&gt;
&lt;td&gt;3.65us&lt;/td&gt;
&lt;td&gt;40.8us&lt;/td&gt;
&lt;td&gt;518us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;35.0ns&lt;/td&gt;
&lt;td&gt;58.1ns&lt;/td&gt;
&lt;td&gt;383ns&lt;/td&gt;
&lt;td&gt;3.87us&lt;/td&gt;
&lt;td&gt;43.2us&lt;/td&gt;
&lt;td&gt;479us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;34.3ns&lt;/td&gt;
&lt;td&gt;59.4ns&lt;/td&gt;
&lt;td&gt;371ns&lt;/td&gt;
&lt;td&gt;3.57us&lt;/td&gt;
&lt;td&gt;40.4us&lt;/td&gt;
&lt;td&gt;452us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;34.8ns&lt;/td&gt;
&lt;td&gt;59.7ns&lt;/td&gt;
&lt;td&gt;399ns&lt;/td&gt;
&lt;td&gt;3.78us&lt;/td&gt;
&lt;td&gt;43.1us&lt;/td&gt;
&lt;td&gt;547us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;32.3ns&lt;/td&gt;
&lt;td&gt;53.8ns&lt;/td&gt;
&lt;td&gt;297ns&lt;/td&gt;
&lt;td&gt;3.21us&lt;/td&gt;
&lt;td&gt;38.3us&lt;/td&gt;
&lt;td&gt;466us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;32.4ns&lt;/td&gt;
&lt;td&gt;59.8ns&lt;/td&gt;
&lt;td&gt;296ns&lt;/td&gt;
&lt;td&gt;3.31us&lt;/td&gt;
&lt;td&gt;38.2us&lt;/td&gt;
&lt;td&gt;475us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;32.0ns&lt;/td&gt;
&lt;td&gt;54.0ns&lt;/td&gt;
&lt;td&gt;333ns&lt;/td&gt;
&lt;td&gt;3.32us&lt;/td&gt;
&lt;td&gt;38.7us&lt;/td&gt;
&lt;td&gt;447us&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Even on the biggest vector, this is a very fast operation, once vectorized and
parallelized. At this speed, some of the differences observed may not be highly
significant. Again clang-based versions are the fastest versions on this code,
but by a small margin.  There also seems to be a slight regression in gcc-7.1,
but again quite small.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="matrix-matrix-multiplication-gemm"&gt;
&lt;h3&gt;Matrix Matrix multiplication (GEMM)&lt;/h3&gt;
&lt;p&gt;The next benchmark is testing the performance of a Matrix-Matrix Multiplication,
an operation known as GEMM in the BLAS nomenclature. In that case, we test both
the naive and the optimized vectorized implementation. To save some horizontal
space, I've split the tables in two.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="23%"&gt;
&lt;col width="12%"&gt;
&lt;col width="14%"&gt;
&lt;col width="15%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sgemm (std)&lt;/th&gt;
&lt;th class="head"&gt;10&lt;/th&gt;
&lt;th class="head"&gt;20&lt;/th&gt;
&lt;th class="head"&gt;40&lt;/th&gt;
&lt;th class="head"&gt;60&lt;/th&gt;
&lt;th class="head"&gt;80&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;7.04us&lt;/td&gt;
&lt;td&gt;50.15us&lt;/td&gt;
&lt;td&gt;356.42us&lt;/td&gt;
&lt;td&gt;1.18ms&lt;/td&gt;
&lt;td&gt;3.41ms&lt;/td&gt;
&lt;td&gt;5.56ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;8.14us&lt;/td&gt;
&lt;td&gt;74.77us&lt;/td&gt;
&lt;td&gt;513.64us&lt;/td&gt;
&lt;td&gt;1.72ms&lt;/td&gt;
&lt;td&gt;4.05ms&lt;/td&gt;
&lt;td&gt;7.92ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;8.03us&lt;/td&gt;
&lt;td&gt;64.78us&lt;/td&gt;
&lt;td&gt;504.41us&lt;/td&gt;
&lt;td&gt;1.69ms&lt;/td&gt;
&lt;td&gt;4.02ms&lt;/td&gt;
&lt;td&gt;7.87ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;7.95us&lt;/td&gt;
&lt;td&gt;65.00us&lt;/td&gt;
&lt;td&gt;508.84us&lt;/td&gt;
&lt;td&gt;1.69ms&lt;/td&gt;
&lt;td&gt;4.02ms&lt;/td&gt;
&lt;td&gt;7.84ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;3.58us&lt;/td&gt;
&lt;td&gt;28.59us&lt;/td&gt;
&lt;td&gt;222.36us&lt;/td&gt;
&lt;td&gt;0.73ms&lt;/td&gt;
&lt;td&gt;1.77us&lt;/td&gt;
&lt;td&gt;3.41ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;4.00us&lt;/td&gt;
&lt;td&gt;25.47us&lt;/td&gt;
&lt;td&gt;190.56us&lt;/td&gt;
&lt;td&gt;0.61ms&lt;/td&gt;
&lt;td&gt;1.45us&lt;/td&gt;
&lt;td&gt;2.80ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;4.00us&lt;/td&gt;
&lt;td&gt;25.38us&lt;/td&gt;
&lt;td&gt;189.98us&lt;/td&gt;
&lt;td&gt;0.60ms&lt;/td&gt;
&lt;td&gt;1.43us&lt;/td&gt;
&lt;td&gt;2.81ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="15%"&gt;
&lt;col width="9%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;col width="7%"&gt;
&lt;col width="7%"&gt;
&lt;col width="7%"&gt;
&lt;col width="7%"&gt;
&lt;col width="7%"&gt;
&lt;col width="8%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sgemm (std)&lt;/th&gt;
&lt;th class="head"&gt;200&lt;/th&gt;
&lt;th class="head"&gt;300&lt;/th&gt;
&lt;th class="head"&gt;400&lt;/th&gt;
&lt;th class="head"&gt;500&lt;/th&gt;
&lt;th class="head"&gt;600&lt;/th&gt;
&lt;th class="head"&gt;700&lt;/th&gt;
&lt;th class="head"&gt;800&lt;/th&gt;
&lt;th class="head"&gt;900&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;1200&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;44.16ms&lt;/td&gt;
&lt;td&gt;148.88ms&lt;/td&gt;
&lt;td&gt;455.81ms&lt;/td&gt;
&lt;td&gt;687.96ms&lt;/td&gt;
&lt;td&gt;1.47s&lt;/td&gt;
&lt;td&gt;1.98s&lt;/td&gt;
&lt;td&gt;2.81s&lt;/td&gt;
&lt;td&gt;4.00s&lt;/td&gt;
&lt;td&gt;5.91s&lt;/td&gt;
&lt;td&gt;9.52s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;63.17ms&lt;/td&gt;
&lt;td&gt;213.01ms&lt;/td&gt;
&lt;td&gt;504.83ms&lt;/td&gt;
&lt;td&gt;984.90ms&lt;/td&gt;
&lt;td&gt;1.70s&lt;/td&gt;
&lt;td&gt;2.70s&lt;/td&gt;
&lt;td&gt;4.03s&lt;/td&gt;
&lt;td&gt;5.74s&lt;/td&gt;
&lt;td&gt;7.87s&lt;/td&gt;
&lt;td&gt;14.905&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;64.04ms&lt;/td&gt;
&lt;td&gt;212.12ms&lt;/td&gt;
&lt;td&gt;502.95ms&lt;/td&gt;
&lt;td&gt;981.74ms&lt;/td&gt;
&lt;td&gt;1.69s&lt;/td&gt;
&lt;td&gt;2.69s&lt;/td&gt;
&lt;td&gt;4.13s&lt;/td&gt;
&lt;td&gt;5.85s&lt;/td&gt;
&lt;td&gt;8.10s&lt;/td&gt;
&lt;td&gt;14.08s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;62.57ms&lt;/td&gt;
&lt;td&gt;210.72ms&lt;/td&gt;
&lt;td&gt;499.68ms&lt;/td&gt;
&lt;td&gt;974.94ms&lt;/td&gt;
&lt;td&gt;1.68s&lt;/td&gt;
&lt;td&gt;2.67s&lt;/td&gt;
&lt;td&gt;3.99s&lt;/td&gt;
&lt;td&gt;5.68s&lt;/td&gt;
&lt;td&gt;7.85s&lt;/td&gt;
&lt;td&gt;13.49s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;27.48ms&lt;/td&gt;
&lt;td&gt;90.85ms&lt;/td&gt;
&lt;td&gt;219.34ms&lt;/td&gt;
&lt;td&gt;419.53ms&lt;/td&gt;
&lt;td&gt;0.72s&lt;/td&gt;
&lt;td&gt;1.18s&lt;/td&gt;
&lt;td&gt;1.90s&lt;/td&gt;
&lt;td&gt;2.44s&lt;/td&gt;
&lt;td&gt;3.36s&lt;/td&gt;
&lt;td&gt;5.84s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;22.01ms&lt;/td&gt;
&lt;td&gt;73.90ms&lt;/td&gt;
&lt;td&gt;175.02ms&lt;/td&gt;
&lt;td&gt;340.70ms&lt;/td&gt;
&lt;td&gt;0.58s&lt;/td&gt;
&lt;td&gt;0.93s&lt;/td&gt;
&lt;td&gt;1.40s&lt;/td&gt;
&lt;td&gt;1.98s&lt;/td&gt;
&lt;td&gt;2.79s&lt;/td&gt;
&lt;td&gt;4.69s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;22.33ms&lt;/td&gt;
&lt;td&gt;75.80ms&lt;/td&gt;
&lt;td&gt;181.27ms&lt;/td&gt;
&lt;td&gt;359.13ms&lt;/td&gt;
&lt;td&gt;0.63s&lt;/td&gt;
&lt;td&gt;1.02s&lt;/td&gt;
&lt;td&gt;1.52s&lt;/td&gt;
&lt;td&gt;2.24s&lt;/td&gt;
&lt;td&gt;3.21s&lt;/td&gt;
&lt;td&gt;5.62s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This time, the differences between the different compilers are very significant.
The clang compilers are leading the way by a large margin here, with clang-4.0
being the fastest of them (by another nice margin). Indeed, clang-4.0.1 is
producing code that is, on average, about twice faster than the code generated
by the best GCC compiler. Very interestingly as well, we can see a huge
regression starting from GCC-5.4 and that is still here in GCC-7.1. Indeed, the
best GCC version, in the tested versions, is again GCC-4.9.4. Clang is really
doing an excellent job of compiling the GEMM code.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="21%"&gt;
&lt;col width="14%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;col width="14%"&gt;
&lt;col width="14%"&gt;
&lt;col width="13%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sgemm (vec)&lt;/th&gt;
&lt;th class="head"&gt;10&lt;/th&gt;
&lt;th class="head"&gt;20&lt;/th&gt;
&lt;th class="head"&gt;40&lt;/th&gt;
&lt;th class="head"&gt;60&lt;/th&gt;
&lt;th class="head"&gt;80&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;264.27ns&lt;/td&gt;
&lt;td&gt;0.95us&lt;/td&gt;
&lt;td&gt;3.28us&lt;/td&gt;
&lt;td&gt;14.77us&lt;/td&gt;
&lt;td&gt;23.50us&lt;/td&gt;
&lt;td&gt;60.37us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;271.41ns&lt;/td&gt;
&lt;td&gt;0.99us&lt;/td&gt;
&lt;td&gt;3.31us&lt;/td&gt;
&lt;td&gt;14.811us&lt;/td&gt;
&lt;td&gt;24.116us&lt;/td&gt;
&lt;td&gt;61.00us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;279.72ns&lt;/td&gt;
&lt;td&gt;1.02us&lt;/td&gt;
&lt;td&gt;3.27us&lt;/td&gt;
&lt;td&gt;15.39us&lt;/td&gt;
&lt;td&gt;24.29us&lt;/td&gt;
&lt;td&gt;61.99us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;273.74ns&lt;/td&gt;
&lt;td&gt;0.96us&lt;/td&gt;
&lt;td&gt;3.81us&lt;/td&gt;
&lt;td&gt;15.55us&lt;/td&gt;
&lt;td&gt;31.35us&lt;/td&gt;
&lt;td&gt;71.11us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;296.67ns&lt;/td&gt;
&lt;td&gt;1.34us&lt;/td&gt;
&lt;td&gt;4.18us&lt;/td&gt;
&lt;td&gt;19.93us&lt;/td&gt;
&lt;td&gt;33.15us&lt;/td&gt;
&lt;td&gt;82.60us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;322.68ns&lt;/td&gt;
&lt;td&gt;1.38us&lt;/td&gt;
&lt;td&gt;4.17us&lt;/td&gt;
&lt;td&gt;20.19us&lt;/td&gt;
&lt;td&gt;34.17us&lt;/td&gt;
&lt;td&gt;83.64us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;307.49ns&lt;/td&gt;
&lt;td&gt;1.41us&lt;/td&gt;
&lt;td&gt;4.10us&lt;/td&gt;
&lt;td&gt;19.72us&lt;/td&gt;
&lt;td&gt;33.72us&lt;/td&gt;
&lt;td&gt;84.80us&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="14%"&gt;
&lt;col width="10%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sgemm (vec)&lt;/th&gt;
&lt;th class="head"&gt;200&lt;/th&gt;
&lt;th class="head"&gt;300&lt;/th&gt;
&lt;th class="head"&gt;400&lt;/th&gt;
&lt;th class="head"&gt;500&lt;/th&gt;
&lt;th class="head"&gt;600&lt;/th&gt;
&lt;th class="head"&gt;700&lt;/th&gt;
&lt;th class="head"&gt;800&lt;/th&gt;
&lt;th class="head"&gt;900&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;1200&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;369.52us&lt;/td&gt;
&lt;td&gt;1.62ms&lt;/td&gt;
&lt;td&gt;2.91ms&lt;/td&gt;
&lt;td&gt;7.17ms&lt;/td&gt;
&lt;td&gt;11.74ms&lt;/td&gt;
&lt;td&gt;22.91ms&lt;/td&gt;
&lt;td&gt;34.82ms&lt;/td&gt;
&lt;td&gt;51.67ms&lt;/td&gt;
&lt;td&gt;64.36ms&lt;/td&gt;
&lt;td&gt;111.15ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;387.54us&lt;/td&gt;
&lt;td&gt;1.60ms&lt;/td&gt;
&lt;td&gt;2.97ms&lt;/td&gt;
&lt;td&gt;7.36ms&lt;/td&gt;
&lt;td&gt;12.11ms&lt;/td&gt;
&lt;td&gt;24.37ms&lt;/td&gt;
&lt;td&gt;35.37ms&lt;/td&gt;
&lt;td&gt;52.27ms&lt;/td&gt;
&lt;td&gt;65.72ms&lt;/td&gt;
&lt;td&gt;112.74ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;384.43us&lt;/td&gt;
&lt;td&gt;1.74ms&lt;/td&gt;
&lt;td&gt;3.12ms&lt;/td&gt;
&lt;td&gt;7.16ms&lt;/td&gt;
&lt;td&gt;12.44ms&lt;/td&gt;
&lt;td&gt;24.15ms&lt;/td&gt;
&lt;td&gt;34.87ms&lt;/td&gt;
&lt;td&gt;52.59ms&lt;/td&gt;
&lt;td&gt;70.074ms&lt;/td&gt;
&lt;td&gt;119.22ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;458.05us&lt;/td&gt;
&lt;td&gt;1.81ms&lt;/td&gt;
&lt;td&gt;3.44ms&lt;/td&gt;
&lt;td&gt;7.86ms&lt;/td&gt;
&lt;td&gt;13.43ms&lt;/td&gt;
&lt;td&gt;24.70ms&lt;/td&gt;
&lt;td&gt;36.54ms&lt;/td&gt;
&lt;td&gt;53.47ms&lt;/td&gt;
&lt;td&gt;66.87ms&lt;/td&gt;
&lt;td&gt;117.25ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;494.52us&lt;/td&gt;
&lt;td&gt;1.96ms&lt;/td&gt;
&lt;td&gt;4.80ms&lt;/td&gt;
&lt;td&gt;8.88ms&lt;/td&gt;
&lt;td&gt;18.20ms&lt;/td&gt;
&lt;td&gt;29.37ms&lt;/td&gt;
&lt;td&gt;41.24ms&lt;/td&gt;
&lt;td&gt;60.72ms&lt;/td&gt;
&lt;td&gt;72.28ms&lt;/td&gt;
&lt;td&gt;123.75ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;511.24us&lt;/td&gt;
&lt;td&gt;2.04ms&lt;/td&gt;
&lt;td&gt;4.11ms&lt;/td&gt;
&lt;td&gt;9.46ms&lt;/td&gt;
&lt;td&gt;15.34ms&lt;/td&gt;
&lt;td&gt;27.23ms&lt;/td&gt;
&lt;td&gt;38.27ms&lt;/td&gt;
&lt;td&gt;58.14ms&lt;/td&gt;
&lt;td&gt;72.78ms&lt;/td&gt;
&lt;td&gt;128.60ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;492.28us&lt;/td&gt;
&lt;td&gt;2.03ms&lt;/td&gt;
&lt;td&gt;3.90ms&lt;/td&gt;
&lt;td&gt;9.00ms&lt;/td&gt;
&lt;td&gt;14.31ms&lt;/td&gt;
&lt;td&gt;25.72ms&lt;/td&gt;
&lt;td&gt;37.09ms&lt;/td&gt;
&lt;td&gt;55.79ms&lt;/td&gt;
&lt;td&gt;67.88ms&lt;/td&gt;
&lt;td&gt;119.92ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As for the optimized version, it seems that the two families are reversed.
Indeed, GCC is doing a better job than clang here, and although the margin is
not as big as before, it's still significant. We can still observe a small
regression in GCC versions because the 4.9 version is again the fastest. As for
clang versions, it seems that clang-5.0 (used in zapcc) has had some performance
improvements for this case.&lt;/p&gt;
&lt;p&gt;For this case of matrix-matrix multiplication, it's very impressive that the
differences in the non-optimized code are so significant. And it's also
impressive that each family of compilers has its own strength, clang being
seemingly much better at handling unoptimized code while GCC is better at
handling vectorized code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="convolution-2d"&gt;
&lt;h3&gt;Convolution (2D)&lt;/h3&gt;
&lt;p&gt;The last benchmark that I considered is the case of the valid convolution on 2D
images. The code is quite similar to the GEMM code but more complicated to
optimized due to cache locality.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="19%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sconv2_valid (std)&lt;/th&gt;
&lt;th class="head"&gt;100x50&lt;/th&gt;
&lt;th class="head"&gt;105x50&lt;/th&gt;
&lt;th class="head"&gt;110x55&lt;/th&gt;
&lt;th class="head"&gt;115x55&lt;/th&gt;
&lt;th class="head"&gt;120x60&lt;/th&gt;
&lt;th class="head"&gt;125x60&lt;/th&gt;
&lt;th class="head"&gt;130x65&lt;/th&gt;
&lt;th class="head"&gt;135x65&lt;/th&gt;
&lt;th class="head"&gt;140x70&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;27.93ms&lt;/td&gt;
&lt;td&gt;33.68ms&lt;/td&gt;
&lt;td&gt;40.62ms&lt;/td&gt;
&lt;td&gt;48.23ms&lt;/td&gt;
&lt;td&gt;57.27ms&lt;/td&gt;
&lt;td&gt;67.02ms&lt;/td&gt;
&lt;td&gt;78.45ms&lt;/td&gt;
&lt;td&gt;92.53ms&lt;/td&gt;
&lt;td&gt;105.08ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;37.60ms&lt;/td&gt;
&lt;td&gt;44.94ms&lt;/td&gt;
&lt;td&gt;54.24ms&lt;/td&gt;
&lt;td&gt;64.45ms&lt;/td&gt;
&lt;td&gt;76.63ms&lt;/td&gt;
&lt;td&gt;89.75ms&lt;/td&gt;
&lt;td&gt;105.08ms&lt;/td&gt;
&lt;td&gt;121.66ms&lt;/td&gt;
&lt;td&gt;140.95ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;37.10ms&lt;/td&gt;
&lt;td&gt;44.99ms&lt;/td&gt;
&lt;td&gt;54.34ms&lt;/td&gt;
&lt;td&gt;64.54ms&lt;/td&gt;
&lt;td&gt;76.54ms&lt;/td&gt;
&lt;td&gt;89.87ms&lt;/td&gt;
&lt;td&gt;105.35ms&lt;/td&gt;
&lt;td&gt;121.94ms&lt;/td&gt;
&lt;td&gt;141.20ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;37.55ms&lt;/td&gt;
&lt;td&gt;45.08ms&lt;/td&gt;
&lt;td&gt;54.39ms&lt;/td&gt;
&lt;td&gt;64.48ms&lt;/td&gt;
&lt;td&gt;76.51ms&lt;/td&gt;
&lt;td&gt;92.02ms&lt;/td&gt;
&lt;td&gt;106.16ms&lt;/td&gt;
&lt;td&gt;125.67ms&lt;/td&gt;
&lt;td&gt;143.57ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;15.42ms&lt;/td&gt;
&lt;td&gt;18.59ms&lt;/td&gt;
&lt;td&gt;22.21ms&lt;/td&gt;
&lt;td&gt;26.40ms&lt;/td&gt;
&lt;td&gt;31.03ms&lt;/td&gt;
&lt;td&gt;36.26ms&lt;/td&gt;
&lt;td&gt;42.35ms&lt;/td&gt;
&lt;td&gt;48.87ms&lt;/td&gt;
&lt;td&gt;56.29ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;15.48ms&lt;/td&gt;
&lt;td&gt;18.67ms&lt;/td&gt;
&lt;td&gt;22.34ms&lt;/td&gt;
&lt;td&gt;26.50ms&lt;/td&gt;
&lt;td&gt;31.27ms&lt;/td&gt;
&lt;td&gt;36.58ms&lt;/td&gt;
&lt;td&gt;42.61ms&lt;/td&gt;
&lt;td&gt;49.33ms&lt;/td&gt;
&lt;td&gt;56.80ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;15.29ms&lt;/td&gt;
&lt;td&gt;18.37ms&lt;/td&gt;
&lt;td&gt;22.00ms&lt;/td&gt;
&lt;td&gt;26.10ms&lt;/td&gt;
&lt;td&gt;30.75ms&lt;/td&gt;
&lt;td&gt;35.95ms&lt;/td&gt;
&lt;td&gt;41.85ms&lt;/td&gt;
&lt;td&gt;48.42ms&lt;/td&gt;
&lt;td&gt;55.74ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In that case, we can observe the same as for the GEMM. The clang-based versions
are much producing significantly faster code than the GCC versions. Moreover, we
can also observe the same large regression starting from GCC-5.4.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="21%"&gt;
&lt;col width="11%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sconv2_valid (vec)&lt;/th&gt;
&lt;th class="head"&gt;100x50&lt;/th&gt;
&lt;th class="head"&gt;105x50&lt;/th&gt;
&lt;th class="head"&gt;110x55&lt;/th&gt;
&lt;th class="head"&gt;115x55&lt;/th&gt;
&lt;th class="head"&gt;120x60&lt;/th&gt;
&lt;th class="head"&gt;125x60&lt;/th&gt;
&lt;th class="head"&gt;130x65&lt;/th&gt;
&lt;th class="head"&gt;135x65&lt;/th&gt;
&lt;th class="head"&gt;140x70&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;878.32us&lt;/td&gt;
&lt;td&gt;1.07ms&lt;/td&gt;
&lt;td&gt;1.20ms&lt;/td&gt;
&lt;td&gt;1.68ms&lt;/td&gt;
&lt;td&gt;2.04ms&lt;/td&gt;
&lt;td&gt;2.06ms&lt;/td&gt;
&lt;td&gt;2.54ms&lt;/td&gt;
&lt;td&gt;3.20ms&lt;/td&gt;
&lt;td&gt;4.14ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;853.73us&lt;/td&gt;
&lt;td&gt;1.03ms&lt;/td&gt;
&lt;td&gt;1.15ms&lt;/td&gt;
&lt;td&gt;1.36ms&lt;/td&gt;
&lt;td&gt;1.76ms&lt;/td&gt;
&lt;td&gt;2.05ms&lt;/td&gt;
&lt;td&gt;2.44ms&lt;/td&gt;
&lt;td&gt;2.91ms&lt;/td&gt;
&lt;td&gt;3.13ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;847.95us&lt;/td&gt;
&lt;td&gt;1.02ms&lt;/td&gt;
&lt;td&gt;1.14ms&lt;/td&gt;
&lt;td&gt;1.35ms&lt;/td&gt;
&lt;td&gt;1.74ms&lt;/td&gt;
&lt;td&gt;1.98ms&lt;/td&gt;
&lt;td&gt;2.43ms&lt;/td&gt;
&lt;td&gt;2.90ms&lt;/td&gt;
&lt;td&gt;3.12ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;795.82us&lt;/td&gt;
&lt;td&gt;0.93ms&lt;/td&gt;
&lt;td&gt;1.05ms&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.60ms&lt;/td&gt;
&lt;td&gt;1.77ms&lt;/td&gt;
&lt;td&gt;2.20ms&lt;/td&gt;
&lt;td&gt;2.69ms&lt;/td&gt;
&lt;td&gt;2.81ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;782.46us&lt;/td&gt;
&lt;td&gt;0.93ms&lt;/td&gt;
&lt;td&gt;1.05ms&lt;/td&gt;
&lt;td&gt;1.26ms&lt;/td&gt;
&lt;td&gt;1.60ms&lt;/td&gt;
&lt;td&gt;1.84ms&lt;/td&gt;
&lt;td&gt;2.21ms&lt;/td&gt;
&lt;td&gt;2.65ms&lt;/td&gt;
&lt;td&gt;2.84ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;767.58us&lt;/td&gt;
&lt;td&gt;0.92ms&lt;/td&gt;
&lt;td&gt;1.04ms&lt;/td&gt;
&lt;td&gt;1.25ms&lt;/td&gt;
&lt;td&gt;1.59ms&lt;/td&gt;
&lt;td&gt;1.83ms&lt;/td&gt;
&lt;td&gt;2.20ms&lt;/td&gt;
&lt;td&gt;2.62ms&lt;/td&gt;
&lt;td&gt;2.83ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;782.49us&lt;/td&gt;
&lt;td&gt;0.94ms&lt;/td&gt;
&lt;td&gt;1.06ms&lt;/td&gt;
&lt;td&gt;1.27ms&lt;/td&gt;
&lt;td&gt;1.62ms&lt;/td&gt;
&lt;td&gt;1.83ms&lt;/td&gt;
&lt;td&gt;2.24ms&lt;/td&gt;
&lt;td&gt;2.65ms&lt;/td&gt;
&lt;td&gt;2.85ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This time, clang manages to produce excellent results. Indeed, all the produced
executables are significantly faster than the versions produced by GCC, except
for GCC-7.1 which is producing similar results. The other versions of GCC are
falling behind it seems. It seems that it was only for the GEMM that clang was
having a lot of troubles handling the optimized code.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Clang seems to have recently done a lot of optimizations regarding compilation
time. Indeed, clang-4.0.1 is much faster for compilation than clang-3.9.
Although GCC-7.1 is faster than GCC-6.3, all the GCC versions are slower than
GCC-4.9.4 which is the fastest at compiling code with optimizations. GCC-7.1 is
the fastest GCC version for compiling code in debug mode.&lt;/p&gt;
&lt;p&gt;In some cases, there is almost no difference between different compilers in the
generated code. However, in more  complex algorithms such as the matrix-matrix
multiplication or the two-dimensional convolution, the differences can be quite
significant. In my tests, Clang have shown to be much better at compiling
unoptimized code. However, and especially in the GEMM case, it seems to be worse
than GCC at handling hand-optimized. I will investigate that case and try to
tailor the code so that clang is having a better time with it.&lt;/p&gt;
&lt;p&gt;For me, it's really weird that the GCC regression, apparently starting from
GCC-5.4, has still not been fixed in GCC 7.1. I was thinking of dropping support
for GCC-4.9 in order to go full C++14 support, but now I may have to reconsider
my position. However, seeing that GCC is generally the best at handling
optimized code (especially for GEMM), I may be able to do the transition, since
the optimized code will be used in most cases.&lt;/p&gt;
&lt;p&gt;As for zapcc, although it is still the fastest compiler in debug mode, with the
new speed of clang-4.0.1, its margin is quite small. Moreover, on optimized
build, it's not as fast as GCC. If you use clang and can have access to zapcc,
it's still quite a good option to save some time.&lt;/p&gt;
&lt;p&gt;Overall, I have been quite pleased by clang-4.0.1 and GCC-7.1, the most recent
versions I have been testing. It seems that they did quite some good work.
I will definitely run some more tests with them and try to adapt the code. I'm
still considering whether I will drop support for some older compilers.&lt;/p&gt;
&lt;p&gt;I hope this comparison was interesting :) My next post will probably be about
the difference in performance between my machine learning framework and other
frameworks to train neural networks.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>C++14</category><category>clang</category><category>Compilers</category><category>etl</category><category>gcc</category><category>Performance</category><category>projects</category><guid>https://baptiste-wicht.com/posts/2017/08/compiler-benchmark-gcc-clang-cpp-library-etl.html</guid><pubDate>Mon, 07 Aug 2017 07:16:21 GMT</pubDate></item></channel></rss>