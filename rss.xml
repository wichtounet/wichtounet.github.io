<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Blog blog("Baptiste Wicht");</title><link>http://baptiste-wicht.com/</link><description>Tutorials and short posts about programming, C++, Java, Assembly, Operating Systems Development, Compilers, ...</description><atom:link type="application/rss+xml" href="http://baptiste-wicht.com/rss.xml" rel="self"></atom:link><language>en</language><lastBuildDate>Tue, 17 Oct 2017 18:59:50 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>DLL New Features: Embeddings and Merge layers</title><link>http://baptiste-wicht.com/posts/2017/10/dll-new-features-embeddings-and-merge-layers.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've just finished integrating new features into DLL, my deep learning library.
I've added support for an embeddings layer, a group layer and a merge layer.
This is not yet released, but available in the master branch.&lt;/p&gt;
&lt;p&gt;Embeddings are used more and more these days to learn dense representation of
characters or word. An embedding layer in a neural network transform labels into
a vector. It's generally used as the first layer of the network. The embedding
are learned as part of the network.&lt;/p&gt;
&lt;p&gt;The merge layer allows to create branches in the network. The input is passed to
each sub layer and then the output of each layer is concatenated to form the
output of the merged layers. This can be very useful to use different
convolutional filter sizes.&lt;/p&gt;
&lt;p&gt;The group layer is a simple utility to group layers together. This is mostly to
use with merge layers to form several branches.&lt;/p&gt;
&lt;p&gt;I've put together a new example to use these features on text classification.
The dataset is totally synthetic for now, but this can easily be reproduced with
a normal text classification dataset. This kind of model is called a Character
Convolutional Neural Network.&lt;/p&gt;
&lt;p&gt;Here is the code for example:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// The length of the embedding vector&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;    &lt;span class="c1"&gt;// The word (or sequence) length&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;embedding_network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_network_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;network_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-6"&gt;&lt;/a&gt;        &lt;span class="c1"&gt;// The embedding layer&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;embedding_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-9"&gt;&lt;/a&gt;        &lt;span class="c1"&gt;// The convolutional layers&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-10"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;merge_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-11"&gt;&lt;/a&gt;            &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-12"&gt;&lt;/a&gt;            &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;group_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-13"&gt;&lt;/a&gt;                  &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-14"&gt;&lt;/a&gt;                &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_2d_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-15"&gt;&lt;/a&gt;            &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-16"&gt;&lt;/a&gt;            &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;group_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-17"&gt;&lt;/a&gt;                  &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-18"&gt;&lt;/a&gt;                &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_2d_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-19"&gt;&lt;/a&gt;            &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-20"&gt;&lt;/a&gt;            &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;group_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-21"&gt;&lt;/a&gt;                  &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;embedding&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-22"&gt;&lt;/a&gt;                &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_2d_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-23"&gt;&lt;/a&gt;            &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-24"&gt;&lt;/a&gt;        &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-25"&gt;&lt;/a&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-26"&gt;&lt;/a&gt;        &lt;span class="c1"&gt;// The final softmax layer&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-27"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_layer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;48&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-28"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-29"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;NADAM&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;     &lt;span class="c1"&gt;// Nesterov Adam (NADAM)&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-30"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;                        &lt;span class="c1"&gt;// The mini-batch size&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-31"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;                               &lt;span class="c1"&gt;// Shuffle before each epoch&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-32"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-33"&gt;&lt;/a&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-34"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;embedding_network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-35"&gt;&lt;/a&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-36"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Display the network and dataset&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-37"&gt;&lt;/a&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-38"&gt;&lt;/a&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-39"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Train the network for performance sake&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-40"&gt;&lt;/a&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-41"&gt;&lt;/a&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-42"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Test the network on train set&lt;/span&gt;
&lt;a name="rest_code_6260278376ea46fa920f56df3fe99fae-43"&gt;&lt;/a&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The network starts with an embedding layer. The embedding is then passed to
three convolutional layers with different filter sizes, each followed by
a pooling layer. The outputs of the three layers are merged at the end of the
merge layer. Finally, a softmax layer is used for classification.&lt;/p&gt;
&lt;p&gt;This kind of model can be very powerful and is used regularly. These new
features make for a much larger variety of models that can be build with the DLL
library.&lt;/p&gt;
&lt;p&gt;The full code with the dataset generation can be found online:
&lt;a class="reference external" href="https://github.com/wichtounet/dll/blob/master/examples/src/char_cnn.cpp"&gt;char_cnn.cpp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The next feature I want to focus on is recurrent neural networks. I'll probably
try a single RNN layer first and then upgrade to multi-layers and LSTM and maybe
GRU.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>deep learning</category><category>dll</category><category>Machine Learning</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/10/dll-new-features-embeddings-and-merge-layers.html</guid><pubDate>Tue, 17 Oct 2017 17:50:40 GMT</pubDate></item><item><title>I successfully defended my Ph.D.</title><link>http://baptiste-wicht.com/posts/2017/10/i-successfully-defended-my-phd.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I'm happy to announce that I've successfully defended my thesis "Deep Learning
Features for Image Processing". After four years, I've defended it officially in
front of the thesis committed last Friday and then again two days ago I've
successfully publicly defended in front of my friends, family and colleagues.&lt;/p&gt;
&lt;p&gt;I'm now a "Doctor of Philosophy in Computer Science :)&lt;/p&gt;
&lt;p&gt;I will update my thesis with the last comments in November and send the final
version to the university. At which point, I'll publish it on this website as
well.&lt;/p&gt;&lt;/div&gt;</description><category>Machine Learning</category><category>Personal</category><category>thesis</category><guid>http://baptiste-wicht.com/posts/2017/10/i-successfully-defended-my-phd.html</guid><pubDate>Sun, 15 Oct 2017 15:16:29 GMT</pubDate></item><item><title>Budgetwarrior: Track assets and portfolio, savings rates and auto-completion</title><link>http://baptiste-wicht.com/posts/2017/10/budgetwarrior-track-assets-portfolio-savings-rates-auto-completion.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;This last month, I've been reading quite a few blogs about personal finance and
I've decided to integrate more features into budgetwarrior. This post is about
three new features that I've integrated. It's not yet a new release, so if you
want to test this version, you'll have to compile it from the &lt;em&gt;master&lt;/em&gt; branch on
Git.&lt;/p&gt;
&lt;p&gt;As it was last time, the values on my screenshots have all been randomized.&lt;/p&gt;
&lt;p&gt;If you have several assets with different distributions, I believe it is a great
value to have them all shown at the same time. Especially if you want to change
the distribution of your portfolio or if you plan big changes in it.&lt;/p&gt;
&lt;div class="section" id="track-assets"&gt;
&lt;h2&gt;Track assets&lt;/h2&gt;
&lt;p&gt;The first feature I've added is a feature to precisely track each of your assets
independently. And you can also track the allocation of your portfolio in terms
of stocks, bonds and cash. The tool also lets you set the desired distribution
of your assets and will compute the difference that you should make in order to
comply to your desired distribution.&lt;/p&gt;
&lt;p&gt;First, you need to define all your asset classes (your accounts, funds, and
stocks, ...) and their distribution with &lt;code&gt;budget asset add&lt;/code&gt;. It also
supports to set a currency. The default currency is now CHF, but you can set it
in the configuration file, for instance &lt;code&gt;default_currency=USD&lt;/code&gt;. You can
see your assets using &lt;code&gt;budget asset&lt;/code&gt;:&lt;/p&gt;
&lt;img alt="View of your assets" src="http://baptiste-wicht.com/images/budgetwarrior_assets.png"&gt;
&lt;p&gt;You can then set the value of your assets using &lt;code&gt;budget asset value add&lt;/code&gt;.
The system will save all the values of your assets. For now, only the last value
is used in the application to display. In the future, I plan to add new reports
for evolution of the portfolio over time. You can see your current net worth
with the &lt;code&gt;budget asset value&lt;/code&gt;:&lt;/p&gt;
&lt;img alt="View of your portfolio" src="http://baptiste-wicht.com/images/budgetwarrior_asset_values.png"&gt;
&lt;p&gt;The different currencies will all be converted to the default currency.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="savings-rate"&gt;
&lt;h2&gt;Savings rate&lt;/h2&gt;
&lt;p&gt;The second change I did is to compute the savings rate of each month and year.
The savings rate is simply the portion of your income that you are able to save
each month. The savings rate for a year is simple the average of the savings
rate of each month.&lt;/p&gt;
&lt;p&gt;The savings rate of the month can be seen with &lt;code&gt;budget overview month&lt;/code&gt;:&lt;/p&gt;
&lt;img alt="Savings rate of the month" src="http://baptiste-wicht.com/images/budgetwarrior_savings_rate.png"&gt;
&lt;p&gt;The saving rates of each month can also be seen in the overview of the year with
&lt;code&gt;budget overview year&lt;/code&gt;:&lt;/p&gt;
&lt;img alt="Savings rate of the year" src="http://baptiste-wicht.com/images/budgetwarrior_savings_rate_year.png"&gt;
&lt;p&gt;This shows the savings rate of each month, the average of the year and the
average of the current year up to the current month.&lt;/p&gt;
&lt;p&gt;The savings rate is a very important metric of your budget. In my case, it's
currently way too low and made me realize I really need to save more. Any
savings rate below 10% is too low. There are no rule as too much it should be,
but I'd like to augment mine to at least 20% next year.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="auto-completion"&gt;
&lt;h2&gt;Auto-completion&lt;/h2&gt;
&lt;p&gt;The last feature is mostly some quality-of-life improvement. Some of the inputs
in the console can now be completed. It's not really auto-completion per se, but
you can cycle through the list of possible values using the UP and DOWN.&lt;/p&gt;
&lt;p&gt;This makes it much easier to set some values such as asset names (in
&lt;code&gt;budget asset value add&lt;/code&gt; for instance), account names and objective types
and sources. I'm trying to make the input of values easier.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I don't know exactly what else will be integrated in this feature, but I may
already improve some visualization for asset values. If I learn something new
about personal finance that I may integrate in the tool, I'll do it as well.&lt;/p&gt;
&lt;p&gt;If you are interested by the sources or want to install this version,
you can download them on Github:
&lt;a class="reference external" href="https://github.com/wichtounet/budgetwarrior"&gt;budgetwarrior&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The new features are in the &lt;em&gt;master&lt;/em&gt; branch.&lt;/p&gt;
&lt;p&gt;If you have a suggestion for a new features or you found a bug, please post an
issue on Github, I'd be glad to help you.&lt;/p&gt;
&lt;p&gt;If you have any comment, don't hesitate to contact me, either by letting a
comment on this post or by email.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>budgetwarrior</category><category>C++</category><category>Linux</category><guid>http://baptiste-wicht.com/posts/2017/10/budgetwarrior-track-assets-portfolio-savings-rates-auto-completion.html</guid><pubDate>Thu, 12 Oct 2017 17:40:14 GMT</pubDate></item><item><title>Deep Learning Library 1.0 - Fast Neural Network Library</title><link>http://baptiste-wicht.com/posts/2017/10/deep-learning-library-10-fast-neural-network-library.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;img alt="DLL Logo" class="align-center" src="http://baptiste-wicht.com/images/dll_logo.png"&gt;
&lt;p&gt;I'm very happy to announce the release of the first version of Deep Learning
Library (DLL) 1.0. DLL is a neural network library with a focus on speed and
ease of use.&lt;/p&gt;
&lt;p&gt;I started working on this library about 4 years ago for my Ph.D. thesis.
I needed a good library to train and use Restricted Boltzmann Machines (RBMs)
and at this time there was no good support for it. Therefore, I decided to write
my own. It now has very complete support for the RBM and the Convolutional RBM
(CRBM) models. Stacks of RBMs (or Deep Belief Networks (DBNs)) can be pretrained
using Contrastive Divergence and then either fine-tuned with mini-batch gradient
descent or Conjugate Gradient or used as a feature extractor. Over the years,
the library has been extended to handle Artificial Neural Networks (ANNs) and
Convolutional Neural Networks (CNNs). The network is also able to train regular
auto-encoders. Several advanced layers such as Dropout or Batch Normalization
are also available as well as adaptive learning rates techniques such as
Adadelta and Adam. The library also has integrated support for a few datasets:
MNIST, CIFAR-10 and ImageNet.&lt;/p&gt;
&lt;p&gt;This library can be used using a C++ interface. The library is fully
header-only. It requires a C++14 compiler, which means a minimum of clang 3.9 or
GCC 6.3.&lt;/p&gt;
&lt;p&gt;In this post, I'm going to present a few examples on using the library and give
some information about the performance of the library and the roadmap for the
project.&lt;/p&gt;
&lt;p class="more"&gt;&lt;a href="http://baptiste-wicht.com/posts/2017/10/deep-learning-library-10-fast-neural-network-library.html"&gt;Read more…&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>dll</category><category>etl</category><category>GPU</category><category>Machine Learning</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2017/10/deep-learning-library-10-fast-neural-network-library.html</guid><pubDate>Sat, 07 Oct 2017 13:42:16 GMT</pubDate></item><item><title>Expression Templates Library (ETL) 1.2 - Complete GPU support</title><link>http://baptiste-wicht.com/posts/2017/10/expression-templates-library-etl-1-2-complete-gpu-support.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;img alt="ETL Logo" class="align-center" src="http://baptiste-wicht.com/images/logo.png"&gt;
&lt;p&gt;I'm happy to announce the version 1.2 of my Expression Templates Library (ETL):
ETL 1.2, two months after &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/08/expression-templates-library-etl-11.html"&gt;I released the version 1.1&lt;/a&gt;.
This version features much better GPU Support, a few new features and a lot of
changes in the internal code.&lt;/p&gt;
&lt;div class="section" id="gpu-support"&gt;
&lt;h2&gt;GPU Support&lt;/h2&gt;
&lt;p&gt;Before, only algorithms such as 4D convolution or matrix-matrix multiplication
were computed in the GPU and lots of operations were causing copies between CPU
and GPU version. Now, the support for basic operations has also been completed
and therefore, expressions like this:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_e97eaba5087f47d0aee5f7ce1c6431bc-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Can be computed entirely on GPU.&lt;/p&gt;
&lt;p&gt;Each matrix and vector containers have a secondary GPU memory space.  During the
execution, the status of both memory spaces is being managed and when necessary,
copies are made between two spaces. In the best case, there should only be
initial copies to the GPU and then everything should be done on the GPU. I've
also considered using Unified Memory in place of this system, but this is
a problem for fast matrix and I'd rather not have two different systems.&lt;/p&gt;
&lt;p&gt;If you have an expression such as &lt;code&gt;c = a + b * 2&lt;/code&gt;, it can be entirely computed
on GPU, however, it will be computed in two GPU operations such as:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_850c1f8548ae4ab89679a31d2e62b2d6-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;a name="rest_code_850c1f8548ae4ab89679a31d2e62b2d6-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;t1&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is not perfect in terms of performance but this will be done without any
copies between CPU and GPU memory. I plan to improve this system with a bit more
complex operations to avoid too many GPU operations, but there will always be
more operations than in CPU where this can easily be done in one go.&lt;/p&gt;
&lt;p&gt;There are a few expressions that are not computable on the GPU, such as random
generations. A few transformations are also not fully compatible with GPU.
Moreover, if you access an element with operators &lt;code&gt;[]&lt;/code&gt; or &lt;code&gt;()&lt;/code&gt;, this
will invalidate the GPU memory and force an update to the CPU memory.&lt;/p&gt;
&lt;p&gt;GPU operations are not implemented directly in ETL, there are coming from
various libraries. ETL is using NVIDIA CUDNN, CUFFT and CUDNN for most
algorithms. Moreover, for other operations, I've implemented a libraries with
simple GPU operations: ETL-GPU-BLAS (EGBLAS). You can have a look at
&lt;a class="reference external" href="https://github.com/wichtounet/etl-gpu-blas"&gt;egblas&lt;/a&gt; if you are interested.&lt;/p&gt;
&lt;p&gt;My Deep Learning Library (DLL) project is based on ETL and its performances are
mostly dependent on ETL's performances. Now that ETL fully supports GPU, the
GPU performance of DLL is much improved. You may remember a few weeks ago
I posted &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/08/dll-blazing-fast-neural-network-library.html"&gt;very high CPU performance of DLL&lt;/a&gt;.
Now, I've run again the tests to see the GPU performance with DLL. Here is the
performance for training a small CNN on the MNIST data set:&lt;/p&gt;
&lt;img alt="Performances for training a Convolutional Neural Network on MNIST" class="align-center" src="http://baptiste-wicht.com/images/etl_12_dll_gpu_mnist.png"&gt;
&lt;p&gt;As you can see, the performances on GPU are now excellent. DLL's performances
are on par with Tensorflow and Keras!&lt;/p&gt;
&lt;p&gt;The next results are for training a much larger CNN on ImageNet, with the time
necessary to train a single batch:&lt;/p&gt;
&lt;img alt="Performances for training a Convolutional Neural Network on Imagenet" class="align-center" src="http://baptiste-wicht.com/images/etl_12_dll_gpu_imagenet.png"&gt;
&lt;p&gt;Again, using the new version of ETL inside DLL has led to excellent performance.
The framework is again on par with TensorFlow and Keras and faster than all the
other frameworks. The large difference between DLL and Tensorflow and Keras is
due to the inefficiency of reading the dataset in the two frameworks, so the
performance of the three framework themselves are about the same.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="other-changes"&gt;
&lt;h2&gt;Other Changes&lt;/h2&gt;
&lt;p&gt;The library also has a few other new features. Logarithms of base 2 and base 10
are now supported in complement to the base e that was already available before.
Categorical Cross Entropy (CCE) computation is also available now, the CCE loss
and error can be computed for one or many samples. Convolutions have also been
improved in that you can use mixed types in both the image and the kernel and
different storage order as well. Nevertheless, the most optimized version
remains the version with the same storage order and the same data type.&lt;/p&gt;
&lt;p&gt;I've also made a major change in the way implementations are selected for each
operation. The tests and the benchmark are using a system to force the selection
of an algorithm. This system is now disabled by default. This makes the
compilation much faster by default. Since it's not necessary in most cases, this
will help regular use cases of the library by compiling much faster.&lt;/p&gt;
&lt;p&gt;Overall, the support for complex numbers has been improved in ETL. There are
more routines that are supported and &lt;code&gt;etl::complex&lt;/code&gt; is better supported
throughout the code. I'll still work on this in the future to make it totally
complete.&lt;/p&gt;
&lt;p&gt;The internal code also has a few new changes. First, all traits have been
rewritten to use variable templates instead of struct traits. This makes the
code much nicer in my opinion. Moreover, I've started experimenting with C++17
&lt;code&gt;if constexpr&lt;/code&gt;. Most of the if conditions that can be transformed to if
constexpr have been annotated with comments that I can quickly enable or disable
so that I can test the impact of C++17, especially on compilation time.&lt;/p&gt;
&lt;p&gt;Finally, a few bugs have been fixed. ETL is now working better with parallel
BLAS library. There should not be issues with double parallelization in ETL and
BLAS. There was a slight bug in the Column-Major matrix-matrix multiplication
kernel. Binary operations with different types in the left and right hand sides
was also problematic with vectorization. The last bug was about GPU status in
case ETL containers were moved.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="what-s-next"&gt;
&lt;h2&gt;What's next ?&lt;/h2&gt;
&lt;p&gt;I don't yet know exactly on which features I'm going to focus for the next
version of ETL. I plan to focus a bit more in the near future on Deep Learning
Library (DLL) for which I should release the version 1.0 soon. I also plan to
start support for Recurrent Neural Networks on it, so that will take me quite
some time.&lt;/p&gt;
&lt;p&gt;Nevertheless, I'm still planning to consider the switch to C++17, since it is
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/09/how-i-made-deep-learning-library-38-faster-to-compile-optimization-and-cpp17-if-constexpr.html"&gt;a bit faster to compile ETL with if constexpr&lt;/a&gt;. The next version of ETL will also probably have GPU-support for
integers, at least in the cases that depend on the etl-gpu-blas library, which
is the standard operators. I also plan to improve the support for complex
numbers, especially in terms of performance and tests. Hopefully, I will have also time (and motivation)
to start working on  the sparse capabilities of ETL. It really needs much more
unit tests and the performance should be improved as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="download-etl"&gt;
&lt;h2&gt;Download ETL&lt;/h2&gt;
&lt;p&gt;You can download ETL &lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;on Github&lt;/a&gt;. If you
only interested in the 1.2 version, you can look at the
&lt;a class="reference external" href="https://github.com/wichtounet/etl/releases"&gt;Releases pages&lt;/a&gt; or clone the tag
1.2. There are several branches:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;master&lt;/em&gt; Is the eternal development branch, may not always be stable&lt;/li&gt;
&lt;li&gt;&lt;em&gt;stable&lt;/em&gt; Is a branch always pointing to the last tag, no development here&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the future release, there always will tags pointing to the corresponding
commits. You can also have access to previous releases on Github or via the
release tags.&lt;/p&gt;
&lt;p&gt;The documentation is still a bit sparse. There are a few examples and the Wiki,
but there still is work to be done. If you have questions on how to use or
configure the library, please don't hesitate.&lt;/p&gt;
&lt;p&gt;Don't hesitate to comment this post if you have any comment on this library or
any question. You can also open an Issue on Github if you have a problem using
this library or propose a Pull Request if you have any contribution you'd like
to make to the library.&lt;/p&gt;
&lt;p&gt;Hope this may be useful to some of you :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>C++17</category><category>Compilers</category><category>dll</category><category>etl</category><category>GPU</category><category>Performance</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/10/expression-templates-library-etl-1-2-complete-gpu-support.html</guid><pubDate>Mon, 02 Oct 2017 08:49:02 GMT</pubDate></item><item><title>C++11 Performance tip: Update on when to use std::pow ?</title><link>http://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-update-when-to-use-std-pow.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;A few days ago, I published a post comparing the
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-when-to-use-std-pow.html"&gt;performance of std::pow against direct multiplications&lt;/a&gt;. When not compiling with -ffast-math, direct multiplication was significantly faster than &lt;code&gt;std::pow&lt;/code&gt;, around two orders of magnitude faster when comparing &lt;code&gt;x * x * x&lt;/code&gt; and &lt;code&gt;code:std::pow(x, 3)&lt;/code&gt;.
One comment that I've got was to test for which &lt;code&gt;n&lt;/code&gt; is
&lt;code&gt;code:std::pow(x, n)&lt;/code&gt; becoming faster than multiplying in a loop. Since
std::pow is using a special algorithm to perform the computation rather than be
simply loop-based multiplications, there may be a point after which it's more interesting to use the
algorithm rather than a loop. So I decided to do the tests. You can also find
the result in the original article, which I've updated.&lt;/p&gt;
&lt;p&gt;First, our pow function:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_52b382b39caf497cb43a418034153a77-1"&gt;&lt;/a&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="nf"&gt;my_pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_52b382b39caf497cb43a418034153a77-2"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_52b382b39caf497cb43a418034153a77-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_52b382b39caf497cb43a418034153a77-4"&gt;&lt;/a&gt;    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_52b382b39caf497cb43a418034153a77-5"&gt;&lt;/a&gt;        &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_52b382b39caf497cb43a418034153a77-6"&gt;&lt;/a&gt;        &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_52b382b39caf497cb43a418034153a77-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_52b382b39caf497cb43a418034153a77-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_52b382b39caf497cb43a418034153a77-9"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_52b382b39caf497cb43a418034153a77-10"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;And now, let's see the performance. I've compiled my benchmark with GCC 4.9.3
and running on my old Sandy Bridge processor. Here are the results for 1000
calls to each functions:&lt;/p&gt;
&lt;div id="graph_std_pow_my_pow_1" style="width: 700px; height: 400px;"&gt;&lt;/div&gt;&lt;p&gt;We can see that between &lt;code&gt;n=100&lt;/code&gt; and &lt;code&gt;n=110&lt;/code&gt;, &lt;code&gt;std::pow(x, n)&lt;/code&gt;
starts to be faster than &lt;code&gt;my_pow(x, n)&lt;/code&gt;. At this point, you should only
use &lt;code&gt;std::pow(x, n)&lt;/code&gt;.  Interestingly too, the time for &lt;code&gt;std::pow(x,
n)&lt;/code&gt; is decreasing. Let's see how is the performance with higher range of
&lt;code&gt;n&lt;/code&gt;:&lt;/p&gt;
&lt;div id="graph_std_pow_my_pow_2" style="width: 700px; height: 400px;"&gt;&lt;/div&gt;&lt;p&gt;We can see that the pow function time still remains stable while our loop-based
pow function still increases linearly. At &lt;code&gt;n=1000&lt;/code&gt;, &lt;code&gt;std::pow&lt;/code&gt; is
one order of magnitude faster than &lt;code&gt;my_pow&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Overall, if you do not care much about extreme accuracy, you may consider using
you own pow function for small-ish (integer) &lt;code&gt;n&lt;/code&gt; values. After
&lt;code&gt;n=100&lt;/code&gt;, it becomes more interesting to use &lt;code&gt;std::pow&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you want more results on the subject, you take a look at the
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-when-to-use-std-pow.html"&gt;original article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are interested in the code of this benchmark, it's available online:
&lt;a class="reference external" href="https://github.com/wichtounet/articles/blob/master/src/bench_pow_my_pow.cpp"&gt;bench_pow_my_pow.cpp&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript" src="https://www.google.com/jsapi"&gt;&lt;/script&gt;
&lt;script type="text/javascript"&gt;google.load('visualization', '1.0', {'packages':['corechart']});&lt;/script&gt;
&lt;script type="text/javascript"&gt;
function draw_graph_pow_my_pow_1(){
var data = google.visualization.arrayToDataTable([
['n', 'my_pow(x, n)', 'std::pow(x, n)'],
['10',   2,     127],
['20',   17,     123],
['30',   26,     127],
['40',   36,     123],
['50',   43,     123],
['60',   55,     123],
['70',   72,     123],
['80',   85,     123],
['90',   102,    126],
['100',  114,    125],
['110',  131,    115],
['120',  144,    111],
['130',  165,    111],
['140',  173,    108],
['150',  189,    107],
['160',  202,    112],
['170',  219,    106],
['180',  232,    105],
['190',  249,    108],
['200',  261,    105],
]);
var graph = new google.visualization.LineChart(document.getElementById('graph_std_pow_my_pow_1'));
var options = {curveType: "function",title: "std::pow(x, 2) (float)",animation: {duration:1200, easing:"in"},width: 700, height: 400,hAxis: {title:"Number of elements", slantedText:true},vAxis: {viewWindow: {min:0}, title:"us"}};
graph.draw(data, options);
}
function draw_graph_pow_my_pow_2(){
var data = google.visualization.arrayToDataTable([
['n', 'my_pow(x, n)', 'std::pow(x, n)'],
['100',  114,    125],
['200',  261,    105],
['300',  410,    104],
['400',  558,    104],
['500',  708,    104],
['600',  855,    104],
['700',  1002,   104],
['800',  1148,   104],
['900',  1300,   104],
['1000', 1442,   104],
]);
var graph = new google.visualization.LineChart(document.getElementById('graph_std_pow_my_pow_2'));
var options = {curveType: "function",title: "std::pow(x, 2) (float)",animation: {duration:1200, easing:"in"},width: 700, height: 400,hAxis: {title:"Number of elements", slantedText:true},vAxis: {viewWindow: {min:0}, title:"us"}};
graph.draw(data, options);
}
function draw_all(){
draw_graph_pow_my_pow_1();
draw_graph_pow_my_pow_2();
}
google.setOnLoadCallback(draw_all);
&lt;/script&gt;&lt;/div&gt;</description><category>Benchmark</category><category>C++</category><category>C++11</category><category>Performances</category><category>Tip</category><guid>http://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-update-when-to-use-std-pow.html</guid><pubDate>Fri, 22 Sep 2017 09:21:07 GMT</pubDate></item><item><title>How I made my Deep Learning Library 38% faster to compile (Optimization and C++17 if constexpr)</title><link>http://baptiste-wicht.com/posts/2017/09/how-i-made-deep-learning-library-38-faster-to-compile-optimization-and-cpp17-if-constexpr.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;My Deep Learning Library (DLL) project is a C++ library for training and using
artificial neural networks (you can take a look at
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html"&gt;this post about DLL&lt;/a&gt;
if you want more information).&lt;/p&gt;
&lt;p&gt;While I made a lot of effort to make it as fast as possible to train and run
neural networks, the compilation time has been steadily going up and is becoming
quite annoying. This library is heavily templated and all the matrix operations
are done using my Expression Templates Library (ETL) which is more than
template-heavy itself.&lt;/p&gt;
&lt;p&gt;In this post, I'll present two techniques with which I've been able to reduce
the total compilation of the DLL unit tests by up to 38%.&lt;/p&gt;
&lt;p class="more"&gt;&lt;a href="http://baptiste-wicht.com/posts/2017/09/how-i-made-deep-learning-library-38-faster-to-compile-optimization-and-cpp17-if-constexpr.html"&gt;Read more…&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>C++17</category><category>clang</category><category>Compilers</category><category>dll</category><category>etl</category><category>gcc</category><category>Performance</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/09/how-i-made-deep-learning-library-38-faster-to-compile-optimization-and-cpp17-if-constexpr.html</guid><pubDate>Thu, 21 Sep 2017 17:44:34 GMT</pubDate></item><item><title>C++11 Performance tip: When to use std::pow ?</title><link>http://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-when-to-use-std-pow.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Update: I've added a new section for larger values of &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Recently, I've been wondering about the performance of &lt;code&gt;std::pow(x, n)&lt;/code&gt;.
I'm talking here about the case when &lt;code&gt;n&lt;/code&gt; is an integer. In the case when
&lt;code&gt;n&lt;/code&gt; is not an integer, I believe, you should always use &lt;code&gt;std::pow&lt;/code&gt;
or use another specialized library.&lt;/p&gt;
&lt;p&gt;In case when n is an integer, you can actually replace it with the direct
equivalent (for instance &lt;code&gt;std::pow(x, 3) = x * x x&lt;/code&gt;). If n is very large,
you'd rather write a loop of course ;) In practice, we generally use powers of
two and three much more often than power of 29, although that could happen. Of
course, it especially make sense to wonder about this if the pow is used inside
a loop. If you only use it once outside a loop, that won't be any difference on
the overall performance.&lt;/p&gt;
&lt;p&gt;Since I'm mostly interested in single precision performance (neural networks are
only about single precision), the first benchmarks will be using &lt;code&gt;float&lt;/code&gt;.&lt;/p&gt;
&lt;p class="more"&gt;&lt;a href="http://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-when-to-use-std-pow.html"&gt;Read more…&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Benchmark</category><category>C++</category><category>C++11</category><category>Performances</category><category>Tip</category><guid>http://baptiste-wicht.com/posts/2017/09/cpp11-performance-tip-when-to-use-std-pow.html</guid><pubDate>Mon, 18 Sep 2017 05:50:44 GMT</pubDate></item><item><title>budgetwarrior 0.4.2 - Budget summary and improved fortune reports</title><link>http://baptiste-wicht.com/posts/2017/09/budgetwarrior-042-budget-summary-improved-fortune-reports.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Almost three years ago, &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2014/09/budgetwarrior-041-expense-templates-and-year-projection.html"&gt;I published the version 0.4.1 of budgetwarrior&lt;/a&gt;. Since then, I've been using this tool almost every day to manage my personal budget. This is the only tool I use to keep track of my expenses and earnings and it makes a great tool for me. I recently felt that it was missing a few features and added them and polished a few things as well and release a new version with all the new stuff. This new version is probably nothing fancy, but a nice upgrade of the tool.&lt;/p&gt;
&lt;p&gt;Don't pay too much attention to the values in the images since I've randomized
all the data for the purpose of this post (new feature, by the way :P).&lt;/p&gt;
&lt;div class="section" id="new-summary-view"&gt;
&lt;h2&gt;New summary view&lt;/h2&gt;
&lt;p&gt;I've added a new report with &lt;code&gt;budget summary&lt;/code&gt;:&lt;/p&gt;
&lt;img alt="/images/budgetwarrior_042_summary.png" src="http://baptiste-wicht.com/images/budgetwarrior_042_summary.png"&gt;
&lt;p&gt;This view gives concise information about the current state of your accounts. It
also gives information about your yearly and monthly objectives. Finally, it
also gives information about the last two fortune values that you've set.
I think this make a great kind of dashboard to view most of the information. If
your terminal is large enough, the three parts will be shown side by side.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="improved-fortune-report"&gt;
&lt;h2&gt;Improved fortune report&lt;/h2&gt;
&lt;p&gt;I've made a few improvements to the &lt;code&gt;budget fortune&lt;/code&gt; view:&lt;/p&gt;
&lt;img alt="/images/budgetwarrior_042_fortune.png" src="http://baptiste-wicht.com/images/budgetwarrior_042_fortune.png"&gt;
&lt;p&gt;It now display the time between the different fortune values and it compute the
average savings (or avg losses) per day in each interval and in average from the
beginning of the first value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="various-changes"&gt;
&lt;h2&gt;Various changes&lt;/h2&gt;
&lt;p&gt;The balance does not propagate over the years anymore. This should mainly change
the behaviour of &lt;code&gt;budget overview&lt;/code&gt;. I don't think it was very
smart to propagate it all the time. The balance now starts at zero for each
year. If you want the old system, you can use the multi_year_balance=true option
in the .budgetrc configuration file.&lt;/p&gt;
&lt;p&gt;The recurring expenses do not use an internal configuration value. This does not
change anything for the behaviour, but means that if you sync between different
machines, it will avoid a lot of possible conflicts :)&lt;/p&gt;
&lt;p&gt;Fixed a few bugs with inconsistency between the different views and reports.
Another bug that was fixed is that &lt;code&gt;budget report&lt;/code&gt; was not always displaying the
first month of the year correctly, this is now fixed.&lt;/p&gt;
&lt;p&gt;The graphs display in &lt;code&gt;budget report&lt;/code&gt; are now automatically adapted to width of
your terminal. Finally, the &lt;code&gt;budget overview&lt;/code&gt; command also displays more
information about the comparison with the previous month.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="installation"&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;If you are on Gentoo, you can install it using layman:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
layman -a wichtounet
emerge -a budgetwarrior
&lt;/pre&gt;
&lt;p&gt;If you are on Arch Linux, you can use this &lt;a class="reference external" href="https://github.com/StreakyCobra/aur-budgetwarrior"&gt;AUR repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For other systems, you'll have to install from sources:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
git clone --recursive git://github.com/wichtounet/budgetwarrior.git
cd budgetwarrior
make
sudo make install
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;A brief tutorial is available on Github: &lt;a class="reference external" href="https://github.com/wichtounet/budgetwarrior/wiki/Start-tutorial"&gt;Starting guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are interested by the sources, you can download them on Github:
&lt;a class="reference external" href="https://github.com/wichtounet/budgetwarrior"&gt;budgetwarrior&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you have any suggestion for a new feature or an improvement to the tool or
you found a bug, please post an issue on Github, I'd be glad to help you. You
can post a comment directly on this post :)&lt;/p&gt;
&lt;p&gt;If you have any other comment, don't hesitate to contact me, either by letting a
comment on this post or by email.&lt;/p&gt;
&lt;p&gt;I hope that this application can be useful to some of you command-line adepts :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>budgetwarrior</category><category>C++</category><category>Gentoo</category><category>Linux</category><category>Releases</category><guid>http://baptiste-wicht.com/posts/2017/09/budgetwarrior-042-budget-summary-improved-fortune-reports.html</guid><pubDate>Thu, 14 Sep 2017 18:42:39 GMT</pubDate></item><item><title>C++11 Concurrency Tutorial - Part 5: Futures</title><link>http://baptiste-wicht.com/posts/2017/09/cpp11-concurrency-tutorial-futures.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've been recently reminded that a long time ago I was doing a series of
tutorial on C++11 Concurrency. For some reason, I haven't continued these
tutorials.  The next post in the series was supposed to be about Futures, so I'm
finally going to do it :)&lt;/p&gt;
&lt;p&gt;Here are the links to the current posts of the C++11 Concurrency Tutorial:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/03/cpp11-concurrency-part1-start-threads.html"&gt;Part 1: Start Threads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/03/cp11-concurrency-tutorial-part-2-protect-shared-data.html"&gt;Part 2: Protect Shared Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/04/c11-concurrency-tutorial-advanced-locking-and-condition-variables.html"&gt;Part 3: Advanced Locking and condition variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/07/c11-concurrency-tutorial-part-4-atomic-type.html"&gt;Part 4: Atomic Types&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, we are going to talk about futures, more precisely
&lt;code&gt;std::future&amp;lt;T&amp;gt;&lt;/code&gt;. What is a future ? It's a very nice and simple mechanism
to work with asynchronous tasks. It also has the advantage of decoupling you
from the threads themselves, you can do multithreading without using
&lt;code&gt;std::thread&lt;/code&gt;. The future itself is a structure pointing to a result that
will be computed in the future. How to create a future ? The simplest way is to
use &lt;code&gt;std::async&lt;/code&gt; that will create an asynchronous task and return
a &lt;code&gt;std::future&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let's start with the simplest of the examples:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_a0b782c0e9794ea7a06f3cd7b84dc744-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;thread&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_a0b782c0e9794ea7a06f3cd7b84dc744-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;future&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_a0b782c0e9794ea7a06f3cd7b84dc744-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_a0b782c0e9794ea7a06f3cd7b84dc744-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_a0b782c0e9794ea7a06f3cd7b84dc744-5"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_a0b782c0e9794ea7a06f3cd7b84dc744-6"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;future&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_a0b782c0e9794ea7a06f3cd7b84dc744-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm a thread"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_a0b782c0e9794ea7a06f3cd7b84dc744-8"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_a0b782c0e9794ea7a06f3cd7b84dc744-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_a0b782c0e9794ea7a06f3cd7b84dc744-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_a0b782c0e9794ea7a06f3cd7b84dc744-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_a0b782c0e9794ea7a06f3cd7b84dc744-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_a0b782c0e9794ea7a06f3cd7b84dc744-13"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Nothing really special here. &lt;code&gt;std::async&lt;/code&gt; will execute the task that we
give it (here a lambda) and return a &lt;code&gt;std::future&lt;/code&gt;. Once you use the
&lt;code&gt;get()&lt;/code&gt; function on a future, it will wait until the result is available
and return this result to you once it is. The &lt;code&gt;get()&lt;/code&gt; function is then
blocking. Since the lambda, is a void lambda, the returned future is of type
&lt;code&gt;std::future&amp;lt;void&amp;gt;&lt;/code&gt; and &lt;code&gt;get()&lt;/code&gt; returns &lt;code&gt;void&lt;/code&gt; as well. It is
very important to know that you cannot call &lt;code&gt;get&lt;/code&gt; several times on the
same future. Once the result is consumed, you cannot consume it again! If you
want to use the result several times, you need to store it yourself after you
called &lt;code&gt;get()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let's see with something that returns a value and actually takes some time
before returning it:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;thread&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;future&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-4"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;chrono&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-6"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;future&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-8"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-9"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-10"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-12"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Do something else ?&lt;/span&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-13"&gt;&lt;/a&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-14"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-15"&gt;&lt;/a&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-16"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cda8edd43d0a4b09aaeb6f3081573610-17"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This time, the future will be of the time &lt;code&gt;std::future&amp;lt;int&amp;gt;&lt;/code&gt; and thus
&lt;code&gt;get()&lt;/code&gt; will also return an &lt;code&gt;int&lt;/code&gt;. &lt;code&gt;std::async&lt;/code&gt; will again
launch a task in an asynchronous way and &lt;code&gt;future.get()&lt;/code&gt; will wait for the
answer. What is interesting, is that you can do something else before the call
to future.&lt;/p&gt;
&lt;p&gt;But &lt;code&gt;get()&lt;/code&gt; is not the only interesting function in &lt;code&gt;std::future&lt;/code&gt;.
You also have &lt;code&gt;wait()&lt;/code&gt; which is almost the same as &lt;code&gt;get()&lt;/code&gt; but does
not consume the result. For instance, you can wait for several futures and then
consume their result together. But, more interesting are the
&lt;code&gt;wait_for(duration)&lt;/code&gt; and &lt;code&gt;wait_until(timepoint)&lt;/code&gt; functions. The
first one wait for the result at most the given time and then returns and the
second one wait for the result at most until the given time point. I think that
&lt;code&gt;wait_for&lt;/code&gt; is more useful in practices, so let's discuss it further.
Finally, an interesting function is &lt;code&gt;bool valid()&lt;/code&gt;. When you use
&lt;code&gt;get()&lt;/code&gt; on the future, it will consume the result, making &lt;code&gt;valid()
returns :code:`false&lt;/code&gt;. So, if you intend to check multiple times for a future,
you should use &lt;code&gt;valid()&lt;/code&gt; first.&lt;/p&gt;
&lt;p&gt;One possible scenario would be if you have several asynchronous tasks, which is
a common scenario. You can imagine that you want to process the results as fast
as possible, so you want to ask the futures for their result several times. If
no result is available, maybe you want to do something else. Here is a possible
implementation:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;thread&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;future&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-4"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;chrono&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-6"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-8"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-9"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-10"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;f2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-14"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-15"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-17"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;f3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](){&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-19"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;666&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-20"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-21"&gt;&lt;/a&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-22"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;timeout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;milliseconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-23"&gt;&lt;/a&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-24"&gt;&lt;/a&gt;    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()){&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-25"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wait_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future_status&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ready&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-26"&gt;&lt;/a&gt;            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Task1 is done! "&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-27"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-28"&gt;&lt;/a&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-29"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wait_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future_status&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ready&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-30"&gt;&lt;/a&gt;            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Task2 is done! "&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-31"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-32"&gt;&lt;/a&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-33"&gt;&lt;/a&gt;        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;valid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wait_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future_status&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ready&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-34"&gt;&lt;/a&gt;            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Task3 is done! "&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-35"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-36"&gt;&lt;/a&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-37"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm doing my own work!"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-38"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-39"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm done with my own work!"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-40"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-41"&gt;&lt;/a&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-42"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Everything is done, let's go back to the tutorial"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-43"&gt;&lt;/a&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-44"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_58242deea52649409e8e4766a5be6a65-45"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The three tasks are started asynchronously with &lt;code&gt;std::async&lt;/code&gt; and the
resulting &lt;code&gt;std::future&lt;/code&gt; are stored. Then, as long as one of the tasks is
not complete, we query each three task and try to process its result. If no
result is available, we simply do something else. This example is important to
understand, it covers pretty much every concept of the futures.&lt;/p&gt;
&lt;p&gt;One interesting thing that remains is that you can pass parameters to your task
via &lt;code&gt;std::async&lt;/code&gt;. Indeed, all the extra parameters that you pass to
&lt;code&gt;std::async&lt;/code&gt; will be passed to the task itself. Here is an example of
spawning tasks in a loop with different parameters:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;thread&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;future&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-3"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-4"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;chrono&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-5"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;vector&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-7"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-11"&gt;&lt;/a&gt;        &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;emplace_back&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-12"&gt;&lt;/a&gt;            &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;chrono&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-13"&gt;&lt;/a&gt;            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-14"&gt;&lt;/a&gt;        &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-15"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-17"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Start querying"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-18"&gt;&lt;/a&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-19"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nl"&gt;future&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-20"&gt;&lt;/a&gt;      &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-21"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-22"&gt;&lt;/a&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-23"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_3b2147c682c04d7fa38a4eb06490bff7-24"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Pretty practical :) All The created &lt;code&gt;std::future&amp;lt;size_t&amp;gt;&lt;/code&gt; are stored in
a &lt;code&gt;std::vector&lt;/code&gt; and then are all queried for their result.&lt;/p&gt;
&lt;p&gt;Overall, I think &lt;code&gt;std::future&lt;/code&gt; and &lt;code&gt;std::async&lt;/code&gt; are great tool that
can simplify your asynchronous code a lot. They allow you to make pretty
advanced stuff while keeping the complexity of the code to a minimum.&lt;/p&gt;
&lt;p&gt;I hope this long-due post is going to be interesting to some of you :)
The code for this post is available &lt;a class="reference external" href="https://github.com/wichtounet/articles/tree/master/src/threads/part5"&gt;on Github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I do not yet know if there will be a next installment in the series. I've
covered pretty much everything that is available in C++11 for concurrency. I may
cover the parallel algorithms of C++17 in a following post. If you have any
suggestion for the next post, don't hesitate to post a comment or contact me
directly by email.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>C++11 Concurrency Tutorial</category><category>Concurrency</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2017/09/cpp11-concurrency-tutorial-futures.html</guid><pubDate>Tue, 12 Sep 2017 13:05:08 GMT</pubDate></item><item><title>Simplify your type traits with C++14 variable templates</title><link>http://baptiste-wicht.com/posts/2017/08/simplify-your-type-traits-with-c%2B%2B14-variable-templates.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Often if you write templated code, you have to write and use a lot of different
traits. In this article, I'll focus on the traits that are representing values,
typically a boolean value. For instance, std::is_const, std::is_same or
std::is_reference are type traits provided by the STL. They are giving you some
information at compile time for a certain type. If you need to write a type
traits, let's say is_float, here is how you would maybe do it in C++11:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_45b11d66ae264c94b9aa702fd745c672-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_45b11d66ae264c94b9aa702fd745c672-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_45b11d66ae264c94b9aa702fd745c672-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_45b11d66ae264c94b9aa702fd745c672-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;or a bit nicer with a template type alias and std::integral constant:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_5b7514df48de41949d3cb2865dffa98c-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_5b7514df48de41949d3cb2865dffa98c-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;integral_constant&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;or since is_same is itself a type traits, you can also directly alias it:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_38f6a207f06249eaa4e2e427d8e07c1a-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_38f6a207f06249eaa4e2e427d8e07c1a-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This makes for some very nice syntax, but we still have a type rather than a value.&lt;/p&gt;
&lt;p&gt;Note that in some cases, you cannot use the using technique since it cannot be
specialized and you often need specialization to write some more advanced
traits.&lt;/p&gt;
&lt;p&gt;And then you would use your traits to do something specific based on that
information. For instance with a very basic example:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_06a0f8637289401b880e671fcd672628-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_06a0f8637289401b880e671fcd672628-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_06a0f8637289401b880e671fcd672628-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is_float&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_06a0f8637289401b880e671fcd672628-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm a float"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_06a0f8637289401b880e671fcd672628-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_06a0f8637289401b880e671fcd672628-6"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm not a float"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_06a0f8637289401b880e671fcd672628-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_06a0f8637289401b880e671fcd672628-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Really nothing fancy here, but that will be enough as examples.&lt;/p&gt;
&lt;p&gt;Even though all this works pretty, it can be made better on two points. First,
every time you use a traits, you need to use the value member (via ::value).
Secondly, every time you declare a new traits, you have to declare a new type or
a type alias. But all you want is a boolean value.&lt;/p&gt;
&lt;p&gt;C++14 introduced a new feature, variable templates. As their name indicates,
they are variables, parametrized with a type. This allows us to write type
traits without using a type alias or struct, meaning we have a real value
instead of a type. If we rewrite our is_float traits with variable templates, we
have the following:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_794ce898460249cf8128e8f03cdbabf0-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_794ce898460249cf8128e8f03cdbabf0-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I think it's much nicer, the intent is clearly stated and there is no
unnecessary code. Moreover, it's also nicer to use:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_4a69bf93945e4964a57a9536d106353a-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4a69bf93945e4964a57a9536d106353a-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_4a69bf93945e4964a57a9536d106353a-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is_float&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_4a69bf93945e4964a57a9536d106353a-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm a float"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_4a69bf93945e4964a57a9536d106353a-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_4a69bf93945e4964a57a9536d106353a-6"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"I'm not a float"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_4a69bf93945e4964a57a9536d106353a-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_4a69bf93945e4964a57a9536d106353a-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;No more ::value everywhere :) I think it's really cool.&lt;/p&gt;
&lt;p&gt;Note that, unlike type alias template, they can be specialized, either fully or
partially, so no more limitation on that side.&lt;/p&gt;
&lt;p&gt;Interestingly, variable templates are used in C++17 to provide helpers for each
type traits with values. For instance, std::is_same will have a std::is_same_v
helper that is a variable template. With that, we can simplify our traits a bit
more:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_32ed21ff43bc4b289e77ba2279eabc04-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_32ed21ff43bc4b289e77ba2279eabc04-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;is_float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same_v&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Personally, I replaced all the type traits inside ETL using variable templates.
If you don't want to do it, you can also introduce helpers like in the C++17 STL
and start using the wrappers when you see fit so that you don't break any code.&lt;/p&gt;
&lt;p&gt;If you want to use this feature, you need a C++14 compiler, such as any version
from GCC5 family or clang 3.6. Although I haven't tested, it should also work on
Microsoft VS2015 Update 2.&lt;/p&gt;
&lt;p&gt;Unfortunately there is a bug in both clang (fixed in clang 3.7) and GCC (fixed
in GCC 6 only) that you may encounter if you start using variable templates in
template classes or variable templates used in another variable templates. If
you plan to use variable templates inside a template, such as something like
this:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;outer_traits&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-4"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;sub_traits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-5"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-8"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;outer_helper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outer_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;sub_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-10"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;outer_helper&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-13"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cadac6599e3749ee8bbb1105d0bb7ef0-14"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;You will encounter a not-helpful at all error message with GCC5 family, such as:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_d50ed49e99294025a1490dc4f343eac5-1"&gt;&lt;/a&gt;test.cpp: In instantiation of ‘constexpr const bool outer_helper&amp;lt;float, float&amp;gt;’:
&lt;a name="rest_code_d50ed49e99294025a1490dc4f343eac5-2"&gt;&lt;/a&gt;test.cpp:14:22:   required from here
&lt;a name="rest_code_d50ed49e99294025a1490dc4f343eac5-3"&gt;&lt;/a&gt;test.cpp:11:20: error: ‘template&amp;lt;class X&amp;gt; constexpr const bool outer_traits&amp;lt;float&amp;gt;::sub_traits&amp;lt;X&amp;gt;’ is not a function template
&lt;a name="rest_code_d50ed49e99294025a1490dc4f343eac5-4"&gt;&lt;/a&gt;     constexpr bool outer_helper = outer_traits&amp;lt;T&amp;gt;::template sub_trait
&lt;a name="rest_code_d50ed49e99294025a1490dc4f343eac5-5"&gt;&lt;/a&gt;                    ^
&lt;a name="rest_code_d50ed49e99294025a1490dc4f343eac5-6"&gt;&lt;/a&gt;test.cpp:11:20: error: ‘sub_traits&amp;lt;X&amp;gt;’ is not a member of ‘outer_traits&amp;lt;float&amp;gt;’
&lt;/pre&gt;&lt;p&gt;It comes from a bug in the handling of variable templates as dependent names. If
you don't come in this cases, you can use GCC5 family directly, otherwise,
you'll have to use GCC6 family only.&lt;/p&gt;
&lt;p&gt;I hope this can help some of you to improve your type traits or at least to
discover the power of the new variable templates. Personally, I've rewritten all
the traits from the ETL library using this new feature and I'm pretty satisfied
with the result. Of course, that means that the compiler support was reduced,
but since I don't have many users, it's not a real issue.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>Compilers</category><category>etl</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/08/simplify-your-type-traits-with-c%2B%2B14-variable-templates.html</guid><pubDate>Tue, 22 Aug 2017 12:45:11 GMT</pubDate></item><item><title>How to fix mdadm RAID5 / RAID6 growing stuck at 0K/s ?</title><link>http://baptiste-wicht.com/posts/2017/08/how-to-fix-mdadm-raid5-raid6-growing-stuck-at-0ks.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I just started growing again my RAID6 array from 12 to 13 disks and
I encountered a new issue. The reshape started, but with a speed of 0K/s. After
some searching, I found a very simple solution:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_ec6ba38cd92b40d9af8372bcb5791b44-1"&gt;&lt;/a&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; max &amp;gt; /sys/block/md0/md/sync_max
&lt;/pre&gt;&lt;p&gt;And the reshape started directly at 50M/s :)&lt;/p&gt;
&lt;p&gt;The solution is the same if you are growing any type of RAID level with parity
(RAID5, RAID6, ...).&lt;/p&gt;
&lt;p&gt;Normally, the issues I have are related to speed not very good. I've written
a post in the post about
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2015/03/how-to-speed-up-raid-5-6-growing-with-mdadm.html"&gt;how to speed up RAID5 / RAID6 growing with mdadm&lt;/a&gt;.
Although RAID5 / RAID6 growing, or another reshape operation, will never be very
fast, you can still speed up the process a lot from a few days to a few hours.
Currently, my reshape is working at 48M/s and I'm looking at around 16 hours of
reshape, but I have 13 disks of 3To, so it's not so bad.&lt;/p&gt;
&lt;p&gt;I hope this very simple tip can be helpful to some of you :)&lt;/p&gt;&lt;/div&gt;</description><category>Gentoo</category><category>Hardware</category><category>Home Server</category><category>Linux</category><guid>http://baptiste-wicht.com/posts/2017/08/how-to-fix-mdadm-raid5-raid6-growing-stuck-at-0ks.html</guid><pubDate>Sat, 12 Aug 2017 10:28:43 GMT</pubDate></item><item><title>DLL: Blazing Fast Neural Network Library</title><link>http://baptiste-wicht.com/posts/2017/08/dll-blazing-fast-neural-network-library.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;A few weeks ago, I talked about all
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html"&gt;the new features of my Deep Learning Library (DLL)&lt;/a&gt;
project. I've mentioned that, on several experiments, DLL was always
significantly faster than some popular deep learning frameworks such as
TensorFlow. I'll now go into more details into this comparison and provide all
the results. So far, the paper we wrote about these results has not been
published, so I'll not provide the paper directly yet.&lt;/p&gt;
&lt;p&gt;For those that may not know, DLL is the project I've been developing to support
my Ph.D. thesis. This is a neural network framework  that supports
Fully-Connected Neural Network (FCNN), Convolutional Neural Network (CNN),
Restricted Boltzmann Machine (RBM), Deep Belief Network (DBN), Convolutional RBM
(CRBM) and Convolutional DBN (CDBN). It also supports a large variety of options
such as Dropout, Batch Normalization and Adaptive Learning Rates. You can read
read the
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html"&gt;previous post&lt;/a&gt;
if you want more information about the new features of the framework. And, as those of
you that read my blog frequently may know, I'm a bit obsessed with performance
optimization, so I've spent a considerable amount of time optimizing
the performance of neural network training, on CPU. Since, at the beginning of my
thesis, I had no access to GPU for training, I've focused on CPU. Although there
is now support for GPU, the gains are not yet important enough.&lt;/p&gt;
&lt;div class="section" id="evaluation"&gt;
&lt;h2&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;To see how fast, or not, the library was, it was compared against five popular
machine learning libraries:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Caffe, installed from sources&lt;/li&gt;
&lt;li&gt;TensorFlow 1.0, from pip&lt;/li&gt;
&lt;li&gt;Keras 2.0, from pip&lt;/li&gt;
&lt;li&gt;Torch, installed from sources&lt;/li&gt;
&lt;li&gt;DeepLearning4J 0.7, from Maven&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I've run four different experiments with all these frameworks and compared the
efficiency of each of them for training the same neural networks with the same
options. In each case, the training or testing error have also been compared to
ensure that each framework is doing roughly the same. I wont present here the
details, but in each experiment DLL showed around the same accuracies as the
other frameworks. I will only focus on the speed results in this article.&lt;/p&gt;
&lt;p&gt;Each experiment is done once with only CPU and once with a GPU. For DLL, I only
report the CPU time in both modes, since it's more stable and more optimized.&lt;/p&gt;
&lt;p&gt;The code for the evaluation is available online on the
&lt;a class="reference external" href="https://github.com/wichtounet/frameworks"&gt;Github repository of the frameworks project&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="mnist-fully-connected-neural-network"&gt;
&lt;h2&gt;MNIST: Fully Connected Neural Network&lt;/h2&gt;
&lt;p&gt;The first experiment is performed on The MNIST data set. It consists of 60'000
grayscale images of size 28x28. The goal is to classify each image of a digit
from 0 to 9. To solve this task, I trained a very small fully-connected neural
network with 500 hidden units in the first layer, 250 in the second and 10 final
hidden units (or output units) for classification. The first two layers are
using the logistic sigmoid activation function and the last layer is using the
softmax activation function. The network is trained for 50 epochs with a
categorical cross entropy loss, with mini-batches of 100 images. Here are
results of this experiment:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Training time performance for the different frameworks on the Fully-Connected Neural Network experiment, on MNIST." src="http://baptiste-wicht.com/images/dll_fcnn.png"&gt;
&lt;p class="caption"&gt;Training time performance for the different frameworks on the Fully-Connected
Neural Network experiment, on MNIST. All the times are in seconds.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In DLL mode, the DLL framework is the clear winner here! It's about 35% faster
than TensorFlow and Keras which are coming at the second place. DLL is more than
four times slower than DLL and the last two frameworks (Caffe and
DeepLearning4J) are five times slower than DLL! Once we add a GPU to the system,
the results are very different. Caffe is now the fastest framework, three times
faster than DLL. DLL is less than two times slower than Keras and TensorFlow.
Interestingly, DLL is still faster than Torch and DeepLearning4J.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="mnist-convolutional-neural-network"&gt;
&lt;h2&gt;MNIST: Convolutional Neural Network&lt;/h2&gt;
&lt;p&gt;Although a Fully-Connected Neural Network is an interesting tool, the trend now
is to use Convolutional Neural Network which have proved very efficient at
solving a lot of problems. The second experiment is also using the same data
set. Again, it's a rather small network. The first layer is a convolutional
layer with 8 5x5 kernels, followed by max pooling layer with 2x2 kernel. They
are followed by one more convolutional layers with 8 5x5 kernels and a 2x2 max
pooling layer. These first four layers are followed by two fully-connected
layers, the first with 150 hidden units and the last one with 10 output units.
The activation functions are the same as for the first network, as is the
training procedure. This takes significantly longer to train than the first
network because of the higher complexity of the convolutional layers compared to
the fully-connected layers even though they have much less weights. The results
are present in the next figure:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Training time performance for the different frameworks on the Convolutional Neural Network experiment, on MNIST." src="http://baptiste-wicht.com/images/dll_cnn.png"&gt;
&lt;p class="caption"&gt;Training time performance for the different frameworks on the Convolutional
Neural Network experiment, on MNIST. All the times are in seconds.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Again, on CPU, DLL is the clear winner, by a lot! It's already 3.6 times faster
than the second frameworks Keras and TensorFlow, more than four times faster
than Caffe and Torch and 8 times faster than DeepLearning4J that is proving very
slow on this experiment. Once a GPU is added, Keras and TensorFlow are about
twice faster than DLL. However, DLL is still faster than the other frameworks
even though they are taking advantage of the GPU.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cifar-10"&gt;
&lt;h2&gt;CIFAR-10&lt;/h2&gt;
&lt;p&gt;The second data set that is tested is the CIFAR-10 data set. It's an object
recognition with 10 classes for classification. The training set is composed of
50'000 colour images for 32x32 pixels. The network that is used for this data
set is similar in architecture than the first network, but has more parameters.
The first convolutional layer now has 12 5x5 kernels and the second
convolutional layer has 24 3x3 kernels. The pooling layers are the same. The
first fully-connected has 64 hidden units and the last one has 10 output units.
The last layer again use a softmax activation function while the other layers
are using Rectifier Linear Units (ReLU). The training is done in the same manner
as for the two first networks. Unfortunately, it was not possible to train
DeepLearning4J on this data set, even though there is official support for this
data set. Since I've had no answer to my question regarding this issue, the
results are simply removed from this experiment. It may not seem so but it's
considerably longer to train this network because of the larger number of input
channels and larger number of convolutional kernels in each layer. Let's get to
the results now:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Training time performance for the different frameworks on the Convolutional Neural Network experiment, on CIFAR-10." src="http://baptiste-wicht.com/images/dll_cifar10.png"&gt;
&lt;p class="caption"&gt;Training time performance for the different frameworks on the Convolutional
Neural Network experiment, on CIFAR-10. All the times are in seconds.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;DLL is still the fastest on CPU, but the margin is less than before. It's about
40% faster than TensorFlow and Keras, twice faster than Torch and 2.6 times
faster than Caffe. Once a GPU is added, DLL is about as fast as Torch but slower
than the other three frameworks. TensorFlow and Keras are about four times
faster than DLL while Caffe is about twice faster than DLL. We can see that
with this larger network, the GPU becomes more interesting and that there is
a smaller margin for improvements compared to the other frameworks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="imagenet"&gt;
&lt;h2&gt;ImageNet&lt;/h2&gt;
&lt;p&gt;The last experiment is made on the ImageNet data set. I used the ILSVRC 2012
subset, that consists "only" of about 1.2 million images for training. I've
resized all the images to 256x256 pixels, this makes for 250 times more colour
values than a MNIST image. This dimension and the number of images makes it
impractical to keep the dataset in memory. The images must be loaded in batch
from the disk. No random cropping or mirroring was performed. The network is
much larger to solve this task. The network starts with 5 pairs of convolutional
layers and max pooling layers. The convolutional layers have 3x3 kernels, 16 for
the first two layers and 32 for the three following one. The five max pooling
layers use 2x2 kernels. Each convolutional layer uses zero-padding so that their
output features are the same dimensions as the input. They are followed by two
fully-connected layer. The first one with 2048 hidden units and the last one
with 1000 output units (one for each class). Except for the last layer, using
softmax, the layers all uses ReLU. The network is trained with mini-batches of
128 images (except for DeepLearning4J and Torch, which can only use 64 images on
the amount of RAM available on my machine). To ease the comparison, I report the
time necessary to train one batch of data (or two for DeepLearning4J and Torch).
The results, presented in logarithmic scale because of DeepLearning4J disastrous
results, are as follows:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Training time performance for the different frameworks on the Convolutional Neural Network experiment, on ImageNet." src="http://baptiste-wicht.com/images/dll_imagenet.png"&gt;
&lt;p class="caption"&gt;Training time performance for the different frameworks on the Convolutional
Neural Network experiment, on ImageNet. The times are the time necessary to
train a batch of 128 images. All the times are in milliseconds.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For this final experiment, DLL is again significantly faster than all the other
frameworks. It's about 40% faster than Keras, twice faster than TensorFlow and
Caffe and more than three times faster than Torch. Although 40% may seem not
that much, don't forget that this kind of training may take days, so it can save
you a lot of time. All the frameworks are much faster than DeepLearning4J. Based
on several posts on the internet, I suspect that this comes from the model of
GPU I have been used (GTX 960), but all the other frameworks seem to handle this
card pretty well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I hope this is not too much of a bragging post :P We can see that my efforts to
make the code as fast as possible have paid :) As was shown in the experiments,
my DLL framework is always the fastest framework when the neural network is
trained on CPU. I'm quite pleased with the results since I've done a lot of work
to optimize the speed as much as possible and since I'm competing with
well-known libraries that have been developed by several persons.  Moreover, the
accuracies of the trained networks is similar to that of the networks trained
with the other frameworks. Even when the other frameworks are using GPU, the
library still remains competitive, although never the fastest.&lt;/p&gt;
&lt;p&gt;In the next step (I've no idea when I'll have the time though), I will want to
focus on GPU speed. This will mostly come from a better support of the GPU in
the ETL library on which DLL is based. I have many ideas to improve it a lot,
but it will take me a lot of time.&lt;/p&gt;
&lt;p&gt;If you want more information on the DLL library, you can have a look at
&lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;its Github repository&lt;/a&gt; and especially at
&lt;a class="reference external" href="https://github.com/wichtounet/dll/tree/master/examples/src"&gt;the few examples&lt;/a&gt;.
You can also have a look at &lt;a class="reference external" href="https://baptiste-wicht.com/categories/dll.html"&gt;my posts about DLL&lt;/a&gt;.
Finally, don't hesitate to comment or contact me through Github issues if you
have comments or problems with this post, the library or anything ;)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>dll</category><category>etl</category><category>GPU</category><category>Machine Learning</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/08/dll-blazing-fast-neural-network-library.html</guid><pubDate>Fri, 11 Aug 2017 09:09:14 GMT</pubDate></item><item><title>Compiler benchmark GCC and Clang on C++ library (ETL)</title><link>http://baptiste-wicht.com/posts/2017/08/compiler-benchmark-gcc-clang-cpp-library-etl.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;It's been a while since I've done a benchmark of different compilers on C++
code. Since I've recently
&lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/08/expression-templates-library-etl-11.html"&gt;released the version 1.1 of my ETL project&lt;/a&gt;
(an optimized matrix/vector computation library with expression templates), I've
decided to use it as the base of my benchmark. It's a C++14 library with a lot
of templates. I'm going to compile the full test suite (124 test cases). This is
done directly on the last release (1.1) code. I'm going to compile once in debug
mode and once in release_debug (release plus debug symbols and assertions) and
record the times for each compiler. The tests were compiled with support for
every option in ETL to account to maximal compilation time. Each compilation was
made using four threads (make -j4). I'm also going to test a few of the
benchmarks to see the difference in runtime performance between the code
generated by each compiler. The benchmark will be compiled in release mode and
its compilation time recorded as well.&lt;/p&gt;
&lt;p&gt;I'm going to test the following compilers:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;GCC-4.9.4&lt;/li&gt;
&lt;li&gt;GCC-5.4.0&lt;/li&gt;
&lt;li&gt;GCC-6.3.0&lt;/li&gt;
&lt;li&gt;GCC-7.1.0&lt;/li&gt;
&lt;li&gt;clang-3.9.1&lt;/li&gt;
&lt;li&gt;clang-4.0.1&lt;/li&gt;
&lt;li&gt;zapcc-1.0 (commercial, based on clang-5.0 trunk)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All have been installed directly using Portage (Gentoo package manager) except
for clang-4.0.1 that has been installed from sources and zapcc since it does not
have a Gentoo package. Since clang package on Gentoo does not support
multislotting, I had to install one version from source and the other from the
package manager. This is also the reason I'm testing less versions of clang,
simply less practical.&lt;/p&gt;
&lt;p&gt;For the purpose of these tests, the exact same options have been used throughout
all the compilers. Normally, I use different options for clang than for GCC
(mainly more aggressive vectorization options on clang). This may not lead to
the best performance for each compiler, but allows for comparison between the
results with defaults optimization level. Here are the main options used:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;In debug mode: -g&lt;/li&gt;
&lt;li&gt;In release_debug mode: -g -O2&lt;/li&gt;
&lt;li&gt;In release mode: -g -O3 -DNDEBUG -fomit-frame-pointer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In each case, a lot of warnings are enabled and the ETL options are the same.&lt;/p&gt;
&lt;p&gt;All the results have been gathered on a Gentoo machine running on Intel Core
i7-2600 (Sandy Bridge...) @3.4GHz with 4 cores and 8 threads, 12Go of RAM and
a SSD. I do my best to isolate as much as possible the benchmark from
perturbations and that my benchmark code is quite sound, it may well be that
some results are not totally accurate. Moreover, some of the benchmarks are
using multithreading, which may add some noise and unpredictability. When I was
not sure about the results, I ran the benchmarks several time to confirm them
and overall I'm confident of the results.&lt;/p&gt;
&lt;div class="section" id="compilation-time"&gt;
&lt;h2&gt;Compilation Time&lt;/h2&gt;
&lt;p&gt;Let's start with the results of the performance of the compilers themselves:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="31%"&gt;
&lt;col width="15%"&gt;
&lt;col width="31%"&gt;
&lt;col width="23%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;Debug&lt;/th&gt;
&lt;th class="head"&gt;Release_Debug&lt;/th&gt;
&lt;th class="head"&gt;Benchmark&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;402s&lt;/td&gt;
&lt;td&gt;616s&lt;/td&gt;
&lt;td&gt;100s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;403s&lt;/td&gt;
&lt;td&gt;642s&lt;/td&gt;
&lt;td&gt;95s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;399s&lt;/td&gt;
&lt;td&gt;683s&lt;/td&gt;
&lt;td&gt;102s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;371s&lt;/td&gt;
&lt;td&gt;650s&lt;/td&gt;
&lt;td&gt;105s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;380s&lt;/td&gt;
&lt;td&gt;807s&lt;/td&gt;
&lt;td&gt;106s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;260s&lt;/td&gt;
&lt;td&gt;718s&lt;/td&gt;
&lt;td&gt;92s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;221s&lt;/td&gt;
&lt;td&gt;649s&lt;/td&gt;
&lt;td&gt;108s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note: For Release_Debug and Benchmark, I only use three threads with zapcc,
because 12Go of RAM is not enough memory for four threads.&lt;/p&gt;
&lt;p&gt;There are some very significant differences between the different compilers.
Overall, clang-4.0.1 is by far the fastest free compiler for Debug mode. When
the tests are compiled with optimizations however, clang is falling behind.
It's quite impressive how clang-4.0.1 manages to be so much faster than
clang-3.9.1 both in debug mode and release mode. Really great work by the clang
team here! With these optimizations, clang-4.0.1 is almost on par with gcc-7.1
in release mode.  For GCC, it seems that the cost of optimization has been going
up quite significantly. However, GCC 7.1 seems to have made optimization faster
and standard compilation much faster as well. If we take into account zapcc,
it's the fastest compiler on debug mode, but it's slower than several gcc
versions on release mode.&lt;/p&gt;
&lt;p&gt;Overall, I'm quite impressed by the performance of clang-4.0.1 which seems
really fast! I'll definitely make more tests with this new version of the
compiler in the near future. It's also good to see that g++-7.1 also did make
the build faster than gcc-6.3. However, the fastest gcc version for optimization
is still gcc-4.9.4 which is already an old branch with low C++ standard support.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="runtime-performance"&gt;
&lt;h2&gt;Runtime Performance&lt;/h2&gt;
&lt;p&gt;Let's now take a look at the quality of the generated code. For some of the
benchmarks, I've included two versions of the algorithm. &lt;em&gt;std&lt;/em&gt; is the most
simple algorithm (the naive one) and &lt;em&gt;vec&lt;/em&gt; is the hand-crafted vectorized and
optimized implementation. All the tests were done on single-precision floating
points.&lt;/p&gt;
&lt;div class="section" id="dot-product"&gt;
&lt;h3&gt;Dot product&lt;/h3&gt;
&lt;p&gt;The first benchmark that is run is to compute the dot product between two
vectors. Let's look first at the naive version:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="13%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="7%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;dot (std)&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;th class="head"&gt;500&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;10000&lt;/th&gt;
&lt;th class="head"&gt;100000&lt;/th&gt;
&lt;th class="head"&gt;1000000&lt;/th&gt;
&lt;th class="head"&gt;2000000&lt;/th&gt;
&lt;th class="head"&gt;3000000&lt;/th&gt;
&lt;th class="head"&gt;4000000&lt;/th&gt;
&lt;th class="head"&gt;5000000&lt;/th&gt;
&lt;th class="head"&gt;10000000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;64.96ns&lt;/td&gt;
&lt;td&gt;97.12ns&lt;/td&gt;
&lt;td&gt;126.07ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;25.91us&lt;/td&gt;
&lt;td&gt;326.49us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.92ms&lt;/td&gt;
&lt;td&gt;2.55ms&lt;/td&gt;
&lt;td&gt;3.22ms&lt;/td&gt;
&lt;td&gt;6.36ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;72.96ns&lt;/td&gt;
&lt;td&gt;101.62ns&lt;/td&gt;
&lt;td&gt;127.89ns&lt;/td&gt;
&lt;td&gt;1.90us&lt;/td&gt;
&lt;td&gt;23.39us&lt;/td&gt;
&lt;td&gt;357.63us&lt;/td&gt;
&lt;td&gt;1.23ms&lt;/td&gt;
&lt;td&gt;1.91ms&lt;/td&gt;
&lt;td&gt;2.57ms&lt;/td&gt;
&lt;td&gt;3.20ms&lt;/td&gt;
&lt;td&gt;6.32ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;73.31ns&lt;/td&gt;
&lt;td&gt;102.88ns&lt;/td&gt;
&lt;td&gt;130.16ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;24.314us&lt;/td&gt;
&lt;td&gt;339.13us&lt;/td&gt;
&lt;td&gt;1.47ms&lt;/td&gt;
&lt;td&gt;2.16ms&lt;/td&gt;
&lt;td&gt;2.95ms&lt;/td&gt;
&lt;td&gt;3.70ms&lt;/td&gt;
&lt;td&gt;6.69ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;70.20ns&lt;/td&gt;
&lt;td&gt;104.09ns&lt;/td&gt;
&lt;td&gt;130.98ns&lt;/td&gt;
&lt;td&gt;1.90us&lt;/td&gt;
&lt;td&gt;23.96us&lt;/td&gt;
&lt;td&gt;281.47us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.93ms&lt;/td&gt;
&lt;td&gt;2.58ms&lt;/td&gt;
&lt;td&gt;3.19ms&lt;/td&gt;
&lt;td&gt;6.33ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;64.69ns&lt;/td&gt;
&lt;td&gt;98.69ns&lt;/td&gt;
&lt;td&gt;128.60ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;23.33us&lt;/td&gt;
&lt;td&gt;272.71us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.91ms&lt;/td&gt;
&lt;td&gt;2.56ms&lt;/td&gt;
&lt;td&gt;3.19ms&lt;/td&gt;
&lt;td&gt;6.37ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;60.31ns&lt;/td&gt;
&lt;td&gt;96.34ns&lt;/td&gt;
&lt;td&gt;128.90ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;22.87us&lt;/td&gt;
&lt;td&gt;270.21us&lt;/td&gt;
&lt;td&gt;1.23ms&lt;/td&gt;
&lt;td&gt;1.91ms&lt;/td&gt;
&lt;td&gt;2.55ms&lt;/td&gt;
&lt;td&gt;3.18ms&lt;/td&gt;
&lt;td&gt;6.35ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;61.14ns&lt;/td&gt;
&lt;td&gt;96.92ns&lt;/td&gt;
&lt;td&gt;125.95ns&lt;/td&gt;
&lt;td&gt;1.89us&lt;/td&gt;
&lt;td&gt;23.84us&lt;/td&gt;
&lt;td&gt;285.80us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.92ms&lt;/td&gt;
&lt;td&gt;2.55ms&lt;/td&gt;
&lt;td&gt;3.16ms&lt;/td&gt;
&lt;td&gt;6.34ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The differences are not very significant between the different compilers. The
clang-based compilers seem to be the compilers producing the fastest code.
Interestingly, there seem to have been a big regression in gcc-6.3 for large
containers, but that has been fixed in gcc-7.1.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="13%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="9%"&gt;
&lt;col width="7%"&gt;
&lt;col width="8%"&gt;
&lt;col width="9%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="9%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;dot (vec)&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;th class="head"&gt;500&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;10000&lt;/th&gt;
&lt;th class="head"&gt;100000&lt;/th&gt;
&lt;th class="head"&gt;1000000&lt;/th&gt;
&lt;th class="head"&gt;2000000&lt;/th&gt;
&lt;th class="head"&gt;3000000&lt;/th&gt;
&lt;th class="head"&gt;4000000&lt;/th&gt;
&lt;th class="head"&gt;5000000&lt;/th&gt;
&lt;th class="head"&gt;10000000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;48.34ns&lt;/td&gt;
&lt;td&gt;80.53ns&lt;/td&gt;
&lt;td&gt;114.97ns&lt;/td&gt;
&lt;td&gt;1.72us&lt;/td&gt;
&lt;td&gt;22.79us&lt;/td&gt;
&lt;td&gt;354.20us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.89ms&lt;/td&gt;
&lt;td&gt;2.52ms&lt;/td&gt;
&lt;td&gt;3.19ms&lt;/td&gt;
&lt;td&gt;6.55ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;47.16ns&lt;/td&gt;
&lt;td&gt;77.70ns&lt;/td&gt;
&lt;td&gt;113.66ns&lt;/td&gt;
&lt;td&gt;1.72us&lt;/td&gt;
&lt;td&gt;22.71us&lt;/td&gt;
&lt;td&gt;363.86us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.89ms&lt;/td&gt;
&lt;td&gt;2.52ms&lt;/td&gt;
&lt;td&gt;3.19ms&lt;/td&gt;
&lt;td&gt;6.56ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;46.39ns&lt;/td&gt;
&lt;td&gt;77.67ns&lt;/td&gt;
&lt;td&gt;116.28ns&lt;/td&gt;
&lt;td&gt;1.74us&lt;/td&gt;
&lt;td&gt;23.39us&lt;/td&gt;
&lt;td&gt;452.44us&lt;/td&gt;
&lt;td&gt;1.45ms&lt;/td&gt;
&lt;td&gt;2.26ms&lt;/td&gt;
&lt;td&gt;2.87ms&lt;/td&gt;
&lt;td&gt;3.49ms&lt;/td&gt;
&lt;td&gt;7.52ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;49.70ns&lt;/td&gt;
&lt;td&gt;80.40ns&lt;/td&gt;
&lt;td&gt;115.77ns&lt;/td&gt;
&lt;td&gt;1.71us&lt;/td&gt;
&lt;td&gt;22.46us&lt;/td&gt;
&lt;td&gt;355.16us&lt;/td&gt;
&lt;td&gt;1.21ms&lt;/td&gt;
&lt;td&gt;1.85ms&lt;/td&gt;
&lt;td&gt;2.49ms&lt;/td&gt;
&lt;td&gt;3.14ms&lt;/td&gt;
&lt;td&gt;6.47ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;46.13ns&lt;/td&gt;
&lt;td&gt;78.01ns&lt;/td&gt;
&lt;td&gt;114.70ns&lt;/td&gt;
&lt;td&gt;1.66us&lt;/td&gt;
&lt;td&gt;22.82us&lt;/td&gt;
&lt;td&gt;359.42us&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.88ms&lt;/td&gt;
&lt;td&gt;2.53ms&lt;/td&gt;
&lt;td&gt;3.16ms&lt;/td&gt;
&lt;td&gt;6.50ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;45.59ns&lt;/td&gt;
&lt;td&gt;74.90ns&lt;/td&gt;
&lt;td&gt;111.29ns&lt;/td&gt;
&lt;td&gt;1.57us&lt;/td&gt;
&lt;td&gt;22.47us&lt;/td&gt;
&lt;td&gt;351.31us&lt;/td&gt;
&lt;td&gt;1.23ms&lt;/td&gt;
&lt;td&gt;1.85ms&lt;/td&gt;
&lt;td&gt;2.49ms&lt;/td&gt;
&lt;td&gt;3.12ms&lt;/td&gt;
&lt;td&gt;6.45ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;45.11ns&lt;/td&gt;
&lt;td&gt;75.04ns&lt;/td&gt;
&lt;td&gt;111.28ns&lt;/td&gt;
&lt;td&gt;1.59us&lt;/td&gt;
&lt;td&gt;22.46us&lt;/td&gt;
&lt;td&gt;357.32us&lt;/td&gt;
&lt;td&gt;1.25ms&lt;/td&gt;
&lt;td&gt;1.89ms&lt;/td&gt;
&lt;td&gt;2.53ms&lt;/td&gt;
&lt;td&gt;3.15ms&lt;/td&gt;
&lt;td&gt;6.47ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If we look at the optimized version, the differences are even slower. Again, the
clang-based compilers are producing the fastest executables, but are closely
followed by gcc, except for gcc-6.3 in which we can still see the same
regression as before.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="logistic-sigmoid"&gt;
&lt;h3&gt;Logistic Sigmoid&lt;/h3&gt;
&lt;p&gt;The next test is to check the performance of the sigmoid operation. In that
case, the evaluator of the library will try to use parallelization and
vectorization to compute it. Let's see how the different compilers fare:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="22%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="13%"&gt;
&lt;col width="15%"&gt;
&lt;col width="13%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sigmoid&lt;/th&gt;
&lt;th class="head"&gt;10&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;10000&lt;/th&gt;
&lt;th class="head"&gt;100000&lt;/th&gt;
&lt;th class="head"&gt;1000000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;8.16us&lt;/td&gt;
&lt;td&gt;5.23us&lt;/td&gt;
&lt;td&gt;6.33us&lt;/td&gt;
&lt;td&gt;29.56us&lt;/td&gt;
&lt;td&gt;259.72us&lt;/td&gt;
&lt;td&gt;2.78ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;7.07us&lt;/td&gt;
&lt;td&gt;5.08us&lt;/td&gt;
&lt;td&gt;6.39us&lt;/td&gt;
&lt;td&gt;29.44us&lt;/td&gt;
&lt;td&gt;266.27us&lt;/td&gt;
&lt;td&gt;2.96ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;7.13us&lt;/td&gt;
&lt;td&gt;5.32us&lt;/td&gt;
&lt;td&gt;6.45us&lt;/td&gt;
&lt;td&gt;28.99us&lt;/td&gt;
&lt;td&gt;261.81us&lt;/td&gt;
&lt;td&gt;2.86ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;7.03us&lt;/td&gt;
&lt;td&gt;5.09us&lt;/td&gt;
&lt;td&gt;6.24us&lt;/td&gt;
&lt;td&gt;28.61us&lt;/td&gt;
&lt;td&gt;252.78us&lt;/td&gt;
&lt;td&gt;2.71ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;7.30us&lt;/td&gt;
&lt;td&gt;5.25us&lt;/td&gt;
&lt;td&gt;6.57us&lt;/td&gt;
&lt;td&gt;30.24us&lt;/td&gt;
&lt;td&gt;256.75us&lt;/td&gt;
&lt;td&gt;1.99ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;7.47us&lt;/td&gt;
&lt;td&gt;5.14us&lt;/td&gt;
&lt;td&gt;5.77us&lt;/td&gt;
&lt;td&gt;26.03us&lt;/td&gt;
&lt;td&gt;235.87us&lt;/td&gt;
&lt;td&gt;1.81ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;7.51us&lt;/td&gt;
&lt;td&gt;5.26us&lt;/td&gt;
&lt;td&gt;6.48us&lt;/td&gt;
&lt;td&gt;28.86us&lt;/td&gt;
&lt;td&gt;258.31us&lt;/td&gt;
&lt;td&gt;1.95ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Interestingly, we can see that gcc-7.1 is the fastest for small vectors while
clang-4.0 is the best for producing code for larger vectors. However, except for
the biggest vector size, the difference is not really significantly. Apparently,
there is a regression in zapcc (or clang-5.0) since it's slower than clang-4.0
at the same level as clang-3.9.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="y-alpha-x-y-axpy"&gt;
&lt;h3&gt;y = alpha * x + y (axpy)&lt;/h3&gt;
&lt;p&gt;The third benchmark is the well-known axpy (y = alpha * x + y). This is entirely
resolved by expressions templates in the library, no specific algorithm is used.
Let's see the results:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="24%"&gt;
&lt;col width="13%"&gt;
&lt;col width="13%"&gt;
&lt;col width="11%"&gt;
&lt;col width="13%"&gt;
&lt;col width="13%"&gt;
&lt;col width="14%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;saxpy&lt;/th&gt;
&lt;th class="head"&gt;10&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;10000&lt;/th&gt;
&lt;th class="head"&gt;100000&lt;/th&gt;
&lt;th class="head"&gt;1000000&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;38.1ns&lt;/td&gt;
&lt;td&gt;61.6ns&lt;/td&gt;
&lt;td&gt;374ns&lt;/td&gt;
&lt;td&gt;3.65us&lt;/td&gt;
&lt;td&gt;40.8us&lt;/td&gt;
&lt;td&gt;518us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;35.0ns&lt;/td&gt;
&lt;td&gt;58.1ns&lt;/td&gt;
&lt;td&gt;383ns&lt;/td&gt;
&lt;td&gt;3.87us&lt;/td&gt;
&lt;td&gt;43.2us&lt;/td&gt;
&lt;td&gt;479us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;34.3ns&lt;/td&gt;
&lt;td&gt;59.4ns&lt;/td&gt;
&lt;td&gt;371ns&lt;/td&gt;
&lt;td&gt;3.57us&lt;/td&gt;
&lt;td&gt;40.4us&lt;/td&gt;
&lt;td&gt;452us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;34.8ns&lt;/td&gt;
&lt;td&gt;59.7ns&lt;/td&gt;
&lt;td&gt;399ns&lt;/td&gt;
&lt;td&gt;3.78us&lt;/td&gt;
&lt;td&gt;43.1us&lt;/td&gt;
&lt;td&gt;547us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;32.3ns&lt;/td&gt;
&lt;td&gt;53.8ns&lt;/td&gt;
&lt;td&gt;297ns&lt;/td&gt;
&lt;td&gt;3.21us&lt;/td&gt;
&lt;td&gt;38.3us&lt;/td&gt;
&lt;td&gt;466us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;32.4ns&lt;/td&gt;
&lt;td&gt;59.8ns&lt;/td&gt;
&lt;td&gt;296ns&lt;/td&gt;
&lt;td&gt;3.31us&lt;/td&gt;
&lt;td&gt;38.2us&lt;/td&gt;
&lt;td&gt;475us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;32.0ns&lt;/td&gt;
&lt;td&gt;54.0ns&lt;/td&gt;
&lt;td&gt;333ns&lt;/td&gt;
&lt;td&gt;3.32us&lt;/td&gt;
&lt;td&gt;38.7us&lt;/td&gt;
&lt;td&gt;447us&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Even on the biggest vector, this is a very fast operation, once vectorized and
parallelized. At this speed, some of the differences observed may not be highly
significant. Again clang-based versions are the fastest versions on this code,
but by a small margin.  There also seems to be a slight regression in gcc-7.1,
but again quite small.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="matrix-matrix-multiplication-gemm"&gt;
&lt;h3&gt;Matrix Matrix multiplication (GEMM)&lt;/h3&gt;
&lt;p&gt;The next benchmark is testing the performance of a Matrix-Matrix Multiplication,
an operation known as GEMM in the BLAS nomenclature. In that case, we test both
the naive and the optimized vectorized implementation. To save some horizontal
space, I've split the tables in two.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="23%"&gt;
&lt;col width="12%"&gt;
&lt;col width="14%"&gt;
&lt;col width="15%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sgemm (std)&lt;/th&gt;
&lt;th class="head"&gt;10&lt;/th&gt;
&lt;th class="head"&gt;20&lt;/th&gt;
&lt;th class="head"&gt;40&lt;/th&gt;
&lt;th class="head"&gt;60&lt;/th&gt;
&lt;th class="head"&gt;80&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;7.04us&lt;/td&gt;
&lt;td&gt;50.15us&lt;/td&gt;
&lt;td&gt;356.42us&lt;/td&gt;
&lt;td&gt;1.18ms&lt;/td&gt;
&lt;td&gt;3.41ms&lt;/td&gt;
&lt;td&gt;5.56ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;8.14us&lt;/td&gt;
&lt;td&gt;74.77us&lt;/td&gt;
&lt;td&gt;513.64us&lt;/td&gt;
&lt;td&gt;1.72ms&lt;/td&gt;
&lt;td&gt;4.05ms&lt;/td&gt;
&lt;td&gt;7.92ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;8.03us&lt;/td&gt;
&lt;td&gt;64.78us&lt;/td&gt;
&lt;td&gt;504.41us&lt;/td&gt;
&lt;td&gt;1.69ms&lt;/td&gt;
&lt;td&gt;4.02ms&lt;/td&gt;
&lt;td&gt;7.87ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;7.95us&lt;/td&gt;
&lt;td&gt;65.00us&lt;/td&gt;
&lt;td&gt;508.84us&lt;/td&gt;
&lt;td&gt;1.69ms&lt;/td&gt;
&lt;td&gt;4.02ms&lt;/td&gt;
&lt;td&gt;7.84ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;3.58us&lt;/td&gt;
&lt;td&gt;28.59us&lt;/td&gt;
&lt;td&gt;222.36us&lt;/td&gt;
&lt;td&gt;0.73ms&lt;/td&gt;
&lt;td&gt;1.77us&lt;/td&gt;
&lt;td&gt;3.41ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;4.00us&lt;/td&gt;
&lt;td&gt;25.47us&lt;/td&gt;
&lt;td&gt;190.56us&lt;/td&gt;
&lt;td&gt;0.61ms&lt;/td&gt;
&lt;td&gt;1.45us&lt;/td&gt;
&lt;td&gt;2.80ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;4.00us&lt;/td&gt;
&lt;td&gt;25.38us&lt;/td&gt;
&lt;td&gt;189.98us&lt;/td&gt;
&lt;td&gt;0.60ms&lt;/td&gt;
&lt;td&gt;1.43us&lt;/td&gt;
&lt;td&gt;2.81ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="15%"&gt;
&lt;col width="9%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;col width="7%"&gt;
&lt;col width="7%"&gt;
&lt;col width="7%"&gt;
&lt;col width="7%"&gt;
&lt;col width="7%"&gt;
&lt;col width="8%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sgemm (std)&lt;/th&gt;
&lt;th class="head"&gt;200&lt;/th&gt;
&lt;th class="head"&gt;300&lt;/th&gt;
&lt;th class="head"&gt;400&lt;/th&gt;
&lt;th class="head"&gt;500&lt;/th&gt;
&lt;th class="head"&gt;600&lt;/th&gt;
&lt;th class="head"&gt;700&lt;/th&gt;
&lt;th class="head"&gt;800&lt;/th&gt;
&lt;th class="head"&gt;900&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;1200&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;44.16ms&lt;/td&gt;
&lt;td&gt;148.88ms&lt;/td&gt;
&lt;td&gt;455.81ms&lt;/td&gt;
&lt;td&gt;687.96ms&lt;/td&gt;
&lt;td&gt;1.47s&lt;/td&gt;
&lt;td&gt;1.98s&lt;/td&gt;
&lt;td&gt;2.81s&lt;/td&gt;
&lt;td&gt;4.00s&lt;/td&gt;
&lt;td&gt;5.91s&lt;/td&gt;
&lt;td&gt;9.52s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;63.17ms&lt;/td&gt;
&lt;td&gt;213.01ms&lt;/td&gt;
&lt;td&gt;504.83ms&lt;/td&gt;
&lt;td&gt;984.90ms&lt;/td&gt;
&lt;td&gt;1.70s&lt;/td&gt;
&lt;td&gt;2.70s&lt;/td&gt;
&lt;td&gt;4.03s&lt;/td&gt;
&lt;td&gt;5.74s&lt;/td&gt;
&lt;td&gt;7.87s&lt;/td&gt;
&lt;td&gt;14.905&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;64.04ms&lt;/td&gt;
&lt;td&gt;212.12ms&lt;/td&gt;
&lt;td&gt;502.95ms&lt;/td&gt;
&lt;td&gt;981.74ms&lt;/td&gt;
&lt;td&gt;1.69s&lt;/td&gt;
&lt;td&gt;2.69s&lt;/td&gt;
&lt;td&gt;4.13s&lt;/td&gt;
&lt;td&gt;5.85s&lt;/td&gt;
&lt;td&gt;8.10s&lt;/td&gt;
&lt;td&gt;14.08s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;62.57ms&lt;/td&gt;
&lt;td&gt;210.72ms&lt;/td&gt;
&lt;td&gt;499.68ms&lt;/td&gt;
&lt;td&gt;974.94ms&lt;/td&gt;
&lt;td&gt;1.68s&lt;/td&gt;
&lt;td&gt;2.67s&lt;/td&gt;
&lt;td&gt;3.99s&lt;/td&gt;
&lt;td&gt;5.68s&lt;/td&gt;
&lt;td&gt;7.85s&lt;/td&gt;
&lt;td&gt;13.49s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;27.48ms&lt;/td&gt;
&lt;td&gt;90.85ms&lt;/td&gt;
&lt;td&gt;219.34ms&lt;/td&gt;
&lt;td&gt;419.53ms&lt;/td&gt;
&lt;td&gt;0.72s&lt;/td&gt;
&lt;td&gt;1.18s&lt;/td&gt;
&lt;td&gt;1.90s&lt;/td&gt;
&lt;td&gt;2.44s&lt;/td&gt;
&lt;td&gt;3.36s&lt;/td&gt;
&lt;td&gt;5.84s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;22.01ms&lt;/td&gt;
&lt;td&gt;73.90ms&lt;/td&gt;
&lt;td&gt;175.02ms&lt;/td&gt;
&lt;td&gt;340.70ms&lt;/td&gt;
&lt;td&gt;0.58s&lt;/td&gt;
&lt;td&gt;0.93s&lt;/td&gt;
&lt;td&gt;1.40s&lt;/td&gt;
&lt;td&gt;1.98s&lt;/td&gt;
&lt;td&gt;2.79s&lt;/td&gt;
&lt;td&gt;4.69s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;22.33ms&lt;/td&gt;
&lt;td&gt;75.80ms&lt;/td&gt;
&lt;td&gt;181.27ms&lt;/td&gt;
&lt;td&gt;359.13ms&lt;/td&gt;
&lt;td&gt;0.63s&lt;/td&gt;
&lt;td&gt;1.02s&lt;/td&gt;
&lt;td&gt;1.52s&lt;/td&gt;
&lt;td&gt;2.24s&lt;/td&gt;
&lt;td&gt;3.21s&lt;/td&gt;
&lt;td&gt;5.62s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This time, the differences between the different compilers are very significant.
The clang compilers are leading the way by a large margin here, with clang-4.0
being the fastest of them (by another nice margin). Indeed, clang-4.0.1 is
producing code that is, on average, about twice faster than the code generated
by the best GCC compiler. Very interestingly as well, we can see a huge
regression starting from GCC-5.4 and that is still here in GCC-7.1. Indeed, the
best GCC version, in the tested versions, is again GCC-4.9.4. Clang is really
doing an excellent job of compiling the GEMM code.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="21%"&gt;
&lt;col width="14%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;col width="14%"&gt;
&lt;col width="14%"&gt;
&lt;col width="13%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sgemm (vec)&lt;/th&gt;
&lt;th class="head"&gt;10&lt;/th&gt;
&lt;th class="head"&gt;20&lt;/th&gt;
&lt;th class="head"&gt;40&lt;/th&gt;
&lt;th class="head"&gt;60&lt;/th&gt;
&lt;th class="head"&gt;80&lt;/th&gt;
&lt;th class="head"&gt;100&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;264.27ns&lt;/td&gt;
&lt;td&gt;0.95us&lt;/td&gt;
&lt;td&gt;3.28us&lt;/td&gt;
&lt;td&gt;14.77us&lt;/td&gt;
&lt;td&gt;23.50us&lt;/td&gt;
&lt;td&gt;60.37us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;271.41ns&lt;/td&gt;
&lt;td&gt;0.99us&lt;/td&gt;
&lt;td&gt;3.31us&lt;/td&gt;
&lt;td&gt;14.811us&lt;/td&gt;
&lt;td&gt;24.116us&lt;/td&gt;
&lt;td&gt;61.00us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;279.72ns&lt;/td&gt;
&lt;td&gt;1.02us&lt;/td&gt;
&lt;td&gt;3.27us&lt;/td&gt;
&lt;td&gt;15.39us&lt;/td&gt;
&lt;td&gt;24.29us&lt;/td&gt;
&lt;td&gt;61.99us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;273.74ns&lt;/td&gt;
&lt;td&gt;0.96us&lt;/td&gt;
&lt;td&gt;3.81us&lt;/td&gt;
&lt;td&gt;15.55us&lt;/td&gt;
&lt;td&gt;31.35us&lt;/td&gt;
&lt;td&gt;71.11us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;296.67ns&lt;/td&gt;
&lt;td&gt;1.34us&lt;/td&gt;
&lt;td&gt;4.18us&lt;/td&gt;
&lt;td&gt;19.93us&lt;/td&gt;
&lt;td&gt;33.15us&lt;/td&gt;
&lt;td&gt;82.60us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;322.68ns&lt;/td&gt;
&lt;td&gt;1.38us&lt;/td&gt;
&lt;td&gt;4.17us&lt;/td&gt;
&lt;td&gt;20.19us&lt;/td&gt;
&lt;td&gt;34.17us&lt;/td&gt;
&lt;td&gt;83.64us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;307.49ns&lt;/td&gt;
&lt;td&gt;1.41us&lt;/td&gt;
&lt;td&gt;4.10us&lt;/td&gt;
&lt;td&gt;19.72us&lt;/td&gt;
&lt;td&gt;33.72us&lt;/td&gt;
&lt;td&gt;84.80us&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="14%"&gt;
&lt;col width="10%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="8%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sgemm (vec)&lt;/th&gt;
&lt;th class="head"&gt;200&lt;/th&gt;
&lt;th class="head"&gt;300&lt;/th&gt;
&lt;th class="head"&gt;400&lt;/th&gt;
&lt;th class="head"&gt;500&lt;/th&gt;
&lt;th class="head"&gt;600&lt;/th&gt;
&lt;th class="head"&gt;700&lt;/th&gt;
&lt;th class="head"&gt;800&lt;/th&gt;
&lt;th class="head"&gt;900&lt;/th&gt;
&lt;th class="head"&gt;1000&lt;/th&gt;
&lt;th class="head"&gt;1200&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;369.52us&lt;/td&gt;
&lt;td&gt;1.62ms&lt;/td&gt;
&lt;td&gt;2.91ms&lt;/td&gt;
&lt;td&gt;7.17ms&lt;/td&gt;
&lt;td&gt;11.74ms&lt;/td&gt;
&lt;td&gt;22.91ms&lt;/td&gt;
&lt;td&gt;34.82ms&lt;/td&gt;
&lt;td&gt;51.67ms&lt;/td&gt;
&lt;td&gt;64.36ms&lt;/td&gt;
&lt;td&gt;111.15ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;387.54us&lt;/td&gt;
&lt;td&gt;1.60ms&lt;/td&gt;
&lt;td&gt;2.97ms&lt;/td&gt;
&lt;td&gt;7.36ms&lt;/td&gt;
&lt;td&gt;12.11ms&lt;/td&gt;
&lt;td&gt;24.37ms&lt;/td&gt;
&lt;td&gt;35.37ms&lt;/td&gt;
&lt;td&gt;52.27ms&lt;/td&gt;
&lt;td&gt;65.72ms&lt;/td&gt;
&lt;td&gt;112.74ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;384.43us&lt;/td&gt;
&lt;td&gt;1.74ms&lt;/td&gt;
&lt;td&gt;3.12ms&lt;/td&gt;
&lt;td&gt;7.16ms&lt;/td&gt;
&lt;td&gt;12.44ms&lt;/td&gt;
&lt;td&gt;24.15ms&lt;/td&gt;
&lt;td&gt;34.87ms&lt;/td&gt;
&lt;td&gt;52.59ms&lt;/td&gt;
&lt;td&gt;70.074ms&lt;/td&gt;
&lt;td&gt;119.22ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;458.05us&lt;/td&gt;
&lt;td&gt;1.81ms&lt;/td&gt;
&lt;td&gt;3.44ms&lt;/td&gt;
&lt;td&gt;7.86ms&lt;/td&gt;
&lt;td&gt;13.43ms&lt;/td&gt;
&lt;td&gt;24.70ms&lt;/td&gt;
&lt;td&gt;36.54ms&lt;/td&gt;
&lt;td&gt;53.47ms&lt;/td&gt;
&lt;td&gt;66.87ms&lt;/td&gt;
&lt;td&gt;117.25ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;494.52us&lt;/td&gt;
&lt;td&gt;1.96ms&lt;/td&gt;
&lt;td&gt;4.80ms&lt;/td&gt;
&lt;td&gt;8.88ms&lt;/td&gt;
&lt;td&gt;18.20ms&lt;/td&gt;
&lt;td&gt;29.37ms&lt;/td&gt;
&lt;td&gt;41.24ms&lt;/td&gt;
&lt;td&gt;60.72ms&lt;/td&gt;
&lt;td&gt;72.28ms&lt;/td&gt;
&lt;td&gt;123.75ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;511.24us&lt;/td&gt;
&lt;td&gt;2.04ms&lt;/td&gt;
&lt;td&gt;4.11ms&lt;/td&gt;
&lt;td&gt;9.46ms&lt;/td&gt;
&lt;td&gt;15.34ms&lt;/td&gt;
&lt;td&gt;27.23ms&lt;/td&gt;
&lt;td&gt;38.27ms&lt;/td&gt;
&lt;td&gt;58.14ms&lt;/td&gt;
&lt;td&gt;72.78ms&lt;/td&gt;
&lt;td&gt;128.60ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;492.28us&lt;/td&gt;
&lt;td&gt;2.03ms&lt;/td&gt;
&lt;td&gt;3.90ms&lt;/td&gt;
&lt;td&gt;9.00ms&lt;/td&gt;
&lt;td&gt;14.31ms&lt;/td&gt;
&lt;td&gt;25.72ms&lt;/td&gt;
&lt;td&gt;37.09ms&lt;/td&gt;
&lt;td&gt;55.79ms&lt;/td&gt;
&lt;td&gt;67.88ms&lt;/td&gt;
&lt;td&gt;119.92ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As for the optimized version, it seems that the two families are reversed.
Indeed, GCC is doing a better job than clang here, and although the margin is
not as big as before, it's still significant. We can still observe a small
regression in GCC versions because the 4.9 version is again the fastest. As for
clang versions, it seems that clang-5.0 (used in zapcc) has had some performance
improvements for this case.&lt;/p&gt;
&lt;p&gt;For this case of matrix-matrix multiplication, it's very impressive that the
differences in the non-optimized code are so significant. And it's also
impressive that each family of compilers has its own strength, clang being
seemingly much better at handling unoptimized code while GCC is better at
handling vectorized code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="convolution-2d"&gt;
&lt;h3&gt;Convolution (2D)&lt;/h3&gt;
&lt;p&gt;The last benchmark that I considered is the case of the valid convolution on 2D
images. The code is quite similar to the GEMM code but more complicated to
optimized due to cache locality.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="19%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;col width="10%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sconv2_valid (std)&lt;/th&gt;
&lt;th class="head"&gt;100x50&lt;/th&gt;
&lt;th class="head"&gt;105x50&lt;/th&gt;
&lt;th class="head"&gt;110x55&lt;/th&gt;
&lt;th class="head"&gt;115x55&lt;/th&gt;
&lt;th class="head"&gt;120x60&lt;/th&gt;
&lt;th class="head"&gt;125x60&lt;/th&gt;
&lt;th class="head"&gt;130x65&lt;/th&gt;
&lt;th class="head"&gt;135x65&lt;/th&gt;
&lt;th class="head"&gt;140x70&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;27.93ms&lt;/td&gt;
&lt;td&gt;33.68ms&lt;/td&gt;
&lt;td&gt;40.62ms&lt;/td&gt;
&lt;td&gt;48.23ms&lt;/td&gt;
&lt;td&gt;57.27ms&lt;/td&gt;
&lt;td&gt;67.02ms&lt;/td&gt;
&lt;td&gt;78.45ms&lt;/td&gt;
&lt;td&gt;92.53ms&lt;/td&gt;
&lt;td&gt;105.08ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;37.60ms&lt;/td&gt;
&lt;td&gt;44.94ms&lt;/td&gt;
&lt;td&gt;54.24ms&lt;/td&gt;
&lt;td&gt;64.45ms&lt;/td&gt;
&lt;td&gt;76.63ms&lt;/td&gt;
&lt;td&gt;89.75ms&lt;/td&gt;
&lt;td&gt;105.08ms&lt;/td&gt;
&lt;td&gt;121.66ms&lt;/td&gt;
&lt;td&gt;140.95ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;37.10ms&lt;/td&gt;
&lt;td&gt;44.99ms&lt;/td&gt;
&lt;td&gt;54.34ms&lt;/td&gt;
&lt;td&gt;64.54ms&lt;/td&gt;
&lt;td&gt;76.54ms&lt;/td&gt;
&lt;td&gt;89.87ms&lt;/td&gt;
&lt;td&gt;105.35ms&lt;/td&gt;
&lt;td&gt;121.94ms&lt;/td&gt;
&lt;td&gt;141.20ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;37.55ms&lt;/td&gt;
&lt;td&gt;45.08ms&lt;/td&gt;
&lt;td&gt;54.39ms&lt;/td&gt;
&lt;td&gt;64.48ms&lt;/td&gt;
&lt;td&gt;76.51ms&lt;/td&gt;
&lt;td&gt;92.02ms&lt;/td&gt;
&lt;td&gt;106.16ms&lt;/td&gt;
&lt;td&gt;125.67ms&lt;/td&gt;
&lt;td&gt;143.57ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;15.42ms&lt;/td&gt;
&lt;td&gt;18.59ms&lt;/td&gt;
&lt;td&gt;22.21ms&lt;/td&gt;
&lt;td&gt;26.40ms&lt;/td&gt;
&lt;td&gt;31.03ms&lt;/td&gt;
&lt;td&gt;36.26ms&lt;/td&gt;
&lt;td&gt;42.35ms&lt;/td&gt;
&lt;td&gt;48.87ms&lt;/td&gt;
&lt;td&gt;56.29ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;15.48ms&lt;/td&gt;
&lt;td&gt;18.67ms&lt;/td&gt;
&lt;td&gt;22.34ms&lt;/td&gt;
&lt;td&gt;26.50ms&lt;/td&gt;
&lt;td&gt;31.27ms&lt;/td&gt;
&lt;td&gt;36.58ms&lt;/td&gt;
&lt;td&gt;42.61ms&lt;/td&gt;
&lt;td&gt;49.33ms&lt;/td&gt;
&lt;td&gt;56.80ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;15.29ms&lt;/td&gt;
&lt;td&gt;18.37ms&lt;/td&gt;
&lt;td&gt;22.00ms&lt;/td&gt;
&lt;td&gt;26.10ms&lt;/td&gt;
&lt;td&gt;30.75ms&lt;/td&gt;
&lt;td&gt;35.95ms&lt;/td&gt;
&lt;td&gt;41.85ms&lt;/td&gt;
&lt;td&gt;48.42ms&lt;/td&gt;
&lt;td&gt;55.74ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In that case, we can observe the same as for the GEMM. The clang-based versions
are much producing significantly faster code than the GCC versions. Moreover, we
can also observe the same large regression starting from GCC-5.4.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="21%"&gt;
&lt;col width="11%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;col width="9%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;sconv2_valid (vec)&lt;/th&gt;
&lt;th class="head"&gt;100x50&lt;/th&gt;
&lt;th class="head"&gt;105x50&lt;/th&gt;
&lt;th class="head"&gt;110x55&lt;/th&gt;
&lt;th class="head"&gt;115x55&lt;/th&gt;
&lt;th class="head"&gt;120x60&lt;/th&gt;
&lt;th class="head"&gt;125x60&lt;/th&gt;
&lt;th class="head"&gt;130x65&lt;/th&gt;
&lt;th class="head"&gt;135x65&lt;/th&gt;
&lt;th class="head"&gt;140x70&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.4&lt;/td&gt;
&lt;td&gt;878.32us&lt;/td&gt;
&lt;td&gt;1.07ms&lt;/td&gt;
&lt;td&gt;1.20ms&lt;/td&gt;
&lt;td&gt;1.68ms&lt;/td&gt;
&lt;td&gt;2.04ms&lt;/td&gt;
&lt;td&gt;2.06ms&lt;/td&gt;
&lt;td&gt;2.54ms&lt;/td&gt;
&lt;td&gt;3.20ms&lt;/td&gt;
&lt;td&gt;4.14ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;853.73us&lt;/td&gt;
&lt;td&gt;1.03ms&lt;/td&gt;
&lt;td&gt;1.15ms&lt;/td&gt;
&lt;td&gt;1.36ms&lt;/td&gt;
&lt;td&gt;1.76ms&lt;/td&gt;
&lt;td&gt;2.05ms&lt;/td&gt;
&lt;td&gt;2.44ms&lt;/td&gt;
&lt;td&gt;2.91ms&lt;/td&gt;
&lt;td&gt;3.13ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-6.3.0&lt;/td&gt;
&lt;td&gt;847.95us&lt;/td&gt;
&lt;td&gt;1.02ms&lt;/td&gt;
&lt;td&gt;1.14ms&lt;/td&gt;
&lt;td&gt;1.35ms&lt;/td&gt;
&lt;td&gt;1.74ms&lt;/td&gt;
&lt;td&gt;1.98ms&lt;/td&gt;
&lt;td&gt;2.43ms&lt;/td&gt;
&lt;td&gt;2.90ms&lt;/td&gt;
&lt;td&gt;3.12ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-7.1.0&lt;/td&gt;
&lt;td&gt;795.82us&lt;/td&gt;
&lt;td&gt;0.93ms&lt;/td&gt;
&lt;td&gt;1.05ms&lt;/td&gt;
&lt;td&gt;1.24ms&lt;/td&gt;
&lt;td&gt;1.60ms&lt;/td&gt;
&lt;td&gt;1.77ms&lt;/td&gt;
&lt;td&gt;2.20ms&lt;/td&gt;
&lt;td&gt;2.69ms&lt;/td&gt;
&lt;td&gt;2.81ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9.1&lt;/td&gt;
&lt;td&gt;782.46us&lt;/td&gt;
&lt;td&gt;0.93ms&lt;/td&gt;
&lt;td&gt;1.05ms&lt;/td&gt;
&lt;td&gt;1.26ms&lt;/td&gt;
&lt;td&gt;1.60ms&lt;/td&gt;
&lt;td&gt;1.84ms&lt;/td&gt;
&lt;td&gt;2.21ms&lt;/td&gt;
&lt;td&gt;2.65ms&lt;/td&gt;
&lt;td&gt;2.84ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-4.0.1&lt;/td&gt;
&lt;td&gt;767.58us&lt;/td&gt;
&lt;td&gt;0.92ms&lt;/td&gt;
&lt;td&gt;1.04ms&lt;/td&gt;
&lt;td&gt;1.25ms&lt;/td&gt;
&lt;td&gt;1.59ms&lt;/td&gt;
&lt;td&gt;1.83ms&lt;/td&gt;
&lt;td&gt;2.20ms&lt;/td&gt;
&lt;td&gt;2.62ms&lt;/td&gt;
&lt;td&gt;2.83ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++-1.0&lt;/td&gt;
&lt;td&gt;782.49us&lt;/td&gt;
&lt;td&gt;0.94ms&lt;/td&gt;
&lt;td&gt;1.06ms&lt;/td&gt;
&lt;td&gt;1.27ms&lt;/td&gt;
&lt;td&gt;1.62ms&lt;/td&gt;
&lt;td&gt;1.83ms&lt;/td&gt;
&lt;td&gt;2.24ms&lt;/td&gt;
&lt;td&gt;2.65ms&lt;/td&gt;
&lt;td&gt;2.85ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This time, clang manages to produce excellent results. Indeed, all the produced
executables are significantly faster than the versions produced by GCC, except
for GCC-7.1 which is producing similar results. The other versions of GCC are
falling behind it seems. It seems that it was only for the GEMM that clang was
having a lot of troubles handling the optimized code.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Clang seems to have recently done a lot of optimizations regarding compilation
time. Indeed, clang-4.0.1 is much faster for compilation than clang-3.9.
Although GCC-7.1 is faster than GCC-6.3, all the GCC versions are slower than
GCC-4.9.4 which is the fastest at compiling code with optimizations. GCC-7.1 is
the fastest GCC version for compiling code in debug mode.&lt;/p&gt;
&lt;p&gt;In some cases, there is almost no difference between different compilers in the
generated code. However, in more  complex algorithms such as the matrix-matrix
multiplication or the two-dimensional convolution, the differences can be quite
significant. In my tests, Clang have shown to be much better at compiling
unoptimized code. However, and especially in the GEMM case, it seems to be worse
than GCC at handling hand-optimized. I will investigate that case and try to
tailor the code so that clang is having a better time with it.&lt;/p&gt;
&lt;p&gt;For me, it's really weird that the GCC regression, apparently starting from
GCC-5.4, has still not been fixed in GCC 7.1. I was thinking of dropping support
for GCC-4.9 in order to go full C++14 support, but now I may have to reconsider
my position. However, seeing that GCC is generally the best at handling
optimized code (especially for GEMM), I may be able to do the transition, since
the optimized code will be used in most cases.&lt;/p&gt;
&lt;p&gt;As for zapcc, although it is still the fastest compiler in debug mode, with the
new speed of clang-4.0.1, its margin is quite small. Moreover, on optimized
build, it's not as fast as GCC. If you use clang and can have access to zapcc,
it's still quite a good option to save some time.&lt;/p&gt;
&lt;p&gt;Overall, I have been quite pleased by clang-4.0.1 and GCC-7.1, the most recent
versions I have been testing. It seems that they did quite some good work.
I will definitely run some more tests with them and try to adapt the code. I'm
still considering whether I will drop support for some older compilers.&lt;/p&gt;
&lt;p&gt;I hope this comparison was interesting :) My next post will probably be about
the difference in performance between my machine learning framework and other
frameworks to train neural networks.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>C++14</category><category>clang</category><category>Compilers</category><category>etl</category><category>gcc</category><category>Performance</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/08/compiler-benchmark-gcc-clang-cpp-library-etl.html</guid><pubDate>Mon, 07 Aug 2017 07:16:21 GMT</pubDate></item><item><title>Expression Templates Library (ETL) 1.1</title><link>http://baptiste-wicht.com/posts/2017/08/expression-templates-library-etl-11.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;img alt="ETL Logo" class="align-center" src="http://baptiste-wicht.com/images/logo.png"&gt;
&lt;p&gt;It took me longer than I thought, but I'm glad to announce the release of the
version 1.1 of my Expression Templates Library (ETL) project. This is a major
new release with many improvements and new features. It's been almost one month
since the last, and first, release (1.0) was released. I should have done some
minor releases in the mean time, but at least now the library is in a good shape
for major version.&lt;/p&gt;
&lt;p&gt;It may be interesting to note that my machine learning framework (DLL), based on
the ETL library, has shown to be faster than all the tested popular frameworks
(Tensorflow, Keras, Caffee, Torch, DeepLearning4J) for training various neural
networks on CPU. I'll post more details on another post on the coming weeks, but
that shows that special attention to performance has been done in this library
and that it is well adapted to machine learning.&lt;/p&gt;
&lt;p&gt;For those of you that don't follow my blog, ETL is a library providing
Expression Templates for computations on matrix and vector. For instance, if you
have three matrices A, B and C you could write C++ code like this:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_3560cf90c38844ddbcc372fbe5cf9afd-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Or given vectors b, v, h and a matrix W, you could write code like this:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_cd5efbf4e37847a0b2d5f1aab4bcc6e8-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The goal of such library is two-fold. First, this makes the expression more
readable and as close to math as possible. And then, it allows the library to
compute the expressions as fast as possible.  In the first case, the framework
will compute the sum using a vectorized algorithm and then compute the overall
expression using yet again vectorized code. The expression can also be computed
in parallel if the matrices are big enough. In the second case, the
vector-matrix multiplication will be computed first using either hand-code
optimized vectorized or a BLAS routine (depending on configuration options).
Then, all the expression will be executed using vectorized code.&lt;/p&gt;
&lt;div class="section" id="features"&gt;
&lt;h2&gt;Features&lt;/h2&gt;
&lt;p&gt;Many new features have been integrated into the library.&lt;/p&gt;
&lt;p&gt;The support for machine learning operations has been improved. There are now
specific helpers for machine learning in the etl::ml namespace which have names
that are standard to machine learning. A real transposed convolution has been
implemented with support for padding and stride. Batched outer product and
batched bias averaging are also now supported. The activation function support
has also been improved and the derivatives have been reviewed. The pooling
operators have also been improved with stride and padding support. Unrelated to
machine learning, 2D and 3D pooling can also be done in higher dimensional
matrix now.&lt;/p&gt;
&lt;p&gt;New functions are also available for matrices and vectors. The support for
square root has been improved with cubic root and inverse root. Support has also
been added for floor and ceil. Moreover, comparison operators are now available
as well as global functions such as approx_equals.&lt;/p&gt;
&lt;p&gt;New reductions have also been added with support for absolute sum and mean
(asum/asum) and for min_index and max_index, which returns the index of the
minimum element, respectively the maximum. Finally, argmax can now be used to
get the max index in each sub dimensions of a matrix. argmax on a vector is
equivalent to max_index.&lt;/p&gt;
&lt;p&gt;Support for shuffling has also been added. By default, shuffling a vector means
shuffling all elements and shuffling a matrix means shuffling by shuffling the
sub matrices (only the first dimension is shuffled), but shuffling a matrix as
a vector is also possible. Shuffle of two vectors or two matrices in parallel,
is also possible. In that case, the same permutation is applied to both
containers. As a side note, all operations using random generation are also
available with an addition parameter for the random generator, which can help to
improve reproducibility or simply tune the random generator.&lt;/p&gt;
&lt;p&gt;I've also included support for adapters matrices. There are adapters for
hermitian matrices, symmetric matrices and lower and upper triangular matrices.
For now, the framework does not take advantage of this information, this will be
done later, but the framework guarantee the different constrain on the content.&lt;/p&gt;
&lt;p&gt;There are also a few new more minor features. Maybe not so minor, matrices can
now be sliced into sub matrices. With that a matrix can be divided into several
sub matrices and modifying the sub matrices will modify the source matrix. The
sub matrices are available in 2D, 3D and 4D for now. There are also some other
ways of slicing matrix and vectors. It is possible to obtain a slice of its
memory or obtain a slice of its first dimension. Another new feature is that it
is now possible compute the cross product of vectors now. Matrices can be
decomposed into their Q/R decomposition rather than only their PALU
decomposition. Special support has been integrated for matrix and vectors of
booleans. In that case, they support logical operators such as and, not and or.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="performance"&gt;
&lt;h2&gt;Performance&lt;/h2&gt;
&lt;p&gt;I've always considered the performance of this library to be a feature itself.
I consider the library to be quite fast, especially its convolution support,
even though there is still room for improvement. Therefore, many improvements
have been made to the performance of the library since the last release. As said
before, this library was used in a machine learning framework which then proved
faster than most popular neural network frameworks on CPU. I'll present here
the most important new improvements to performance, in no real particular order,
every bit being important in my opinion.&lt;/p&gt;
&lt;p&gt;First, several operations have been optimized to be faster.&lt;/p&gt;
&lt;p&gt;Multiplication of matrices or matrices and vectors are now much faster if one of
the matrix is transposed. Instead of performing the slow transposition,
different kernels are used in order to maximize performance without doing any
transposition, although sometimes transposition is performed when it is faster.
This leads to very significant improvements, up to 10 times faster in the best
case. This is performed for vectorized kernels and also for BLAS and CUBLAS
calls. These new kernels are also directly used when matrices of different
storage order are used. For instance, multiplying a column major matrix with
a row major matrix and storing the result in a column major matrix is now much
more efficient than before. Moreover, the performance of the transpose operation
itself is also much faster than before.&lt;/p&gt;
&lt;p&gt;A lot of machine learning operations have also been highly optimized. All the
pooling and upsample operators are now parallelized and the most used kernel
(2x2 pooling) is now more optimized. 4D convolution kernels (for machine
learning) have been greatly improved. There are now very specialized vectorized
kernels for classic kernel configurations (for instance 3x3 or 5x5) and the
selection of implementations is now smarter than before. The support of padding
is now much better than before for small amount of padding. Moreover, for small
kernels the full convolution can now be evaluated using the valid convolution
kernels directly with some padding, for much faster overall performance. The
exponential operation is now vectorized which allows operations such as sigmoid
or softmax to be much faster.&lt;/p&gt;
&lt;p&gt;Matrices and vector are automatically using aligned memory. This means that
vectorized code can use aligned operations, which may be slightly faster.
Moreover, matrices and vectors are now padded to a multiple of the vector size.
This allows to remove the final non-vectorized remainder loop from the
vectorized code. This is only done for the end of matrices, when they are
accessed in flat way. Contrary to some frameworks, inner dimensions of the
matrix are not padded.  Finally, accesses to 3D and 4D matrices is now much
faster than before.&lt;/p&gt;
&lt;p&gt;Then, the parallelization feature of ETL has been completely reworked. Before,
there was a thread pool for each algorithm that was parallelized. Now, there is
a global thread engine with one thread pool. Since parallelization is not nested
in ETL, this improves performance slightly by greatly diminishing the number of
threads that are created throughout an application. Another big difference in
parallel dispatching is that now it can detect good split based on alignment so
that each split are aligned. This then allows the vectorization process to use
aligned stores and loads instead of unaligned ones which may be faster on some
processors.&lt;/p&gt;
&lt;p&gt;Vectorization has also been greatly improved in ETL. Integer operations are now
automatically vectorized on processors that support this. Before, only floating
points operations were vectorized. The automatic vectorizer now is able to use
non-temporal stores for very large operations. A non-temporal store bypasses the
cache, thus gaining some time. Since very large matrices do not fit in cache
anyway and the cache would end up being overwritten anyway, this is a net gain.
Moreover, the alignment detection in the automatic vectorizer has also been
improved. Support for Fused-Multiply-Add (FMA) operations has also been
integrated in the algorithms that can make use of it (multiplications and
convolutions). The matrix-matrix multiplications and vector-matrix
multiplications now have highly optimized vectorized kernels. They also have
versions for column-major matrices now.  I plan to reintegrate a version of the
GEMM based on BLIS in the future but with more optimizations and support for all
precisions and integers, For my version is still slower than the simple
vectorized version. The sum and the dot product operations now also have
specialized vectorized implementations. The min and max operations are now
automatically-vectorized. Several others algorithms have also their own
vectorized implementations.&lt;/p&gt;
&lt;p&gt;Last, but not least, the GPU support has also been almost completely reworked.
Now, several operations can be chained without any copies between GPU and CPU.
Several new operations have also been added with support to GPU (convolutions,
pooling, sigmoid, ReLU, ...). Moreover, to complement operations that are not
available in any of the supported NVIDIA libraries, I've created a simple
library that can be used to add a few more GPU operations.  Nevertheless a lot
of operations are still missing and only algorithms are available not
expressions (such as c = a + b * 1.0) that are entirely computed on CPU. I have
plans to improve that further, probably for version 1.2. The different contexts
necessary for NVIDIA library can now be cached (using an option from ETL),
leading to much faster code. Only the main handle can be cached so far, I plan
to try to cache all the descriptors, but I don't know yet when that will be
ready. Finally, an option is also available to reuse GPU memory instead of
directly releasing it to CUDA. This is using a custom memory pool and can save
some time. Since this needs to be cleaned (by a call to etl::exit() or using
ETL_PROLOGUE), this is only activated on demand.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="other-changes"&gt;
&lt;h2&gt;Other changes&lt;/h2&gt;
&lt;p&gt;There also have been a lot of refactorings in the code of the library. A lot of
expressions now have less overhead and are specialized for performance.
Moreover, temporary expressions have been totally reworked to be more simple and
maintainable and easier to optimize in the future. It's also probably easier to
add new expressions to the framework now, although that could be even more
simple. There are also less duplicated code now in the different expressions.
Especially, now there are now more SSE and AVX variants in the code. All the
optimized algorithms are now using the vectorization system of the library.&lt;/p&gt;
&lt;p&gt;I also tried my best to reduce the compilation time, based on the unit tests.
This is still not great but better than before. For user code, the next version
should be much faster to compile since I plan to disable forced selection of
implementations by default and only enable it on demand.&lt;/p&gt;
&lt;p&gt;Finally, there also was quite a few bug fixes. Most of them have been found by
the use of the library in the Deep Learning Library (DLL) project. Some were
very small edge cases. For instance, the transposition algorithm was not working
on GPU on rectangular column major matrices. There also was a slight bug in the
Q/R decomposition and in the pooling of 4D matrices.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="what-s-next"&gt;
&lt;h2&gt;What's next ?&lt;/h2&gt;
&lt;p&gt;Next time, I may do some minor release, but I don't yet have a complete plan.
For the next major release (1.2 probably), here is what is planned:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Review the system for selection of algorithms to reduce compilation time&lt;/li&gt;
&lt;li&gt;Review the GPU system to allow more complete support for standard operators&lt;/li&gt;
&lt;li&gt;Switch to C++17: there are many improvements that could be done to the code with C++17 features&lt;/li&gt;
&lt;li&gt;Add support for convolution on mixed types (float/double)&lt;/li&gt;
&lt;li&gt;More tests for sparse matrix&lt;/li&gt;
&lt;li&gt;More algorithms support for sparse matrix&lt;/li&gt;
&lt;li&gt;Reduce the compilation time of the library in general&lt;/li&gt;
&lt;li&gt;Reduce the compilation and execution time of the unit tests&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are pretty big changes, especially the first two, so maybe it'll be split
into several releases. It will really depend on the time I have. As for C++17,
I really want to try it and I have a lot of points that could profit from the
switch, but that will means setting GCC 7.1 and Clang 3.9 as minimum
requirement, which may not be reasonable for every user.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="download-etl"&gt;
&lt;h2&gt;Download ETL&lt;/h2&gt;
&lt;p&gt;You can download ETL &lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;on Github&lt;/a&gt;. If you
only interested in the 1.1 version, you can look at the
&lt;a class="reference external" href="https://github.com/wichtounet/etl/releases"&gt;Releases pages&lt;/a&gt; or clone the tag
1.1. There are several branches:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;master&lt;/em&gt; Is the eternal development branch, may not always be stable&lt;/li&gt;
&lt;li&gt;&lt;em&gt;stable&lt;/em&gt; Is a branch always pointing to the last tag, no development here&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the future release, there always will tags pointing to the corresponding
commits. I'm not following the git flow way, I'd rather try to have a more
linear history with one eternal development branch, rather than an useless
develop branch or a load of other branches for releases.&lt;/p&gt;
&lt;p&gt;The documentation is a bit sparse. There are a few examples and the Wiki, but
there still is work to be done. If you have questions on how to use or configure
the library, please don't hesitate.&lt;/p&gt;
&lt;p&gt;Don't hesitate to comment this post if you have any comment on this library or
any question. You can also open an Issue on Github if you have a problem using
this library or propose a Pull Request if you have any contribution you'd like
to make to the library.&lt;/p&gt;
&lt;p&gt;Hope this may be useful to some of you :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>C++17</category><category>Compilers</category><category>dll</category><category>etl</category><category>GPU</category><category>Performance</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/08/expression-templates-library-etl-11.html</guid><pubDate>Fri, 04 Aug 2017 13:13:03 GMT</pubDate></item><item><title>Update on Deep Learning Library (DLL): Dropout, Batch Normalization, Adaptive Learning Rates, ...</title><link>http://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;It's been a while since I've posted something on this, especially since I had
one month vacation. This year I've been able to integrate a great number of
changes into my Deep Learning Library (DLL) project. It has seen a lot of
refactorings and a lot of new features making it look like a real neural network
library now. In this post, I'll try to outline the last new features and changes
of the library.&lt;/p&gt;
&lt;p&gt;For those that don't know, DLL is a library for neural network training, written
in C++ and for C++. You can train Fully-Connected Neural Networks and
Convolutional Neural Networks. The focus of the framework is on speed and easy
use in C++.&lt;/p&gt;
&lt;p&gt;As for my ETL project and again thanks to my thesis supervisor, the project now
has a logo:&lt;/p&gt;
&lt;img alt="DLL Logo" class="align-center" src="http://baptiste-wicht.com/images/dll_logo.png"&gt;
&lt;div class="section" id="adaptive-learning-rates"&gt;
&lt;h2&gt;Adaptive Learning Rates&lt;/h2&gt;
&lt;p&gt;Before, the framework only supported simple SGD and Momentum updates for the
different parameters of the network. Moreover, it was not very well extendable.
Therefore, I reviewed the system to be able to configure an optimizer for each
network to train. Once that was done, the first thing I did was to add support
for Nesterov Accelerated Gradients (NAG) as a third optimizer. After this,
I realized it was then easy to integrate support for more advanced optimizers
including support for adaptive learning rates. This means that the learning rate
will be adapted for each parameter depending on what the network is learning.
Some of the optimizers even don't need any learning rate. So far, I've
implemented support for the following optimizers: Adagrad, RMSProp, Adam (with
and without bias correction), Adamax (Adam with infinite norm), Nadam (Adam with
Nesterov momentum) and Adadelta (no more learning rate). The user can now choose
the optimizer of its choice, for instance NADAM, as a parameter of the network:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_98f0da6d9a6d4143933f13d57200b55b-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Use a Nadam optimizer&lt;/span&gt;
&lt;a name="rest_code_98f0da6d9a6d4143933f13d57200b55b-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;NADAM&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Another improvement in the same domain is that the learning rate can also be
decayed over time automatically by the optimizer.&lt;/p&gt;
&lt;p&gt;If you want more information on the different optimizers, you can have a look at
this very good article:
&lt;a class="reference external" href="http://ruder.io/optimizing-gradient-descent/"&gt;An overview of gradient descent optimization algorithms&lt;/a&gt;
from Sebastian Ruder.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="better-loss-support"&gt;
&lt;h2&gt;Better loss support&lt;/h2&gt;
&lt;p&gt;Before, DLL was automatically using Categorical Cross Entropy Loss, but it was
not possible to change it and it was not even possible to see the loss over
time. Now, the current value of the loss is displayed after each epoch of
training and the loss used for training is now configurable. So far, only three
different losses are supported, but it it not difficult to add new loss to the
system. The three losses supported are: Categorical Cross Entropy Loss, Binary
Cross Entropy Loss and Mean Squared Error Loss.&lt;/p&gt;
&lt;p&gt;Again, each network can specify the loss to use:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_b70b52a8ec5842ca802d4ad6b64ffd15-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Use a Binary Cross Entropy Loss&lt;/span&gt;
&lt;a name="rest_code_b70b52a8ec5842ca802d4ad6b64ffd15-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;BINARY_CROSS_ENTROPY&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="dropout"&gt;
&lt;h2&gt;Dropout&lt;/h2&gt;
&lt;p&gt;Dropout is a relatively new technique for neural network training. This is
especially made to reduce overfitting since a large number of sub networks will
be trained and it should prevent co-adaptation between different neurons. This
technique is relatively simple. Indeed, it simply randomly sets to zero some of
the input neurons of layers. At each batch, a new mask will be used and this
should lead to a large number of sub networks being trained.&lt;/p&gt;
&lt;p&gt;Here is example of a MLP with Dropout (p=0.5):&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_5d8cf0d10d51437290cac9704d13c031-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_dbn_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_5d8cf0d10d51437290cac9704d13c031-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_5d8cf0d10d51437290cac9704d13c031-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_5d8cf0d10d51437290cac9704d13c031-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dropout_layer_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_5d8cf0d10d51437290cac9704d13c031-5"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_5d8cf0d10d51437290cac9704d13c031-6"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dropout_layer_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_5d8cf0d10d51437290cac9704d13c031-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;SOFTMAX&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_5d8cf0d10d51437290cac9704d13c031-8"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;MOMENTUM&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;     &lt;span class="c1"&gt;// Momentum&lt;/span&gt;
&lt;a name="rest_code_5d8cf0d10d51437290cac9704d13c031-9"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;                          &lt;span class="c1"&gt;// The mini-batch size&lt;/span&gt;
&lt;a name="rest_code_5d8cf0d10d51437290cac9704d13c031-10"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;                                  &lt;span class="c1"&gt;// Shuffle before each epoch&lt;/span&gt;
&lt;a name="rest_code_5d8cf0d10d51437290cac9704d13c031-11"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="batch-normalization"&gt;
&lt;h2&gt;Batch Normalization&lt;/h2&gt;
&lt;p&gt;Batch Normalization is another new technique for training neural networks. This
technique will ensure that each of the layer will receive inputs that look
kind of similar. This is a very large advantage since then you reduce the
different in impact of hyper parameters on different layers. Google reported
much faster training with this technique by getting rid of Dropout and by
increasing the learning rate of training.&lt;/p&gt;
&lt;p&gt;Here is an example of using Batch Normalization in a CNN:&lt;/p&gt;
&lt;pre class="code C++"&gt;&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_dbn_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_normalization_layer_4d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-5"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_layer_2d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-6"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_normalization_layer_4d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-8"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_layer_2d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-9"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-10"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_normalization_layer_2d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-11"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;SOFTMAX&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-12"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ADADELTA&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;     &lt;span class="c1"&gt;// Adadelta&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-13"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;                          &lt;span class="c1"&gt;// The mini-batch size&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-14"&gt;&lt;/a&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;                                  &lt;span class="c1"&gt;// Shuffle the dataset before each epoch&lt;/span&gt;
&lt;a name="rest_code_5587f6973cee4fbba8294e1fb7b6a594-15"&gt;&lt;/a&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;You may notice that the layer is set as 4D so should only be used after
convolutional layer (or after the input). If you want to use it after
fully-connected layers, you can use the 2D version that works the same way.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="better-dataset-support"&gt;
&lt;h2&gt;Better dataset support&lt;/h2&gt;
&lt;p&gt;At the beginning, I designed DLL so that the user could directly pass data for
training in the form of STL Containers such as the std::vector. This is good in
some cases, but in some cases, the user does not know how to read the data , or
does not want to be bothered with it. Therefore, several data sets reader are
now available. Moreover, the entire system has been reworked to use generators
for data. A generator is simply a concept that has some data to produce. The
advantage of this new system is data augmentation is now supported every where
and much more efficiently than before. It is now possible to perform random
cropping and mirroring of images for instance. Moreover, the data augmentation
can be done in a secondary thread so as to be sure that there is always enough
data available for the training.&lt;/p&gt;
&lt;p&gt;The library now has a powerful dataset reader for both MNIST and CIFAR-10 and
the reader for ImageNet is almost ready. The project has already been used and
tested with these three datasets now. Moreover, the support for directly passing
STL containers has been maintained. In this case, a generator is simply created
around the data provided in the container and the generator is then passed to
the system for training.&lt;/p&gt;
&lt;p&gt;Here for instance is how to read MNIST data and scale (divide) all pixel values
by 255:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_50da657adbf34f3c863147af1554d40a-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Load the dataset&lt;/span&gt;
&lt;a name="rest_code_50da657adbf34f3c863147af1554d40a-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_mnist_dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;scale_pre&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{});&lt;/span&gt;
&lt;a name="rest_code_50da657adbf34f3c863147af1554d40a-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_50da657adbf34f3c863147af1554d40a-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_50da657adbf34f3c863147af1554d40a-5"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Train the network&lt;/span&gt;
&lt;a name="rest_code_50da657adbf34f3c863147af1554d40a-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_50da657adbf34f3c863147af1554d40a-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_50da657adbf34f3c863147af1554d40a-8"&gt;&lt;/a&gt;&lt;span class="c1"&gt;// Test the network&lt;/span&gt;
&lt;a name="rest_code_50da657adbf34f3c863147af1554d40a-9"&gt;&lt;/a&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="much-faster-performance"&gt;
&lt;h2&gt;Much faster performance&lt;/h2&gt;
&lt;p&gt;I've spent quite a lot of time improving the performance of the framework. I've
focused on every part of training in order to make training of neural networks
as fast as possible. I've also made a comparison of the framework against
several popular machine learning framework (Caffe, TensorFlow, Keras, Torch and
DeepLearning4J). For instance, here are the results on a small CNN experiment on
MNIST with all the different frameworks in CPU mode and in GPU mode:&lt;/p&gt;
&lt;img alt="DLL Comparison Against other frameworks" class="align-center" src="http://baptiste-wicht.com/images/dll_comparison.png"&gt;
&lt;p&gt;As you can see, DLL is by far the fastest framework on CPU. On GPU, there is
still some work to be done, but this is already ongoing (although a lot of work
remains). This is confirmed on each of the four experiments performed on MNIST,
CIFAR-10 and ImageNet, although the margin is smaller for larger networks (still
about 40% faster than TensorFlow and Keras which are the fastest framework after
DLL on CPU on my tests).&lt;/p&gt;
&lt;p&gt;Overall, DLL is between 2 and 4 times faster than before and is always the
fastest framework for neural network training when training is performed on CPU.&lt;/p&gt;
&lt;p&gt;I proposed a talk about these optimizations and performance for Meeting C++ this
year, but it has unfortunately not been accepted. We also have submitted
a publication about the framework to a conference later this year.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="examples"&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;The project now has a few examples (available &lt;a class="reference external" href="https://github.com/wichtounet/dll/tree/master/examples/src"&gt;here&lt;/a&gt;), well-designed and I try to update them with the latest updates of the framework.&lt;/p&gt;
&lt;p&gt;For instance, here is the CNN example for MNIST (without includes):&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-1"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="cm"&gt;/*argc*/&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;char&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="cm"&gt;/*argv*/&lt;/span&gt; &lt;span class="p"&gt;[])&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-2"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Load the dataset&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_mnist_dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;scale_pre&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{});&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-5"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Build the network&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_dbn_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-8"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-9"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-10"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_layer_2d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-11"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-12"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;mp_layer_2d_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-13"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-14"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dense_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;SOFTMAX&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-15"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;updater_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;MOMENTUM&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;     &lt;span class="c1"&gt;// Momentum&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-16"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;                          &lt;span class="c1"&gt;// The mini-batch size&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-17"&gt;&lt;/a&gt;        &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;                                  &lt;span class="c1"&gt;// Shuffle the dataset before each epoch&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-18"&gt;&lt;/a&gt;    &lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-19"&gt;&lt;/a&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-20"&gt;&lt;/a&gt;    &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-21"&gt;&lt;/a&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-22"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-23"&gt;&lt;/a&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-24"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Display the network and dataset&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-25"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-26"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;display&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-27"&gt;&lt;/a&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-28"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Train the network&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-29"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-30"&gt;&lt;/a&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-31"&gt;&lt;/a&gt;    &lt;span class="c1"&gt;// Test the network on test set&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-32"&gt;&lt;/a&gt;    &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-33"&gt;&lt;/a&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-34"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_55bf2586b6034f428bdde045a4becb8f-35"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="reproducible-results"&gt;
&lt;h2&gt;Reproducible results&lt;/h2&gt;
&lt;p&gt;And last, but maybe not least, I've finally united all the random number
generation code. This means that DLL can now set a global seed and that two
training of the same network and data with the same seed will now produce
exactly the same result.&lt;/p&gt;
&lt;p&gt;The usage is extremely simple:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_850781e48a564363993a79920cfd8d5e-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;set_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;After all these changes, I truly feel that the library is now in a much better
state and could be useful in several projects. I hope that this will be useful
to some more people. Moreover, as you can see by the performance results, the
framework is now extremely efficient at training neural networks on CPU.&lt;/p&gt;
&lt;p&gt;If you want more information, you can consult the
&lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;dll Github Repository&lt;/a&gt;. You can also add
a comment to this post. If you find any problem on the project or have specific
question or request, don't hesitate to open an issue on Github.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>deep learning</category><category>dll</category><category>etl</category><category>Machine Learning</category><category>publications</category><category>thesis</category><guid>http://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html</guid><pubDate>Sun, 16 Jul 2017 13:41:51 GMT</pubDate></item><item><title>Jenkins Tip: Send notifications on fixed builds in declarative pipeline</title><link>http://baptiste-wicht.com/posts/2017/06/jenkins-tip-send-notifications-fixed-builds-declarative-pipeline.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;In &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/06/jenkins-declarative-pipeline-and-awesome-github-integration.html"&gt;my previous post&lt;/a&gt;, I presented a few news about Jenkins and about the fact that I switched to declarative pipelines and Github Organization support for my projects.&lt;/p&gt;
&lt;p&gt;The main issue I had with this system is that I lost the ability to get
notifications on build that recover. Normally, I would get an email indicating
that build X was back to normal, but I haven't found a way to solve that for
declarative pipeline.&lt;/p&gt;
&lt;p&gt;By following a few posts on StackOverflow, I now have the solution and it is the
same problem that was already present in scripted pipelines. Namely, the status
of the current build is not set early enough for the notification.  Basically,
you have to set the notification yourself. Here is what a declarative pipeline
looks like:&lt;/p&gt;
&lt;pre class="code groovy"&gt;&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;pipeline&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;agent&lt;/span&gt; &lt;span class="n"&gt;any&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;stages&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-6"&gt;&lt;/a&gt;        &lt;span class="c1"&gt;// Normal Stages&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-8"&gt;&lt;/a&gt;        &lt;span class="n"&gt;stage&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'success'&lt;/span&gt;&lt;span class="o"&gt;){&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-9"&gt;&lt;/a&gt;            &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-10"&gt;&lt;/a&gt;                &lt;span class="n"&gt;script&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-11"&gt;&lt;/a&gt;                    &lt;span class="n"&gt;currentBuild&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'SUCCESS'&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-12"&gt;&lt;/a&gt;                &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-13"&gt;&lt;/a&gt;            &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-14"&gt;&lt;/a&gt;        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-15"&gt;&lt;/a&gt;    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-17"&gt;&lt;/a&gt;    &lt;span class="n"&gt;post&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;failure&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-19"&gt;&lt;/a&gt;            &lt;span class="n"&gt;script&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-20"&gt;&lt;/a&gt;                &lt;span class="n"&gt;currentBuild&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'FAILURE'&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-21"&gt;&lt;/a&gt;            &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-22"&gt;&lt;/a&gt;        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-23"&gt;&lt;/a&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-24"&gt;&lt;/a&gt;        &lt;span class="n"&gt;always&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-25"&gt;&lt;/a&gt;            &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="o"&gt;([&lt;/span&gt;&lt;span class="n"&gt;$class&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'Mailer'&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-26"&gt;&lt;/a&gt;                &lt;span class="nl"&gt;notifyEveryUnstableBuild:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-27"&gt;&lt;/a&gt;                &lt;span class="nl"&gt;recipients:&lt;/span&gt; &lt;span class="s2"&gt;"baptiste.wicht@gmail.com"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-28"&gt;&lt;/a&gt;                &lt;span class="nl"&gt;sendToIndividuals:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;])&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-29"&gt;&lt;/a&gt;        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-30"&gt;&lt;/a&gt;    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_fa9325ba41204aaf9cce7b65733e1a52-31"&gt;&lt;/a&gt;&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;There are two important things here. First, a new stage (success) is added that
simply set the result of the current build to SUCCESS once it is done. It must
be the last stage on the pipeline. This could also be added as the last step of
the last stage instead of adding a new stage, but I think it's clearer like
this. The second thing is the failure block in which the result of the current
build is set to FAILURE. With these two things, the Mailer plugin now sends
notification when a build has been fixed.&lt;/p&gt;
&lt;p&gt;I hope that will help some of you. I personally think that it should be much
easier than that. All this boilerplate is polluting the pipeline that should be
kept more maintainable, but for now it seems, it's the nicest way to achieve
that, short of handling all conditions in the post block and sending mail
directly there, but that would result in even more boilerplate code.&lt;/p&gt;&lt;/div&gt;</description><category>Continuous Integration</category><category>Jenkins</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2017/06/jenkins-tip-send-notifications-fixed-builds-declarative-pipeline.html</guid><pubDate>Wed, 07 Jun 2017 07:52:57 GMT</pubDate></item><item><title>Jenkins Declarative Pipeline and Awesome Github Integration</title><link>http://baptiste-wicht.com/posts/2017/06/jenkins-declarative-pipeline-and-awesome-github-integration.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;This post is about some news about Jenkins and how I've updated my Jenkins
usage. This may be a bit of an enthusiastic post ;)&lt;/p&gt;
&lt;p&gt;At the beginning of Jenkins, the best way to define the commands to be executed
for your builds was simply to write the commands in the Jenkins interface. This
worked quite well. Later on, Jenkins introduced the notion of Pipeline. Instead
of a single set of commands to be executed, the build was defined in
multi-stages pipeline of commands. This is defined as a Groovy script. One big
advantage of this is that all the code for creating the build is inside the
repository. This has the advantage that each build is reproducible. This enabled
to define complex pipelines of commands for your builds. Moreover, this also
allows to have a clean view of which steps are failing and which steps are
taking how much of the time of the build. For instance, here it's a view of the
pipeline steps for my DLL project:&lt;/p&gt;
&lt;img alt="Jenkins stage view for DLL pipeline." class="align-center" src="http://baptiste-wicht.com/images/jenkins_pipeline_dll.png"&gt;
&lt;p&gt;I think that's pretty cool :)&lt;/p&gt;
&lt;p&gt;They recently added a new feature, the declarative pipelines. Instead of
scripting the Pipeline in Groovy, the new system uses its own syntax, completely
declarative, to put blocks together and add ways of doing actions at specific
points and setting environment and so on. I think the new syntax is much nicer
than the Groovy scripted Pipeline way, so I started converting my scripts. I'll
give an example in a few paragraphs. But first, I'd like to talk about Github
integration. Before, every time I created a new project, I add to add it to
Jenkins by creating a new project, updating the link to the Github project and
a few things in order to add it. This is not so bad but what if you want to
build on several branches and keep track of the status of the branches and maybe
of the Pull Requests as well. All of this is now very simple. You can now
declare the Github organizations (and users) you are of building projects from
and the projects inside the organization will be automatically detected as long
as they have a Jenkinsfile inside. That means that you'll never have to create
a project yourself or handle branches. Indeed, all the created projects can now
handle multiples. For instance, here is the status of the two current branches
of my dll project:&lt;/p&gt;
&lt;img alt="Jenkins branches for DLL Github project" class="align-center" src="http://baptiste-wicht.com/images/jenkins_github_branches.png"&gt;
&lt;p&gt;It's maybe not the best example since one branch is failing and the other is
unstable, but you can see that you can track the builds for each branch in
a nice way.&lt;/p&gt;
&lt;p&gt;A very good feature of this integration is that Jenkins will now automatically
marks commits on your Github with the status of your builds at no cost! For
instance, here is the status on my ETL project after I configured on Jenkins and
made the first builds:&lt;/p&gt;
&lt;img alt="Jenkins marking commits in Github" class="align-center" src="http://baptiste-wicht.com/images/jenkins_github_integration_marks.png"&gt;
&lt;p&gt;Pretty cool I think :)&lt;/p&gt;
&lt;p&gt;Another nice thing in Jenkins is the Blue Ocean interface. This is an
alternative interface, especially well-suited for multi-branch projects and
pipelines. It looks much more modern and I think it's quite good. Here are a few
views of it:&lt;/p&gt;
&lt;p&gt;The Activity view for the last events of the project:&lt;/p&gt;
&lt;img alt="Jenkins Blue Ocean Activity view for DLL project" class="align-center" src="http://baptiste-wicht.com/images/jenkins_blue_ocean_dll_activity.png"&gt;
&lt;p&gt;The Branches view for the status of each branch:&lt;/p&gt;
&lt;img alt="Jenkins Blue Ocean Branches view for DLL project" class="align-center" src="http://baptiste-wicht.com/images/jenkins_blue_ocean_dll_branches.png"&gt;
&lt;p&gt;The view of the status of a build:&lt;/p&gt;
&lt;img alt="Jenkins Blue Ocean view of a build for DLL project" class="align-center" src="http://baptiste-wicht.com/images/jenkins_blue_ocean_dll_build.png"&gt;
&lt;p&gt;The status of the tests for a given build:&lt;/p&gt;
&lt;img alt="Jenkins Blue Ocean view of a build tests for DLL project" class="align-center" src="http://baptiste-wicht.com/images/jenkins_blue_ocean_dll_build_tests.png"&gt;
&lt;p&gt;It's likely that it won't appeal to everyone, but I think it's pretty nice.&lt;/p&gt;
&lt;p&gt;If we get back to the declarative Pipeline, here is the declarative pipeline for
my Expression Templates Library (ETL) project:&lt;/p&gt;
&lt;pre class="code groovy"&gt;&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;pipeline&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;agent&lt;/span&gt; &lt;span class="n"&gt;any&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;environment&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-5"&gt;&lt;/a&gt;       &lt;span class="n"&gt;CXX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"g++-4.9.4"&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-6"&gt;&lt;/a&gt;       &lt;span class="n"&gt;LD&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"g++-4.9.4"&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-7"&gt;&lt;/a&gt;       &lt;span class="n"&gt;ETL_MKL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'true'&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-8"&gt;&lt;/a&gt;    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;stages&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-11"&gt;&lt;/a&gt;        &lt;span class="n"&gt;stage&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'git'&lt;/span&gt;&lt;span class="o"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-12"&gt;&lt;/a&gt;            &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-13"&gt;&lt;/a&gt;                &lt;span class="n"&gt;checkout&lt;/span&gt;&lt;span class="o"&gt;([&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-14"&gt;&lt;/a&gt;                    &lt;span class="n"&gt;$class&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'GitSCM'&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-15"&gt;&lt;/a&gt;                    &lt;span class="nl"&gt;branches:&lt;/span&gt; &lt;span class="n"&gt;scm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;branches&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-16"&gt;&lt;/a&gt;                    &lt;span class="nl"&gt;doGenerateSubmoduleConfigurations:&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-17"&gt;&lt;/a&gt;                    &lt;span class="nl"&gt;extensions:&lt;/span&gt; &lt;span class="n"&gt;scm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;extensions&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;$class&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'SubmoduleOption'&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nl"&gt;disableSubmodules:&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nl"&gt;recursiveSubmodules:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nl"&gt;reference:&lt;/span&gt; &lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nl"&gt;trackingSubmodules:&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;]],&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-18"&gt;&lt;/a&gt;                    &lt;span class="nl"&gt;submoduleCfg:&lt;/span&gt; &lt;span class="o"&gt;[],&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-19"&gt;&lt;/a&gt;                    &lt;span class="nl"&gt;userRemoteConfigs:&lt;/span&gt; &lt;span class="n"&gt;scm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;userRemoteConfigs&lt;/span&gt;&lt;span class="o"&gt;])&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-20"&gt;&lt;/a&gt;            &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-21"&gt;&lt;/a&gt;        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-22"&gt;&lt;/a&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-23"&gt;&lt;/a&gt;        &lt;span class="n"&gt;stage&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'pre-analysis'&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-24"&gt;&lt;/a&gt;            &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-25"&gt;&lt;/a&gt;                &lt;span class="n"&gt;sh&lt;/span&gt; &lt;span class="s1"&gt;'cppcheck --xml-version=2 -j3 --enable=all --std=c++11 `git ls-files "*.hpp" "*.cpp"` 2&amp;gt; cppcheck_report.xml'&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-26"&gt;&lt;/a&gt;                &lt;span class="n"&gt;sh&lt;/span&gt; &lt;span class="s1"&gt;'sloccount --duplicates --wide --details include/etl test workbench &amp;gt; sloccount.sc'&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-27"&gt;&lt;/a&gt;                &lt;span class="n"&gt;sh&lt;/span&gt; &lt;span class="s1"&gt;'cccc include/etl/*.hpp test/*.cpp workbench/*.cpp || true'&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-28"&gt;&lt;/a&gt;            &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-29"&gt;&lt;/a&gt;        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-30"&gt;&lt;/a&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-31"&gt;&lt;/a&gt;        &lt;span class="n"&gt;stage&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'build'&lt;/span&gt;&lt;span class="o"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-32"&gt;&lt;/a&gt;            &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-33"&gt;&lt;/a&gt;                &lt;span class="n"&gt;sh&lt;/span&gt; &lt;span class="s1"&gt;'make clean'&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-34"&gt;&lt;/a&gt;                &lt;span class="n"&gt;sh&lt;/span&gt; &lt;span class="s1"&gt;'make -j6 release'&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-35"&gt;&lt;/a&gt;            &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-36"&gt;&lt;/a&gt;        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-37"&gt;&lt;/a&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-38"&gt;&lt;/a&gt;        &lt;span class="n"&gt;stage&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'test'&lt;/span&gt;&lt;span class="o"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-39"&gt;&lt;/a&gt;            &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-40"&gt;&lt;/a&gt;                &lt;span class="n"&gt;sh&lt;/span&gt; &lt;span class="s1"&gt;'ETL_THREADS=-j6 ETL_GPP=g++-4.9.4 LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/opt/intel/mkl/lib/intel64:/opt/intel/lib/intel64\" ./scripts/test_runner.sh'&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-41"&gt;&lt;/a&gt;                &lt;span class="n"&gt;archive&lt;/span&gt; &lt;span class="s1"&gt;'catch_report.xml'&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-42"&gt;&lt;/a&gt;                &lt;span class="n"&gt;junit&lt;/span&gt; &lt;span class="s1"&gt;'catch_report.xml'&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-43"&gt;&lt;/a&gt;            &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-44"&gt;&lt;/a&gt;        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-45"&gt;&lt;/a&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-46"&gt;&lt;/a&gt;        &lt;span class="n"&gt;stage&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'sonar-master'&lt;/span&gt;&lt;span class="o"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-47"&gt;&lt;/a&gt;            &lt;span class="n"&gt;when&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-48"&gt;&lt;/a&gt;                &lt;span class="n"&gt;branch&lt;/span&gt; &lt;span class="s1"&gt;'master'&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-49"&gt;&lt;/a&gt;            &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-50"&gt;&lt;/a&gt;            &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-51"&gt;&lt;/a&gt;                &lt;span class="n"&gt;sh&lt;/span&gt; &lt;span class="s2"&gt;"/opt/sonar-runner/bin/sonar-runner"&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-52"&gt;&lt;/a&gt;            &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-53"&gt;&lt;/a&gt;        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-54"&gt;&lt;/a&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-55"&gt;&lt;/a&gt;        &lt;span class="n"&gt;stage&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'sonar-branch'&lt;/span&gt;&lt;span class="o"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-56"&gt;&lt;/a&gt;            &lt;span class="n"&gt;when&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-57"&gt;&lt;/a&gt;                &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-58"&gt;&lt;/a&gt;                    &lt;span class="n"&gt;branch&lt;/span&gt; &lt;span class="s1"&gt;'master'&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-59"&gt;&lt;/a&gt;                &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-60"&gt;&lt;/a&gt;            &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-61"&gt;&lt;/a&gt;            &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-62"&gt;&lt;/a&gt;                &lt;span class="n"&gt;sh&lt;/span&gt; &lt;span class="s2"&gt;"/opt/sonar-runner/bin/sonar-runner -Dsonar.branch=${env.BRANCH_NAME}"&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-63"&gt;&lt;/a&gt;            &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-64"&gt;&lt;/a&gt;        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-65"&gt;&lt;/a&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-66"&gt;&lt;/a&gt;        &lt;span class="n"&gt;stage&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'bench'&lt;/span&gt;&lt;span class="o"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-67"&gt;&lt;/a&gt;            &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-68"&gt;&lt;/a&gt;                &lt;span class="n"&gt;build&lt;/span&gt; &lt;span class="nl"&gt;job:&lt;/span&gt; &lt;span class="s1"&gt;'etl - benchmark'&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nl"&gt;wait:&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-69"&gt;&lt;/a&gt;            &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-70"&gt;&lt;/a&gt;        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-71"&gt;&lt;/a&gt;    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-72"&gt;&lt;/a&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-73"&gt;&lt;/a&gt;    &lt;span class="n"&gt;post&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-74"&gt;&lt;/a&gt;        &lt;span class="n"&gt;always&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-75"&gt;&lt;/a&gt;            &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="o"&gt;([&lt;/span&gt;&lt;span class="n"&gt;$class&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'Mailer'&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-76"&gt;&lt;/a&gt;                &lt;span class="nl"&gt;notifyEveryUnstableBuild:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-77"&gt;&lt;/a&gt;                &lt;span class="nl"&gt;recipients:&lt;/span&gt; &lt;span class="s2"&gt;"baptiste.wicht@gmail.com"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-78"&gt;&lt;/a&gt;                &lt;span class="nl"&gt;sendToIndividuals:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;])&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-79"&gt;&lt;/a&gt;        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-80"&gt;&lt;/a&gt;    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b5cc2ab00ac34619bb4ecd4021c3c749-81"&gt;&lt;/a&gt;&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;There is nothing really fancy about, it's probably average. Moreover, since I'm
not an expert on pipelines and I've just discovered declarative pipelines, it
may not be optimal, but it works. As you'll see there are some problems
I haven't been able to fix.&lt;/p&gt;
&lt;p&gt;The first part declares the environment variables for the build. Then, the
multiple build stages are listed. The first stage checkout the code from the
SCM. This ugly piece of code is here to allow to checkout the submodules. It is
the only solution I have found so far. It's very ugly but it works. The second
steps is simply some basic static analysis. The next step is the classical build
step. Then, the tests are run. In that case, I'm using a script because the
tests are compiled with several different sets of options and it was much easier
to put that in a script that in the Pipeline. Moreover, that also means I can
run them standalone. The variables in the line to run the script is another
problem I haven't been able to fix so far. If I declare these variables in an
environment block, they are not passed to the script for some reason, so I had
to use this ugly line. The next two blocks are for Sonar analysis. If you start
with Sonar, you can simply the second block that passed the branch information
to Sonar. Unfortunately, Sonar is very limited in terms of Git branches. Each
branch is considered as another totally different project. That means the false
positives defined in the master branch will not be used in the second branch.
Therefore, I kept a clean master and several different projects for the other
branches. Once Sonar improves this branch handling stuff, if they ever do, I'll
be able to get rid of one of these conditional stages. The last stage is simple
running the benchmark job. Finally, the post block is using the Mailer plugin to
send failed builds information. Again, there is a problem here since this does
not send "Back to normal" information as it used to do before. I've asked this
question on StackOverflow, but haven't received an answer so far. I'll post
a better solution on this blog once I have one. If any of you have some
solutions to these problems, don't hesitate to post in the comments below or to
contact me on Github.&lt;/p&gt;
&lt;p&gt;Here it is. I really think Jenkins is getting even greater now with all this
cool stuff and I advice you to try it out!&lt;/p&gt;&lt;/div&gt;</description><category>Continuous Integration</category><category>Jenkins</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2017/06/jenkins-declarative-pipeline-and-awesome-github-integration.html</guid><pubDate>Sun, 04 Jun 2017 18:52:10 GMT</pubDate></item><item><title>C++ Containers Benchmark: vector/list/deque and plf::colony</title><link>http://baptiste-wicht.com/posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Already more than three years ago, I've written a &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2012/12/cpp-benchmark-vector-list-deque.html"&gt;benchmark of some of the STL containers&lt;/a&gt;,
namely the vector, the list and the deque. Since this article was very popular,
I decided to improve the benchmarks and collect again all the results. There are
now more benchmarks and some problems have been fixed in the benchmark code.
Moreover, I have also added a new container, the plf::colony. Therefore, there
are four containers tested:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The std::vector: This is a dynamically-resized array of elements. All the
elements are contiguous in memory. If an element is inserted or removed it at
a position other than the end, the following elements will be moved to fill
the gap or to open a gap. Elements can be accessed at random position in
constant time. The array is resized so that it can several more elements, not
resized at each insert operation. This means that insertion at the end of the
container is done in amortized constant time.&lt;/li&gt;
&lt;li&gt;The std::deque: The deque is a container that offer constant time insertion
both at the front and at the back of the collection. In current c++ libraries,
it is implementation as a collection of dynamically allocated fixed-size
array. Not all elements are contiguous, but depending on the size of the data
type, this still has good data locality. Access to a random element is also
done in constant time, but with more overhead than the vector. For insertions
and removal at random positions, the elements are shifted either to the front
or to the back meaning that it is generally faster than the vector, by twice
in average.&lt;/li&gt;
&lt;li&gt;The std::list: This is a doubly-linked list. It supports constant time
insertions at any position of the collection. However, it does not support
constant time random access. The elements are obviously not contiguous, since
they are all allocated in nodes. For small elements, this collection has
a very big memory overhead.&lt;/li&gt;
&lt;li&gt;The plf::colony: This container is a non-standard container which is
unordered, it means that the insertion order will not necessarily be
preserved. It provides strong iterators guarantee, pointers to non-erased
element are not invalidated by insertion or erasure. It is especially tailored
for high-insertion/erasure workloads. Moreover, it is also specially optimized
for non-scalar types, namely structs and classes with relatively large data
size (greater than 128 bits on the official documentation). Its implementation
is more complicated than the other containers. It is also implemented as
a list of memory blocks, but they are of increasingly large sizes. When
elements are erased, there position is not removed, but marked as erased so
that it can be reused for fast insertion later on. This container uses the
same conventions as the standard containers and was proposed for inclusion to
the standard library, which is the main reason why it's included in this
benchmark. If you want more information, you can consult the
&lt;a class="reference external" href="http://plflib.org/colony.htm"&gt;official website&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the text and results, the namespaces will be omitted. Note that I have only
included sequence containers in my test. These are the most common containers in
practices and these also the containers I'm the most familiar with. I could have
included multiset in this benchmark, but the interface and purpose being
different, I didn't want the benchmark to be confusing.&lt;/p&gt;
&lt;p&gt;All the examples are compiled with g++-4.9.4 (-std=c++11 -march=native -O2) and
run on a Gentoo Linux machine with an Intel Core i7-4770 at 3.4GHz.&lt;/p&gt;
&lt;p&gt;For each graph, the vertical axis represent the amount of time necessary to
perform the operations, so the lower values are the better. The horizontal axis
is always the number of elements of the collection. For some graph, the
logarithmic scale could be clearer, a button is available after each graph to
change the vertical scale to a logarithmic scale.&lt;/p&gt;
&lt;p&gt;The tests are done with several different data types. The trivial data types are
varying in size, they hold an array of longs and the size of the array varies to
change the size of the data type. The non-trivial data type is composed of
a string (just long enough to avoid SSO (Small String Optimization) (even though
I'm using GCC)). The non-trivial data types comes in a second version with
noexcept move operations.  Not all results are presented for each data types if
there are not significant differences between in order to keep this article
relatively short (it's already probably too long :P).&lt;/p&gt;
&lt;p class="more"&gt;&lt;a href="http://baptiste-wicht.com/posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html"&gt;Read more…&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>C++</category><category>C++11</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2017/05/cpp-containers-benchmark-vector-list-deque-plf-colony.html</guid><pubDate>Sun, 21 May 2017 10:46:23 GMT</pubDate></item><item><title>Speed up TensorFlow inference by compiling it from source</title><link>http://baptiste-wicht.com/posts/2017/05/speed-up-tensorflow-inference-compiling-from-source.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;The most simple way to install TensorFlow is to work in a virtual Python
environment and simply to use either the TensorFlow official packages in pip or
use one of the official wheels for distributions.  There is one big problem with
that technique and it's the fact that the binaries are precompiled so that they
fit as many hardware configuration as possible. This is normal from Google since
generating precompiled binaries for all the possible combinations of processor
capabilities would be a nightmare. This is not a problem for GPU
since the CUDA Libraries will take care of the difference from one graphics card
to another. But it is a problem with CPU performance. Indeed, different
processors have different capabilities. For instance, the vectorization
capabilities are different from processor to processor (SSE, AVX, AVX2,
AVX-512F, FMA, ...). All those options can make a significant difference in the
performance of the programs. Although most of the machine learning training
occurs on GPU most of the time, the inference is mostly done on the CPU.
Therefore, it probably remains important to be as fast as possible on CPU.&lt;/p&gt;
&lt;p&gt;So if you care about performance on CPU, you should install TensorFlow from
sources directly yourself. This will allow compilation of the TensorFlow sources
with -march=native which will enable all the hardware capabilities of machine on
which you are compiling the library.&lt;/p&gt;
&lt;p&gt;Depending on your problem, this may give you some nice speedup. In my case, on
a very small Recurrent Neural Network, it made inference about 20% faster.  On
a larger problem and depending on your processor, you may gain much more than
that. If you are training on CPU, this may make a very large difference in total
time.&lt;/p&gt;
&lt;p&gt;Installing TensorFlow is sometimes a bit cumbersome. You'll likely have to
compile Bazel from sources as well and depending on your processor, it may take
a long time to finish. Nevertheless, I have successfully compiled TensorFlow
from sources on several machines now without too many problems. Just pay close
attention to the options you are setting while configuring TensorFlow, for
instance CUDA configuration if you want GPU support.&lt;/p&gt;
&lt;p&gt;I hope this little trick will help you gain some time :)&lt;/p&gt;
&lt;p&gt;Here is the &lt;a class="reference external" href="https://www.tensorflow.org/install/install_sources"&gt;link to compile TensorFlow from source&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>CPU</category><category>GPU</category><category>Intel</category><category>Machine Learning</category><category>Performance</category><category>tensorflow</category><guid>http://baptiste-wicht.com/posts/2017/05/speed-up-tensorflow-inference-compiling-from-source.html</guid><pubDate>Wed, 10 May 2017 12:18:33 GMT</pubDate></item><item><title>Update on Expression Templates Library (ETL)</title><link>http://baptiste-wicht.com/posts/2017/05/update-on-expression-templates-library-etl.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;It's been a while since I've &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2016/09/expression-templates-library-etl-10.html"&gt;released the version 1.0 of ETL&lt;/a&gt;. There is some work to do before I release the next version, but I wanted to give you a quick update on what has been going on for ETL in the last months. There has been a lot of changes in the library and the next version will be a major update when I'm done with some refactorings and improvements.&lt;/p&gt;
&lt;p&gt;Thanks to my thesis supervisor, the project now has a logo:&lt;/p&gt;
&lt;img alt="ETL Logo" class="align-center" src="http://baptiste-wicht.com/images/logo.png"&gt;
&lt;p&gt;There are quite a few new features, although probably nothing really major. The
support for square root has been improved with cubic root and inverse root.
Vectors can now be transformed using floor and ceil. Cross product of vector has
been implemented as well. Batched outer product and batched bias averaging (for
machine learning) are now supported. Reductions have also been improved with
absolute sum and mean (asum/asum) support and min_index and max_index. argmax
can now be used to get the max index in each sub dimensions. Matrix can now be
decomposed into their Q/R decomposition rather than only their PALU
decomposition. The matrices can now be sliced by getting only a sub part of the
matrix. The pooling operators  have also been improved with stride and padding
support. Matrices and vectors can also be shuffled. Moreover, a few adapters
are now available for hermitian matrices, symmetric matrices and lower and upper
matrices. So far the support for these adapters is not huge, but they are
guaranteed to validate their constraints.&lt;/p&gt;
&lt;p&gt;Several operations have been optimized for speed. All the pooling and upsample
operators are now parallelized and the most used kernel (2x2 pooling) is now
more optimized. 4D convolution kernels (for machine learning) have been greatly
improved. There are now very specialized vectorized kernels for classic kernel
configurations (for instance 3x3 or 5x5) and the selection of implementations is
now smarter than before. The support of padding now much better than before for
small amount of padding. Moreover, for small kernels the full convolution can
now be evaluated using the valid convolution kernels directly with some padding,
for much faster overall performance. Matrix-matrix multiplication with
transposed matrices is now much faster when using BLAS kernels. Indeed, the
transposition is not performed but handled inside the kernels. Moreover, the
performance of the transposition itself is also much faster. Finally, accesses
to 3D and 4D matrices is now much faster than before.&lt;/p&gt;
&lt;p&gt;The parallelization feature of ETL has been completely reworked. Before, there
was a thread pool for each algorithm that was parallelized. Now, there is
a global thread engine with one thread pool. Since parallelization is not nested
in ETL, this improves performance slightly by greatly diminishing the number of
threads that are created throughout an application.&lt;/p&gt;
&lt;p&gt;Vectorization has also been greatly improved in ETL. Integer operations are now
automatically vectorized on processors that support this. The automatic
vectorizer now is able to use non-temporal stores for very large operations.
A non-temporal store bypasses the cache, thus gaining some time. Since very
large matrices do not fit in cache, this is a net gain. Moreover, the alignment
detection in the automatic vectorizer has also been improved. Support for
Fused-Multiply-Add (FMA) operations has also been integrated in the algorithms
that can make use of it. The matrix-matrix multiplications and vector-matrix
multiplications now have optimized vectorized kernels. They also have versions
for column-major matrices now. The old egblas version of the gemm, based on BLIS
kernels, has been removed since it was only supporting double-precision and was
not faster than the new vectorized algorithm. I plan to reintegrate a version of
the GEMM based on BLIS in the future but with more optimizations and support for
all precisions and integers. The sum and the dot product now also have
specialized vectorized implementations. The min and max operations are now
automatically-vectorized.&lt;/p&gt;
&lt;p&gt;The GPU has also been almost completely reworked. Now, operations can be chained
without any copies between GPU and CPU. Several new operations have also been
added with support to GPU. Moreover, to complement operations that are not
available in any of the supported NVIDIA libraries, I've created a simple
library that can be used to add a few more GPU operations. Nevertheless a lot of
operations are still missing and only algorithms are available not expressions
(such as c = a + b * 1.0) that are entirely computed on CPU. I have plans to
improve that further, but probably not before the version 1.2.&lt;/p&gt;
&lt;p&gt;There also have been a lot of refactorings in the code of the library. A lot of
expressions now have less overhead and are specialized for performance.
Moreover, temporary expressions are currently being reworked in order to be more
simple and maintainable and easier to optimize in the future.&lt;/p&gt;
&lt;p&gt;Finally, there also was quite a few bug fixes. Most of them have been found by
the use of the library in the Deep Learning Library (DLL) project.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>etl</category><category>GPU</category><category>Performance</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/05/update-on-expression-templates-library-etl.html</guid><pubDate>Sat, 06 May 2017 19:31:48 GMT</pubDate></item><item><title>Home Automation: Power Meter and Integration of Zwave into Domoticz</title><link>http://baptiste-wicht.com/posts/2017/04/home-automation-power-meter-integration-zwave-domoticz.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've improved a bit my home automation installation. It's been a while since
the last upgrade, but unfortunately I cannot afford as many upgrades as I would
like :P&lt;/p&gt;
&lt;p&gt;For a long time I wanted to monitor the power consumption of a few of my
appliances in my apartment. Especially my Linux servers so that I could try to
improve the consumption and reduce my bill on the long run. Unfortunately, there
are very few options for power meter in Switzerland due to the special type of
plug we have. The only option I found is a Zwave power plug. For a while,
I waited to see if I could find other options because Zwave devices are quite
expensive and I would have rather preferred to stay with simpler and cheaper
RF-433 appliances. Since I didn't find anything, I ordered a ZWave USB
controller from  Aeon Labs (the generation 5). I also ordered two Aeon Labs
Swiss Smart Plug with power meter.&lt;/p&gt;
&lt;p&gt;Here is an image of the Aeon Labs key:&lt;/p&gt;
&lt;img alt="Aeon Labs ZWave USB Key" class="align-center" src="http://baptiste-wicht.com/images/zwave_usb.jpg"&gt;
&lt;p&gt;And of the power meter in usage:&lt;/p&gt;
&lt;img alt="ZWave power meter" class="align-center" src="http://baptiste-wicht.com/images/power_meter.jpg"&gt;
&lt;p&gt;Integration of ZWave into Domoticz was extremely easy. I just plugged the USB
key, restarted Domoticz (seems necessary for it to pick the new tty) and added
new hardware "OpenZWave USB" with the correct serial port. From there, there are
two main ways to add new devices. The first is to remove the USB key and use the
synchronization button on both the key and the device close to each other. The
other way is to use the "Include Node" option on Domoticz and then press the
synchronization button on the device to detect the new device. I used the second
option since it seemed simpler and it worked perfectly. I did that for my two
plugs and it worked fine. Directly after this, 5 new devices were added for each
of the plug. One for the voltage, one for the current , two for the  usage (I
don't know why there is two, but they are both reporting the same value) and one
for the switch on/off. I was a bit afraid that only the On/Off part of the smart
plug would work on Domoticz, but I had absolutely no problem.&lt;/p&gt;
&lt;p&gt;Here is for instance the power usage of last 24 hours on my television system:&lt;/p&gt;
&lt;img alt="Power usage on television system" class="align-center" src="http://baptiste-wicht.com/images/domoticz_power_usage.png"&gt;
&lt;p&gt;For now, I haven't integrated this information on any rule, but I plan to
monitor this information in the coming weeks and try to improve my consumption,
especially for my servers. I also plan to purchase more of these plugs once my
home automation budget can be upgraded.&lt;/p&gt;
&lt;p&gt;On another note, I also purchased a Chacon wall remote switch working in RF-433.
Although it is quite cheap, I'm very disappointed by the quality of this switch.
I add to straighten myself the pins that are attached to the battery because
there was no contact. After that, it worked correctly and it is able to work
with the RFLink module.&lt;/p&gt;
&lt;p&gt;I have to say that I'm quite satisfied with ZWave devices with this experience.
Even though I still feel it is way too expensive, it is high quality and have
a good finishing. I'll probably purchase more ZWave devices in the future. I'm
especially interested in The Aeotec 6 in 1 sensor for temperature humidity,
motion, light, UV and vibration. This would allow me to have much information in
each room with only one sensor in place of several sensors in each room like
I currently have.&lt;/p&gt;
&lt;p&gt;I still have a few Milight Bulbs and LEDS to install with a secondary Milight
bridge that I will install in the coming week, but I probably won't do a post
about this.&lt;/p&gt;&lt;/div&gt;</description><category>Domoticz</category><category>Home Automation</category><category>Personal</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/04/home-automation-power-meter-integration-zwave-domoticz.html</guid><pubDate>Sun, 23 Apr 2017 14:43:13 GMT</pubDate></item><item><title>Publications: Deep Learning Features for Handwritten Keyword Spotting</title><link>http://baptiste-wicht.com/posts/2017/04/publications-deep-learning-features-handwritten-keyword-spotting.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;After my previous post about my publication on CPU performance optimization,
I wanted to talk a bit about two publications on Handwritten Keyword Spotting,
in which we extract features with Convolutional RBM RBM&lt;/p&gt;
&lt;p&gt;We published two different papers:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.researchgate.net/publication/306081095_Keyword_Spotting_with_Convolutional_Deep_Belief_Networks_and_Dynamic_Time_Warping"&gt;Keyword Spotting With Convolutional Deep Belief Networks and Dynamic Time Warping&lt;/a&gt;, in the Proceedings of the International Conference on Artificial Neural Networks (ICANN-2016), Barcelona, Spain&lt;/li&gt;
&lt;li&gt;Mixed Handwritten and printed digit recognition in Sudoku With Convolutional Deep Belief Network (Link will come), in the Proceedings of the International Conference on Pattern Recognition (ICPR-2016), Cancun, Mexico&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second paper is mostly a large extension of the first one, so I'll focus on
the complete version.&lt;/p&gt;
&lt;p&gt;On a side note, I also co-authored a third paper:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.researchgate.net/publication/312486359_Inkball_Models_as_Features_for_Handwriting_Recognition"&gt;Inkball Models as Features for Handwriting Recognition&lt;/a&gt;, in the Proceedings of the International Conference on Frontiers of Handwriting Recognition (ICFHR-2016), Shenzen, China&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We mostly used our existing system to generate features for a comparison between
different set of features for handwritten keyword spotting. It was my first time
in China and I enjoyed the stay a lot. I also had the chance to meet my
girlfriend in Shenzen, all the more reason to mention this publication :)&lt;/p&gt;
&lt;p&gt;Back on the main subject. The idea behind these publications is to
a Convolutional Deep Belief Network (CDBN) to extract features from the images
and then pass these features to either a Dynamic Time Warping (DTW) algorithm or
an Hidden Markov Model (HMM). The following image describe the overall system:&lt;/p&gt;
&lt;img alt="Keyword Spotting System" class="align-center" src="http://baptiste-wicht.com/images/kws_system.png"&gt;
&lt;p&gt;The features are extracted from preprocessed normalized binary images. Using
a sliding window, moving from left to right, one pixel at a time, the features
are extracted on each window. The feature extractor is a Convolutional Deep
Belief Network, trained fully unsupervised. The features are then normalized so
that each feature group sum to one and then each has zero-mean and
unit-variance. The network used for feature extraction is depicted in the
following image:&lt;/p&gt;
&lt;img alt="Convolutional Deep Belief Network features" class="align-center" src="http://baptiste-wicht.com/images/kws_network.png"&gt;
&lt;p&gt;Two Convolutional Restricted Boltzmann Machines (CRBMs) are used, each followed
by a max pooling layer.&lt;/p&gt;
&lt;p&gt;Once the features are extracted, they can be passed to the classifier for
keyword spotting scoring. We tested our features with two different approaches
for word scoring. The first one is a template matching strategy, Dynamic Time
Warping (DTW), is a very simple measure of distance between two sequences of
different length. The two sequences are warped non-linearly to minimize the
distance between each pair of features. A template from the training set is
compared to the word image being evaluated. This works pretty well for simple
data sets but fails when the writing styles of the test set are not known in the
training set. The second classifier is more powerful and trained, a Hidden
Markov Model (HMM). Character models are trained using the entire training set.
From these character models, a keyword model as well as an unconstrained model
(the filler model) are constructed. The probability of these two models is
computed using Viterbi and the final score is computed using log-odds scoring of
these two models using the filler model as a form of normalization.&lt;/p&gt;
&lt;p&gt;This technique was evaluated on three datasets (George Washington (GW), Parzival
(PAR) and IAM offline database (IAM)). Our features were compared with three
reference feature sets, one heuristic and two local feature sets.&lt;/p&gt;
&lt;p&gt;The results for DTW:&lt;/p&gt;
&lt;img alt="Keyword Spotting Results with Dynamic Time Warping" class="align-center" src="http://baptiste-wicht.com/images/kws_results_dtw.png"&gt;
&lt;p&gt;Overall, our features exhibit better performance than the other reference.
Except for the Mean Average Precision on the PAR data set. The very low
performance on PAR with DTW is explained by the fact mentioned earlier that it
has poor generalization to unknown writing styles.&lt;/p&gt;
&lt;p&gt;The results for HMM:&lt;/p&gt;
&lt;img alt="Keyword Spotting Results with Hidden Markov Model" class="align-center" src="http://baptiste-wicht.com/images/kws_results_hmm.png"&gt;
&lt;p&gt;With HMM, our features are always better than the other feature sets. However,
the margin of improvement is smaller than when using DTW.&lt;/p&gt;
&lt;p&gt;Overall, the proposed system proved quite powerful and was able to outperform
the three tested feature sets on three datasets for keyword spotting.&lt;/p&gt;
&lt;p&gt;You can find the &lt;a class="reference external" href="https://github.com/wichtounet/word_spotting"&gt;C++ implementation on Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As for my thesis, I have finished the writings about a month ago and it is now
in the hands on my supervisor.&lt;/p&gt;
&lt;p&gt;If you want to have a look, the
&lt;a class="reference external" href="http://baptiste-wicht.com/stories/publications.html"&gt;list of my publications&lt;/a&gt;
is available on this website.&lt;/p&gt;
&lt;p&gt;If you want more details on this project, don't hesitate to ask here or on
Github, or read the papers :)&lt;/p&gt;
&lt;p&gt;I hope the next post about my publications will be about the finalization of my
thesis :)&lt;/p&gt;&lt;/div&gt;</description><category>deep learning</category><category>dll</category><category>publications</category><category>thesis</category><guid>http://baptiste-wicht.com/posts/2017/04/publications-deep-learning-features-handwritten-keyword-spotting.html</guid><pubDate>Fri, 21 Apr 2017 18:29:39 GMT</pubDate></item><item><title>Partial type erasing in Deep Learning Library (DLL) to improve compilation time</title><link>http://baptiste-wicht.com/posts/2017/03/partial-type-erasing-deep-learning-library-dll-improve-compilation-time.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;In a previous post, I compared the &lt;a class="reference external" href="https://baptiste-wicht.com/posts/2017/03/disappointing-zapcc-performance-on-deep-learning-library-dll.html"&gt;compilation time on my Deep Learning Library (DLL) project with different compilers&lt;/a&gt;. I realized that the compilation times were quickly going unreasonable for this library, especially for compiling the unit cases which clearly hurts the development of the library. Indeed, you want to be able to run the unit tests reasonably quickly after you integrated new changes.&lt;/p&gt;
&lt;div class="section" id="reduce-the-compilation-time"&gt;
&lt;h2&gt;Reduce the compilation time&lt;/h2&gt;
&lt;p&gt;The first thing I did was to split the compilation in three executables: one for
the unit tests, one for the various performance tests and one for the various other
miscellaneous tests. With this, it is much faster to compile only the unit test
cases.&lt;/p&gt;
&lt;p&gt;But this can be improved significantly more. In DLL a network is a variadic
template containing the list of layers, in order. In DLL, there are two main
different ways of declaring a neural networks. In the first version, the fast
version, the layers directly know their sizes:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_f16feeb38686463faeb78f276a61043d-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;a name="rest_code_f16feeb38686463faeb78f276a61043d-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_f16feeb38686463faeb78f276a61043d-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_f16feeb38686463faeb78f276a61043d-4"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_f16feeb38686463faeb78f276a61043d-5"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_f16feeb38686463faeb78f276a61043d-6"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;400&lt;/span&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;hidden&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;unit_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;SOFTMAX&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_f16feeb38686463faeb78f276a61043d-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sgd_trainer&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_f16feeb38686463faeb78f276a61043d-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_f16feeb38686463faeb78f276a61043d-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_f16feeb38686463faeb78f276a61043d-10"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;pretrain&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_f16feeb38686463faeb78f276a61043d-11"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;In my opinion, this is the best way to use DLL. This is the fastest and the
clearest. Moreover, the dimensions of the network can be validated at compile
time, which is always better than at runtime. However, the dimensions of the
network cannot be changed at runtime.  For this, there is a different version,
the dynamic version:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-4"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-5"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-6"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;hidden&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;unit_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;SOFTMAX&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sgd_trainer&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-11"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;layer_get&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;init_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-12"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;layer_get&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;init_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-13"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;layer_get&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;init_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-14"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;layer_get&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-15"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;layer_get&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-16"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;layer_get&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-18"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;pretrain&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_7dba155d64d04952aa1bff84e2570a48-19"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is a bit more verbose, but the configuration can be changed at runtime with
this system. Moreover, this is also faster to compile. On the other hand, there
is some performance slowdown.&lt;/p&gt;
&lt;p&gt;There is also a third version that is a hybrid of the first version:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_c95085d4c8b34af3ace718c536a89849-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;network_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;a name="rest_code_c95085d4c8b34af3ace718c536a89849-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dyn_dbn_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_c95085d4c8b34af3ace718c536a89849-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_layers&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
&lt;a name="rest_code_c95085d4c8b34af3ace718c536a89849-4"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_c95085d4c8b34af3ace718c536a89849-5"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_c95085d4c8b34af3ace718c536a89849-6"&gt;&lt;/a&gt;            &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;rbm_desc&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;400&lt;/span&gt;    &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;hidden&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;unit_type&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;SOFTMAX&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;layer_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_c95085d4c8b34af3ace718c536a89849-7"&gt;&lt;/a&gt;        &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sgd_trainer&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dll&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dbn_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_c95085d4c8b34af3ace718c536a89849-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_c95085d4c8b34af3ace718c536a89849-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;network&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_unique&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;network_t&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_c95085d4c8b34af3ace718c536a89849-10"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;pretrain&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_c95085d4c8b34af3ace718c536a89849-11"&gt;&lt;/a&gt;&lt;span class="n"&gt;network&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fine_tune&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Only one line was changed compared to the first version, &lt;code&gt;dbn_desc&lt;/code&gt;
becomes &lt;code&gt;dyn_dbn_desc&lt;/code&gt;. What this changes is that all the layers are
automatically transformed into their dynamic versions and all the parameters are
propagated at runtime. This is a form a type erasing since the sizes will not be
propagated at compilation time. But this is simple since the types are simply
transformed from one type to another directly. Behind the scene, it's the
dynamic version using the front-end of the fast version. This is almost as fast
to compile as the dynamic version, but the code is much better. It executes the
same as the dynamic version.&lt;/p&gt;
&lt;p&gt;If we compare the compilation time of the three versions when compiling a single
network and 5 different networks with different architectures, we get the
following results (with clang):&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="52%"&gt;
&lt;col width="48%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Model&lt;/th&gt;
&lt;th class="head"&gt;Time [s]&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;1 Fast&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;1 Dynamic&lt;/td&gt;
&lt;td&gt;16.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;1 Hybrid&lt;/td&gt;
&lt;td&gt;16.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5 Fast&lt;/td&gt;
&lt;td&gt;114&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5 Dynamic&lt;/td&gt;
&lt;td&gt;16.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5 Hybrid&lt;/td&gt;
&lt;td&gt;21.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Even with one single network, the compilation time is reduced by 44%. When five
different networks are compilation, time is reduced by 85%. This can be
explained easily. Indeed, for the hybrid and dynamic versions, the layers will
have the same type and therefore a lot of template instantiations will only be
done once instead of five times. This makes a lot of difference since almost
everything is template inside the library.&lt;/p&gt;
&lt;p&gt;Unfortunately, this also has an impact on the runtime of the network:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="26%"&gt;
&lt;col width="41%"&gt;
&lt;col width="32%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Model&lt;/th&gt;
&lt;th class="head"&gt;Pretrain [s]&lt;/th&gt;
&lt;th class="head"&gt;Train [s]&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Fast&lt;/td&gt;
&lt;td&gt;195&lt;/td&gt;
&lt;td&gt;114&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Dynamic&lt;/td&gt;
&lt;td&gt;203&lt;/td&gt;
&lt;td&gt;123&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Hybrid&lt;/td&gt;
&lt;td&gt;204&lt;/td&gt;
&lt;td&gt;122&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;On average, for dense models, the slowdown is between 4% and 8%. For
convolutional models, it is between 10% and 25%. I will definitely work on
trying to make the dynamic and especially the hybrid version faster in the
future, most on the work should be on the matrix library (ETL) that is used.&lt;/p&gt;
&lt;p&gt;Since for test cases, a 20% increase in runtime is not really a problem, tests
being fast already, I decided to add an option to DLL so that everything can be
compiled by default in hybrid model. By using a compilation flag, all the
&lt;code&gt;dbn_desc&lt;/code&gt; are becoming &lt;code&gt;dyn_dbn_desc&lt;/code&gt; and therefore each used
network is becoming a hybrid network. Without a single change in the code, the
compilation time of the entire library can be significantly improved, as seen in
the next section.  This can also be used in user code to improve compilation
time during debugging and experiments and can be turned off for the final
training.&lt;/p&gt;
&lt;p&gt;On my Continuous Integration system, I will build the system in both
configurations. This is not really an issue, since my personal machine at home
is more powerful than what I have available here.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="results"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;On a first experiment, I measured the difference before and after this change on
the three executables of the library, with gcc:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="23%"&gt;
&lt;col width="26%"&gt;
&lt;col width="26%"&gt;
&lt;col width="26%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Model&lt;/th&gt;
&lt;th class="head"&gt;Unit [s]&lt;/th&gt;
&lt;th class="head"&gt;Perf [s]&lt;/th&gt;
&lt;th class="head"&gt;Misc [s]&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Before&lt;/td&gt;
&lt;td&gt;1029&lt;/td&gt;
&lt;td&gt;192&lt;/td&gt;
&lt;td&gt;937&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;After&lt;/td&gt;
&lt;td&gt;617&lt;/td&gt;
&lt;td&gt;143&lt;/td&gt;
&lt;td&gt;619&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup&lt;/td&gt;
&lt;td&gt;40.03%&lt;/td&gt;
&lt;td&gt;25.52%&lt;/td&gt;
&lt;td&gt;33.93%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is clear that the speedups are very significant! The compilation is between
25% and 40% faster with the new option. Overall, this is a speedup of 36%!
I also noticed that the compilation takes significantly less memory than before.
Therefore, I decided to rerun the compiler benchmark on the library. In the
previous experiment, zapcc was taking so much memory that it was impossible to
use more than one thread. Let's see how it is faring now. The time to compile
the full unit tests is computed for each compiler. Let's start in debug mode:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="23%"&gt;
&lt;col width="19%"&gt;
&lt;col width="19%"&gt;
&lt;col width="19%"&gt;
&lt;col width="19%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Debug&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j3&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;clang-3.9&lt;/td&gt;
&lt;td&gt;527&lt;/td&gt;
&lt;td&gt;268&lt;/td&gt;
&lt;td&gt;182&lt;/td&gt;
&lt;td&gt;150&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;gcc-4.9.3&lt;/td&gt;
&lt;td&gt;591&lt;/td&gt;
&lt;td&gt;303&lt;/td&gt;
&lt;td&gt;211&lt;/td&gt;
&lt;td&gt;176&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;gcc-5.3.0&lt;/td&gt;
&lt;td&gt;588&lt;/td&gt;
&lt;td&gt;302&lt;/td&gt;
&lt;td&gt;209&lt;/td&gt;
&lt;td&gt;175&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc-1.0&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;375&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;187&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;126&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;121&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This time, zapcc is able to scale to four threads without problems. Moreover, it
is always the fastest compiler, by a significant margin, in this configuration.
It is followed by clang and then by gcc for which both versions are about the
same speed.&lt;/p&gt;
&lt;p&gt;If we compile again in release mode:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="24%"&gt;
&lt;col width="20%"&gt;
&lt;col width="20%"&gt;
&lt;col width="20%"&gt;
&lt;col width="16%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Release&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j3&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;clang-3.9&lt;/td&gt;
&lt;td&gt;1201&lt;/td&gt;
&lt;td&gt;615&lt;/td&gt;
&lt;td&gt;421&lt;/td&gt;
&lt;td&gt;356&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;gcc-4.9.3&lt;/td&gt;
&lt;td&gt;1041&lt;/td&gt;
&lt;td&gt;541&lt;/td&gt;
&lt;td&gt;385&lt;/td&gt;
&lt;td&gt;321&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;gcc-5.3.0&lt;/td&gt;
&lt;td&gt;1114&lt;/td&gt;
&lt;td&gt;579&lt;/td&gt;
&lt;td&gt;412&lt;/td&gt;
&lt;td&gt;348&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc-1.0&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;897&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;457&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;306&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;306&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The difference in compilation time is very large, it's twice slower to compile
with all optimizations enabled. It also takes significantly more memory. Indeed,
zapcc was not able to compile with 4 threads. Nevertheless, even the results
with three threads are better than the other compilers using four threads. zapcc
is clearly the winner again on this test, followed by gcc4-9 which is faster
than gcc-5.3 which is itself faster than clang. It seems that while clang is
better at frontend than gcc, it is slower for optimizations. Note that this may
also be an indication that clang performs more optimizations than gcc and may
not be slower.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;By using some form of type erasing to simplify the templates types at compile
time, I was able to reduce the overall compilation time of my Deep Learning
Library (DLL) by 36%. Moreover, this can be done by switching a simple
compilation flag. This also very significantly reduce the memory used during the
compilation, allowing zapcc to to compile with up to three threads, compared
with only one before. This makes zapcc the fastest compiler again on this
benchmark. Overall, this will make debugging much easier on this library and
will save me a lot of time.&lt;/p&gt;
&lt;p&gt;In the future, I plan to try to improve compilation time even more. I have a few
ideas, especially in ETL that should significantly improve the compilation time
but that will require a lot of time to implement, so that will likely have to
wait a while. In the coming days, I plan to work on the performance of DLL,
especially for stochastic gradient descent.&lt;/p&gt;
&lt;p&gt;If you want more information on DLL, you can check out the
&lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;dll Github repository&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>clang</category><category>Compilers</category><category>dll</category><category>etl</category><category>gcc</category><category>zapcc</category><guid>http://baptiste-wicht.com/posts/2017/03/partial-type-erasing-deep-learning-library-dll-improve-compilation-time.html</guid><pubDate>Wed, 15 Mar 2017 06:43:44 GMT</pubDate></item><item><title>Use clang-tidy for static analysis and integration in Sonarqube</title><link>http://baptiste-wicht.com/posts/2017/03/clang-tidy-static-analysis-integration-in-sonarqube.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;clang-tidy is an extensive linter C++. It provides a complete framework for
analysis of C++ code. Some of the checks are very simple but some of them are
very complete and most of the checks from the clang-static-analyzer are
integrated into clang-tidy.&lt;/p&gt;
&lt;div class="section" id="usage"&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;If you want to see the list of checks available on clang-tidy, you can use the
list-checks options:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_097a78d40f4046e993cb65035e8f026b-1"&gt;&lt;/a&gt;clang-tidy -list-checks
&lt;/pre&gt;&lt;p&gt;You can then choose the tests you are interested in and perform an analysis of
your code. For, it is highly recommended to use a Clang compilation database,
you can have a look at Bear to generate this compilation database if you don't
have it yet. The usage of clang-tidy, is pretty simple, you set the list of
checks you want, the header on which you want to have warnings reported and the
list of source files to analyse:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_5c3d2b46a40542d1a886b67a6cf0ce0f-1"&gt;&lt;/a&gt;clang-tidy -checks='*' -header-filter="^include" -p . src/*.cpp
&lt;/pre&gt;&lt;p&gt;You'll very likely see a lot of warnings. And you will very likely see a lot of
false positives and a lot of warnings you don't agree too. For insance, there
are a lot of warnings from the CPP Core Guidelines and the Google Guidelines
that I don't follow in my coding. You should not take the complete list of tests
as rule, you should devise your own list of what you really want to fix in your
code. If you want to disable one check X, you can use the - operation:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_cbbaba3d60ea4314a400a9f7c0c8fa08-1"&gt;&lt;/a&gt;clang-tidy -checks='*,-X' -header-filter="^include" -p . src/*.cpp
&lt;/pre&gt;&lt;p&gt;You can also enable the checks one by one or parts of them with *:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_91ce81be43cc4b0ab4d00b1f298e655a-1"&gt;&lt;/a&gt;clang-tidy -checks='google-*' -header-filter="^include" -p . src/*.cpp
&lt;/pre&gt;&lt;p&gt;One problem with the clang-tidy tool is that it is utterly slow, especially if
you enable the clang-static-analyzer checks. Moreover, if you use it like it is
set before, it will only use one thread for the complete set of files. This may
not be an issue on small projects, but this will definitely be a big issue for
large projects and template-heavy code (like my ETL project). You could create
an implicit target into your Makefile to use it on each file independently and
then use the -j option of make to make them in parallel, but it not really
practical.&lt;/p&gt;
&lt;p&gt;For this, I just discovered that clang propose a Python script,
run-clang-tidy.py that does it all for us! On Gentoo, it is installed at
/usr/share/run-clang-tidy.py.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_42a80053a2734bd5bf4fa14d7825c424-1"&gt;&lt;/a&gt;run-clang-tidy.py -checks='*' -header-filter="^include" -p . -j9
&lt;/pre&gt;&lt;p&gt;This will automatically run clang-tidy on each file from the compilation
database and use 9 threads to perform the checks. This is definitely much
faster. For me, this is the best way to run clang-tidy.&lt;/p&gt;
&lt;p&gt;One small point I don't like is that the script always print the list of enabled
checks. For, this I changed this line in the script:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_fde3c156e5484523a31f93d4698c577b-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;invocation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clang_tidy_binary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'-list-checks'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;with:&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_10fb77fb275a4f698765be87bf2e4395-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;invocation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clang_tidy_binary&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This makes it more quiet.&lt;/p&gt;
&lt;p&gt;One thing I didn't mention is that clang-tidy is able to fix some of the errors
directly if you use the -fix option. Personally, I don't like this, but for
a large code base and a carefully selected set of checks, this could be really
useful. Note that not all the checks are automatically fixable by clang-tidy.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="results"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;I have run clang-tidy on my cpp-utils library and here some interesting results.
I have not run all the checks, here is the command I used:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_607905eaa8774e0aa00e4715eec695d6-1"&gt;&lt;/a&gt;/usr/share/clang/run-clang-tidy.py -p . -header-filter '^include/cpp_utils' -checks='cert-*,cppcoreguidelines-*,google-*,llvm-*,misc-*,modernize-*,performance-*,readility-*,-cppcoreguidelines-pro-type-reinterpret-cast,-cppcoreguidelines-pro-bounds-pointer-arithmetic,-google-readability-namespace-comments,-llvm-namespace-comment,-llvm-include-order,-google-runtime-references' -j9 2&amp;gt;/dev/null  | /usr/bin/zgrep -v "^clang-tidy"
&lt;/pre&gt;&lt;p&gt;Let's go over some warnings I got:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_4341cfb8f71949b4bf99d22fcd3d8ffa-1"&gt;&lt;/a&gt;include/cpp_utils/assert.hpp:91:103: warning: consider replacing 'long' with 'int64' [google-runtime-int]
&lt;a name="rest_code_4341cfb8f71949b4bf99d22fcd3d8ffa-2"&gt;&lt;/a&gt;void assertion_failed_msg(const CharT* expr, const char* msg, const char* function, const char* file, long line) {
&lt;a name="rest_code_4341cfb8f71949b4bf99d22fcd3d8ffa-3"&gt;&lt;/a&gt;                                                                                                      ^
&lt;/pre&gt;&lt;p&gt;I got this one several times. It is indeed more portable to use &lt;code&gt;int64&lt;/code&gt; rather than &lt;code&gt;long&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_12d0bca3b5ed43f0a550dcef7b18a988-1"&gt;&lt;/a&gt;include/cpp_utils/aligned_allocator.hpp:53:9: warning: use 'using' instead of 'typedef' [modernize-use-using]
&lt;a name="rest_code_12d0bca3b5ed43f0a550dcef7b18a988-2"&gt;&lt;/a&gt;        typedef aligned_allocator&amp;lt;U, A&amp;gt; other;
&lt;a name="rest_code_12d0bca3b5ed43f0a550dcef7b18a988-3"&gt;&lt;/a&gt;        ^
&lt;/pre&gt;&lt;p&gt;This one is part of the modernize checks, indicating that one should use
&lt;code&gt;using&lt;/code&gt; rather than a &lt;code&gt;typedef&lt;/code&gt; and I completely agree.&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_d0d13f55c06f47cd9c2992f049f1e972-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;include&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;cpp_utils&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;aligned_allocator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nl"&gt;hpp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nl"&gt;warning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;use&lt;/span&gt; &lt;span class="err"&gt;'&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;define&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;trivial&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="n"&gt;constructor&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;modernize&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;use&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_d0d13f55c06f47cd9c2992f049f1e972-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;aligned_allocator&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;a name="rest_code_d0d13f55c06f47cd9c2992f049f1e972-3"&gt;&lt;/a&gt;    &lt;span class="o"&gt;^&lt;/span&gt;
&lt;a name="rest_code_d0d13f55c06f47cd9c2992f049f1e972-4"&gt;&lt;/a&gt;                        &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Another one from the modernize checks that I really like. This is completely
true.&lt;/p&gt;
&lt;!-- code.:

include/cpp_utils/maybe_parallel.hpp:33:5: warning: constructors that are callable with a single argument must be marked explicit to avoid unintentional implicit conversions [google-explicit-constructor]
    thread_pool(Args... /*args*/){
    ^
    explicit --&gt;
&lt;p&gt;I don't agree that every constructor with one argument should be explicit,
sometimes you want implicit conversion. Nevertheless, this particular case is
very interesting since it is variadic, it can have one template argument and as
thus it can be implicitly converted from anything, which is pretty bad I think.&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_41f1325a0f0a4bb3b426dfc8dec16a61-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;array_wrapper&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nl"&gt;cpp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nl"&gt;warning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt; &lt;span class="n"&gt;casts&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="n"&gt;discouraged&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;use&lt;/span&gt; &lt;span class="k"&gt;reinterpret_cast&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;google&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;readability&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;casting&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_41f1325a0f0a4bb3b426dfc8dec16a61-2"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mem&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;malloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_41f1325a0f0a4bb3b426dfc8dec16a61-3"&gt;&lt;/a&gt;                 &lt;span class="o"&gt;^&lt;/span&gt;
&lt;a name="rest_code_41f1325a0f0a4bb3b426dfc8dec16a61-4"&gt;&lt;/a&gt;                 &lt;span class="k"&gt;reinterpret_cast&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;*&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;         &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;On this one, I completely agree, C-style casts should be avoided and much
clearer C++ style casts should be preferred.&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_2363916daf3248c1a3c905bfdeb1ee5f-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;home&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;wichtounet&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;dev&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;cpp_utils_test&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;include&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;cpp_utils&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;aligned_allocator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nl"&gt;hpp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;126&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nl"&gt;warning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;thrown&lt;/span&gt; &lt;span class="n"&gt;exception&lt;/span&gt; &lt;span class="n"&gt;type&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;nothrow&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt; &lt;span class="n"&gt;constructible&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cert&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;err60&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;cpp&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;a name="rest_code_2363916daf3248c1a3c905bfdeb1ee5f-2"&gt;&lt;/a&gt;            &lt;span class="k"&gt;throw&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;length_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"aligned_allocator&amp;lt;T&amp;gt;::allocate() - Integer overflow."&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_2363916daf3248c1a3c905bfdeb1ee5f-3"&gt;&lt;/a&gt;                  &lt;span class="o"&gt;^&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is one of the checks I don't agree with. Even though it makes sense to
prefer exception that are nothrow copy constructible, they should be caught by
const reference anyway. Moreover, this is here an exception from the standard
library.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_a9b73a5c8cf8408093759d5ec4b4d46c-1"&gt;&lt;/a&gt;/home/wichtounet/dev/cpp_utils_test/include/cpp_utils/aligned_allocator.hpp:141:40: warning: do not use const_cast [cppcoreguidelines-pro-type-const-cast]
&lt;a name="rest_code_a9b73a5c8cf8408093759d5ec4b4d46c-2"&gt;&lt;/a&gt;        free((reinterpret_cast&amp;lt;void**&amp;gt;(const_cast&amp;lt;std::remove_const_t&amp;lt;T&amp;gt;*&amp;gt;(ptr)))[-1]);
&lt;a name="rest_code_a9b73a5c8cf8408093759d5ec4b4d46c-3"&gt;&lt;/a&gt;                                       ^
&lt;/pre&gt;&lt;p&gt;In general, I agree that using const_cast should be avoided as much as possible.
But there are some cases where they make sense. In this particular case, I don't
modify the object itself but some memory before the object that is unrelated and
I initialize myself.&lt;/p&gt;
&lt;p&gt;I also had a few false positives, but overall nothing too bad. I'm quite
satisfied with the quality of the results. I'll fix these warnings in the coming
week.&lt;/p&gt;
&lt;p&gt;Integration in Sonarqube&lt;/p&gt;
&lt;p&gt;The sonar-cxx plugin just integrated support for clang-tidy in main. You need
to build the version yourself, the 0.9.8-SNAPSHOT version. You then can use
something like this in your sonar-project.properties file:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_3b2c06213ac94d8bac9a28d7e67c5be0-1"&gt;&lt;/a&gt;sonar.cxx.clangtidy.reportPath=clang-tidy-report
&lt;/pre&gt;&lt;p&gt;and sonar-cxx will parse the results and integrate the issues in your sonar
report.&lt;/p&gt;
&lt;p&gt;Here is an example:&lt;/p&gt;
&lt;img alt="/images/sonar-cxx-clang-tidy.png" src="http://baptiste-wicht.com/images/sonar-cxx-clang-tidy.png"&gt;
&lt;p&gt;You can see two of the warnings from clang-tidy :)&lt;/p&gt;
&lt;p&gt;For now, I haven't integrate this in my Continuous Integration system because
I'm still having issues with clang-tidy and the compilation database. Because
the compilation contains absolute paths to the file and to the current
directory, it cannot be shared directly between servers. I have to find a way to
fix that so that clang-tidy can use on the other computer. I'll probably wait
till the sonar-cxx 0.9.8 version is released before integrating all this in
Sonarqube, but this is a great news for this plugin :)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;clang-tidy is C++ linter that can analyze your code and checks for hundreds of
problems in it. With it, I have found some very interesting problems in the code
of my cpp_utils library. Moreover, you can now integrate it Sonarqube by using
the sonar-cxx plugin. Since it is a bit slow, I'll probably not integrate it in
my bigger projects, but I'll integrate at least in the cpp_utils library when
sonar-cxx 0.9.8 will be released.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>projects</category><category>Sonar</category><guid>http://baptiste-wicht.com/posts/2017/03/clang-tidy-static-analysis-integration-in-sonarqube.html</guid><pubDate>Sat, 11 Mar 2017 08:54:00 GMT</pubDate></item></channel></rss>