<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Blog blog("Baptiste Wicht");</title><link>http://baptiste-wicht.com/</link><description>Tutorials and short posts about programming, C++, Java, Assembly, Operating Systems Development, Compilers, ...</description><atom:link href="http://baptiste-wicht.com/rss.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Fri, 10 Mar 2017 10:24:16 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Disappointing zapcc performance on Deep Learning Library (DLL)</title><link>http://baptiste-wicht.com/posts/2017/03/disappointing-zapcc-performance-on-deep-learning-library-dll.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;One week ago, zapcc 1.0 was released and I've observed it to be much faster than the other
compilers in terms of compile time. This can be seen when
&lt;a class="reference external" href="http://baptiste-wicht.com/posts/2017/03/release-zapcc-10-fast-cpp-compiler.html"&gt;I tested it on my Expression Templates Library (ETL)&lt;/a&gt;. It was almost four
times faster than clang 3.9 and about 2.5 times faster than GCC.&lt;/p&gt;
&lt;p&gt;The ETL library is quite heavy to compile, but still reasonable. This is not the
case for my Deep Learning Library (DLL) where compiling all the test cases takes
a very long time. I have to admit that I have been going overboard with
templates and such and I have now to pay the price. In practice, for the users
of the library, this is not a big problem since only one or two neural networks
will be compiled (and it will take hours to train), but in the test cases, there
are hundreds of them and this is a huge pain. Anyway, enough with the ramble,
I figured it would be very good to test zapcc on it and see what I can gain from
using it.&lt;/p&gt;
&lt;p&gt;In this article, when I speak of a compiler thread, I mean an instance of the
processor, so it's really a process in the Linux world.&lt;/p&gt;
&lt;div class="section" id="results"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;However, I soon realized that I would have more issues than I thought. The first
problem is the memory consumed by zapcc. Indeed, it is based on clang and
I always had problem with huge memory consumption from clang on this library and
zapcc has even bigger memory consumption because some information is cached
between runs. The amount of memory that zapcc is able to cache can be configured
in the configuration file. By default, it can use 1.5Go of memory. When zapcc
goes over the memory limit, it simply wipes out its caches. This means that all
the gain for the next compilation will be lost, since the cache will have to be
rebuilt from scratch. This is not a hard limit for the compilation itself.
Indeed, if the compilation itself takes 3Go, it will still be able to complete
it, but it is likely that the cache will be wiped after the compilation.&lt;/p&gt;
&lt;p&gt;When I tried compiling using several threads, it soon used all my memory and
crashed. The same occurs with clang but I can still compile with 3 or 4 threads
without too much issues on this computer. The same also occurs with GCC but it
can still handle 4 or 5 threads (depending on the order of the compilation
units).&lt;/p&gt;
&lt;p&gt;The tests are performed on my desktop computer at work, which is not really
good... I have 12Go of RAM (I had to ask for extra...) and an old Sandy Bridge
processor, but at least I have an SSD (also had to ask for extra).&lt;/p&gt;
&lt;p&gt;I started with testing with only one compiler thread. For zapcc, I set the
maximum memory limit to 8Go. Even with such a limit, the zapcc server restarted
more than 10 times during the compilation of the 84 test cases. After this first
experiment, I increased the number of threads to 2 for each compiler, using 4Go
limit for zapcc. The limit is for each server and each parallel thread will
spawn a new server, so the effective limit is the number of threads times the
limit. Even with two threads, I was unable to finish a compilation with zapcc.
This is quite disappoint for me since clang is able to run with 4 threads in
parallel. Moreover, a big problem with that is that the servers are not always
killed when there is no no more memory, they just hang and use all the memory of
the computer, which is evidently really inconvenient for service processes. When
this happens with clang or gcc, the compiler simply crashes and the memory is
released and make is interrupted. Since zapcc is not able to work with more than
one thread on this computer, the results are the ones with one thread. I was
also surprised to be able to compile the library with clang and four threads,
this was not possible before clang-3.9.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="36%"&gt;
&lt;col width="17%"&gt;
&lt;col width="17%"&gt;
&lt;col width="15%"&gt;
&lt;col width="15%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j3&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;gcc-4.9.3&lt;/td&gt;
&lt;td&gt;2250.95&lt;/td&gt;
&lt;td&gt;1256.36&lt;/td&gt;
&lt;td&gt;912.67&lt;/td&gt;
&lt;td&gt;760.84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;gcc-5.3.0&lt;/td&gt;
&lt;td&gt;2305.37&lt;/td&gt;
&lt;td&gt;1279.49&lt;/td&gt;
&lt;td&gt;918.08&lt;/td&gt;
&lt;td&gt;741.38&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang-3.9&lt;/td&gt;
&lt;td&gt;2047.61&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;1102.93&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;899.13&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;730.42&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc-1.0&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;1483.73&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1483.73&lt;/td&gt;
&lt;td&gt;1483.73&lt;/td&gt;
&lt;td&gt;1483.73&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Difference against Clang&lt;/td&gt;
&lt;td&gt;-27.55%&lt;/td&gt;
&lt;td&gt;+25.69%&lt;/td&gt;
&lt;td&gt;+39.37%&lt;/td&gt;
&lt;td&gt;+50.77%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC-5.3&lt;/td&gt;
&lt;td&gt;-35.66%&lt;/td&gt;
&lt;td&gt;+13.75%&lt;/td&gt;
&lt;td&gt;+38.09%&lt;/td&gt;
&lt;td&gt;+50.03%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC-4.9&lt;/td&gt;
&lt;td&gt;-34.08%&lt;/td&gt;
&lt;td&gt;+15.30%&lt;/td&gt;
&lt;td&gt;+38.50%&lt;/td&gt;
&lt;td&gt;+48.75%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If we look at the results with only one thread, we can see that there still are
some significant improvements when using zapcc, but nowhere near as good as what
was seen in the compilation of ETL. Here, the compilation time is reduced by 34%
compared to gcc and by 27% compared to clang. This is not bad, since it is
faster than the other compilers, but I would have expected better speedups. We
can see that g++-4.9 is slightly faster than g++-5.3, but this is not really
a significant difference. I'm actually very surprised to find that clang is
faster than g++ on this experiment. On ETL, it is always very significantly
slower and before, it was also significantly slower on DLL. I was so used to
this, that I stopped using it on this project. I may have to reconsider my
position when working on this project.&lt;/p&gt;
&lt;p&gt;Let's look at the results with more than two threads. Even with two threads,
every compiler is faster than zapcc. Indeed, zapcc is slower than Clang by 25%
and slower than GCC by about 15%. If we use more threads, the other compilers
are becoming even faster and the slowdowns of zapcc are more important. When
using four threads, zapcc is about 48% slower than gcc and about 50% slower than
clang. This is really showing one big downside of zapcc that has a very large
memory consumption. When it is used to compile really heavy template code, it is
failing very early to use more processes. And even when there is enough memory,
the speedups are not as great as for relatively simpler code.&lt;/p&gt;
&lt;p&gt;One may argue that this is not a fair comparison since zapcc does not have the
same numbers of threads. However, considering that this is the best zapcc can do
on this machine, I would argue that this is a fair comparison in this limited
experimental setting. If we were to have a big machine for compilation, which
I don't have at work, the zapcc results would likely be more interesting, but in
this specific limited case, it shows that zapcc suffers from its high memory
consumption. It should also be taken into account that this experiment was done
with almost nothing else running on the machine (no browser for instance) to
have as much memory as possible available for the compilers. This is not
a common use case.  Most of the days, when I compile something, I have my
browser open, which makes a large difference in memory available, and several
other applications (but consoles and vim instances do not really consume memory
:D).&lt;/p&gt;
&lt;p&gt;This experiment made me realize that the compilation times for this library were
quickly becoming crazy. Most of the time, the complete test suite is only
compiled on my Continuous Integration machine at home which has a much faster
processor and much more RAM. Therefore, it is relatively fast since it uses more
threads to compile.  Nevertheless, this is not a good point that the unit tests
takes so much time to compile. I plan to split the test cases in several sets.
Because, currently the real unit tests are compiled with the performance tests
and other various tests. I'll probably end up generating three executables. This
will help greatly during development. Moreover, I also have a technique to
decrease the compilation time by erasing some template parameters at compilation
time. This is already ready, but has currently a runtime overhead that I will
try to remove and then use this technique everywhere to get back to reasonable
compilation times. I'll also try to see if I can find obvious compilation
bottlenecks in the code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;To conclude, while zapcc brings some very interesting compilation speedups in
some cases like in my ETL library, it also has some downsides, namely
&lt;strong&gt;huge memory consumption&lt;/strong&gt;. This memory consumption may prevent the use of several
compiler threads and render zapcc much less interesting than other compilers.&lt;/p&gt;
&lt;p&gt;When trying to compile my DLL library on a machine with 12Go of RAM with two
zapcc threads, it was impossible for me to make it complete. While zapcc was
faster with one thread than the other compilers, they were able to use up to
four threads and in the end &lt;strong&gt;zapcc was about twice slower than clang&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I knew that zapcc memory consumption was very large, but I would have not have
expected something so critical. Another feature that would be interesting in
zapcc would be to set a max memory hard limit for the server instead of simply
a limit on the cache they are able to keep in memory. This would prevent hanging
the complete computer when something goes wrong.&lt;/p&gt;
&lt;p&gt;I had a good surprise with clang that was actually faster than GCC and also able
to work with four threads in parallel. This was not the case with previous
version of clang. On ETL, it is still significantly slower than GCC though.&lt;/p&gt;
&lt;p&gt;For now, I'll continue using clang on this DLL project and use zapcc only on my
ETL project. I'll also focus on improving the compilation time on this project
and make it reasonable again.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>compiler</category><category>dll</category><category>gcc</category><category>projects</category><category>zapcc</category><guid>http://baptiste-wicht.com/posts/2017/03/disappointing-zapcc-performance-on-deep-learning-library-dll.html</guid><pubDate>Thu, 09 Mar 2017 12:41:06 GMT</pubDate></item><item><title>Migrated from owncloud 5 to Nextcloud 11</title><link>http://baptiste-wicht.com/posts/2017/03/migrated-from-owncloud-5-to-nextcloud-11.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;For several years now I've been using Owncloud running on one of my servers.
I'm using simply using as a simple synchronization, I don't use any of the tons
of fancy features they keep adding. Except from several synchronization issues,
I haven't had too much issues with it.&lt;/p&gt;
&lt;p&gt;However, I have had a very bad time with updates of Owncloud. The last time
I tried, already long ago, was to upgrade from 5.0 to 6.0 and I never succeeded
without losing all the configuration and having to do the resync. Therefore,
I've still an Owncloud 5.0 running. From this time, I had to say that I've been
lazy and didn't try again to upgrade it. Recently, I've received several mails
indicating that this is a security threat.&lt;/p&gt;
&lt;p&gt;Since I was not satisfied with updates in Owncloud and its security has been
challenged recently, I figured it would be a good moment to upgrade to Nextcloud
which is a very active fork of Owncloud that was forked by developers of
Owncloud.&lt;/p&gt;
&lt;p&gt;I haven't even tried to do an upgrade from such an old version to the last
version of Nextcloud, it was doomed to fail. Therefore, I made a new clean
installation. Since I only use the sync feature of the tool, it does not really
matter, it is just some time lost to sync everything again, but nothing too bad.&lt;/p&gt;
&lt;p&gt;I configured a new PostgreSQL on one of my servers for the new database and then
installed Nextcloud 11 on Gentoo. It's a bit a pain to have a working Nginx
configuration for Nextcloud, I don't advice to do it by hand, better take one
from the official documentation, you'll also gain some security. One very bad
thing in the installation process is that you cannot choose the database prefix,
it's set like Owncloud. The problem with that is that you cannot install both
Owncloud and Nextcloud on the same database which would be more practical for
testing purpose. It's a bit retarded in my opinion, but not a big problem in the
end. Other than these two points, everything went well and it was installation
pretty nicely. Then, you should have your user ready to go.&lt;/p&gt;
&lt;img alt="Nextcloud view" class="align-center" src="http://baptiste-wicht.com/images/nextcloud.png"&gt;
&lt;p&gt;As for the interface, I don't think there is a lot to tell here. Most of it is
what you would except from this kind of tool. Moreover, I very rarely use the
web interface or any of the feature that are not the sync feature. One thing
that is pretty cool I think is the monitoring graphs in the Admin section of the
interface. You can the number of users connected, the memory used and the CPU
load. It's pretty useful if you share your Nextcloud between a lot of different
users.&lt;/p&gt;
&lt;p&gt;I didn't have any issue with the sync either. I used the nextcloud-client
package on Gentoo directly and it worked perfectly directly. It took about 10
minutes to sync everything again (about 5GB). I'll have to do the same thing on
my other computer as well, but I don't think I'll have any issue.&lt;/p&gt;
&lt;p&gt;So far, I cannot say that this is better than Owncloud, I just hope the next
upgrade will fare better than they did on Owncloud. Moreover, I also hope that
the security that they promise is really here and I won't have any problem with
it. I'll see in the future!&lt;/p&gt;&lt;/div&gt;</description><category>Gentoo</category><category>Linux</category><category>Server</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2017/03/migrated-from-owncloud-5-to-nextcloud-11.html</guid><pubDate>Fri, 03 Mar 2017 08:08:58 GMT</pubDate></item><item><title>Release of zapcc 1.0 - Fast C++ compiler</title><link>http://baptiste-wicht.com/posts/2017/03/release-zapcc-10-fast-cpp-compiler.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;If you remember, I recently wrote about &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html"&gt;zapcc C++ compilation speed against gcc 5.4 and clang 3.9&lt;/a&gt; in which I was comparing the beta version of zapcc against gcc and clang.&lt;/p&gt;
&lt;p&gt;I just been informed that zapcc was just released in version 1.0. I though it
was a good occasion to test it again. It will be compared against gcc-4.9,
gcc-5.3 and clang-3.9. This version is based on the trunk of clang-5.0.&lt;/p&gt;
&lt;p&gt;Again, I will use my Expression Template Library (&lt;a class="reference external" href="https://github.com/wichtounet/etl/"&gt;ETL&lt;/a&gt;) project. This is a purely header-only
library with lots of templates. I'm going to compile the full test cases. This
is a perfect example for long compilation times.&lt;/p&gt;
&lt;p&gt;The current tests are made on the last version of the library and with slightly
different parameters for compilation, therefore the absolute times are not
comparable, but the speedups should be comparable.&lt;/p&gt;
&lt;p&gt;Just like last time, I have configured zapcc to let is use 2Go RAM per caching
server, which is the maximum allowed. Moreover, I killed the servers before each
tests.&lt;/p&gt;
&lt;div class="section" id="debug-results"&gt;
&lt;h2&gt;Debug results&lt;/h2&gt;
&lt;p&gt;Let's start with a debug build, with no optimizations enabled. Every build will
use four threads. This is the equivalent of doing make -j4 debug/bin/etl_test
without the link step.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="73%"&gt;
&lt;col width="27%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt; &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.3&lt;/td&gt;
&lt;td&gt;190.09s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.3.0&lt;/td&gt;
&lt;td&gt;200.92s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9&lt;/td&gt;
&lt;td&gt;313.85&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;81.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;3.86&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC-5.3&lt;/td&gt;
&lt;td&gt;2.47&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC-4.9&lt;/td&gt;
&lt;td&gt;2.33&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The speedups are even more impressive than last time! zapcc is &lt;strong&gt;almost four
times fast than clang-3.9&lt;/strong&gt; and around &lt;strong&gt;2.5 times faster than GCC-5.3&lt;/strong&gt;.
Interestingly, we can see that gcc-5.3 is slighly slower than GCC-4.9.&lt;/p&gt;
&lt;p&gt;It seems that they have the compiler even faster!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="release-results"&gt;
&lt;h2&gt;Release results&lt;/h2&gt;
&lt;p&gt;Let's look now how the results are looking with optimizations enabled. Again,
every build will use four threads. This is the equivalent of doing make -j4
release_debug/bin/etl_test without the link step.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="75%"&gt;
&lt;col width="25%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt; &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.3&lt;/td&gt;
&lt;td&gt;252.99&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;g++-5.3.0&lt;/td&gt;
&lt;td&gt;264.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9&lt;/td&gt;
&lt;td&gt;361.65&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;237.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;1.51&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC-5.3&lt;/td&gt;
&lt;td&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC-4.9&lt;/td&gt;
&lt;td&gt;1.06&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can see that this time the speedups are not as interesting as they were.
Very interestingly, it's the compiler that suffers the more from the
optimization overhead. Indeed, zapcc is three times slower in release mode than
it was in debug mode. Nevertheless, it still manages to beat the three other
compilers, by about 10% for Gcc and 50% than clang, which is already
interesting.&lt;/p&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;To conclude, we have observed that zapcc is always faster than the three
compilers tested in this experiment. Moreover, in debug mode, the speedups are
very significant, it was almost 4 times faster than clang and around 2.5 faster
than gcc.&lt;/p&gt;
&lt;p&gt;I haven't seen any problem with the tool, it's like clang and it should generate
code of the same performance, but just compile it much faster. One problem
I have with zapcc is that it is not based on an already released version of
clang but on the trunk. That means it is hard to be compare with the exact same
version of clang and it is also a risk of running into clang bugs.&lt;/p&gt;
&lt;p&gt;Although the prices have not been published yet, it is indicated on the website
that zapcc is free for non-commercial entities. Which is really great.&lt;/p&gt;
&lt;p&gt;If you want more information, you can go to the
&lt;a class="reference external" href="https://www.zapcc.com/"&gt;official website of zapcc&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>compiler</category><category>etl</category><category>gcc</category><category>projects</category><category>zapcc</category><guid>http://baptiste-wicht.com/posts/2017/03/release-zapcc-10-fast-cpp-compiler.html</guid><pubDate>Thu, 02 Mar 2017 13:50:04 GMT</pubDate></item><item><title>Home Automation: First attempt at voice control with Jarvis</title><link>http://baptiste-wicht.com/posts/2017/02/home-automation-first-attempt-voice-control-jarvis.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I have several devices in my home that can be controller via Domoticz, a few
power outlets, a few lights (more are coming), my Kodi home theater. And I have
a lot of sensors and information gathered by Domoticz. All of this is working
quite well, but I have only a few actuators and intelligence (motion sensor,
button and some automation via Lua script).&lt;/p&gt;
&lt;p&gt;My next objective was to add voice control to my system. If I was living in
United States or United Kingdom I would directly an Amazon Dot or even an Amazon
Echo, but they are not available in Switzerland. I could have arranged for
delivery, but if I want my system to be useful to several people, I need to have
in French. It's the same problem with the Google Home system. So, no other way
than custom solutions.&lt;/p&gt;
&lt;p&gt;Since I had an extra Raspberry Pi 2, I based my system on this. I bought a Trust
Mico microphone and a Trust Compact speakers and installed them on the Pi. Both
peripherals are working quite well.&lt;/p&gt;
&lt;p&gt;You can have a closer look at my microphone:&lt;/p&gt;
&lt;img alt="Trust Mico microphone for Jarvis Home Automation Voice Control" class="align-center" src="http://baptiste-wicht.com/images/jarvis_mic.jpg"&gt;
&lt;p&gt;and the complete installation:&lt;/p&gt;
&lt;img alt="Jarvis Home Automation Voice Control around my TV" class="align-center" src="http://baptiste-wicht.com/images/jarvis_full.jpg"&gt;
&lt;p&gt;The Raspberry Pi is on the bottom, the speakers below the TV, left and right and
the microphone on the top right.&lt;/p&gt;
&lt;p&gt;For the voice control software, I decided to go with Jarvis. It seems to me that
this is the best suited software for this kind of project. Moreover, it supports
French natively which seems good. I also tried Jasper, but this has been such
a pain to install that I gave up.&lt;/p&gt;
&lt;p&gt;Jarvis is reasonably easy to install if you have a recent Raspbian image. It
took some time to install the dependencies, but in the end it was not difficult.
The installation process has a step-by-step wizard help so it's really easy to
configure everything.&lt;/p&gt;
&lt;p&gt;However, even if it's easy to install, it's easy to configure correctly. The
first thing is to configure the hotword to activate commands. There are several
options, but I used snowboy which is offline and is made for hotword
recognition. This worked quite well, you just have to train a model with the
hotword to recognize the voice. After this, the problems started... You then
have to configure audio for the commands themselves. There are 6 parameters for
audio capture (noise levels to start and stop the capture, silence levels, ...)
and no help to tune them. So basically, I tried a lot of combinations until
I had something working reasonably well. When you are in debug mode, you can
listen to what the system captured. These parameters are dependent on your
environment and on your microphone and on your voice. I may be dumb but it took
several hours and a lot of tries to get a configuration working. After this, you
have to choose the engine for recognition of the commands. Unfortunately, all
the good options are online so everything you'll say as commands after the
hotword will be sent online. I first tried Bing, but I had very poor recognition
rate. I then switched to wit.ai which gave me better results. In the end, I have
about 60% recognition rate, which is not great at all, but at least some phrases
are working almost all the time while others are always failing. Another problem
I have with this is the large delay between commands and action. It takes almost
five seconds between the end of my sentence and the time where the lights in my
living room are tuned on or off by Jarvis via Domoticz.&lt;/p&gt;
&lt;p&gt;So far, I'm a bit disappointed by the quality of the system, but maybe I was
hoping for too much. I have been able to control a few of my appliances but not
really reliably. Another thing I have realized is that when counting the
Raspberry Pi, its enclosure the Microphone and the speakers, this system is more
costly than an Amazon Dot and seem highly inferior (and is much less good
looking).&lt;/p&gt;
&lt;p&gt;I'll try to improve the current system with better configuration and commands in
the coming days and I will maybe try another system for voice control. I still
hope Amazon Alexa systems or Google Home are made available in
France/Switzerland not too far in the future, since I believe these systems are
a better solution than custom made systems, at least for now. Moreover, next
month, I plan to integrate ZWave into my systems with a few sensors, complete
the lighting installation and add new motion sensors. This should make it more
useful. And hopefully, by this time, I should have a good voice control system,
but I'm not too hopeful.&lt;/p&gt;
&lt;p&gt;Don't hesitate to comment or contact me if you have questions about this
installation or want to share experience about voice control in home automation.
If you want more details about this, dont' hesitate to ask as well ;)&lt;/p&gt;&lt;/div&gt;</description><category>Home Automation</category><category>Personal</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/02/home-automation-first-attempt-voice-control-jarvis.html</guid><pubDate>Tue, 14 Feb 2017 17:34:19 GMT</pubDate></item><item><title>Publication: CPU Performance Optimizations for RBM and CRBM</title><link>http://baptiste-wicht.com/posts/2017/02/publication-cpu-performance-optimizations-rbm-crbm.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Recently, we have published a paper about performance optimizations that may
interest you.&lt;/p&gt;
&lt;p&gt;The paper is &lt;a class="reference external" href="https://www.researchgate.net/publication/307908790_On_CPU_Performance_Optimization_of_Restricted_Boltzmann_Machine_and_Convolutional_RBM"&gt;On CPU Performance Optimizations for Restricted Boltzmann Machine and Convolutional RBM&lt;/a&gt;, published in the Proceedings of the Artificial Neural Networks and Pattern Recognition workshop (ANNPR-2016). I've presented this paper in Germany, at Ulm.&lt;/p&gt;
&lt;p&gt;Although most of the performance research going on is focused on GPU, there are
still of research laboratories that are only equipped with CPU and it remains
important to be as fast as possible on CPU. Moreover, this is something
I really like.&lt;/p&gt;
&lt;p&gt;For this publication, I have tried to make my Restricted Boltzmann Machine (RBM)
and Convolutional RBM (CRBM) implementations in my DLL library as fast as
possible.&lt;/p&gt;
&lt;p&gt;The first part of the article is about Restricted Boltzmann Machine (RBM) which
are a form of dense Artificial Neural Network (ANN). Their training is very
similar to that of the ANN with Gradient Descent. Four different network
configurations are being tested.&lt;/p&gt;
&lt;p&gt;First, mini-batch training is shown to be much faster than online training, even
when online training is performed in parallel. Once mini-batch training is used,
BLAS operations are used in order to get as much performance as possible on the
different operations, mainly the Matrix Matrix Multiplication with the use of
the GEMM operation from the Intel Math Kernel Library (MKL). Moreover, the
parallel version of the MKL is also used to get even more performance. When all
these optimizations are performed, speedups of 11 to 30 are obtained compared to
the online training, depending on the network configurations. This final version
is able  to perform one epoch of Contrastive Divergence in 4 to 15 seconds
depending on the network, for 60000 images.&lt;/p&gt;
&lt;p&gt;The second part of the article is about Convolutional Restricted Boltzmann
Machine (CRBM). This is almost the equivalent of a Convolutional Neural Network
(CNN). Again four different networks are evaluated.&lt;/p&gt;
&lt;p&gt;The main problem with CRBM is that there are no standard implementations of the
convolution operation that is really fast. Therefore, it is not possible to
simply use a BLAS library to make the computation as fast as possible. The first
optimization that was tried is to vectorize the convolutions. With this, the
speedups have been between 1.1 and 1.9 times faster. I'm not really satisfied
with these results since in fact per convolution the speedups are much better.
Moreover, I have since been able to obtain better speedups but the deadline was
too short to include them in this paper. I'll try to talk about these
improvements in more details on this blog. What is more interesting to to
parallellize the different convolutions since they are mostly independent. This
can bring a speedup of the amount of cores available on the machine. Since
convolutions are extremely memory hungry, virtual cores with Hyper Threading
generally does not help. An interesting optimization is to use a Matrix
Multiplication to compute several valid convolutions at once.  This can give an
additional speedup between 1.6 and 2.2 compared to the vectorized version. While
it is possible to use the FFT to reduce the full convolution as well, in our
experiment the images were not big enough for this to be interesting. The final
speedups are about 10 times faster with these optimizations.&lt;/p&gt;
&lt;p&gt;We have obtained pretty good and I'm happy we have been published. However, I'm
not very satisfied with these results since I've been able to get even faster
since this and when compared with other frameworks, DLL is actually quite
competitive. I'll try to publish something new in the future.&lt;/p&gt;
&lt;p&gt;If you want more information, you can have a look at the paper. If you want to
look at the code, you can have a look at my projects:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;Expression Templates Library (ETL)&lt;/a&gt;: For
the Matrix Multiplication and Convolutions&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/wichtounet/dll"&gt;Deep Learning Library (DLL)&lt;/a&gt;: For the RBM
and CRBM implementations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Don't hesitate to ask any questions if you want more information :)&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>CPU</category><category>crbm</category><category>dbn</category><category>deep learning</category><category>dll</category><category>etl</category><category>Performances</category><category>publications</category><category>rbm</category><guid>http://baptiste-wicht.com/posts/2017/02/publication-cpu-performance-optimizations-rbm-crbm.html</guid><pubDate>Tue, 07 Feb 2017 16:33:33 GMT</pubDate></item><item><title>Publications - Sudoku Recognition with Deep Belief Network</title><link>http://baptiste-wicht.com/posts/2017/01/publications-sudoku-recognition-with-deep-belief-network.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I recently realized that I never talked about my publications on this website...
I thought it was time to start. I'll start to write a few posts about my earlier
publications and then I'll try to write something for the new ones not too late.&lt;/p&gt;
&lt;p&gt;For the story, I'm currently a PHD student at the University of Fribourg, in
Switzerland. My PHD is about the use of Deep Learning technologies to
automatically extract features from images. I have developed my Deep Learning
Library (DLL) project for this thesis. We have published a few articles on the
various projects that we tackled during the thesis. I'll try to go in order.&lt;/p&gt;
&lt;p&gt;At the beginning of the thesis, I used Restricted Boltzmann Machine and Deep
Belief Network to perform digit recognition on images of Sudoku taken with
a phone camera. We published two papers on this subject.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.researchgate.net/publication/282303748_Camera-based_Sudoku_recognition_with_deep_belief_network"&gt;Camera-based Sudoku Recognition with Deep Belief Network&lt;/a&gt;, in the Proceedings of the International Conference on Soft Computing and Pattern Recognition (SOCPAR-2014)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.researchgate.net/publication/307545305_Mixed_handwritten_and_printed_digit_recognition_in_Sudoku_with_Convolutional_Deep_Belief_Network"&gt;Mixed Handwritten and printed digit recognition in Sudoku With Convolutional Deep Belief Network&lt;/a&gt;, in the Proceedings of the International Conference on Document Analysis and Recognition (ICDAR-2015)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Sudoku grid and digits are detected using standard image processing
techniques:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;The image is first converted to grayscale, then a median blur is applied to
remove noise and the image is binarized using Adapteive Thresholding&lt;/li&gt;
&lt;li&gt;The edges are detected using the Canny algorithm. From these, the lines are
detected using a Progressive Probabilistic Hough Transform&lt;/li&gt;
&lt;li&gt;Using a connected component analysis, the segments of lines are clustered
together to detect the Sudoku Grid&lt;/li&gt;
&lt;li&gt;The cells are then detected inside the grid using the inner lines and contour
detection is used to isolate the digits.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here is one of the original images from our dataset:&lt;/p&gt;
&lt;img alt="Original image from our dataset" class="align-center" src="http://baptiste-wicht.com/images/image124.jpg"&gt;
&lt;p&gt;Here are the detected characters from the previous image:&lt;/p&gt;
&lt;img alt="Detected digits from our application" class="align-center" src="http://baptiste-wicht.com/images/image124_char.jpg"&gt;
&lt;p&gt;Once all the digits have been found they are passed to a Deep Belief Network for
recognition. A Deep Belief Network is composed of several Restricted Boltzmann
Machines (RBM) that are stacked. The network is pretrained, by training each
RBM, in turn, with Contrastive Divergence. This algorithm basically trains each
RBM as an auto-encoder and learns a good feature representation of the inputs.
Once all the layers have been trained, the network can then be trained as
a regular neural network with Stochastic Gradient Descent.&lt;/p&gt;
&lt;p&gt;In the second paper, the images of Sudoku are containing both computer printed
and handwritten digits (the grid is already filled). The other difference is
that the second system used a Convolutional DBN instead of DBN. The difference
being that each layer is a Convolutional RBM. Such a model will learn a set of
small filters that will be applied to each position of the image.&lt;/p&gt;
&lt;p&gt;On the second version of the dataset, we have been able to achieve 99.14% of
recognition of the digits or 92.5% of fully-recognized grid  with the
Convolutional Network.&lt;/p&gt;
&lt;p&gt;You can find the &lt;a class="reference external" href="https://github.com/wichtounet/sudoku_recognizer"&gt;C++ implementation on Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want to have a look, I've updated the
&lt;a class="reference external" href="http://baptiste-wicht.com/stories/publications.html"&gt;list of my publications&lt;/a&gt;
on this website.&lt;/p&gt;
&lt;p&gt;If you want more details on this project, don't hesitate to ask here or on
Github, or read the paper :)
The next post about my publications will probably be about CPU performances!&lt;/p&gt;&lt;/div&gt;</description><category>deep learning</category><category>dll</category><category>Personal</category><category>publications</category><guid>http://baptiste-wicht.com/posts/2017/01/publications-sudoku-recognition-with-deep-belief-network.html</guid><pubDate>Fri, 20 Jan 2017 07:56:46 GMT</pubDate></item><item><title>Add Milight smart lights to my Domoticz home automation system</title><link>http://baptiste-wicht.com/posts/2017/01/add-milight-smart-lights-to-domoticz-home-automation-system.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Recently, &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2017/01/new-home-automation-system-with-domoticz.html#"&gt;I switched from my hand crafted home automation system to Domoticz&lt;/a&gt;.
This allows me to easily integrate new smart devices and remote controllable
peripherals without much effort. I plan to relate my effort in having fun
controlling my home :)&lt;/p&gt;
&lt;p&gt;I'm now able to control lights in two rooms with Domoticz. The most well-known
smart bulbs are the Philips Hue. However, they are stupidly expensive. There are
a lot of alternatives. I've ordered some Milight light bulbs and controller to
test with Domoticz. I didn't order a lot of them because I wanted to make sure
they would work with my system.
Milight system is working over Wifi. There are several components to a Milight
system:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The LED Light Bulbs with Red/Green/Blue/White channels&lt;/li&gt;
&lt;li&gt;The Wifi Controller that is able to control 4 zones&lt;/li&gt;
&lt;li&gt;An RGBW Controller for LED strip&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first two are necessary for any installation, the third is to control an
RGBW LED strip. This list is not exhaustive, it's only the components that
I have used. It is important to note that a single Wifi controller can only
control 4 zones. There are also remotes, but I have not bought one since I plan
to use them only from Domoticz and maybe smartphone.&lt;/p&gt;
&lt;p&gt;The installation of the controller is relatively easy. You need to download the
Milight 2.0 application on the Android Play Store (or the equivalent for IOS).
Then, you can power on the Wifi Controller. It'll create a new Wifi on which you
can then connect on your phone. Then, you can use the application on your phone
to configure the device and make it connect to your home wireless network. Once,
this is done, you can connect your phone back to your home network. You can then
use the Milight application to configure your device. I highly recommend to set
a static IP address to the controller. The way I did it is simply to set a fixed
IP address on my DHCP server based on the MAC address of the MAC controller but
you can simply do the configuration in the application or in the web interface
of the controller (the user and password combination is admin:admin).&lt;/p&gt;
&lt;p&gt;Here is the look of the controller (next to my RFLink):&lt;/p&gt;
&lt;img alt="Milight wifi controller" class="align-center" src="http://baptiste-wicht.com/images/system.jpg"&gt;
&lt;p&gt;(My phone is starting to die, hence the very bad quality of images)&lt;/p&gt;
&lt;p&gt;You can then install your LED light bulbs. For, open first the remote on your
Android Milight application. Then plug the light bulb without power first. Then
power on the light and press once on one of the I buttons on one of the zones on
the remote. This will link the LED to the selected zone on the controller. You
can then control the light from your phone. Remember, only four zones and
therefore four lights per controller.&lt;/p&gt;
&lt;p&gt;The installation for a LED strip is not much complicated. You need to plug the
4 wires (or 5 wires if your have an actual RGBW LED) into the corresponding
inputs on the controller. Then, you power it and you can link it to a zone like
a normal light bulb!&lt;/p&gt;
&lt;img alt="LEDS in my Living Room controlled by Milight" class="align-center" src="http://baptiste-wicht.com/images/leds.jpg"&gt;
&lt;p&gt;It works really well and directly without problems.&lt;/p&gt;
&lt;p&gt;The last step is of course to configure your controller in Domoticz. It is
really easy to do. You need to add a new hardware of each of the Milight
controller. It is listed under the name "Limitless/AppLamp/Mi Light with
LAN/WiFi interface". You then can set the IP address and the port by default is
8899. Once you did configure the hardware, you'll see new devices appear in the
Devices list. There will one device for each zone and one device to control all
four zones at once. You can add the device you already configured as switches.
From the Switches interface you can turn the lamp on and off and you can&lt;/p&gt;
&lt;img alt="Domoticz Light Bulbs control" class="align-center" src="http://baptiste-wicht.com/images/domoticz_rgbw.png"&gt;
&lt;p&gt;You can then put them on your floor plan or control them from your rules.&lt;/p&gt;
&lt;p&gt;So far, I'm pretty satisfied with this Milight system. The Android application
is of poor quality but aside from this is pretty good and the price is very
fair. I'm also really satisfied with the Domoticz support. The only that is sad
is that the Domoticz Android application does not support RGBW control of the
lamps, only on and off, but that is already cool.&lt;/p&gt;
&lt;p&gt;Now that all of this is working well, I've ordered a few more light bulbs to
cover all my rooms and a few LED controller to control (and install first) the
other LEDS that I have in mind.&lt;/p&gt;
&lt;p&gt;On another note, I've also added a new outside temperature sensor outside my
apartment. It is a very cheap Chinese sensor, bought on Ebay, based on the
XT200 system that is working very well with RFLink.&lt;/p&gt;
&lt;p&gt;The next step in my system is probably to integrate Voice Control, but I don't
know exactly which way I'll go. I ordered a simple microphone that I intend to
plug on my spare Raspberry Pi, but I don't know if the range will be enough to
cover a room. Ideally, I would like to use an Amazon Dot, but they are not
available in Switzerland. I'll probably write more on the subject once I've
found an adequate solution. Another idea I've is to integrate support for ZWave
via OpenZWave and then add a few more cool sensors that I haven't found on an
cheaper system.&lt;/p&gt;
&lt;p&gt;I hope this is interesting and don't hesitate if you have any question about my
home automation project. You can expect a few more posts about this as soon as
In improve it :)&lt;/p&gt;&lt;/div&gt;</description><category>Home Automation</category><category>Personal</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/01/add-milight-smart-lights-to-domoticz-home-automation-system.html</guid><pubDate>Sun, 15 Jan 2017 21:00:25 GMT</pubDate></item><item><title>Vivaldi + Vimium = Finally no more Firefox!</title><link>http://baptiste-wicht.com/posts/2017/01/vivaldi-vimium-finally-no-more-firefox.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've been using the Pentadactyl Firefox extension for a long time. This
extensions "vimifies" Firefox and it does a very good job of it. This is
probably the best extension I have ever seen on any browser. This post is really
not against Pentadactyl, this is a great addon and it still works great.&lt;/p&gt;
&lt;p&gt;However, I have been more and more dissatisfied of Mozilla Firefox over the
years. Indeed, the browser is becoming slower and slower all the time and I'm
experiencing more and more issues on Gentoo with it. But the biggest problem
I've with Firefox right now is the philosophy of the developers that is really
crappy. Currently, there is only one thing that is good in Firefox compared to
the other browsers, its extensions. Basically, an extension in Firefox can do
almost anything. Pentadactyl is able to transform most of the interface and get
rid of all of the useless parts of the interface. It is currently impossible to
do so in other browsers. These powerful addons are using the XUL/XPCOM
programming interface to do so. Pentadactyl is the only reason I've kept to
Firefox so long. But Firefox has announced, already more than a year ago, that
it will deprecate its XUL/XPCOM interface in favour of webextensions. This means
that a lot of very good addons will not be able to work anymore once the
deprecation has been completed. Several writers of popular Firefox have
announced that they will not even try to port their addons and some addons will
simply not be possible anymore. This is the case for Pentadactyl which is on the
line for when the deprecation occurs. The data for deprecated has already been
delayed but is likely to come anyway.&lt;/p&gt;
&lt;p&gt;For several months, I've been looking at several possible replacements for my
current Pentadactyl browser. I've tried qutebrowser, but it is really too
limited in terms of features so far. I've also tried again Chromium which is
a great browser but unfortunately, there are very few possibilities for addons
to modify the interface. Vimium is a great addon for Chromium which is basically
the very much more lightweight alternative to Pentadactyl. It has much less
features, but most of the missing features are simply things that cannot be done
in Chromium.&lt;/p&gt;
&lt;p&gt;Only recently did I test Vivaldi. Vivaldi is a free multi-platform browser,
based on Chromium and supporting Chromium extensions. The major difference with
Chrome is how the UI is customizable, due to the use of a dynamic UI, stylable
with CSS. With the customizability of Vivaldi plus the great shortcuts and
vim-like behaviour of vimium, I really feel like I found a new Pentadactyl with
the advantage of not having to bear Firefox!&lt;/p&gt;
&lt;p&gt;Here is how it is looking with the follow URLs feature from vimium:&lt;/p&gt;
&lt;img alt="View of my Vivaldi browser" class="align-center" src="http://baptiste-wicht.com/images/vivaldi.png"&gt;
&lt;p&gt;Note: The gray bar on the left is the console to the left and the top kind of
bar is awesome wm, they are not part of the browser.&lt;/p&gt;
&lt;p&gt;I'm using the dark theme with native windows. I've disabled the address bar,
moved the tab bar to the bottom and completely hidden the side panel. All that
remained was the title bar and the scroll bar.&lt;/p&gt;
&lt;p&gt;To get rid of the title bar, you can use CSS. First, you have to only display
the Vivaldi button in the settings page. Then, you can use this custom CSS:&lt;/p&gt;
&lt;pre class="code CSS"&gt;&lt;a name="rest_code_0f18c7d313314e4d97f7f4d51e4e682c-1"&gt;&lt;/a&gt;&lt;span class="nt"&gt;button&lt;/span&gt;&lt;span class="nc"&gt;.vivaldi&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_0f18c7d313314e4d97f7f4d51e4e682c-2"&gt;&lt;/a&gt;    &lt;span class="nb"&gt;display&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;none&lt;/span&gt; &lt;span class="cp"&gt;!important&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_0f18c7d313314e4d97f7f4d51e4e682c-3"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_0f18c7d313314e4d97f7f4d51e4e682c-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_0f18c7d313314e4d97f7f4d51e4e682c-5"&gt;&lt;/a&gt;&lt;span class="nn"&gt;#header&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_0f18c7d313314e4d97f7f4d51e4e682c-6"&gt;&lt;/a&gt;    &lt;span class="nb"&gt;min-height&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="cp"&gt;!important&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_0f18c7d313314e4d97f7f4d51e4e682c-7"&gt;&lt;/a&gt;    &lt;span class="nb"&gt;z-index&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;auto&lt;/span&gt; &lt;span class="cp"&gt;!important&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_0f18c7d313314e4d97f7f4d51e4e682c-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_0f18c7d313314e4d97f7f4d51e4e682c-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_0f18c7d313314e4d97f7f4d51e4e682c-10"&gt;&lt;/a&gt;&lt;span class="nc"&gt;.button-toolbar.home&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nb"&gt;display&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;none&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;to hide the title completely! To get rid of the scroll bar, you need to use the
Stylish extension and use this custom CSS:&lt;/p&gt;
&lt;pre class="code CSS"&gt;&lt;a name="rest_code_98c819850c7e4cd4bae24ad6c8ab5110-1"&gt;&lt;/a&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;:-webkit-scrollbar&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;display&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nb"&gt;none&lt;/span&gt; &lt;span class="cp"&gt;!important&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;width&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;0px&lt;/span&gt;&lt;span class="p"&gt;;}&lt;/span&gt;
&lt;a name="rest_code_98c819850c7e4cd4bae24ad6c8ab5110-2"&gt;&lt;/a&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;:-webkit-scrollbar-button&lt;/span&gt;&lt;span class="o"&gt;,:&lt;/span&gt;&lt;span class="nd"&gt;:-webkit-scrollbar-track&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;display&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nb"&gt;none&lt;/span&gt; &lt;span class="cp"&gt;!important&lt;/span&gt;&lt;span class="p"&gt;;}&lt;/span&gt;
&lt;a name="rest_code_98c819850c7e4cd4bae24ad6c8ab5110-3"&gt;&lt;/a&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;:-webkit-scrollbar-thumb&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;display&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;none&lt;/span&gt; &lt;span class="cp"&gt;!important&lt;/span&gt;&lt;span class="p"&gt;;}&lt;/span&gt;
&lt;a name="rest_code_98c819850c7e4cd4bae24ad6c8ab5110-4"&gt;&lt;/a&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nd"&gt;:-webkit-scrollbar-track&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;display&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;none&lt;/span&gt; &lt;span class="cp"&gt;!important&lt;/span&gt;&lt;span class="p"&gt;;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;And then, no more scroll bar :)&lt;/p&gt;
&lt;p&gt;If you want to have full HTML5 video support, you need to install extra codecs.
On Gentoo, I've uploaded a ebuild on my overlay (wichtounet on layman) with the
name vivaldi-ffmpeg-codecs and everything should be working fine :)&lt;/p&gt;
&lt;p&gt;Vimium is clearly inferior to Pentadactyl in that for instance it only works in
web page, not in system page and you still have to use the browser for a few
things, but it does not seem too bar so far. Moreover, I wasn't using all the
features of Pentadactyl. I haven't been used this browser for a long time, so
maybe there are things that I will miss from Pentadactyl, but I won't certainly
miss Firefox!&lt;/p&gt;&lt;/div&gt;</description><category>Gentoo</category><category>Google</category><category>Personal</category><category>Web</category><guid>http://baptiste-wicht.com/posts/2017/01/vivaldi-vimium-finally-no-more-firefox.html</guid><pubDate>Sat, 14 Jan 2017 19:55:26 GMT</pubDate></item><item><title>New home automation system with Domoticz</title><link>http://baptiste-wicht.com/posts/2017/01/new-home-automation-system-with-domoticz.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;As you may remember, I started going into Home Automation last year with a &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/08/asgard-home-automation-project.html"&gt;C++ project, Asgard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Unfortunately, I haven't add the time to improve this project further and make
it really useful and I have so many C++ projects already, so I decided to switch
to using an existing project.&lt;/p&gt;
&lt;div class="section" id="choice"&gt;
&lt;h2&gt;Choice&lt;/h2&gt;
&lt;p&gt;I needed a system that was at least able to handle everything my existing system
does:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Control my RF-433 Temperature Sensors (Oregon Temperature Sensors)&lt;/li&gt;
&lt;li&gt;Control my RF-433 Remote Button&lt;/li&gt;
&lt;li&gt;Control my smart power outlets&lt;/li&gt;
&lt;li&gt;Ping and wake-on-lan my different computers&lt;/li&gt;
&lt;li&gt;Be installable on a Raspberry Pi&lt;/li&gt;
&lt;li&gt;Control Skadi/Kodi Home Cinema&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Of course, it also had to be open source and in a language that I can write
without agonizing pain (no Ruby, no JavaScript, ideally no Python (I hate a lot
of languages...)).&lt;/p&gt;
&lt;p&gt;It's impressive the number of home automation projects that exist today. It's
really a hot topic, more than I though. After a few hours of research, I've
finally settled on Domoticz. It seemed that it could handle all my home
automation devices that I already had. Domoticz is an open source home
automation system. It's written mainly in C++, which is an advantage for me
since I'll be able to work inside it if necessary. It has a quite modern HTML5
interface. It has a lot of support for hardware and a lot of bridges for other
controllers as well. One of the reason I chose it is also the floor-planner
feature that looks really cool.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="installation"&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;First of all, I cleaned the hardware of my Raspberry Pi. I got rid of my
breadboard and the components on it. Then, I put the Raspberry Pi on a nicer box
that I had laying around. One problem with Domoticz is that it does not support
directly RF433 devices, but need a controller in the middle. At first,
I believed I could simply connect them through GPIO and lets Domoticz do the
rest, but it's not possible. I ordered the different pieces for a RFLink
controller at nodo-shops. It is running on a Arduino Mega and is connected to
the Pi. The very big advantage is that they handle a lot of protocols directly,
so you only have to read the serial port to get information on RF-433 events.&lt;/p&gt;
&lt;p&gt;Here is how the current system look:&lt;/p&gt;
&lt;img alt="Asgard automation system hardware" class="align-center" src="http://baptiste-wicht.com/images/home_automation_hardware.jpg"&gt;
&lt;p&gt;The black box is the Arduino Mega with RFLink software while the Transparent is
simply the Raspberry on which Domoticz is installed.&lt;/p&gt;
&lt;p&gt;As for the installation of Domoticz, it's really simple. They also have images of
Raspberry Pi directly, but since my Raspberry was already installed with
Raspbian, I installed Domoticz on it directly. A simple matter of curl and bash:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_12f8fd57ff2c43449795c44ffc4f88b9-1"&gt;&lt;/a&gt;sudo curl -L install.domoticz.com &lt;span class="p"&gt;|&lt;/span&gt; sudo bash
&lt;/pre&gt;&lt;p&gt;After a few questions, your installation should be complete. You can browse your
Domoticz installation at the IP of your Pi and at the given port.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="home-automation-with-domoticz"&gt;
&lt;h2&gt;Home Automation with Domoticz&lt;/h2&gt;
&lt;p&gt;There are several concepts in Domoticz that are important. At first, it seemed
a bit weird to me, but in the end it works really well and you get used to it
pretty fast.&lt;/p&gt;
&lt;p&gt;The first concept is the hardware. It is not necessary hardware directly, but
rather a driver to some piece of information of switches than a real hardware in
my opinion. For instance, the RFLink Gateway is considered hardware as well as
the Forecast weather API. Here is all the hardware I've configured:&lt;/p&gt;
&lt;img alt="Domoticz Hardware Configuration" class="align-center" src="http://baptiste-wicht.com/images/domoticz_hardware.png"&gt;
&lt;p&gt;I've got some sensors from the motherboard, the Kodi interaction, the driver to
ping the different computers, the Forecast weather with Dark Sky and Weather
Underground, the RFLink communication, the wake-on-lan and some dummies virtual
switches.&lt;/p&gt;
&lt;p&gt;On its own, an hardware is useless, but it can create devices that will be used
for home automation. A device is sub device of a particular driver. In some
cases, the devices will be created automatically. For instance, RF Link can
create devices as soon as they are detected. For wake-on-lan, you have to
configure MAC Addresses and they will be added as devices. And so on...&lt;/p&gt;
&lt;p&gt;There are a lot of types of devices, also called switches. Some devices can do
some actions, for instance the wake-on-lan devices, while others are information
based such as ping or temperature sensors detected from the RFLink. For
instance, here are my temperature sensors:&lt;/p&gt;
&lt;img alt="Domoticz Temperature Sensors" class="align-center" src="http://baptiste-wicht.com/images/domoticz_temperature.png"&gt;
&lt;p&gt;As for my hardware I already had, I haven't add too many issues with them. The
Oregon Temperature Sensors worked out of the box without any issues as well my
old RF433 Ninja Block temperature sensor. The smart power outlets have been
pretty easy to use as well. The most problem I've add was with the remote
buttons. It seems to many that it should be the easiest, but RFLink has a lot of
problem with them, they are detected several times and are highly unreliable.
Interestingly you can also change the icon of most device to a large variety of
existing devices so that they would look more realistic.&lt;/p&gt;
&lt;p&gt;Another feature I find pretty cool is the floor plan. You can create plans for
each floor of your house and then create rooms inside and finally attach devices
to each room and place inside the plan. Unfortunately, to draw the plan, you'll
have to use an external tool. I've used floorplanner for this, but you may use
any plan you have as long as you have an image.&lt;/p&gt;
&lt;p&gt;Here is my floor plan with the devices (the scale is terribly off :P) :&lt;/p&gt;
&lt;img alt="Domoticz Floor Plan" class="align-center" src="http://baptiste-wicht.com/images/domoticz_floorplan.png"&gt;
&lt;p&gt;With that you can directly see the status of each of your sensors in each room.
Moreover, you can also activate some devices directly from the floor plan as
well :)&lt;/p&gt;
&lt;p&gt;Normally, my Somfy blinds are supposed to work with the RFLink controller.
I have managed to control my blinds but only when the RFLink was within 50
centimeters of the Blinds, which is clearly not acceptable. I don't know from
where the problem comes. It may come from my Blinds controller inside the wall
or may come from bad antenna of the RFLink or from a bug in RFLink, so from now
I cannot control my blinds.&lt;/p&gt;
&lt;p&gt;I have several D-Link cameras at home. You can also add cameras to Domoticz, but
don't expect a lot of support on this side. It's pretty poor indeed. The only
useful you can do in Domoticz with Cameras is to take a screenshot from them.
You cannot control the Camera or even have a decent video stream and cannot do
motion detection directly. I hope that the Camera support will improve in the
future. One thing that would be good is to add a view of the cameras in the
floor plan, but it is not possible currently.&lt;/p&gt;
&lt;p&gt;What I did is to use Zoneminder to manage the cameras and send alert to Domoticz
when motion is detect with the REST API of Domoticz. Unfortunately that means
two systems to manage the Cameras. Zoneminder is a very good piece of software,
but one of the ugliest I've ever seen (even I can do a better web interface...)
and quite obscure to configure. But the numbers of cameras it can handle and the
capabilities to control the camera is very very powerful.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="events-and-actions"&gt;
&lt;h2&gt;Events and actions&lt;/h2&gt;
&lt;p&gt;No home automation system would be complete without an events and rules system.
In Domoticz, you have several ways to create rules.&lt;/p&gt;
&lt;p&gt;The first solution is to create group. A group is simply a set of switches that
are all activated together. This is pretty limited, but can be useful. The
second solution is a scene. A scene is activated by a device and can set several
switches to on or off after the activation. The problem with this is that you
have to listen for an action as activation, you cannot use an existing device or
a rule with a value. For me, these two possibilities are way too limited and
I don't really a reason to use them.&lt;/p&gt;
&lt;p&gt;The best solution is to script an handler. For that, there are two ways, either
you define visually your script with blocky, for instance:&lt;/p&gt;
&lt;img alt="Domoticz Blockly Rule" class="align-center" src="http://baptiste-wicht.com/images/domoticz_blockly.png"&gt;
&lt;p&gt;This seemed so cool that I wanted to do all my rules with that. Unfortunately,
it is really limited. The main limitation is that you cannot nest if blocks. In
fact the interface lets you do, but it doesn't work afterwards. The other
limitation is that it has no notion of time. For instance, it's very hard to
create an action if a computer is shutdown for more than 60 seconds. You can do
it, but you end up creating virtual devices which are turned off after some
delay and it gets really complicated really soon...&lt;/p&gt;
&lt;p&gt;In the end, I used Lua script directly. These scripts have the information about
the last values of each device and the last updated time as well. It's much
easier with that to create powerful rules especially when you want a notion of
time. There are plenty of examples on the Domoticz forum about very powerful
rules that you can create. Unfortunately, there are some limitations. For
instance, you cannot send several actions to the same device in the same script
Moreover, you also have to decode the values of the sensors yourself. For
instance, for a temperature and humidity sensor, you'll have to parse the values
and extract the one you need by hand.&lt;/p&gt;
&lt;p&gt;So far, I haven't created many rules, only four. The first rule is that with the
push of one remote button I can power on my smart socket and turn on one of my
computers in my office. Then, if both the computers on this smart sockets have
been off for more than 60 seconds, the power is shut off on the smart socket.
And I have done almost the same for my media center and TV in my living room.&lt;/p&gt;
&lt;p&gt;In the future, I'll probably only use the Lua scripting capability. In my
opinion, it's better to have all the scripts in the same language for
maintainability reasons. Once my futures devices arrive, I'll have a few more
rules to code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;To wrap up, I'd say that I'm pretty satisfied with my new home automation system
with Domoticz. It's not a perfect tool, but it has a lot of features and is
working quite well in general. I would recommend you to try it if you want
a good home automation system, fully featured and relatively easy to use.&lt;/p&gt;
&lt;p&gt;I've several more things planned for this system. I plan to test a real motion
sensor rather than rely only on the cameras which are focused on the doors and
windows. I also plan to add an outdoor temperature sensor. And more
interestingly, I'm going to try to integrate smart bulbs from Milight to be able
to control my lights from the system. These are the shot term projects that
I want to do, but I have many more ideas!&lt;/p&gt;
&lt;p&gt;I'm also sure that there are some features from Domoticz that I have overlooked
or not discovered.&lt;/p&gt;
&lt;p&gt;I'll probably write more posts on Home Automation on the coming months.&lt;/p&gt;
&lt;p&gt;For more information on Domoticz, you can consult
&lt;a class="reference external" href="http://domoticz.com/"&gt;the official Domoticz website&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>Home Automation</category><category>Personal</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2017/01/new-home-automation-system-with-domoticz.html</guid><pubDate>Sun, 08 Jan 2017 15:08:42 GMT</pubDate></item><item><title>PVS-Studio on C++ Library Review</title><link>http://baptiste-wicht.com/posts/2016/12/pvs-studio-on-cpp-library-review.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;PVS-Studio is a commercial static analyzer for C, C++ and C#. It works in both
Windows and Linux.&lt;/p&gt;
&lt;p&gt;It has been a long time since I wanted to test it on my projects. I contacted
The PVS-Studio team and they gave me a temporary license so that I can test the
tool and make a review.&lt;/p&gt;
&lt;p&gt;I tried the static analyzer on my Expression Templates Library (ETL) project.
This is a heavily-templated C++ library. I tried it on Linux of course.&lt;/p&gt;
&lt;div class="section" id="usage"&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;The installation is very simple, simply untar an archive and put the executables
in your PATH (or use absolute paths). There are also some deb and rpm packages
for some distributions. You need strace to make the analyzer work, it should be
available on any Linux platform.&lt;/p&gt;
&lt;p&gt;The usage of PVS-Studio on Linux should be straightforward. First, you can use the
analyzer directly with make and it will detect the invocations of the compiler.
For instance, here is the command I used for ETL:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_0f877ca773884ee58b7f0820e698fbae-1"&gt;&lt;/a&gt;pvs-studio-analyzer trace -- make -j5 debug/bin/etl_test
&lt;/pre&gt;&lt;p&gt;Note that you can use several threads without issues, which is really great.
There does not seem to be any slowdown at this stage, probably only collecting
compiler arguments.&lt;/p&gt;
&lt;p&gt;This first step creates a strace_out file that will be used by the next stage.&lt;/p&gt;
&lt;p&gt;Once, the compilation has been analyzed, you can generate the results with the
analyze function, for which you'll need a license. Here is what I did:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_c6156e68e04943b38bf39be447d43737-1"&gt;&lt;/a&gt;pvs-studio-analyzer analyze -l ~/pvs_studio/PVS-Studio.lic -j5
&lt;/pre&gt;&lt;p&gt;Unfortunately, this didn't work for me:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_5a49cc1f76a3411d9f395f497c700042-1"&gt;&lt;/a&gt;No compilation units found
&lt;a name="rest_code_5a49cc1f76a3411d9f395f497c700042-2"&gt;&lt;/a&gt;Analysis finished in 0:00:00.00
&lt;/pre&gt;&lt;p&gt;Apparently, it's not able to use the strace_out it generated itself...&lt;/p&gt;
&lt;p&gt;Another possibility is to use the compilation database from clang to use
PVS-Studio. So I generated my compile_commands.json file again (it was not up to
date...) with &lt;a class="reference external" href="https://github.com/rizsotto/Bear"&gt;Bear&lt;/a&gt;. And then, you only
need to run the analyze step:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_4142a43651ba4bd8a21b890a40ca1231-1"&gt;&lt;/a&gt;pvs-studio-analyzer analyze -l ~/pvs_studio/PVS-Studio.lic -j5
&lt;/pre&gt;&lt;p&gt;Make sure you have the same compiler configured than the one used to generate
the compilation database to avoid errors with compiler arguments.&lt;/p&gt;
&lt;p&gt;Unfortunately, this just printed a load of crap on my console:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_cc46d0c91c3245b9af49d93bbd1938d1-1"&gt;&lt;/a&gt;(L8Pu(]-'Lo8h&amp;gt;uo(_uv(uo2(-&amp;gt;'2h_u(uo2(uvU2K h&amp;gt;'o8a=}Lkk;x[G^%cuaa8acr[VS%
&lt;a name="rest_code_cc46d0c91c3245b9af49d93bbd1938d1-2"&gt;&lt;/a&gt;$ckUaoc8 c'8&amp;gt;_-o-8&amp;gt;U2cu/kau==-8&amp;gt;c-=cU2]Uf=c U2=u%c&amp;amp;kU__-&amp;gt;j}c@uvu2%cJ
&lt;a name="rest_code_cc46d0c91c3245b9af49d93bbd1938d1-3"&gt;&lt;/a&gt;(L8Pu(]-'Lo8h&amp;gt;uo(_uv(uo2(-&amp;gt;'2h_u(uo2(uvU2K h&amp;gt;'o8a=}Lkk;JVJ^%cuaa8acr[VS%
&lt;a name="rest_code_cc46d0c91c3245b9af49d93bbd1938d1-4"&gt;&lt;/a&gt;$ckUaoc8 c'8&amp;gt;_-o-8&amp;gt;U2cu/kau==-8&amp;gt;c-=cU2]Uf=c U2=u%c&amp;amp;kU__-&amp;gt;j}c@uvu2%cJ
&lt;a name="rest_code_cc46d0c91c3245b9af49d93bbd1938d1-5"&gt;&lt;/a&gt;(L8Pu(]-'Lo8h&amp;gt;uo(_uv(uo2(-&amp;gt;'2h_u(uo2(uvU2K h&amp;gt;'o8a=}Lkk;*[G^%cuaa8acr[VS%
&lt;a name="rest_code_cc46d0c91c3245b9af49d93bbd1938d1-6"&gt;&lt;/a&gt;$ckUaoc8 c'8&amp;gt;_-o-8&amp;gt;U2cu/kau==-8&amp;gt;c-=cU2]Uf=c U2=u%c&amp;amp;kU__-&amp;gt;j}c@uvu2%cJ
&lt;a name="rest_code_cc46d0c91c3245b9af49d93bbd1938d1-7"&gt;&lt;/a&gt;(L8Pu(]-'Lo8h&amp;gt;uo(_uv(uo2(-&amp;gt;'2h_u(uo2(uvU2K h&amp;gt;'o8a=}Lkk;b[b^%cuaa8acr[VS%
&lt;a name="rest_code_cc46d0c91c3245b9af49d93bbd1938d1-8"&gt;&lt;/a&gt;$ckUaoc8 c'8&amp;gt;_-o-8&amp;gt;U2cu/kau==-8&amp;gt;c-=cU2]Uf=c U2=u%c&amp;amp;kU__-&amp;gt;j}c@uvu2%cJ
&lt;a name="rest_code_cc46d0c91c3245b9af49d93bbd1938d1-9"&gt;&lt;/a&gt;(L8Pu(]-'Lo8h&amp;gt;uo(_uv(uo2(-&amp;gt;'2h_u(uo2(uvU2K h&amp;gt;'o8a=}Lkk;[[x^%cuaa8acr[VS%
&lt;a name="rest_code_cc46d0c91c3245b9af49d93bbd1938d1-10"&gt;&lt;/a&gt;$ckUaoc8 c'8&amp;gt;_-o-8&amp;gt;U2cu/kau==-8&amp;gt;c-=cU2]Uf=c U2=u%c&amp;amp;kU__-&amp;gt;j}c@uvu2%cJ
&lt;/pre&gt;&lt;p&gt;Pretty nice, isn't it ?&lt;/p&gt;
&lt;p&gt;Let's try again in a file:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_854a9e9de0c44011a009a0136a7518d0-1"&gt;&lt;/a&gt;pvs-studio-analyzer analyze -o results.log -l ~/pvs_studio/PVS-Studio.lic -j5
&lt;/pre&gt;&lt;p&gt;The time is quite reasonable for the analysis, it took much less time than the
compilation time. In total, it took 88 seconds to analyze all the files. It's
much faster than the clang static analyzer.&lt;/p&gt;
&lt;p&gt;This time it worked, but the log file is not readable, you need to convert it
again:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_de64e02638c2481a998ddd7c667b51ae-1"&gt;&lt;/a&gt;plog-converter -t errorfile -o errors results.log
&lt;/pre&gt;&lt;p&gt;And finally, you can read the results of the analysis in the errors file.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="results"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;Overall, PVS-Studio found 236 messages in the ETL library, I was expecting more.
I also wish there was an HTML report that include the source code as well as the
error message. I had to lookup at the code for each message (you could integrate
it in vim and then use the quickfix window to do that). There are some
visualization but in things like QtCreator or LibreOffice which I don't have nor
want on my computer.&lt;/p&gt;
&lt;p&gt;Let's look at the results. For each message, I'll include the message from
PVS-Studio and the code if it's relevant.&lt;/p&gt;
&lt;p&gt;The first is about using the comma:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_65e2e147a8164ed985e23f359f55680e-1"&gt;&lt;/a&gt;include/etl/traits.hpp:674:1: error: V521 Such expressions using the ',' operator are dangerous. Make sure the expression is correct.
&lt;a name="rest_code_65e2e147a8164ed985e23f359f55680e-2"&gt;&lt;/a&gt;include/etl/traits.hpp:674:1: error: V685 Consider inspecting the return statement. The expression contains a comma.
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_38d30dd60268474eb2dcccbcfe4fa6e0-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_38d30dd60268474eb2dcccbcfe4fa6e0-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;dimensions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;noexcept&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_38d30dd60268474eb2dcccbcfe4fa6e0-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;etl_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dimensions&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_38d30dd60268474eb2dcccbcfe4fa6e0-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Here I'm simply using the comma operand to ignore expr to avoid a warning. To
make this compile in C++11, you need to do it in one line otherwise it's not
a constexpr function. It's probably not perfect to use this construct, but there
is no problem here.&lt;/p&gt;
&lt;p&gt;There is a bunch of these, let's filter them, it remains 207 warnings. Let's
jump to the next one:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_0e5d0eee11504dcb9f4ec917bbf64242-1"&gt;&lt;/a&gt;include/etl/impl/blas/fft.hpp:29:1: error: V501 There are identical sub-expressions to the left and to the right of the '==' operator: (DFTI_SINGLE) == DFTI_SINGLE
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_072a12d0e7f545fc914be5a24216bbaa-1"&gt;&lt;/a&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;fft_kernel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;complex&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;*&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;complex&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;*&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_072a12d0e7f545fc914be5a24216bbaa-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;DFTI_DESCRIPTOR_HANDLE&lt;/span&gt; &lt;span class="n"&gt;descriptor&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_072a12d0e7f545fc914be5a24216bbaa-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_072a12d0e7f545fc914be5a24216bbaa-4"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;in_ptr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;const_cast&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="o"&gt;*&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;static_cast&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="o"&gt;*&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;in&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_072a12d0e7f545fc914be5a24216bbaa-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_072a12d0e7f545fc914be5a24216bbaa-6"&gt;&lt;/a&gt;    &lt;span class="n"&gt;DftiCreateDescriptor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;descriptor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DFTI_SINGLE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DFTI_COMPLEX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;//Specify size and precision&lt;/span&gt;
&lt;a name="rest_code_072a12d0e7f545fc914be5a24216bbaa-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;DftiSetValue&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;descriptor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DFTI_PLACEMENT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DFTI_NOT_INPLACE&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;         &lt;span class="c1"&gt;//Out of place FFT&lt;/span&gt;
&lt;a name="rest_code_072a12d0e7f545fc914be5a24216bbaa-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;DftiCommitDescriptor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;descriptor&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;                                   &lt;span class="c1"&gt;//Finalize the descriptor&lt;/span&gt;
&lt;a name="rest_code_072a12d0e7f545fc914be5a24216bbaa-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;DftiComputeForward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;descriptor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;in_ptr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;                        &lt;span class="c1"&gt;//Compute the Forward FFT&lt;/span&gt;
&lt;a name="rest_code_072a12d0e7f545fc914be5a24216bbaa-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;DftiFreeDescriptor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;descriptor&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;                                    &lt;span class="c1"&gt;//Free the descriptor&lt;/span&gt;
&lt;a name="rest_code_072a12d0e7f545fc914be5a24216bbaa-11"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Unfortunately, the error is inside the MKL library. Here, I really don't think
it's an issue. There is pack of them. I forgot to exclude non-ETL code from the
results. Once filter from all dependencies, 137 messages remain.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_05d62eb888bb480c97e7a1a35623b6ed-1"&gt;&lt;/a&gt;include/etl/eval_functors.hpp:157:1: warning: V560 A part of conditional expression is always false: !padding.
&lt;/pre&gt;&lt;p&gt;This is true, but not an issue since padding is a configuration constant that
enables the use of padding in vector and matrices. There was 27 of these at
different locations and with different configuration variables.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_a989b9a3362945b8a3905ea906dd3a61-1"&gt;&lt;/a&gt;include/etl/op/sub_view.hpp:161:1: note: V688 The 'i' function argument possesses the same name as one of the class members, which can result in a confusion.
&lt;/pre&gt;&lt;p&gt;This is again true, but not a bug in this particular case. It is still helpful and
I ended up changing these to avoid confusion. Again, there was a few of these.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_6aa20005c5704daa838e6498310f1058-1"&gt;&lt;/a&gt;etl/test/src/conv_multi_multi.cpp:23:1: error: V573 Uninitialized variable 'k' was used. The variable was used to initialize itself.
&lt;/pre&gt;&lt;p&gt;This one is in the test code:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_c10ca0a0e3944ca3a5e65825e307446f-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_c10ca0a0e3944ca3a5e65825e307446f-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_c10ca0a0e3944ca3a5e65825e307446f-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;C_ref&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv_2d_valid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt; &lt;span class="c1"&gt;// HERE&lt;/span&gt;
&lt;a name="rest_code_c10ca0a0e3944ca3a5e65825e307446f-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_c10ca0a0e3944ca3a5e65825e307446f-5"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I don't see any error, k is initialized correctly to zero in the first loop.
This is a &lt;strong&gt;false positive&lt;/strong&gt; for me. There were several of these in different
places. It seems to that the use of the operator() is confusing for PVS-Studio.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_09396412f4b6456a953b36f89b3acf4f-1"&gt;&lt;/a&gt;include/etl/traits.hpp:703:1: note: V659 Declarations of functions with 'rows' name differ in the 'const' keyword only, but the bodies of these functions have different composition. This is suspicious and can possibly be an error. Check lines: 693, 703.
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_670da214ce194cb790006982e866c097-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cpp_disable_if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;decay_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;is_fast&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_670da214ce194cb790006982e866c097-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//693&lt;/span&gt;
&lt;a name="rest_code_670da214ce194cb790006982e866c097-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;etl_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_670da214ce194cb790006982e866c097-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_670da214ce194cb790006982e866c097-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_670da214ce194cb790006982e866c097-6"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cpp_enable_if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;decay_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;is_fast&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_670da214ce194cb790006982e866c097-7"&gt;&lt;/a&gt;&lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;noexcept&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//703&lt;/span&gt;
&lt;a name="rest_code_670da214ce194cb790006982e866c097-8"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;etl_traits&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_670da214ce194cb790006982e866c097-9"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Unfortunately, this is again a &lt;strong&gt;false positive&lt;/strong&gt; because PVS-Studio failed to
recognized SFINAE and therefore the warning is wrong.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_e2e67a6018434d6bb6a3ec0edabcee9e-1"&gt;&lt;/a&gt;include/etl/builder/expression_builder.hpp:345:1: note: V524 It is odd that the body of '&amp;gt;&amp;gt;=' function is fully equivalent to the body of '*=' function.
&lt;/pre&gt;&lt;p&gt;This one is interesting indeed. It is true that they are exactly because in ETL
&amp;gt;&amp;gt; is used for scalar element-wise multiplication. This is quite interesting that
PVS-Studio points that out. There was a few of these oddities but all were
normal in the library.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_da4a8294a57b47c095353bb7b5e02d38-1"&gt;&lt;/a&gt;etl/test/src/compare.cpp:23:1: error: V501 There are identical sub-expressions to the left and to the right of the '!=' operator: a != a
&lt;/pre&gt;&lt;p&gt;Again, it is nice that PVS-Studio finds that, but this is done on purpose on the
tests to compare an object to itself. If I remove all the oddities in the test
cases, there are only 17 left in the headers. None of the warnings on the test
case was serious, but there was no more false positives either, so that's great.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_99c4df94548a4f478bb6ea555f061578-1"&gt;&lt;/a&gt;include/etl/impl/vec/sum.hpp:92:1: error: V591 Non-void function should return a value.
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_a4430ea0a3db43b38f18eb5c8d37f0ac-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cpp_disable_if&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;vec_enabled&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;all_vectorizable&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;vector_mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_a4430ea0a3db43b38f18eb5c8d37f0ac-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;value_t&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_a4430ea0a3db43b38f18eb5c8d37f0ac-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_a4430ea0a3db43b38f18eb5c8d37f0ac-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_a4430ea0a3db43b38f18eb5c8d37f0ac-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unused&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_a4430ea0a3db43b38f18eb5c8d37f0ac-6"&gt;&lt;/a&gt;    &lt;span class="n"&gt;cpp_unreachable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"vec::sum called with invalid parameters"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_a4430ea0a3db43b38f18eb5c8d37f0ac-7"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This one is interesting. It's not a false positive since indeed the function
does not return a value, but there is a __builtin_unreachable() inside the
function and it cannot be called. In my opinion, the static analyzer should be
able to handle that, but this is really a corner case.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_bbf342a9ab5a4fdea9321329759eb0d9-1"&gt;&lt;/a&gt;include/etl/sparse.hpp:148:1: note: V550 An odd precise comparison: a == 0.0. It's probably better to use a comparison with defined precision: fabs(A - B) &amp;lt; Epsilon.
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_08fc07b4aa444c86860c25d58d8afcb4-1"&gt;&lt;/a&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="nf"&gt;is_zero&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_08fc07b4aa444c86860c25d58d8afcb4-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_08fc07b4aa444c86860c25d58d8afcb4-3"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is not false, but again this is intended because of the comparison to zero
for a sparse matrix. There were 10 of these in the same class.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_f480505ac131457ea6b2b0a159d61caf-1"&gt;&lt;/a&gt;include/etl/impl/blas/fft.hpp:562:1: note: V656 Variables 'a_padded', 'b_padded' are initialized through the call to the same function. It's probably an error or un-optimized code. Consider inspecting the 'etl::size(c)' expression. Check lines: 561, 562.
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_27954fe29563415e83b1528254c587ca-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;dyn_vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;complex&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a_padded&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_27954fe29563415e83b1528254c587ca-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;dyn_vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;complex&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b_padded&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;It's indeed constructed with the same size, but for me I don't think it's an
odd pattern. I would not consider that as a warning, especially since it's
a constructor and not a assignment.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_d8678f88dcd847d38c5bc388430011a1-1"&gt;&lt;/a&gt;include/etl/dyn_base.hpp:312:1: warning: V690 The 'dense_dyn_base' class implements a copy constructor, but lacks the '=' operator. It is dangerous to use such a class.
&lt;/pre&gt;&lt;p&gt;This is again a kind of corner case in the library because it's a base class
and the assignment is different between the sub classes and not a real
assignment in the C++ sense.&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_36185ca299e54bb5926abdf7835c33b1-1"&gt;&lt;/a&gt;include/etl/impl/reduc/conv_multi.hpp:657:1: warning: V711 It is dangerous to create a local variable within a loop with a same name as a variable controlling this loop.
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_b599b76418a2439e91a5672a640b890d-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b599b76418a2439e91a5672a640b890d-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b599b76418a2439e91a5672a640b890d-3"&gt;&lt;/a&gt;        &lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv_temp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_b599b76418a2439e91a5672a640b890d-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b599b76418a2439e91a5672a640b890d-5"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is again a false positive... It really seems that PVS-Studio is not able to
handle the operator().&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_f7a4da08181947069ce12a3ad7813279-1"&gt;&lt;/a&gt;include/etl/impl/pooling.hpp:396:1: error: V501 There are identical sub-expressions to the left and to the right of the '||' operator: P1 || P2 || P1
&lt;/pre&gt;&lt;pre class="code cpp"&gt;&lt;a name="rest_code_b5cf2e1981b04fcd8d729cd61c418e60-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;C1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;C2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;C3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;S1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;S2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;S3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;P1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;P2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;P3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_b5cf2e1981b04fcd8d729cd61c418e60-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b5cf2e1981b04fcd8d729cd61c418e60-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;o1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;C1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;P1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;S1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b5cf2e1981b04fcd8d729cd61c418e60-4"&gt;&lt;/a&gt;    &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;o2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;C2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;P2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;S2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b5cf2e1981b04fcd8d729cd61c418e60-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;o3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;C3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;P3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;S3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b5cf2e1981b04fcd8d729cd61c418e60-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_b5cf2e1981b04fcd8d729cd61c418e60-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;P1&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;P2&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;P1&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Last but not least, this time, it's entirely true and it's in fact a bug in my
code! The condition should be written like this:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_7ab745b4816149b6b0b75d4e5b83eff2-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;P1&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;P2&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;P3&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is now fixed in the master of ETL.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The installation was pretty easy, but the usage was not as easy as it could
because the first method by analyzing the build system did not work.
Fortunately, the system supports using the Clang compilation database directly
and therefore it was possible to use.&lt;/p&gt;
&lt;p&gt;Overall, it found 236 warnings on my code base (heavily templated library).
Around 50 of them were in some of the extend libraries, but I forgot to filter
them out. The quality of the results is pretty good in my opinion. It was able
to &lt;strong&gt;find a bug&lt;/strong&gt; in my implementation of pooling with padding. Unfortunately,
there was quite a few false positives, due to SFINAE, bad handling of the
operator() and no handling of __builtin_unreachable. The remaining were all
correct, but were not bug considering their usages.&lt;/p&gt;
&lt;p&gt;To conclude, I think it's a great static analyzer that is really fast compared
to other one in the market. There are a few false positives, but it's really not
bad compared to other tools and some of the messages are really great. An HTML
report including the source code would be great as well.&lt;/p&gt;
&lt;p&gt;If you want more information, you can consult
&lt;a class="reference external" href="http://www.viva64.com/en/pvs-studio/"&gt;the official site&lt;/a&gt;. There is even a way
to use it on open-source code for free, but you have to add comments on top of
each of your files.&lt;/p&gt;
&lt;p&gt;I hope it was helpful ;)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>C++14</category><category>Review</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2016/12/pvs-studio-on-cpp-library-review.html</guid><pubDate>Tue, 20 Dec 2016 08:40:12 GMT</pubDate></item><item><title>C++ Compiler benchmark on Expression Templates Library (ETL)</title><link>http://baptiste-wicht.com/posts/2016/12/cpp-compiler-benchmark-on-expression-templates-library-etl.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;script src="https://code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"&gt;&lt;/script&gt;
&lt;script src="https://code.highcharts.com/highcharts.js"&gt;&lt;/script&gt;
&lt;script src="https://code.highcharts.com/modules/exporting.js"&gt;&lt;/script&gt;&lt;p&gt;In my Expression Templates Library (ETL) project, I have a lot of template heavy
code that needs to run as fast as possible and that is quite intensive to
compile. In this post, I'm going to compare the performance of a few of the
kernels produced by different compilers. I've got GCC 5.4, GCC 6.20 and clang
3.9. I also included zapcc which is based on clang 4.0.&lt;/p&gt;
&lt;p&gt;These tests have been run on an Haswell processor. The automatic parallelization
of ETL has been turned off for these tests.&lt;/p&gt;
&lt;p&gt;Keep in mind that some of the diagrams are presented in logarithmic form.&lt;/p&gt;
&lt;div class="section" id="vector-multiplication"&gt;
&lt;h2&gt;Vector multiplication&lt;/h2&gt;
&lt;p&gt;The first kernel is a very simple one, simple element-wise multiplication of two
vectors. Nothing fancy here.&lt;/p&gt;
&lt;div id="mul_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('mul_container', {
        chart: { type: 'column' },
        title: { text: 'Element-wise Vector Multiplication' },
        xAxis: {
            categories: ['10', '100', '1000', '10000', '100000', '1000000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [0.021, 0.040, 0.215, 2.07, 32.1, 403]
        },
        {
            name: 'g++-6.2', data: [0.021, 0.037, 0.208, 2.17, 32.1, 376]
        },
        {
            name: 'clang-3.9', data: [0.027, 0.045, 0.243, 2.43, 32.7, 389]
        },
        {
            name: 'zapcc-4.0', data: [0.026, 0.047, 0.321, 2.5, 32.8, 411]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;For small vectors, clang is significantly slower than gcc-5.4 and gcc6.2. On
vectors from 100'000 elements, the speed is comparable for each compiler,
depending on the memory bandwidth. Overall, gcc-6.2 produces the fastest code
here. clang-4.0 is slightly slower than clang-3.9, but nothing dramatic.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="vector-exponentiation"&gt;
&lt;h2&gt;Vector exponentiation&lt;/h2&gt;
&lt;p&gt;The second kernel is computing the exponentials of each elements of a vector and
storing them in another vector.&lt;/p&gt;
&lt;div id="exp_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('exp_container', {
        chart: { type: 'column' },
        title: { text: 'Element-wise Vector Exponentiation' },
        xAxis: {
            categories: ['10', '100', '1000', '10000', '100000', '1000000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [0.0478, 0.137, 1.12, 9.79, 97.5, 959]
        },
        {
            name: 'g++-6.2', data: [0.0474, 0.132, 1.11, 9.71, 97, 1000]
        },
        {
            name: 'clang-3.9', data: [0.0492, 0.136, 0.959, 9.24, 92.9, 914]
        },
        {
            name: 'zapcc-4.0', data: [0.0488, 0.142, 0.952, 9.25, 91.9, 915]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;Interestingly, this time, clang versions are significantly faster for medium to
large vectors, from 1000 elements and higher, by about 5%. There is no
significant differences between the different versions of each compiler.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="matrix-matrix-multiplication"&gt;
&lt;h2&gt;Matrix-Matrix Multiplication&lt;/h2&gt;
&lt;p&gt;The next kernel I did benchmark with the matrix-matrix multiplication operation.
In that case, the kernel is hand-unrolled and vectorized.&lt;/p&gt;
&lt;div id="gemm_container_small" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;div id="gemm_container_large" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('gemm_container_small', {
        chart: { type: 'column' },
        title: { text: 'Matrix Matrix Multiplication (small)', },
        xAxis: {
            categories: ['10x10', '20x20', '40x40', '60x60', '80x80', '100x100']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [0.159, 0.815, 2.637, 13.849, 17.281, 78.903]
        },
        {
            name: 'g++-6.2', data: [0.162, 0.802, 2.431, 13.531, 17.274, 74.02]
        },
        {
            name: 'clang-3.9', data: [0.179, 1.218, 2.391, 14.981, 15.142, 61.548]
        },
        {
            name: 'zapcc-4.0', data: [0.159, 0.836, 2.712, 13.426, 15.114, 62.241]
        }
        ]
    });
    Highcharts.chart('gemm_container_large', {
        chart: { type: 'column' },
        title: { text: 'Matrix Matrix Multiplication (large)', },
        xAxis: {
            categories: ['200x200', '300x300', '400x400', '500x500', '600x600', '700x700', '800x800', '900x900', '1000x1000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [275.219, 1371, 1837, 5177, 6667, 14981, 17037, 31492, 32813]
        },
        {
            name: 'g++-6.2', data: [267.776, 1362, 1808, 5297, 6859, 15166, 15664, 30666, 33067]
        },
        {
            name: 'clang-3.9', data: [266.033, 1230, 1789, 4825, 6969, 14488, 15916, 30872, 33186]
        },
        {
            name: 'zapcc-4.0', data: [267.806, 1237, 1820, 4909, 7035, 15191, 18193, 33127, 37346]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;There are few differences between the compilers. The first thing is that for
some sizes such as 80x80 and 100x100, clang is significantly faster than GCC, by
more than 10%. The other interesting fact is that for large matrices
zapcc-clang-4.0 is always slower than clang-3.9 which is itself on par with the
two GCC versions. In my opinion, it comes from a regression in clang trunk but
it could also come from zapcc itself.&lt;/p&gt;
&lt;div id="std_gemm_container_large" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('std_gemm_container_large', {
        chart: { type: 'column' },
        title: { text: 'Matrix Matrix Multiplication (naive)', },
        xAxis: {
            categories: ['200x200', '300x300', '400x400', '500x500', '600x600', '700x700', '800x800', '900x900', '1000x1000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [1.195, 4.891, 10.467, 22.400, 33.399,
            58.401, 77.150, 121.392, 148.469]
        },
        {
            name: 'g++-6.2', data: [1.109, 4.540, 9.964, 21.359, 31.904,
            55.282, 72.690, 113.52, 143.27]
        },
        {
            name: 'clang-3.9', data: [0.893, 3.710, 7.287, 16.244, 23.920,
            43.342, 56.771, 91.870, 112.309]
        },
        {
            name: 'zapcc-4.0', data: [5.088, 16.909, 39.632, 77.194, 133.15,
            214.539, 316.01, 447.715, 612.255]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;The results are much more interesting here! First, there is a huge regression in
clang-4.0 (or in zapcc for that matter). Indeed, it is up to 6 times slower than
clang-3.9. Moreover, the clang-3.9 is always significantly faster than gcc-6.2.
Finally, there is a small improvement in gcc-6.2 compared to gcc 5.4.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fast-fourrier-transform"&gt;
&lt;h2&gt;Fast-Fourrier Transform&lt;/h2&gt;
&lt;p&gt;The following kernel is the performance of a hand-crafted Fast-Fourrier
transform implementation.&lt;/p&gt;
&lt;div id="fft_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('fft_container', {
        chart: { type: 'column' },
        title: { text: 'Fast Fourrier Transform', },
        xAxis: {
            categories: ['100', '1000', '10000', '100000', '1000000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [2.640, 27.515, 308.239, 3427.4, 41695.9]
        },
        {
            name: 'g++-6.2', data: [2.578, 26.194, 298.97, 3348.82, 40783.8]
        },
        {
            name: 'clang-3.9', data: [3.047, 30.514, 333.403, 3569.36,43860.6]
        },
        {
            name: 'zapcc-4.0', data: [3.199,33.304,317.135,4025.18,48445.3]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;On this benchmark, gcc-6.2 is the clear winner. It is significantly faster
than clang-3.9 and clang-4.0. Moreover, gcc-6.2 is also faster than gcc-5.4.
On the contrary, clang-4.0 is significantly slower than clang-3.9 except on one
configuration (10000 elements).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="d-convolution"&gt;
&lt;h2&gt;1D Convolution&lt;/h2&gt;
&lt;p&gt;This kernel is about computing the 1D valid convolution of two vectors.&lt;/p&gt;
&lt;div id="conv1_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('conv1_container', {
        chart: { type: 'column' },
        title: { text: '1D convolution (optimized)', },
        xAxis: {
            categories: ['1000x500', '2000x1000', '3000x1500', '4000x2000',
            '5000x2500', '6000x3000', '7000x3500', '8000x4000', '9000x4500',
            '10000x5000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [11.710, 41.002, 91.201, 158.178,
            248.985, 353.695, 486.676, 634.53, 867.101, 1082.62]
        },
        {
            name: 'g++-6.2', data: [9.307, 40.921, 90.327, 158.734, 248.892,
            354.582, 488.38, 636.899, 869.637, 1084.86]
        },
        {
            name: 'clang-3.9', data: [13.404, 41.409, 95.094, 162.339,
            256.143, 362.34, 498.66, 651.352, 886.465, 1092.24]
        },
        {
            name: 'zapcc-4.0', data: [13.528, 40.886, 94.473, 159.917,
            252.992, 356.63, 493.653, 640.348, 872.282, 1091.36]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;While clang-4.0 is faster than clang-3.9, it is still slightly slower than both
gcc versions. On the GCC side, there is not a lot of difference except on the
1000x500 on which gcc-6.2 is 25% faster.&lt;/p&gt;
&lt;p&gt;And here are the results with the naive implementation:&lt;/p&gt;
&lt;div id="std_conv1_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('std_conv1_container', {
        chart: { type: 'column' },
        title: { text: '1D convolution (naive)', },
        xAxis: {
            categories: ['1000x500', '2000x1000', '3000x1500', '4000x2000',
            '5000x2500', '6000x3000', '7000x3500', '8000x4000', '9000x4500',
            '10000x5000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [0.350, 1.452, 3.260, 5.823, 9.116,
            13.155, 17.922, 23.438, 29.705, 36.683]
        },
        {
            name: 'g++-6.2', data: [0.350, 1.457, 3.262, 5.823, 9.120,
            13.152, 17.922, 23.436, 29.687, 36.665]
        },
        {
            name: 'clang-3.9', data: [0.216, 0.873, 1.974, 3.517, 5.501,
            7.921, 10.793, 14.11, 17.867, 22.068]
        },
        {
            name: 'zapcc-4.0', data: [0.215, 0.873, 1.972, 3.514, 5.501,
            7.928, 10.799, 14.11, 17.879, 22.065]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;Again, on the naive version, clang is much faster than GCC on the naive, by
about 65%. This is a really large speedup.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;2D Convolution&lt;/h2&gt;
&lt;p&gt;This next kernel is computing the 2D valid convolution of two matrices&lt;/p&gt;
&lt;div id="conv2_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('conv2_container', {
        chart: { type: 'column' },
        title: { text: '2D Convolution (optimized)', },
        xAxis: {
            categories: ['100x50', '105x50', '110x55', '115x55', '120x60',
            '125x60', '130x65', '135x65', '140x70']
        },
        yAxis: {
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [327.399, 367.389, 441.457, 576.021,
            762.268, 794, 994.06, 1261.71, 1360.57]
        },
        {
            name: 'g++-6.2', data: [327.764, 367.379, 441.993, 572.241,
            761.741, 784.605, 991.717, 1266.55, 1361.59]
        },
        {
            name: 'clang-3.9', data: [330.199, 364.253, 443.483, 580.676,
            763.772, 777.39, 1000.53, 1267.75, 1375.51]
        },
        {
            name: 'zapcc-4.0', data: [339.358, 364.756, 443.807, 575.917,
            761.248, 784.695, 992.29, 1265.04, 1367.33]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;There is no clear difference between the compilers in this code. Every compiler
here has up and down.&lt;/p&gt;
&lt;p&gt;Let's look at the naive implementation of the 2D convolution (units are
milliseconds here not microseconds):&lt;/p&gt;
&lt;div id="std_conv2_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('std_conv2_container', {
        chart: { type: 'column' },
        title: { text: '2D Convolution (naive)', },
        xAxis: {
            categories: ['100x50', '105x50', '110x55', '115x55', '120x60',
            '125x60', '130x65', '135x65', '140x70']
        },
        yAxis: {
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [9.501,11.458,13.888, 16.489, 19.634,
            22.898, 27.012, 31.246, 36.269]
        },
        {
            name: 'g++-6.2', data: [9.502, 11.464, 13.903, 16.484, 19.642,
            22.994, 27.004, 31.248, 36.26]
        },
        {
            name: 'clang-3.9', data: [5.880, 7.136, 8.610, 10.226, 12.164,
            14.247, 17.024, 19.577, 22.510]
        },
        {
            name: 'zapcc-4.0', data: [5.875, 7.091, 8.661, 10.241, 12.218,
            14.302, 16.777, 19.424, 22.472]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;This time the difference is very large! Indeed, clang versions are about 60%
faster than the GCC versions! This is really impressive. Even though this does
not comes close to the optimized. It seems the vectorizer of clang is much more
efficient than the one from GCC.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id2"&gt;
&lt;h2&gt;4D Convolution&lt;/h2&gt;
&lt;p&gt;The final kernel that I'm testing is the batched 4D convolutions that is used a
lot in Deep Learning. This is not really a 4D convolution, but a large number
of 2D convolutions applied on 4D tensors.&lt;/p&gt;
&lt;div id="conv4_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('conv4_container', {
        chart: { type: 'column' },
        title: { text: '4D Convolution', },
        xAxis: {
            categories: ['2x6x3x28x16', '2x6x3x28x16', '2x6x3x28x16',
            '2x6x3x28x16', '2x6x3x28x16', '2x6x3x28x16', '2x6x3x28x16',
            '2x6x3x28x16', '2x6x3x28x16']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [0.095, 0.402, 1.083, 2.237, 3.988,
            6.474, 9.985, 14.132, 19.539]
        },
        {
            name: 'g++-6.2', data: [0.089, 0.413, 1.081, 2.224, 3.990,
            6.462, 9.815, 14.118, 19.612]
        },
        {
            name: 'clang-3.9', data: [0.090, 0.416, 1.108, 2.277, 4.077,
            6.587, 10.024, 14.359, 20.006]
        },
        {
            name: 'zapcc-4.0', data: [0.088, 0.406, 1.080, 2.237, 3.987,
            6.484, 9.827, 14.130, 19.569]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;Again, there are very small differences between each version. The best versions
are the most recent versions of the compiler gcc-6.2 and clang-4.0 on a tie.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Overall, we can see two trends in these results. First, when working with
highly-optimized code, the choice of compiler will not make a huge difference.
On these kind of kernels, gcc-6.2 tend to perform faster than the other
compilers, but only by a very slight margin, except in some cases. On the other
hand, when working with naive implementations, clang versions really did perform
much better than GCC. The clang compiled versions of the 1D and 2D convolutions
are more than 60% faster than their GCC counter parts. This is really
impressive. Overall, clang-4.0 seems to have several performance regressions,
but since it's not still a work in progress, I would not be suprised if these
regressions are not present in the final version. Since the clang-4.0 version is
in fact the clang version used by zapcc, it's also possible that zapcc is
introducing new performance regressions.&lt;/p&gt;
&lt;p&gt;Overall, my advice would be to use GCC-6.2 (or 5.4) on hand-optimized kernels
and clang when you have mostly naive implementations. However, keep in mind that
at least for the example shown here, the naive version optimized by the compiler
never comes close to the highly-optimized version.&lt;/p&gt;
&lt;p&gt;As ever, takes this with a grain of salt, it's only been tested on one project
and one machine, you may obtain very different results on other projects and on
other processors.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>Compilers</category><category>gcc</category><category>Performance</category><category>templates</category><guid>http://baptiste-wicht.com/posts/2016/12/cpp-compiler-benchmark-on-expression-templates-library-etl.html</guid><pubDate>Sun, 11 Dec 2016 13:17:30 GMT</pubDate></item><item><title>zapcc C++ compilation speed against gcc 5.4 and clang 3.9</title><link>http://baptiste-wicht.com/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;A week ago, I compared the &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html"&gt;compilation time performance of zapcc against gcc-4.9.3 and clang-3.7&lt;/a&gt;. On debug builds, zapcc was about 2 times faster than gcc and 3 times faster than clang. In this post, I'm going to try some more recent compilers, namely gcc 5.4 and clang 3.9 on the same project. If you want more information on zapcc, read the previous posts, this post will concentrate on results.&lt;/p&gt;
&lt;p&gt;Again, I use my Expression Template Library
(&lt;a class="reference external" href="https://github.com/wichtounet/etl/"&gt;ETL&lt;/a&gt;). This is a purely header-only
library with lots of templates. I'm going to compile the full test cases.&lt;/p&gt;
&lt;p&gt;The results of the two articles are not directly comparable, since they were
obtained on two different computers. The one on which the present results are
done has a less powerful and only 16Go of RAM compared to the 32Go of RAM of my
build machine. Also take into account that that the present results were
obtained on a Desktop machine, there can be some perturbations from background
tasks.&lt;/p&gt;
&lt;p&gt;Just like on the previous results, it does not help using more threads than
physical cores, therefore, the results were only computed on up to 4 cores on
this machine.&lt;/p&gt;
&lt;p&gt;The link time is not taken into account on the results.&lt;/p&gt;
&lt;div class="section" id="debug-build"&gt;
&lt;h2&gt;Debug build&lt;/h2&gt;
&lt;p&gt;Let's start with the result of the debug build.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="55%"&gt;
&lt;col width="15%"&gt;
&lt;col width="15%"&gt;
&lt;col width="15%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;469s&lt;/td&gt;
&lt;td&gt;230s&lt;/td&gt;
&lt;td&gt;130s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9&lt;/td&gt;
&lt;td&gt;710s&lt;/td&gt;
&lt;td&gt;371s&lt;/td&gt;
&lt;td&gt;218s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;214s&lt;/td&gt;
&lt;td&gt;112s&lt;/td&gt;
&lt;td&gt;66s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;3.31&lt;/td&gt;
&lt;td&gt;3.31&lt;/td&gt;
&lt;td&gt;3.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC&lt;/td&gt;
&lt;td&gt;2.19&lt;/td&gt;
&lt;td&gt;2.05&lt;/td&gt;
&lt;td&gt;1.96&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The results are almost the same as the previous test. zapcc is 3.3 times faster
to compile than Clang and around 2 times faster than GCC. It seems that GCC 5.4
is a bit faster than GCC 4.9.3 while clang 3.9 is a bit slower than clang 3.7,
but nothing terribly significant.&lt;/p&gt;
&lt;p&gt;Overall, for debug builds, zapcc can bring a very significant improvement to
your compile times.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="release-build"&gt;
&lt;h2&gt;Release build&lt;/h2&gt;
&lt;p&gt;Let's see what is the status of Release builds. Since the results are comparable
between the numbers of threads, the results here are just for one thread.&lt;/p&gt;
&lt;p&gt;This is more time consuming since a lot of optimizations are enabled and more
features from ETL are enabled as well.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="79%"&gt;
&lt;col width="21%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;782s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9&lt;/td&gt;
&lt;td&gt;960s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;640s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC&lt;/td&gt;
&lt;td&gt;1.22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;On a release build, the speedups are much less interesting. Nevertheless, they
are still significant. zapcc is still 1.2 times faster than gcc and 1.5 times
faster than clang. Then speedup against clang 3.9 is significantly higher than
it was on my experiment with clang 3.7, it's possible that clang 3.9 is slower
or simply has new optimization passes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The previous conclusion still holds with modern version of compilers: zapcc is
much faster than other compilers on Debug builds of template heavy code. More
than 3 times faster than clang-3.9 and about 2 times faster than gcc-5.4. Since
it's based on clang, there should not be any issue compiling projects that
already compile with a recent clang. Even though the speedups are less
interesting on a release build, it is still significantly, especially compared
against clang.&lt;/p&gt;
&lt;p&gt;I'm really interested in finding out what will be the pricing for zapcc once
out of the beta or if they will be able to get even faster!&lt;/p&gt;
&lt;p&gt;For the comparison with gcc 4.9.3 and clang 3.7, you can have a look at
&lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html"&gt;this article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want more information about zapcc, you can go to the
&lt;a class="reference external" href="https://www.zapcc.com/"&gt;official website of zapcc&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>compiler</category><category>etl</category><category>gcc</category><category>meta</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html</guid><pubDate>Mon, 05 Dec 2016 17:46:09 GMT</pubDate></item><item><title>New design: Faster and mobile compatible</title><link>http://baptiste-wicht.com/posts/2016/11/new-design-faster-and-mobile-compatible.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've finally taken the time to improve the design of the website!&lt;/p&gt;
&lt;p&gt;The site was becoming slower and slower, the design was not responsive at all
and was an horror on mobile.&lt;/p&gt;
&lt;p&gt;I've changed the design to focus more on content and removed superfluous things
such as the Google profile or slow things such as the 3D tag cloud. Moreover,
the design is now responsive again. It was a matter of removing a lot of bad
things I did in the CSS. Instead of having a vertical and an horizontal bars,
I now have only one vertical bar with both the navigation and a bit more
information. With these changes, the design is now also working on mobile phone!
It's about time.&lt;/p&gt;
&lt;p&gt;Moreover, I've also spent quite some time working on the speed of the website.
For this, I've bundled most of the JS and CSS files together and reduced them.
Moreover, the static files are now hosted and cached by CloudFlare. I've also
removed the 3D tag cloud which was quite slow. The Google API usage for the
Google profile badge were also quite slow. Overall, the index page is now really
fast. The article pages are also much faster but it's not perfect, especially
because of Disqus that does tons of requests and redirects everywhere. I've also
got rid of the Disqus ads which were really insignificant in the end. It may
take a while for the ads to disappear according to Disqus.&lt;/p&gt;
&lt;p&gt;I know that it's still not perfect, but I hope that user experience on the blog
is now improved for all readers and now article can be read on mobile normally.
I'll try to continue monitoring the speed and usability of the website to see if
I can improve it further in the coming days.&lt;/p&gt;
&lt;p&gt;If you have any issue on the updated website, don't hesitate to let me know
either by commenting on this post or sending me an email (check the Contact
page).&lt;/p&gt;&lt;/div&gt;</description><category>Performance</category><category>The site</category><category>Web</category><guid>http://baptiste-wicht.com/posts/2016/11/new-design-faster-and-mobile-compatible.html</guid><pubDate>Mon, 28 Nov 2016 06:55:48 GMT</pubDate></item><item><title>zapcc - a faster C++ compiler</title><link>http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Update: For a comparison against more modern compiler versions, you can read: &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html"&gt;zapcc C++ compilation speed against gcc 5.4 and clang 3.9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I just joined the private beta program of zapcc. Zapcc is a c++ compiler, based
on Clang which aims at being much faster than other C++ compilers. How they are
doing this is using a caching server that saves some of the compiler structures,
which should speed up compilation a lot. The private beta is free, but once the
compiler is ready, it will be a commercial compiler.&lt;/p&gt;
&lt;p&gt;Every C++ developer knows that compilation time can quickly be an issue when
programs are getting very big and especially when working with template-heavy
code.&lt;/p&gt;
&lt;p&gt;To benchmark this new compiler, I use my Expression Template Library
(&lt;a class="reference external" href="https://github.com/wichtounet/etl/"&gt;ETL&lt;/a&gt;). This is a purely header-only
library with lots of templates. There are lots of test cases which is what I'm
going to compile. I'm going to compare against Clang-3.7 and gcc-4.9.3.&lt;/p&gt;
&lt;p&gt;I have configured zapcc to let is use 2Go RAM per caching server, which is the
maximum allowed. Moreover, I killed the servers before each tests.&lt;/p&gt;
&lt;div class="section" id="debug-build"&gt;
&lt;h2&gt;Debug build&lt;/h2&gt;
&lt;p&gt;Let's start with a debug build. In that configuration, there is no optimization
going on and several of the features of the library (GPU, BLAS, ...) are
disabled. This is the fastest way to compile ETL. I gathered this result on
a 4 core, 8 threads, Intel processor, with an SSD.&lt;/p&gt;
&lt;p&gt;The following table presents the results with different number of threads and
the difference of zapcc compared to the other compilers:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="42%"&gt;
&lt;col width="11%"&gt;
&lt;col width="13%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;th class="head"&gt;-j6&lt;/th&gt;
&lt;th class="head"&gt;-j8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.3&lt;/td&gt;
&lt;td&gt;350s&lt;/td&gt;
&lt;td&gt;185s&lt;/td&gt;
&lt;td&gt;104s&lt;/td&gt;
&lt;td&gt;94s&lt;/td&gt;
&lt;td&gt;91s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.7&lt;/td&gt;
&lt;td&gt;513s&lt;/td&gt;
&lt;td&gt;271s&lt;/td&gt;
&lt;td&gt;153s&lt;/td&gt;
&lt;td&gt;145s&lt;/td&gt;
&lt;td&gt;138s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;158s&lt;/td&gt;
&lt;td&gt;87s&lt;/td&gt;
&lt;td&gt;47s&lt;/td&gt;
&lt;td&gt;44s&lt;/td&gt;
&lt;td&gt;42s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;3.24&lt;/td&gt;
&lt;td&gt;3.103&lt;/td&gt;
&lt;td&gt;3.25&lt;/td&gt;
&lt;td&gt;3.29&lt;/td&gt;
&lt;td&gt;3.28&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC&lt;/td&gt;
&lt;td&gt;2.21&lt;/td&gt;
&lt;td&gt;2.12&lt;/td&gt;
&lt;td&gt;2.21&lt;/td&gt;
&lt;td&gt;2.13&lt;/td&gt;
&lt;td&gt;2.16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The result is pretty clear! zapcc is around &lt;strong&gt;three times faster than Clang&lt;/strong&gt; and around
&lt;strong&gt;two times faster than GCC&lt;/strong&gt;. This is pretty impressive!&lt;/p&gt;
&lt;p&gt;For those that think than Clang is always faster than GCC, keep in mind that
this is not the case for template-heavy code such as this library. In all my
tests, Clang has always been slower and much memory hungrier than GCC on
template-heavy C++ code. And sometimes the difference is very significant.&lt;/p&gt;
&lt;p&gt;Interestingly, we can also see that going past the physical cores is not really
interesting on this computer. On some computer, the speedups are interesting,
but not on this one. Always benchmark!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="release-build"&gt;
&lt;h2&gt;Release build&lt;/h2&gt;
&lt;p&gt;We have seen the results on a debug build, let's now compare on something a bit
more timely, a release build with all options of ETL enabled (GPU, BLAS, ...),
which should make it significantly longer to compile.&lt;/p&gt;
&lt;p&gt;Again, the table:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="40%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;th class="head"&gt;-j6&lt;/th&gt;
&lt;th class="head"&gt;-j8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.3&lt;/td&gt;
&lt;td&gt;628s&lt;/td&gt;
&lt;td&gt;336s&lt;/td&gt;
&lt;td&gt;197s&lt;/td&gt;
&lt;td&gt;189s&lt;/td&gt;
&lt;td&gt;184s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.7&lt;/td&gt;
&lt;td&gt;663s&lt;/td&gt;
&lt;td&gt;388s&lt;/td&gt;
&lt;td&gt;215s&lt;/td&gt;
&lt;td&gt;212s&lt;/td&gt;
&lt;td&gt;205s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;515s&lt;/td&gt;
&lt;td&gt;281s&lt;/td&gt;
&lt;td&gt;173s&lt;/td&gt;
&lt;td&gt;168s&lt;/td&gt;
&lt;td&gt;158s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;1.28&lt;/td&gt;
&lt;td&gt;1.38&lt;/td&gt;
&lt;td&gt;1.24&lt;/td&gt;
&lt;td&gt;1.26&lt;/td&gt;
&lt;td&gt;1.29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC&lt;/td&gt;
&lt;td&gt;1.21&lt;/td&gt;
&lt;td&gt;1.30&lt;/td&gt;
&lt;td&gt;1.13&lt;/td&gt;
&lt;td&gt;1.12&lt;/td&gt;
&lt;td&gt;1.16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This time, we can see that the difference is much lower. Zapcc is &lt;strong&gt;between 1.2
and 1.4 times faster than Clang&lt;/strong&gt; and &lt;strong&gt;between 1.1 and 1.3 times faster than
GCC&lt;/strong&gt;. This shows that most of the speedups from zapcc are in the front end of
the compiler. This is not a lot but still significant over long builds,
especially if you have few threads where the absolute difference would be
higher.&lt;/p&gt;
&lt;p&gt;We can also observe that Clang is now almost on par with GCC which shows that
optimization is faster in Clang while front and backend is faster in gcc.&lt;/p&gt;
&lt;p&gt;You also have to keep in mind that zapcc memory usage is higher than Clang
because of all the caching. Moreover, the server are still up in between
compilations, so this memory usage stays between builds, which may not be what
you want.&lt;/p&gt;
&lt;p&gt;As for runtime, I have not seen any significant difference in performance
between the clang version and the zapcc. According to the official benchmarks
and documentation, there should not be any difference in that between zapcc and
the version of clang on which zapcc is based.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="incremental-build"&gt;
&lt;h2&gt;Incremental build&lt;/h2&gt;
&lt;p&gt;Normally, zapcc should shine at incremental building, but I was unable to show
any speedup when changing a single without killing the zapcc servers. Maybe
I did something wrong in my usage of zapcc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, we can see that zapcc is always faster than both GCC and Clang,
on my template-heavy library. Moreover, on debug builds, it is much faster than
any of the two compilers, being more than 2 times faster than GCC and more than
3 times faster than clang. This is really great. Moreover, I have not seen any
issue with the tool so far, it can seamlessly replace Clang without problem.&lt;/p&gt;
&lt;p&gt;It's a bit weird that you cannot allocate more than 2Go to the zapcc servers.&lt;/p&gt;
&lt;p&gt;For a program, that's really impressive. I hope that they are continuing the
good work and especially that this motivates other compilers to improve the
speed of compilation (especially of templates).&lt;/p&gt;
&lt;p&gt;If you want more information, you can go to the
&lt;a class="reference external" href="https://www.zapcc.com/"&gt;official website of zapcc&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>compiler</category><category>etl</category><category>gcc</category><category>projects</category><category>zapcc</category><guid>http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html</guid><pubDate>Sat, 26 Nov 2016 12:17:50 GMT</pubDate></item><item><title>Blazing fast unit test compilation with doctest 1.1</title><link>http://baptiste-wicht.com/posts/2016/09/blazing-fast-unit-test-compilation-with-doctest-11.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;You may remember &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/06/reduce-compilation-time-by-another-16-with-catch.html"&gt;my quest for faster compilation times&lt;/a&gt;. I had made several changes to the Catch test framework macros in order to save some compilation at the expense of my test code looking a bit less nice:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_dd6efd82be7245cc82ab1a2ea6e2ae35-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;//Before&lt;/span&gt;
&lt;a name="rest_code_dd6efd82be7245cc82ab1a2ea6e2ae35-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE_EQUALS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;//After&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The first line is a little bit better, but using several optimizations, I was
able to dramatically change the compilation time of the test cases of ETL. In
the end, I don't think that the difference between the two lines justifies the
high overhead in compilation times.&lt;/p&gt;
&lt;div class="section" id="doctest"&gt;
&lt;h2&gt;doctest&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/onqtam/doctest"&gt;doctest&lt;/a&gt; is a framework quite similar to
Catch but that claims to be much lighter. I tested doctest 1.0 early on, but at
this point it was actually slower than Catch and especially slower than my
versions of the macro.&lt;/p&gt;
&lt;p&gt;Today, doctest 1.1 was released with promises of being even lighter than before
and providing several new ways of speeding up compilation. If you want the
results directly, you can take a look at the next section.&lt;/p&gt;
&lt;p&gt;First of all, this new version improved the basic macros to make expression
decomposition faster. When you use the standard REQUIRE macro, the expression is
composed by using several template techniques and operator overloading. This is
really slow to compile. By removing the need for this decomposition, the fast
Catch macros are much faster to compile.&lt;/p&gt;
&lt;p&gt;Moreover, doctest 1.1 also introduces CHECK_EQ that does not any expression
decomposition. This is close to what I did in my macros expect that it is
directly integrated into the framework and preserves all its features. It is
also possible to bypass the expression checking code by using FAST_CHECK_EQ
macro. In that case, the exceptions are not captured. Finally, a new
configuration option is introduced (DOCTEST_CONFIG_SUPER_FAST_ASSERTS) that
removes some features related to automatic debugger breaks. Since I don't use
the debugger features and I don't need to capture exception everywhere (it's
sufficient for me that the test fails completely if an exception is thrown), I'm
more than eager to use these new features.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="results"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;For evaluation, I have compiled the complete test suite of ETL, with 1 thread,
using gcc 4.9.3 with various different options, starting from Catch to doctest
1.1 with all compilation time features. Here are the results, in seconds:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="29%"&gt;
&lt;col width="12%"&gt;
&lt;col width="14%"&gt;
&lt;col width="22%"&gt;
&lt;col width="23%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Version&lt;/th&gt;
&lt;th class="head"&gt;Time&lt;/th&gt;
&lt;th class="head"&gt;VS Catch&lt;/th&gt;
&lt;th class="head"&gt;VS Fast Catch&lt;/th&gt;
&lt;th class="head"&gt;VS doctest 1.0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Catch&lt;/td&gt;
&lt;td&gt;724.22&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Fast Catch&lt;/td&gt;
&lt;td&gt;464.52&lt;/td&gt;
&lt;td&gt;-36%&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;doctest 1.0&lt;/td&gt;
&lt;td&gt;871.54&lt;/td&gt;
&lt;td&gt;+20%&lt;/td&gt;
&lt;td&gt;+87%&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;doctest 1.1&lt;/td&gt;
&lt;td&gt;614.67&lt;/td&gt;
&lt;td&gt;-16%&lt;/td&gt;
&lt;td&gt;+32%&lt;/td&gt;
&lt;td&gt;-30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;REQUIRE_EQ&lt;/td&gt;
&lt;td&gt;493.97&lt;/td&gt;
&lt;td&gt;-32%&lt;/td&gt;
&lt;td&gt;+6%&lt;/td&gt;
&lt;td&gt;-43%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;FAST_REQUIRE_EQ&lt;/td&gt;
&lt;td&gt;439.09&lt;/td&gt;
&lt;td&gt;-39%&lt;/td&gt;
&lt;td&gt;-6%&lt;/td&gt;
&lt;td&gt;-50%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;SUPER_FAST_ASSERTS&lt;/td&gt;
&lt;td&gt;411.11&lt;/td&gt;
&lt;td&gt;-43%&lt;/td&gt;
&lt;td&gt;-12%&lt;/td&gt;
&lt;td&gt;-53%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can see, doctest 1.1 is much faster to compile than doctest 1.0! This is
really great news. Moreover, it is already 16% faster than Catch. When all the
features are used, doctest is 12% faster than my stripped down versions of Catch
macros (and 43% faster than Catch standard macros). This is really cool! It
means that I don't have to do any change in the code (no need to strip macros
myself) and I can gain a lot of compilation time compared to the bare Catch
framework.&lt;/p&gt;
&lt;p&gt;I really think the author of doctest did a great job with the new version.
Although this was not of as much interest for me, there are also a lot of
other changes in the new version. You can consult the
&lt;a class="reference external" href="https://github.com/onqtam/doctest/blob/master/CHANGELOG.md"&gt;changelog&lt;/a&gt; if you want more information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Overall, doctest 1.1 is much faster to compile than doctest 1.0. Moreover, it
offers very fast macros for test assertions that are much faster to compile
than Catch versions and even faster than the versions I created myself to reduce
compilation time. I really thing this is a great advance for doctest. When
compiling with all the optimizations, doctest 1.1 saves me 50 seconds in
compilation time compared to the fast version of Catch macro and more than
5 minutes compared to the standard version of Catch macros.&lt;/p&gt;
&lt;p&gt;I'll probably start using doctest on my development machine. For now, I'll keep
Catch as well since I need it to generate the unit test reports in XML format
for Sonarqube. Once this feature appears in doctest, I'll probably drop Catch
from ETL and DLL&lt;/p&gt;
&lt;p&gt;If you need blazing fast compilation times for your unit tests, doctest 1.1 is
probably the way to go.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>Catch</category><category>Compilers</category><category>doctest</category><category>etl</category><category>gcc</category><category>Performances</category><category>Tests</category><category>time</category><guid>http://baptiste-wicht.com/posts/2016/09/blazing-fast-unit-test-compilation-with-doctest-11.html</guid><pubDate>Wed, 21 Sep 2016 19:45:13 GMT</pubDate></item><item><title>Short review of Bullseye Coverage</title><link>http://baptiste-wicht.com/posts/2016/09/short-review-of-bullseye-coverage.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;a class="reference external" href="http://www.bullseye.com/"&gt;Bullseye&lt;/a&gt; is a commercial Code Coverage analyzer.
It is fully-featured with an export to HTML, to XML and even a specific GUI to
see the application.It costs about 800$, with a renewal fee of about 200$ per
year.&lt;/p&gt;
&lt;p&gt;I'm currently using gcov and passing the results to Sonar. This works well, but
there are several problems. First, I need to use gcovr to generate the XML file,
that means two tools. Then, gcov has no way to merge coverage reports. In my
tests of ETL, I have seven different profiles being tested and I need the
overall coverage report. lcov has a merge feature but it is slow as hell (it
takes longer to merge the coverage files than to compile and run the complete
test suite seven times...). For now, I'm using a C++ program that I wrote to
combine the XML files or a Python script that does that, but neither are perfect
and it needs maintenance. Finally, it's impossible to exclude some code from the
coverage report (there is code that isn't meant to be executed (exceptional
code)). For now, I'm using yet another C++ program  that I wrote to do this from
comments in code.&lt;/p&gt;
&lt;p&gt;Bullseye does have all these feature, so I got an evaluation license online and
tried this tool and wrote a short review of it.&lt;/p&gt;
&lt;div class="section" id="usage"&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;The usage is pretty simple. You put the coverage executables in your PATH
variable and activate coverage globally. Then, we you compile, the compiler
calls will be intercepted and a coverage file will be generated. When the
compilation is done, run the program and the coverage measurements will be
filled.&lt;/p&gt;
&lt;p&gt;The coverage results can then be exported to HTML (or XML) or visualized using
the CoverageBrowser tool:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Screenshot of Bullseye main coverage view" src="http://baptiste-wicht.com/images/bullseye_view.png"&gt;
&lt;p class="caption"&gt;The main view of the Bullseye tool code coverage results&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It's a pretty good view of the coverage result. You have a breakdown by folders,
by file, by function and finally by condition. You can view directly the source
code:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Screenshot of Bullseye source code coverage view" src="http://baptiste-wicht.com/images/bullseye_source_view.png"&gt;
&lt;p class="caption"&gt;The source view of the Bullseye tool code coverage results&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you want to exclude some code from your coverage reports, you can use
a pragma:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_b3f3b5c1073048e2941f876b0edd5b56-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;switch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b3f3b5c1073048e2941f876b0edd5b56-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;one&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b3f3b5c1073048e2941f876b0edd5b56-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;two&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b3f3b5c1073048e2941f876b0edd5b56-4"&gt;&lt;/a&gt;    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;three&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b3f3b5c1073048e2941f876b0edd5b56-5"&gt;&lt;/a&gt;    &lt;span class="cp"&gt;#pragma BullseyeCoverage off&lt;/span&gt;
&lt;a name="rest_code_b3f3b5c1073048e2941f876b0edd5b56-6"&gt;&lt;/a&gt;    &lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;abort&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_b3f3b5c1073048e2941f876b0edd5b56-7"&gt;&lt;/a&gt;    &lt;span class="cp"&gt;#pragma BullseyeCoverage on&lt;/span&gt;
&lt;a name="rest_code_b3f3b5c1073048e2941f876b0edd5b56-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;So that the condition won't be set as uncovered.&lt;/p&gt;
&lt;p&gt;As for the coverage, it's pretty straightforward. For example:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_670afcee1a924ec183b33c7ce062cb65-1"&gt;&lt;/a&gt;covmerge -c -ffinal.cov sse.cov avx.cov
&lt;/pre&gt;&lt;p&gt;and it's really fast. Unfortunately, the merging is only done at the function
level, not at the statement or at the condition level. This is a bit
disappointing, especially from a commercial tool. Nevertheless, it works well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;To conclude, Bullseye seems to be a pretty good tool. It has more features than
standard gcov coverage and all features are well integrated together. I have
only covered the features I was interested in, there are plenty of other things
you can look at on the &lt;a class="reference external" href="http://www.bullseye.com/"&gt;official website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, if you don't need the extra features such as the visualizer (or use
something like Sonar for this), or the merge or code excluding, it's probably
not worth paying the price for it. In my case, since the merge is not better
than my C++ tool (both do almost the same and my tool does some basic line
coverage merging as well) and I don't need the visualizer, I won't pay the price
for it. Moreover, they don't have student or open source licensing, therefore,
I'll continue with my complicated toolchain :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>coverage</category><category>test</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2016/09/short-review-of-bullseye-coverage.html</guid><pubDate>Fri, 16 Sep 2016 11:25:44 GMT</pubDate></item><item><title>Expression Templates Library (ETL) 1.0</title><link>http://baptiste-wicht.com/posts/2016/09/expression-templates-library-etl-10.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've just released the first official version of my Expression Templates Library
(ETL for short): The version 1.0.&lt;/p&gt;
&lt;p&gt;Until now, I was using a simple rolling release model, but I think it's now time
to switch to some basic versioning. The project is now at a stable state.&lt;/p&gt;
&lt;p&gt;ETL 1.0 has the following main features:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Smart Expression Templates&lt;/li&gt;
&lt;li&gt;Matrix and vector (runtime-sized and compile-time-sized)&lt;/li&gt;
&lt;li&gt;Simple element-wise operations&lt;/li&gt;
&lt;li&gt;Reductions (sum, mean, max, ...)&lt;/li&gt;
&lt;li&gt;Unary operations (sigmoid, log, exp, abs, ...)&lt;/li&gt;
&lt;li&gt;Matrix multiplication&lt;/li&gt;
&lt;li&gt;Convolution (1D and 2D and higher variations)&lt;/li&gt;
&lt;li&gt;Max Pooling&lt;/li&gt;
&lt;li&gt;Fast Fourrier Transform&lt;/li&gt;
&lt;li&gt;Use of SSE/AVX to speed up operations&lt;/li&gt;
&lt;li&gt;Use of BLAS/MKL/CUBLAS/CUFFT/CUDNN libraries to speed up operations&lt;/li&gt;
&lt;li&gt;Symmetric matrix adapter (experimental)&lt;/li&gt;
&lt;li&gt;Sparse matrix (experimental)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="examples"&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;Here is an example of expressions in ETL:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_bcd5d1f7b3054695a5195ef50cedf6f8-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;fast_matrix&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_bcd5d1f7b3054695a5195ef50cedf6f8-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;fast_matrix&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;4.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;4.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_bcd5d1f7b3054695a5195ef50cedf6f8-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;fast_matrix&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_bcd5d1f7b3054695a5195ef50cedf6f8-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_bcd5d1f7b3054695a5195ef50cedf6f8-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;fast_matrix&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;2.111&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Or another I'm using in my neural networks library:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_3984a3addd7f4c539cd4f2c2e425b347-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;In that case, the vector-matrix multiplication will be executed using a BLAS
kernel (if ETL is configured correclty) and the assignment, the sigmoid and the
addition will be automatically vectorized to use either AVX or SSE depending
on the machine.&lt;/p&gt;
&lt;p&gt;Or with a convolutional layer and a ReLU activation function:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_270c75d8c6ae4a5eb25a4e6b52330d64-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NH1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NH2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_4d_valid_flipped&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NC&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NV1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NV2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v_a&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_270c75d8c6ae4a5eb25a4e6b52330d64-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b_rep&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;h_a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This will automatically be computed either with NVIDIA CUDNN (if available) or
with optimized SSE/AVX kernels.&lt;/p&gt;
&lt;p&gt;For more information, you can take a look at the &lt;a class="reference external" href="https://github.com/wichtounet/etl/wiki"&gt;Reference&lt;/a&gt; on the wiki.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="next-version"&gt;
&lt;h2&gt;Next version&lt;/h2&gt;
&lt;p&gt;For the next version, I'll focus on several things:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Improve matrix-matrix multiplication kernels when BLAS is not available. There
is a lot of room for improvement here&lt;/li&gt;
&lt;li&gt;Complete support for symmetric matrices (currently experimental)&lt;/li&gt;
&lt;li&gt;Maybe some new adapters such as Hermitian matrices&lt;/li&gt;
&lt;li&gt;GPU improvements for some operations that can be done entirely on GPU&lt;/li&gt;
&lt;li&gt;New convolution performanceimprovements&lt;/li&gt;
&lt;li&gt;Perhaps more complete parallel support for some implementations&lt;/li&gt;
&lt;li&gt;Drop some compiler support to use full C++14 support&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="download-etl"&gt;
&lt;h2&gt;Download ETL&lt;/h2&gt;
&lt;p&gt;You can download ETL &lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;on Github&lt;/a&gt;. If you
only interested in the 1.0 version, you can look at the
&lt;a class="reference external" href="https://github.com/wichtounet/etl/releases"&gt;Releases pages&lt;/a&gt; or clone the tag
1.0. There are several branches:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;master&lt;/em&gt; Is the eternal development branch, may not always be stable&lt;/li&gt;
&lt;li&gt;&lt;em&gt;stable&lt;/em&gt; Is a branch always pointing to the last tag, no development here&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the future release, there always will tags pointing to the corresponding
commits. I'm not following the git flow way, I'd rather try to have a more
linear history with one eternal development branch, rather than an useless
develop branch or a load of other branches for releases.&lt;/p&gt;
&lt;p&gt;Don't hesitate to comment this post if you have any comment on this library or
any question. You can also open an Issue on Github if you have a problem using
this library or propose a Pull Request if you have any contribution you'd like
to make to the library.&lt;/p&gt;
&lt;p&gt;Hope this may be useful to some of you :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>Compilers</category><category>etl</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2016/09/expression-templates-library-etl-10.html</guid><pubDate>Fri, 02 Sep 2016 14:12:38 GMT</pubDate></item><item><title>Asgard: Home Automation project</title><link>http://baptiste-wicht.com/posts/2016/08/asgard-home-automation-project.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I have updated my asgard project to make it finally useful for me, so I figured
I'd present the project now.&lt;/p&gt;
&lt;p&gt;Asgard is my project of home automation based on a Raspberry Pi. I started this
project after Ninja Blocks kickstarter company went down and I was left with
useless sensors. So I figured why not have fun creating my own :P I know there
are some other projects out there that are pretty good, but I wanted to do some
more low level stuff for once, so what the hell.&lt;/p&gt;
&lt;p&gt;Of course, everything is written in C++, no surprise here. The project is built
upon a server / drivers architecture. The drivers and the server are talking via
network sockets, so they can be on different machines.  The server is displaying
the data it got on a web interface and also provide a way to trigger actions of
drivers either from the web interface or through the integrated rules engine.
The data are stored in a database, accessed with CPPSqlite3 (probably going to
be replaced by sqlcpp11) and the web server is handled with mongoose (with a c++
interface).&lt;/p&gt;
&lt;p&gt;I must mention that most of the web part of the project was made by a student of
mine, Stéphane Ly, who work on it as part of his study.&lt;/p&gt;
&lt;p&gt;Here is a picture of the Raspberry Pi system (not very pretty ;) ):&lt;/p&gt;
&lt;img alt="Asgard automation system hardware" class="align-center" src="http://baptiste-wicht.com/images/asgard_hardware.jpg"&gt;
&lt;p&gt;I plan to try to fit at least some of it on a nicer box with nicer cables and
such. Moreover, I also plan to add real antennas to the RF transmitter and
receiver, but I haven't received them so far.&lt;/p&gt;
&lt;div class="section" id="sensors"&gt;
&lt;h2&gt;Sensors&lt;/h2&gt;
&lt;p&gt;asgard support several sensors:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;DHT11 Temperature/Humdity Sensor&lt;/li&gt;
&lt;li&gt;WT450 Temperature/Humdity Sensor&lt;/li&gt;
&lt;li&gt;RF Button&lt;/li&gt;
&lt;li&gt;IR Remote&lt;/li&gt;
&lt;li&gt;CPU Temperature Sensor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can see the sensors data displayed on the web interface:&lt;/p&gt;
&lt;img alt="Asgard automation system home page" class="align-center" src="http://baptiste-wicht.com/images/asgard_home.png"&gt;
&lt;/div&gt;
&lt;div class="section" id="actions"&gt;
&lt;h2&gt;Actions&lt;/h2&gt;
&lt;p&gt;There are currently a few actions provided by the drivers:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Wake-On-Lan a computer by its MAC Address&lt;/li&gt;
&lt;li&gt;ITT-1500 smart plugs ON and OFF&lt;/li&gt;
&lt;li&gt;Kodi actions: Pause / Play / Next / Previous on Kodi&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are the rules engine:&lt;/p&gt;
&lt;img alt="Asgard automation system rules page" class="align-center" src="http://baptiste-wicht.com/images/asgard_rules.png"&gt;
&lt;/div&gt;
&lt;div class="section" id="my-home-automation"&gt;
&lt;h2&gt;My home automation&lt;/h2&gt;
&lt;p&gt;I'm currently using this system to monitor the temperature in my appartment.
Nothing great so far because I don't have enough sensors yet. And now, I'm also
using a wireless button to turn on my power socket, wait 2 seconds and then
power on my Kodi Home Theater with wake on lan.&lt;/p&gt;
&lt;p&gt;It's nothing fancy so far, but it's already better than what I had with Ninja
Blocks, except for the ugly hardware ;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="future"&gt;
&lt;h2&gt;Future&lt;/h2&gt;
&lt;p&gt;There are still tons of work on the project and on integration in my home.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;I'm really dissatisfied with the WT450 sensor, I've ordered new Oregon sensors to try to do better.&lt;/li&gt;
&lt;li&gt;I've ordered a few new sensors: Door intrusion detector and motion detector&lt;/li&gt;
&lt;li&gt;The rules system needs to be improve to support multiple conditions&lt;/li&gt;
&lt;li&gt;I plan to add a simple state system to the asgard server&lt;/li&gt;
&lt;li&gt;There are a lot of refactorings necessary in the code and&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, I don't know when I'll work on this again, my work on this project is
pretty episodic to say the least.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="code"&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The code is, as always, available on Github. There are multiple repositories:
&lt;a class="reference external" href="https://github.com/search?q=user%3Awichtounet+asgard"&gt;all asgard repositories&lt;/a&gt;.
It's not that much code for now, about 2000 lines of code, but some of it may be
useful. If you plan to use the system, keep in mind that it was never tested out
of my environment and that there is no documentation so far, but don't hesitate
to open Issues on Github if you have questions or post a comment here.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>asgard</category><category>Home Automation</category><category>Personal</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2016/08/asgard-home-automation-project.html</guid><pubDate>Sat, 27 Aug 2016 20:28:16 GMT</pubDate></item><item><title>Update: Thor, Thesis and Publications</title><link>http://baptiste-wicht.com/posts/2016/08/update-thor-thesis-and-publications.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Since it's been a real while since the last post I've written here, I wanted to
write a short status update.&lt;/p&gt;
&lt;p&gt;I had to serve one month in the army, which does not help at all for
productivity :P Since the update to Boost Spirit X3, I haven't worked on my
eddic compiler again, but I've switched back to my operating system project:
thor. I'm having a lot of fun with it again and it's in much better state than
before.&lt;/p&gt;
&lt;p&gt;We also have been very productive on the publication side, with four new
publications this year in various conferences. I'll update the blog when the
proceedings are published. I'll be going to ICANN 2016 and ANNPR 2016 next week
and probably to ICFHR in October. And of course, I'll go back to Meeting C++ in
November :) As for my thesis, it's finally going great, I've started writing
regularly and it's taking form!&lt;/p&gt;
&lt;div class="section" id="thor"&gt;
&lt;h2&gt;Thor&lt;/h2&gt;
&lt;p&gt;My project Thor Operating System now has much more features than before:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;64bit operating system&lt;/li&gt;
&lt;li&gt;Preemptive Multiprocessing&lt;/li&gt;
&lt;li&gt;Keyboard / Mouse driver&lt;/li&gt;
&lt;li&gt;Full ACPI support with ACPICA&lt;/li&gt;
&lt;li&gt;Read/Write ATA driver&lt;/li&gt;
&lt;li&gt;FAT32 file system support&lt;/li&gt;
&lt;li&gt;HPET/RTC/PIT drivers&lt;/li&gt;
&lt;li&gt;Basic PCI support&lt;/li&gt;
&lt;li&gt;Multi stage booting with FAT32&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since last time, I've fixed tons of bug in the system. Although there are still
some culprit, it's much more stable than before. They were a lot of bugs in the
scheduler with loads of race conditions. I hope I've working through most of
them now.&lt;/p&gt;
&lt;p&gt;I'm currently working on the network stack. I'm able to receive and send packets
using the Realtek 8139 card. I have working support for Ethernet, IP and ARP.
I'm currently working on adding ICMP support. I've come to realize that the
hardest part is not to develop the code here but to find a way to test it.
Network in Qemu is a huge pain in the ass to configure. And then, you need tools
to generate some packets or at least answer to packets send by the virtual
machine, and it's really bad... Nevertheless, it's pretty fun overall :)&lt;/p&gt;
&lt;p&gt;Aside from this, I'm also working on a window manager. I'll try to post an
update on this.&lt;/p&gt;
&lt;p&gt;You can take a look at the &lt;a class="reference external" href="https://github.com/wichtounet/thor-os"&gt;thor sources&lt;/a&gt; if you're interested.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="future"&gt;
&lt;h2&gt;Future&lt;/h2&gt;
&lt;p&gt;For the time being, I'll focus my effort on the thor project. I also have some
development to do on my home automation system: &lt;a class="reference external" href="https://github.com/wichtounet/asgard-server"&gt;asgard-server&lt;/a&gt; that I plan to finalize and deploy in a useful way this weekend in my apartment. You can also expect some updates on my deep learning library where I've started work to make it more user-friendly (kind of). I'm also still waiting on the first stable version of doctest for a new comparison with Catch.&lt;/p&gt;
&lt;p&gt;I really want to try to publish again some more posts on the blog. I'll
especially try to publish some more updates about Thor.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>deep learning</category><category>osdev</category><category>publications</category><category>thor</category><guid>http://baptiste-wicht.com/posts/2016/08/update-thor-thesis-and-publications.html</guid><pubDate>Tue, 23 Aug 2016 05:40:13 GMT</pubDate></item><item><title>eddic 1.2.4: New Boost Spirit X3 parser and minor cleanups</title><link>http://baptiste-wicht.com/posts/2016/06/eddic-124-new-boost-spirit-x3-parser-and-minor-cleanups.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;After almost 2 years, the new version of eddic (the compiler of the EDDI
programming language) is out! eddic 1.2.4&lt;/p&gt;
&lt;p&gt;I haven't worked a lot on this project in the last years, I have been busy with
my Ph.D. related projects (ETL and DLL), my operating systems, cpm, ... I've
mostly worked on the parser to test the new version of Boost Spirit: X3. This
will be described on the next section, with the other changes in the later
section.&lt;/p&gt;
&lt;div class="section" id="new-boost-spirit-x3"&gt;
&lt;h2&gt;New Boost Spirit X3&lt;/h2&gt;
&lt;p&gt;Boost Spirit X3 is a completely revamped version of Boost Spirit X3. It's aimed
at performance, both at compile-time and at runtime and uses recent features of
modern C++. It's not compatible with Boost Spirit Qi, so you'll most likely
have to rewrite a lot of stuff, in the parser, in the Abstract Syntax Tree
(AST) and in the AST passes as well.&lt;/p&gt;
&lt;p&gt;For reference, I'm using the Boost 1.59 version.&lt;/p&gt;
&lt;div class="section" id="pros"&gt;
&lt;h3&gt;Pros&lt;/h3&gt;
&lt;p&gt;Let's start with the pros.&lt;/p&gt;
&lt;p&gt;First, the runtime performance is definitely better. Parsing all my eddi test
cases and samples, &lt;em&gt;takes 42% less time than with the previous parser&lt;/em&gt;. It is
important to know that the old parser was very optimized, with moves instead of
copies and with a static lexer. You can take a look at &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2013/06/improving-eddic-boost-spirit-parser-performances.html"&gt;this post&lt;/a&gt;
to see what was necessary to optimize the old Qi. I think it's a good result
since the new grammar does not use a lexer (x3 does not support it) and does
not need these optimizations. This improvement really was my objective. I'll
try to push it farther in the future.&lt;/p&gt;
&lt;p&gt;Compile-time performance is also much better. It takes 3 times less time to
compile the new parser (1 minute to around 20 seconds). Moreover, the new
parser is now in only one file, rather than being it necessary to split it all
over the place for compile-time performance. Even though it's not really
important for me, it's still good to have :)&lt;/p&gt;
&lt;p&gt;Especially due to the performance point, I've been able to remove some code,
the lexer, the generated static lexer and the special pointers optimizations of
the AST.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cons"&gt;
&lt;h3&gt;Cons&lt;/h3&gt;
&lt;p&gt;Unfortunately, there are some disadvantages of using the new Spirit X3.&lt;/p&gt;
&lt;p&gt;First, the AST needs to be changed. For good parsing performance, you need to
use x3::variant and x3::forward_ast. This is a major pain in the ass since
x3::variant is much less practical to use than boost::variant. Almost
everything is explicit, meaning uglier code than before, in my opinion.
Moreover, you need to work around x3::forward_ast for boost::get, whereas
boost::recursive_wrapper was working better in that matter. I've had to create
my own wrapper around boost::get in order to be able to use the new tree. In my
opinion, this is clearly a regression.&lt;/p&gt;
&lt;p&gt;Secondly, although X3 was also meant to remove the need to use some hacks in
the grammar, I ended up having more hacks than before. For instance, many AST
node have a fake field in order to make X3 happy. I've still had to use the
horrible eps hack at one place. I've had to create a few more rules in order to
fix type deduction that is working differently than before (worse for me). And
for some reasons, I had to replace some expectations from the grammar to make it
parse correctly. This is a really important regression in my opinion, since it
may make the parsing slower and will make the error message less nice.&lt;/p&gt;
&lt;p&gt;The previous error handling system allowed me to track the file from which an
AST node was parsed from. Although the new error handler is a lot nicer than
the old system, it does not have this feature, so I had to work around this by
using new annotation nodes and a new global handler. Overall, it's probably a
bit worse than before, but makes for lighter AST nodes.&lt;/p&gt;
&lt;p&gt;Finally, for some reason, I haven't been able to use the debug option of the
library (lots of compile time errors). That complicated a bit the debugging of
the parser.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="spirit-x3-or-spirit-qi"&gt;
&lt;h3&gt;Spirit X3 or Spirit Qi ?&lt;/h3&gt;
&lt;p&gt;Overall, I have to say I'm a bit disappointed by Spirit X3. Even though it's
faster at runtime and faster to compile, I was really expecting less issues
with it. What I really did not like was all the changes I had to make because
of x3::variant and x3::forward_ast. Overall, I really don't think it was worth
the trouble porting my parser to Spirit X3.&lt;/p&gt;
&lt;p&gt;If you have a new project, I would still consider using Boost Spirit X3.&lt;/p&gt;
&lt;p&gt;If you have an existing parser, I would probably not advice porting it to X3.
Unless you really have issues with parsing performances (and especially if you
have not already optimized QI parser), it's probably not worth the trouble and
all the time necessary for all the changes.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="other-changes"&gt;
&lt;h2&gt;Other changes&lt;/h2&gt;
&lt;p&gt;The other changes are much more minor. First of all, I've gotten rid of CMake.
This project has really made me hate CMake. I have actually gotten rid of it on
all my projects. I'm now using plain Makefiles and having a much better time
with them. I've also replaced boost Program Options with cxxopts. It's a much
more modern approach for program options parsing. Moreover, it's much more
lightweight and it's header only. Only advantages. There also have been lots of
changes to code (still not very good quality though).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="future"&gt;
&lt;h2&gt;Future&lt;/h2&gt;
&lt;p&gt;eddic was my first real project in C++ and this can be seen in the code and the
organization. The quality of the code is really bad now that I read it again.
Some things are actually terrible :P It's probably normal since I was a
beginner in C++ at the time.&lt;/p&gt;
&lt;p&gt;For the future version of the compiler, I want to clean the code a lot more and
focus on the EDDI language adding new features. Moreover, I'll also get rid of
Boost Test Framework by using Catch (or doctest if it is ready).&lt;/p&gt;
&lt;p&gt;As for now, I'm not sure on which project I'm going to focus. Either I'll
continue working on the compiler or I'll start working again on my operating
system (thor-os) in which I was working on process concurrency (without too
much success :P). I'll probably post next updates on this post in the coming
months.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="download"&gt;
&lt;h2&gt;Download&lt;/h2&gt;
&lt;p&gt;You can find the EDDI Compiler sources on the &lt;a class="reference external" href="https://github.com/wichtounet/eddic"&gt;Github repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The version is available in the &lt;em&gt;v1.2.4&lt;/em&gt; tag available in the GitHub
repository, in the releases pages or directly in the &amp;lt;em&amp;gt;master&amp;lt;/em&amp;gt; branch.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>Boost</category><category>C++</category><category>C++14</category><category>eddic</category><guid>http://baptiste-wicht.com/posts/2016/06/eddic-124-new-boost-spirit-x3-parser-and-minor-cleanups.html</guid><pubDate>Sun, 26 Jun 2016 20:35:04 GMT</pubDate></item><item><title>Reduce Catch tests compilation time by another 16%</title><link>http://baptiste-wicht.com/posts/2016/06/reduce-compilation-time-by-another-16-with-catch.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;No, it's not the same post as two days! I've been able to reduce the compilation
time of my test cases by another 16%!&lt;/p&gt;
&lt;p&gt;Two days ago, I posted an article about how &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/05/speedup-compilation-by-13-by-simplifying-unit-test-with-catch.html"&gt;I reduced the compilation time of my tests by 13%&lt;/a&gt;, by bypassing the expression deduction from Catch. I came up with the macro &lt;code class="cpp"&gt;&lt;span class="n"&gt;REQUIRE_EQUALS&lt;/span&gt;&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_dfe7e82b56a04496b66fb37327ccdd4c-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_dfe7e82b56a04496b66fb37327ccdd4c-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;evaluate_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ResultBuilder&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt; &lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_dfe7e82b56a04496b66fb37327ccdd4c-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setResultType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_dfe7e82b56a04496b66fb37327ccdd4c-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setLhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_dfe7e82b56a04496b66fb37327ccdd4c-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setRhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_dfe7e82b56a04496b66fb37327ccdd4c-6"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setOp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"=="&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_dfe7e82b56a04496b66fb37327ccdd4c-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;endExpression&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_dfe7e82b56a04496b66fb37327ccdd4c-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;react&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_dfe7e82b56a04496b66fb37327ccdd4c-9"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_dfe7e82b56a04496b66fb37327ccdd4c-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_dfe7e82b56a04496b66fb37327ccdd4c-11"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define REQUIRE_EQUALS(lhs, rhs) \&lt;/span&gt;
&lt;a name="rest_code_dfe7e82b56a04496b66fb37327ccdd4c-12"&gt;&lt;/a&gt;&lt;span class="cp"&gt;    evaluate_result(Catch::ResultBuilder( "REQUIRE", CATCH_INTERNAL_LINEINFO, #lhs " == " #rhs, Catch::ResultDisposition::Normal ), lhs, rhs);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This has the advantage that the left and right hand sides are directly set, not
deduced with templates and operator overloading. This still has exactly the same
features has the original macro, but it is a bit less nice in the test code.
I was quite happy with that optimization, but it turned out, I was not
aggressive enough in my optimizations.&lt;/p&gt;
&lt;p&gt;Even though it seems simple, the macro is still bloated. There are two
constructors calls: &lt;code class="cpp"&gt;&lt;span class="n"&gt;ResultBuilder&lt;/span&gt;&lt;/code&gt; and &lt;code class="cpp"&gt;&lt;span class="n"&gt;SourceLineInfo&lt;/span&gt;&lt;/code&gt; (hidden behind
&lt;code class="cpp"&gt;&lt;span class="n"&gt;CATCH_INTERNAL_LINEINFO&lt;/span&gt;&lt;/code&gt;). That means that if you test case has 100
assertions, 200 constructor calls will need to be processed by the compiler.
Considering that I have some test files with around 400 assertions, this is
a lot of overhead for nothing. Moreover, two parameters have always the same
value, no need to repeat them every time.&lt;/p&gt;
&lt;p&gt;Simplifying the macro to the minimum led me to this:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_26e5950da3174eb39013c80a518cdb4f-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_26e5950da3174eb39013c80a518cdb4f-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;evaluate_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;char&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;char&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt; &lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_26e5950da3174eb39013c80a518cdb4f-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ResultBuilder&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"REQUIRE"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ResultDisposition&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Flags&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_26e5950da3174eb39013c80a518cdb4f-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setResultType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_26e5950da3174eb39013c80a518cdb4f-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setLhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_26e5950da3174eb39013c80a518cdb4f-6"&gt;&lt;/a&gt;    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setRhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_26e5950da3174eb39013c80a518cdb4f-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setOp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"=="&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_26e5950da3174eb39013c80a518cdb4f-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;endExpression&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_26e5950da3174eb39013c80a518cdb4f-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;react&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_26e5950da3174eb39013c80a518cdb4f-10"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_26e5950da3174eb39013c80a518cdb4f-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_26e5950da3174eb39013c80a518cdb4f-12"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define REQUIRE_EQUALS(lhs, rhs) \&lt;/span&gt;
&lt;a name="rest_code_26e5950da3174eb39013c80a518cdb4f-13"&gt;&lt;/a&gt;&lt;span class="cp"&gt;    evaluate_result(__FILE__, __LINE__, #lhs " == " #rhs, lhs, rhs);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The macro is now a simple function call. Even though the function is a template
function, it will only be compiled for a few types (&lt;code class="cpp"&gt;&lt;span class="kt"&gt;double&lt;/span&gt;&lt;/code&gt; and
&lt;code class="cpp"&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;/code&gt; in my case), whereas the code of the macro would be unconditionally
compiled for each invocation.&lt;/p&gt;
&lt;p&gt;With this new macro and function, the compilation time went down from 664
seconds to 554 seconds! This is &lt;strong&gt;more than 16% reduction in compilation
time&lt;/strong&gt;. When comparing against the original compilation time (without both
optimizations) of 764 seconds, this is a 27% reduction! And there are absolutely
no difference in features.&lt;/p&gt;
&lt;p&gt;This is a really great result, in my opinion. I don't think this can be cut down
more. However, there is still some room for optimization regarding the includes
that Catch need. Indeed, it is very bloated as well. A new test framework,
&lt;a class="reference external" href="https://github.com/onqtam/doctest"&gt;doctest&lt;/a&gt; follows the same philosophy, but
has much smaller include overhead. Once all the necessary features are in
doctest, I may consider adapting my macros for it and using it in place of Catch
is there is some substantial reduction in compilation time.&lt;/p&gt;
&lt;p&gt;If you want to take a look at the code, you can find the adapted code on &lt;a class="reference external" href="https://github.com/wichtounet/etl/blob/master/test/include/fast_catch.hpp"&gt;Github&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>Catch</category><category>Tests</category><guid>http://baptiste-wicht.com/posts/2016/06/reduce-compilation-time-by-another-16-with-catch.html</guid><pubDate>Wed, 01 Jun 2016 05:28:36 GMT</pubDate></item><item><title>Speed up compilation by 13% by simplifying Catch unit tests</title><link>http://baptiste-wicht.com/posts/2016/05/speedup-compilation-by-13-by-simplifying-unit-test-with-catch.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;In the previous two days, I've working on improving compilation time of my
project Expression Templates Library (ETL). I have been able to reduce the
compilation time of the complete test suite from 794 seconds to 764 seconds
(using only one thread). Trying to get further, I started checking what was
taking the most time in a test case when I saw that the REQUIRE calls of &lt;strong&gt;the
test library were taking a large portion of the compilation time!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I have been &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2014/07/catch-powerful-yet-simple-cpp-test-framework.html"&gt;using Catch as my test framework&lt;/a&gt;
for more than two years and it's really been great overall. It is a great tool,
header-only, fully-featured, XML reporting for Sonar, ... It really has
everything I need from a test framework.&lt;/p&gt;
&lt;p&gt;Contrary to some popular test frameworks that provides ASSERT_EQUALS,
ASSERT_GREATER and all fashion of assert macros, Catch only provides one
version: REQUIRE. For instance:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_bac7ccdc54ad4371a043f483e44ba4b6-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_bac7ccdc54ad4371a043f483e44ba4b6-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;5.5&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_bac7ccdc54ad4371a043f483e44ba4b6-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mf"&gt;22.01f&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The left and right part are detected with some smart template and operator
overloading techniques and this makes for very nice test output in case of
errors, for instance:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_a496ca5e66d04ae1bec9bdc1f808bcf1-1"&gt;&lt;/a&gt;test/src/dyn_matrix.cpp:16: FAILED:
&lt;a name="rest_code_a496ca5e66d04ae1bec9bdc1f808bcf1-2"&gt;&lt;/a&gt;  REQUIRE( test_matrix.rows() == 2UL )
&lt;a name="rest_code_a496ca5e66d04ae1bec9bdc1f808bcf1-3"&gt;&lt;/a&gt;with expansion:
&lt;a name="rest_code_a496ca5e66d04ae1bec9bdc1f808bcf1-4"&gt;&lt;/a&gt;  3 == 2
&lt;/pre&gt;&lt;p&gt;I think this is pretty nice and the tests are really clear. However, &lt;em&gt;it comes
with a cost&lt;/em&gt; and I underestimated this at first.&lt;/p&gt;
&lt;p&gt;To overcome this, I create two new macros (and few other variations)
REQUIRE_EQUALS and REQUIRE_DIRECT that simply bypass Catch deduction of the
expression:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-1"&gt;&lt;/a&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;evaluate_result_direct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ResultBuilder&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setResultType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setLhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="s"&gt;"true"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"false"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setOp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;""&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;endExpression&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-6"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-8"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-9"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;evaluate_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ResultBuilder&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt; &lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setResultType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setLhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setRhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-13"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setOp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"=="&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-14"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;endExpression&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-15"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-17"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define REQUIRE_DIRECT(value) \&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-18"&gt;&lt;/a&gt;&lt;span class="cp"&gt;    evaluate_result_direct(Catch::ResultBuilder( "REQUIRE", CATCH_INTERNAL_LINEINFO, #value, Catch::ResultDisposition::Normal ), value);&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-19"&gt;&lt;/a&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-20"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define REQUIRE_EQUALS(lhs, rhs) \&lt;/span&gt;
&lt;a name="rest_code_45f863a86a2a4edabbed1f2332775e8c-21"&gt;&lt;/a&gt;&lt;span class="cp"&gt;    evaluate_result(Catch::ResultBuilder( "REQUIRE", CATCH_INTERNAL_LINEINFO, #lhs " == " #rhs, Catch::ResultDisposition::Normal ), lhs, rhs);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;There is really nothing too special about it, I simply followed the macros and
functions in Catch source code until I found out what to bypass.&lt;/p&gt;
&lt;p&gt;And now, we use them directly:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_d323bd83575b49b984545dea8600da1d-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE_DIRECT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;am_i_true&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_d323bd83575b49b984545dea8600da1d-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE_EQUALS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is a bit less nice and it requires to know a few more macros, I admit, but
it turns out to be much faster (and who really cares about the beauty of test
code anyway...). Indeed, the total compilation time of the tests went from 764
seconds to 664 seconds!  This is &lt;strong&gt;a 13% reduction of the compilation time&lt;/strong&gt;!
I really am impressed of the overhead of this technique. I cannot justify this
slowdown just for a bit nicer test code. Finally, the output in case of error
remains exactly the same as before.&lt;/p&gt;
&lt;p&gt;This proves that sometimes the bottlenecks are not where we expect them :)&lt;/p&gt;
&lt;p&gt;If you are interested, you can find the adapted code on &lt;a class="reference external" href="https://github.com/wichtounet/etl/blob/master/test/include/fast_catch.hpp"&gt;Github&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>Tests</category><guid>http://baptiste-wicht.com/posts/2016/05/speedup-compilation-by-13-by-simplifying-unit-test-with-catch.html</guid><pubDate>Wed, 25 May 2016 10:35:16 GMT</pubDate></item><item><title>Simplify Deep Learning Library usage on Linux and Windows!</title><link>http://baptiste-wicht.com/posts/2016/04/simplify-deep-learning-library-usage-on-linux-and-windows.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;No, I'm not dead ;) I've been very busy with my Ph.D (and playing Path of Exile,
let's be honest...) and haven't had time to write something here in a long time.&lt;/p&gt;
&lt;p&gt;Until now, there was too way to use my
&lt;a class="reference external" href="https://github.com/wichtounet/dll/"&gt;Deep Learning Library (DLL)&lt;/a&gt; project:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Write a C++ program that uses the library&lt;/li&gt;
&lt;li&gt;Install DLL and write a configuration file to define your network and the problem to solve&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first version gives you all the features of the tool and allows you to build
exactly what you need. The second version is a bit more limited, but does not
require any C++ knowledge. However, it still does require a recent C++ compiler
and build system.&lt;/p&gt;
&lt;p&gt;Due to the high C++ requirements that are not met by Visual Studio and the fact
that I don't work on Windows, this platform is not supported by the tool. Until
now!&lt;/p&gt;
&lt;p&gt;I've added a third option to use DLL in the form of a Docker image to make the
second option even easier and allow the use of DLL on Windows. All you need is
Docker, which is available on Linux, Mac and Windows. This is still limited to
the second option in that you need to write a configuration describing the
network, but you need to build DLL and don't need to install all its
dependencies.&lt;/p&gt;
&lt;div class="section" id="usage"&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;To install the image, you can simply use &lt;cite&gt;docker pull&lt;/cite&gt;:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_95ee1d21e0634946929445e453bbe3d1-1"&gt;&lt;/a&gt;docker pull wichtounet/docker-dll
&lt;/pre&gt;&lt;p&gt;Then, to run it, you have to create a folder containing a &lt;cite&gt;dll.conf&lt;/cite&gt; file and
mount in the container at &lt;cite&gt;/dll/data/&lt;/cite&gt;. There are some examples in the
&lt;a class="reference external" href="https://github.com/wichtounet/docker-dll/"&gt;image repository&lt;/a&gt;.  For instance,
on Linux from the cloned repository:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_276428fb67a3453ba45308908375f493-1"&gt;&lt;/a&gt;docker run -v &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;pwd&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/rbm_mnist/:/dll/data/ wichtounet/docker-dll
&lt;/pre&gt;&lt;p&gt;or on Windows:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_9dc5f5fbaf5b4f62b806bca8f20395c4-1"&gt;&lt;/a&gt;docker run -v /c/Users/Baptiste/rbm_mnist/:/dll/data wichtounet/docker-dll
&lt;/pre&gt;&lt;p&gt;This will automatically run the actions specified in the configuration file and
train your network.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I would really have thought this would be harder, but it turned out that Docker
is a very good solution to deploy multiplatform demo tools :)&lt;/p&gt;
&lt;p&gt;As of now, there is only support for mnist data format in the tool in this
form, but I plan to add basic CSV support as well in the near future.&lt;/p&gt;
&lt;p&gt;I hope that this will help people willing to try the library with a simpler
usage.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>deep learning</category><category>dll</category><category>Linux</category><category>Machine Learning</category><category>projects</category><category>Windows</category><guid>http://baptiste-wicht.com/posts/2016/04/simplify-deep-learning-library-usage-on-linux-and-windows.html</guid><pubDate>Fri, 29 Apr 2016 10:48:18 GMT</pubDate></item><item><title>Use templight and Templar to debug C++ templates</title><link>http://baptiste-wicht.com/posts/2016/02/use-templight-and-templar-to-debug-cpp-templates.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;C++ has some very good tools to debug, profile and analyze source files and executables. This all works well for standard runtime program. But, when you are using templates, you sometimes want these tools to act at compile-time. And at this point the support is much more scarce.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/mikael-s-persson/templight"&gt;templight&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/schulmar/Templar"&gt;Templar&lt;/a&gt; and two tools that are trying to fix this issue.&lt;/p&gt;
&lt;p&gt;From the templight site:&lt;/p&gt;
&lt;blockquote&gt;
Templight is a Clang-based tool to profile the time and memory consumption of template instantiations and to perform interactive debugging sessions to gain introspection into the template instantiation process.&lt;/blockquote&gt;
&lt;p&gt;and Templar is a visualization tool for the traces generated by templight.&lt;/p&gt;
&lt;div class="section" id="installation"&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;Unfortunately, the templight installation is not user-friendly at all. You need to clone the complete LLVM/Clang tree and add templight inside it before compiling the complete clang toolchain. But that is the case for all clang-based tools... You also need to patch clang but that may not be necessary in the future. The complete instructions are available &lt;a class="reference external" href="https://github.com/mikael-s-persson/templight#getting-and-compiling-templight"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The installation of Templar is much more convenient:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_d7a713f4424847eb874a522c2f47980f-1"&gt;&lt;/a&gt;git clone https://github.com/schulmar/Templar.git
&lt;a name="rest_code_d7a713f4424847eb874a522c2f47980f-2"&gt;&lt;/a&gt;git checkout feature/templight2
&lt;a name="rest_code_d7a713f4424847eb874a522c2f47980f-3"&gt;&lt;/a&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; Templar
&lt;a name="rest_code_d7a713f4424847eb874a522c2f47980f-4"&gt;&lt;/a&gt;qmake .
&lt;a name="rest_code_d7a713f4424847eb874a522c2f47980f-5"&gt;&lt;/a&gt;make
&lt;a name="rest_code_d7a713f4424847eb874a522c2f47980f-6"&gt;&lt;/a&gt;sudo make install
&lt;/pre&gt;&lt;p&gt;The branch feature/templight2 has much more features than the master and should support both Qt4 and Qt5, but I have only tested it on Qt4.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example"&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Let's use the class Fibonacci function as an example:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-6"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-8"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-11"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-13"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-14"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-15"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-16"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-18"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-19"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Fibonacci&amp;lt;5&amp;gt;:"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_cc6d1af56ba9422ba8597889ba4b229f-20"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Nothing fancy here, we're simply printing the fifth Fibonacci number on the console.&lt;/p&gt;
&lt;p&gt;You can compile it with templight++:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_d5a311eb4fbd4687929b2ba184e291c8-1"&gt;&lt;/a&gt;templight++ -Xtemplight -profiler -Xtemplight -memory -Xtemplight -ignore-system -std&lt;span class="o"&gt;=&lt;/span&gt;c++14 main.cpp
&lt;/pre&gt;&lt;p&gt;All the templight options starts with -Xtemplight and then you can use any clang++ options. This will generate a &lt;em&gt;a.memory.trace.pbf&lt;/em&gt; file in the current directory. You can then run Templar. use File &amp;gt; Open Trace to open the trace file. This should open a window of this sort:&lt;/p&gt;
&lt;img alt="/images/templar.png" class="align-center" src="http://baptiste-wicht.com/images/templar.png"&gt;
&lt;p&gt;The top-left panel contains the source code of the application, automatically
refreshed whenever you move in the template tree. In the top right, there is
the template instantiation graph. In the bottom left, you'll see a list of list
of files to be able to filter them and in the bottom right, you'll see the list
of templates events. You can sort the list of template events by duration which
is really convenient. You can then select Fibonacci&amp;lt;5&amp;gt; by double clicking it in
the list (once sorted, it should be near the top). This should give you a tree
looking something like that:&lt;/p&gt;
&lt;img alt="/images/templar_tree.png" class="align-center" src="http://baptiste-wicht.com/images/templar_tree.png"&gt;
&lt;p&gt;The edgy nodes are template instantiations and the round nodes are template
memoization. We can directly see that each instantiation was only done once. I
think this graph view is really helpful if you need to debug computation done
at compile time. You can see that that not all nodes are displayed, this is
because there is a limit on the displayed depth. Simply click on Fibonacci&amp;lt;3&amp;gt;
and the remaining nodes will be shown.&lt;/p&gt;
&lt;p&gt;I have already used this tool to find the most time-consuming templates in ETL
an DLL. This is a great tool to indicate where you should focus on improving
the template compile-time. I have also been able to find some unnecessary
instantiations that could be avoided (either with SFINAE or with refactorings).&lt;/p&gt;
&lt;p&gt;templight also contains a fully-fledged debugger for template programs, but I haven't tested it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, I would say that templight and Templar are really helping with
template debugging and profiling. There is a real lack of tools in this domain
and I hope to see more tools of this kind in the future. I hope this will help
you develop template-heavy programs or template metaprograms.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>templates</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2016/02/use-templight-and-templar-to-debug-cpp-templates.html</guid><pubDate>Mon, 08 Feb 2016 07:11:18 GMT</pubDate></item><item><title>Improve DLL and ETL Compile Time further</title><link>http://baptiste-wicht.com/posts/2016/01/improve-dll-and-etl-compile-time-further.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;For a while, the compilation time of my matrix/vector computation library (ETL), based on Expression Templates has become more and more problematic. I've already worked on this problem &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2015/06/how-i-improved-a-bit-compile-time-of-etl.html"&gt;here&lt;/a&gt; and &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2015/06/improve-etl-compile-time-with-precompiled-headers.html"&gt;there&lt;/a&gt;, using some general techniques (pragmas, precompiled headers, header removals and so on). On this post, I'll talk about two major improvements I have been able to do directly in the code.&lt;/p&gt;
&lt;div class="section" id="use-of-static-if"&gt;
&lt;h2&gt;Use of static_if&lt;/h2&gt;
&lt;p&gt;Remember &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2015/07/simulate-static_if-with-c11c14.html"&gt;static_if&lt;/a&gt; ? I was able to use it to really reduce the compile time of DLL.&lt;/p&gt;
&lt;p&gt;I wrote a script to time each test case of the DLL project to find the test cases that took the longest to compile. Once I found the best candidate, I isolated the functions that took the longest to compile. It was quite tedious and I did it by hand, primarily by commenting parts of the code and going deeper and deeper in the code. I was quite suprised to find that a single function call (template function of course ;) ) was responsible for 60% of the compilation time of my candidate test case. The function was instantiating a whole bunch of expression templates (to compute the free energy of several models). The function itself was not really optimizable, but what was really interesting is that this function was only used in some very rare cases and that these cases were known at compile-time :) This was a perfect case to use a static_if. And once the call was inside the static_if, the test case was indeed about 60% faster. &lt;strong&gt;This reduced the overall compilation time of DLL by about 30%&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;This could also of course also have been achieved by using two functions, one with the call, one empty and selected by SFINAE (Substitution Failure Is Not An Error). I prefer the statif_if version since this really shows the intent and hides SFINAE behind nicer syntax.&lt;/p&gt;
&lt;p&gt;I was also able to use static_if at other places in the DLL code to avoid instantiating some templates, but the improvements were much less dramatic (about 1% of the total compilation time). I was very lucky to find a single function that accounted for so much compile time. After some more tests, I concluded that much of the compilation time of DLL was spent compiling the Expression Templates from my ETL library so I decided to delve into ETL code directly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="removal-of-std-async"&gt;
&lt;h2&gt;Removal of std::async&lt;/h2&gt;
&lt;p&gt;The second improvement was very surprising. I was working on improving the compilation of ETL and found out that the sum and average reductions of matrices were dramatically slow, about an order of magnitude slower than standard operations on matrices. In parallel (but the two facts are linked), I also found out another weird fact when splitting a file into 10 parts (the file was comprised of 10 test cases). Compiling the 10 parts separarely (and sequentially, not multiple threads) was about 40% faster than compiling the complete file. There was no swapping so it was not a memory issue. This is not expected. Generally, it is faster to compile a big file than to compile its parts separately. The advantage of smaller files is that you can compile them in parallel and that incremental builds are faster (only compile a small part).&lt;/p&gt;
&lt;p&gt;By elimination, I found out that most of the time was spent inside the function that was dispatching in parallel the work for accumulating the sum of a matrix. Here is the function:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;Functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;AccFunctor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-2"&gt;&lt;/a&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;dispatch_1d_acc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Functor&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AccFunctor&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-6"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-7"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-9"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-10"&gt;&lt;/a&gt;            &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-11"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-15"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="nl"&gt;fut&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-16"&gt;&lt;/a&gt;            &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fut&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-17"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-18"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-19"&gt;&lt;/a&gt;        &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-20"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_fc1c40e0dda24e0fbbe55fa0407192b0-21"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;There isn't anything really fancy about this function. This takes one functor that will be done in parallel and one function for accumulation.  It dispatches all the work in batch and then accumulates the results. I tried several things to optimize the compilation time of this function, but nothing worked. The line that was consuming all the time was the std::async line. This function was using std::async because the thread pool that I'm generally using does not support returning values from parallel functors. I decided to use a workaround and use my thread pool and I came out with this version:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;Functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;AccFunctor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-2"&gt;&lt;/a&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;dispatch_1d_acc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Functor&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AccFunctor&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-5"&gt;&lt;/a&gt;        &lt;span class="n"&gt;cpp&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;default_thread_pool&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-7"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-8"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-10"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;sub_functor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-11"&gt;&lt;/a&gt;            &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-12"&gt;&lt;/a&gt;        &lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-13"&gt;&lt;/a&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-14"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-15"&gt;&lt;/a&gt;            &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;do_task&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-16"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-19"&gt;&lt;/a&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-20"&gt;&lt;/a&gt;        &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wait&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-21"&gt;&lt;/a&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-22"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="nl"&gt;fut&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-23"&gt;&lt;/a&gt;            &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fut&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-24"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-25"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-26"&gt;&lt;/a&gt;        &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-27"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_106d5b607d5644b59c30e01148d4a4d7-28"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I simply preallocate space for all the threads and create a new functor calling the input functor and saving its result inside the vector. It is less nice, but it works well. And it compiles MUCH faster. This &lt;strong&gt;reduced the compilation time&lt;/strong&gt; of my biggest test case &lt;strong&gt;by a factor of 8&lt;/strong&gt; (from 344 seconds to 44 seconds). This is really crazy. It also fixed the problem where splitting the test case was faster than big file (it is now twice faster to compile the big files than compiling all the small files separately). &lt;strong&gt;This reduced the total compilation time of dll by about 400%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As of now, I still have no idea why this makes such a big difference. I have looked at the std::async code, but I haven't found a valid reason for this slowdown. If someone has any idea, I'd be very glad to discuss in the comments below.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="improving-the-template-instantiation-tree"&gt;
&lt;h2&gt;Improving the template instantiation tree&lt;/h2&gt;
&lt;p&gt;I recently discovered the templight tool that is a profiler for templates (pretty cool). After some time, I was able to build it and use it on ETL. For now, I haven't been able to reduce compile time a lot, but I have been able to reduce the template instantiation tree a lot seeing that some instantiations were completely useless and I optimized the code to remove them.&lt;/p&gt;
&lt;p&gt;I won't be go into much details here because I plan to write a post on this subject in the coming days.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, I would say that it is pretty hard to improve the compile time of complex C++ programs once you have gone through all the standard methods. However, I was very happy to found that &lt;strong&gt;two optimizations in the source code reduced the overall compilation of DLL by almost 500%&lt;/strong&gt;. I will continue working on this, but for now, the compilation time is much more reasonable.&lt;/p&gt;
&lt;p&gt;I hope the two main facts in this article were interesting. If you have similar experience, comments or ideas for further improvements, I'd be glad to discuss them with you in the comments :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>Compilers</category><category>dll</category><category>etl</category><category>gcc</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2016/01/improve-dll-and-etl-compile-time-further.html</guid><pubDate>Fri, 29 Jan 2016 16:02:34 GMT</pubDate></item></channel></rss>