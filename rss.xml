<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Blog blog("Baptiste Wicht");</title><link>http://baptiste-wicht.com/</link><description>Tutorials and short posts about programming, C++, Java, Assembly, Operating Systems Development, Compilers, ...</description><atom:link href="http://baptiste-wicht.com/rss.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Sun, 11 Dec 2016 19:17:12 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>C++ Compiler benchmark on Expression Templates Library (ETL)</title><link>http://baptiste-wicht.com/posts/2016/12/cpp-compiler-benchmark-on-expression-templates-library-etl.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;script src="https://code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"&gt;&lt;/script&gt;
&lt;script src="https://code.highcharts.com/highcharts.js"&gt;&lt;/script&gt;
&lt;script src="https://code.highcharts.com/modules/exporting.js"&gt;&lt;/script&gt;&lt;p&gt;In my Expression Templates Library (ETL) project, I have a lot of template heavy
code that needs to run as fast as possible and that is quite intensive to
compile. In this post, I'm going to compare the performance of a few of the
kernels produced by different compilers. I've got GCC 5.4, GCC 6.20 and clang
3.9. I also included zapcc which is based on clang 4.0.&lt;/p&gt;
&lt;p&gt;These tests have been run on an Haswell processor. The automatic parallelization
of ETL has been turned off for these tests.&lt;/p&gt;
&lt;p&gt;Keep in mind that some of the diagrams are presented in logarithmic form.&lt;/p&gt;
&lt;div class="section" id="vector-multiplication"&gt;
&lt;h2&gt;Vector multiplication&lt;/h2&gt;
&lt;p&gt;The first kernel is a very simple one, simple element-wise multiplication of two
vectors. Nothing fancy here.&lt;/p&gt;
&lt;div id="mul_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('mul_container', {
        chart: { type: 'column' },
        title: { text: 'Element-wise Vector Multiplication' },
        xAxis: {
            categories: ['10', '100', '1000', '10000', '100000', '1000000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [0.021, 0.040, 0.215, 2.07, 32.1, 403]
        },
        {
            name: 'g++-6.2', data: [0.021, 0.037, 0.208, 2.17, 32.1, 376]
        },
        {
            name: 'clang-3.9', data: [0.027, 0.045, 0.243, 2.43, 32.7, 389]
        },
        {
            name: 'zapcc-4.0', data: [0.026, 0.047, 0.321, 2.5, 32.8, 411]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;For small vectors, clang is significantly slower than gcc-5.4 and gcc6.2. On
vectors from 100'000 elements, the speed is comparable for each compiler,
depending on the memory bandwidth. Overall, gcc-6.2 produces the fastest code
here. clang-4.0 is slightly slower than clang-3.9, but nothing dramatic.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="vector-exponentiation"&gt;
&lt;h2&gt;Vector exponentiation&lt;/h2&gt;
&lt;p&gt;The second kernel is computing the exponentials of each elements of a vector and
storing them in another vector.&lt;/p&gt;
&lt;div id="exp_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('exp_container', {
        chart: { type: 'column' },
        title: { text: 'Element-wise Vector Exponentiation' },
        xAxis: {
            categories: ['10', '100', '1000', '10000', '100000', '1000000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [0.0478, 0.137, 1.12, 9.79, 97.5, 959]
        },
        {
            name: 'g++-6.2', data: [0.0474, 0.132, 1.11, 9.71, 97, 1000]
        },
        {
            name: 'clang-3.9', data: [0.0492, 0.136, 0.959, 9.24, 92.9, 914]
        },
        {
            name: 'zapcc-4.0', data: [0.0488, 0.142, 0.952, 9.25, 91.9, 915]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;Interestingly, this time, clang versions are significantly faster for medium to
large vectors, from 1000 elements and higher, by about 5%. There is no
significant differences between the different versions of each compiler.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="matrix-matrix-multiplication"&gt;
&lt;h2&gt;Matrix-Matrix Multiplication&lt;/h2&gt;
&lt;p&gt;The next kernel I did benchmark with the matrix-matrix multiplication operation.
In that case, the kernel is hand-unrolled and vectorized.&lt;/p&gt;
&lt;div id="gemm_container_small" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;div id="gemm_container_large" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('gemm_container_small', {
        chart: { type: 'column' },
        title: { text: 'Matrix Matrix Multiplication (small)', },
        xAxis: {
            categories: ['10x10', '20x20', '40x40', '60x60', '80x80', '100x100']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [0.159, 0.815, 2.637, 13.849, 17.281, 78.903]
        },
        {
            name: 'g++-6.2', data: [0.162, 0.802, 2.431, 13.531, 17.274, 74.02]
        },
        {
            name: 'clang-3.9', data: [0.179, 1.218, 2.391, 14.981, 15.142, 61.548]
        },
        {
            name: 'zapcc-4.0', data: [0.159, 0.836, 2.712, 13.426, 15.114, 62.241]
        }
        ]
    });
    Highcharts.chart('gemm_container_large', {
        chart: { type: 'column' },
        title: { text: 'Matrix Matrix Multiplication (large)', },
        xAxis: {
            categories: ['200x200', '300x300', '400x400', '500x500', '600x600', '700x700', '800x800', '900x900', '1000x1000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [275.219, 1371, 1837, 5177, 6667, 14981, 17037, 31492, 32813]
        },
        {
            name: 'g++-6.2', data: [267.776, 1362, 1808, 5297, 6859, 15166, 15664, 30666, 33067]
        },
        {
            name: 'clang-3.9', data: [266.033, 1230, 1789, 4825, 6969, 14488, 15916, 30872, 33186]
        },
        {
            name: 'zapcc-4.0', data: [267.806, 1237, 1820, 4909, 7035, 15191, 18193, 33127, 37346]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;There are few differences between the compilers. The first thing is that for
some sizes such as 80x80 and 100x100, clang is significantly faster than GCC, by
more than 10%. The other interesting fact is that for large matrices
zapcc-clang-4.0 is always slower than clang-3.9 which is itself on par with the
two GCC versions. In my opinion, it comes from a regression in clang trunk but
it could also come from zapcc itself.&lt;/p&gt;
&lt;div id="std_gemm_container_large" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('std_gemm_container_large', {
        chart: { type: 'column' },
        title: { text: 'Matrix Matrix Multiplication (naive)', },
        xAxis: {
            categories: ['200x200', '300x300', '400x400', '500x500', '600x600', '700x700', '800x800', '900x900', '1000x1000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [1.195, 4.891, 10.467, 22.400, 33.399,
            58.401, 77.150, 121.392, 148.469]
        },
        {
            name: 'g++-6.2', data: [1.109, 4.540, 9.964, 21.359, 31.904,
            55.282, 72.690, 113.52, 143.27]
        },
        {
            name: 'clang-3.9', data: [0.893, 3.710, 7.287, 16.244, 23.920,
            43.342, 56.771, 91.870, 112.309]
        },
        {
            name: 'zapcc-4.0', data: [5.088, 16.909, 39.632, 77.194, 133.15,
            214.539, 316.01, 447.715, 612.255]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;The results are much more interesting here! First, there is a huge regression in
clang-4.0 (or in zapcc for that matter). Indeed, it is up to 6 times slower than
clang-3.9. Moreover, the clang-3.9 is always significantly faster than gcc-6.2.
Finally, there is a small improvement in gcc-6.2 compared to gcc 5.4.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fast-fourrier-transform"&gt;
&lt;h2&gt;Fast-Fourrier Transform&lt;/h2&gt;
&lt;p&gt;The following kernel is the performance of a hand-crafted Fast-Fourrier
transform implementation.&lt;/p&gt;
&lt;div id="fft_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('fft_container', {
        chart: { type: 'column' },
        title: { text: 'Fast Fourrier Transform', },
        xAxis: {
            categories: ['100', '1000', '10000', '100000', '1000000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [2.640, 27.515, 308.239, 3427.4, 41695.9]
        },
        {
            name: 'g++-6.2', data: [2.578, 26.194, 298.97, 3348.82, 40783.8]
        },
        {
            name: 'clang-3.9', data: [3.047, 30.514, 333.403, 3569.36,43860.6]
        },
        {
            name: 'zapcc-4.0', data: [3.199,33.304,317.135,4025.18,48445.3]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;On this benchmark, gcc-6.2 is the clear winner. It is significantly faster
than clang-3.9 and clang-4.0. Moreover, gcc-6.2 is also faster than gcc-5.4.
On the contrary, clang-4.0 is significantly slower than clang-3.9 except on one
configuration (10000 elements).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="d-convolution"&gt;
&lt;h2&gt;1D Convolution&lt;/h2&gt;
&lt;p&gt;This kernel is about computing the 1D valid convolution of two vectors.&lt;/p&gt;
&lt;div id="conv1_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('conv1_container', {
        chart: { type: 'column' },
        title: { text: '1D convolution (optimized)', },
        xAxis: {
            categories: ['1000x500', '2000x1000', '3000x1500', '4000x2000',
            '5000x2500', '6000x3000', '7000x3500', '8000x4000', '9000x4500',
            '10000x5000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [11.710, 41.002, 91.201, 158.178,
            248.985, 353.695, 486.676, 634.53, 867.101, 1082.62]
        },
        {
            name: 'g++-6.2', data: [9.307, 40.921, 90.327, 158.734, 248.892,
            354.582, 488.38, 636.899, 869.637, 1084.86]
        },
        {
            name: 'clang-3.9', data: [13.404, 41.409, 95.094, 162.339,
            256.143, 362.34, 498.66, 651.352, 886.465, 1092.24]
        },
        {
            name: 'zapcc-4.0', data: [13.528, 40.886, 94.473, 159.917,
            252.992, 356.63, 493.653, 640.348, 872.282, 1091.36]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;While clang-4.0 is faster than clang-3.9, it is still slightly slower than both
gcc versions. On the GCC side, there is not a lot of difference except on the
1000x500 on which gcc-6.2 is 25% faster.&lt;/p&gt;
&lt;p&gt;And here are the results with the naive implementation:&lt;/p&gt;
&lt;div id="std_conv1_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('std_conv1_container', {
        chart: { type: 'column' },
        title: { text: '1D convolution (naive)', },
        xAxis: {
            categories: ['1000x500', '2000x1000', '3000x1500', '4000x2000',
            '5000x2500', '6000x3000', '7000x3500', '8000x4000', '9000x4500',
            '10000x5000']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [0.350, 1.452, 3.260, 5.823, 9.116,
            13.155, 17.922, 23.438, 29.705, 36.683]
        },
        {
            name: 'g++-6.2', data: [0.350, 1.457, 3.262, 5.823, 9.120,
            13.152, 17.922, 23.436, 29.687, 36.665]
        },
        {
            name: 'clang-3.9', data: [0.216, 0.873, 1.974, 3.517, 5.501,
            7.921, 10.793, 14.11, 17.867, 22.068]
        },
        {
            name: 'zapcc-4.0', data: [0.215, 0.873, 1.972, 3.514, 5.501,
            7.928, 10.799, 14.11, 17.879, 22.065]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;Again, on the naive version, clang is much faster than GCC on the naive, by
about 65%. This is a really large speedup.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;2D Convolution&lt;/h2&gt;
&lt;p&gt;This next kernel is computing the 2D valid convolution of two matrices&lt;/p&gt;
&lt;div id="conv2_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('conv2_container', {
        chart: { type: 'column' },
        title: { text: '2D Convolution (optimized)', },
        xAxis: {
            categories: ['100x50', '105x50', '110x55', '115x55', '120x60',
            '125x60', '130x65', '135x65', '140x70']
        },
        yAxis: {
            title: { text: 'Time (us)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'us'},
        series: [
        {
            name: 'g++-5.4', data: [327.399, 367.389, 441.457, 576.021,
            762.268, 794, 994.06, 1261.71, 1360.57]
        },
        {
            name: 'g++-6.2', data: [327.764, 367.379, 441.993, 572.241,
            761.741, 784.605, 991.717, 1266.55, 1361.59]
        },
        {
            name: 'clang-3.9', data: [330.199, 364.253, 443.483, 580.676,
            763.772, 777.39, 1000.53, 1267.75, 1375.51]
        },
        {
            name: 'zapcc-4.0', data: [339.358, 364.756, 443.807, 575.917,
            761.248, 784.695, 992.29, 1265.04, 1367.33]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;There is no clear difference between the compilers in this code. Every compiler
here has up and down.&lt;/p&gt;
&lt;p&gt;Let's look at the naive implementation of the 2D convolution (units are
milliseconds here not microseconds):&lt;/p&gt;
&lt;div id="std_conv2_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('std_conv2_container', {
        chart: { type: 'column' },
        title: { text: '2D Convolution (naive)', },
        xAxis: {
            categories: ['100x50', '105x50', '110x55', '115x55', '120x60',
            '125x60', '130x65', '135x65', '140x70']
        },
        yAxis: {
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [9.501,11.458,13.888, 16.489, 19.634,
            22.898, 27.012, 31.246, 36.269]
        },
        {
            name: 'g++-6.2', data: [9.502, 11.464, 13.903, 16.484, 19.642,
            22.994, 27.004, 31.248, 36.26]
        },
        {
            name: 'clang-3.9', data: [5.880, 7.136, 8.610, 10.226, 12.164,
            14.247, 17.024, 19.577, 22.510]
        },
        {
            name: 'zapcc-4.0', data: [5.875, 7.091, 8.661, 10.241, 12.218,
            14.302, 16.777, 19.424, 22.472]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;This time the difference is very large! Indeed, clang versions are about 60%
faster than the GCC versions! This is really impressive. Even though this does
not comes close to the optimized. It seems the vectorizer of clang is much more
efficient than the one from GCC.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id2"&gt;
&lt;h2&gt;4D Convolution&lt;/h2&gt;
&lt;p&gt;The final kernel that I'm testing is the batched 4D convolutions that is used a
lot in Deep Learning. This is not really a 4D convolution, but a large number
of 2D convolutions applied on 4D tensors.&lt;/p&gt;
&lt;div id="conv4_container" style="min-width: 310px; height:400px; margin: 0 auto; "&gt;&lt;/div&gt;
&lt;script&gt;
$(function () {
    Highcharts.chart('conv4_container', {
        chart: { type: 'column' },
        title: { text: '4D Convolution', },
        xAxis: {
            categories: ['2x6x3x28x16', '2x6x3x28x16', '2x6x3x28x16',
            '2x6x3x28x16', '2x6x3x28x16', '2x6x3x28x16', '2x6x3x28x16',
            '2x6x3x28x16', '2x6x3x28x16']
        },
        yAxis: {
            type: 'logarithmic',
            title: { text: 'Time (ms)' },
            plotLines: [{
                value: 0,
                width: 1,
                color: '#808080'
            }]
        },
        tooltip: {valueSuffix: 'ms'},
        series: [
        {
            name: 'g++-5.4', data: [0.095, 0.402, 1.083, 2.237, 3.988,
            6.474, 9.985, 14.132, 19.539]
        },
        {
            name: 'g++-6.2', data: [0.089, 0.413, 1.081, 2.224, 3.990,
            6.462, 9.815, 14.118, 19.612]
        },
        {
            name: 'clang-3.9', data: [0.090, 0.416, 1.108, 2.277, 4.077,
            6.587, 10.024, 14.359, 20.006]
        },
        {
            name: 'zapcc-4.0', data: [0.088, 0.406, 1.080, 2.237, 3.987,
            6.484, 9.827, 14.130, 19.569]
        }
        ]
    });
});
&lt;/script&gt;&lt;p&gt;Again, there are very small differences between each version. The best versions
are the most recent versions of the compiler gcc-6.2 and clang-4.0 on a tie.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Overall, we can see two trends in these results. First, when working with
highly-optimized code, the choice of compiler will not make a huge difference.
On these kind of kernels, gcc-6.2 tend to perform faster than the other
compilers, but only by a very slight margin, except in some cases. On the other
hand, when working with naive implementations, clang versions really did perform
much better than GCC. The clang compiled versions of the 1D and 2D convolutions
are more than 60% faster than their GCC counter parts. This is really
impressive. Overall, clang-4.0 seems to have several performance regressions,
but since it's not still a work in progress, I would not be suprised if these
regressions are not present in the final version. Since the clang-4.0 version is
in fact the clang version used by zapcc, it's also possible that zapcc is
introducing new performance regressions.&lt;/p&gt;
&lt;p&gt;Overall, my advice would be to use GCC-6.2 (or 5.4) on hand-optimized kernels
and clang when you have mostly naive implementations. However, keep in mind that
at least for the example shown here, the naive version optimized by the compiler
never comes close to the highly-optimized version.&lt;/p&gt;
&lt;p&gt;As ever, takes this with a grain of salt, it's only been tested on one project
and one machine, you may obtain very different results on other projects and on
other processors.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>Compilers</category><category>gcc</category><category>Performance</category><category>templates</category><guid>http://baptiste-wicht.com/posts/2016/12/cpp-compiler-benchmark-on-expression-templates-library-etl.html</guid><pubDate>Sun, 11 Dec 2016 13:17:30 GMT</pubDate></item><item><title>zapcc C++ compilation speed against gcc 5.4 and clang 3.9</title><link>http://baptiste-wicht.com/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;A week ago, I compared the &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html"&gt;compilation time performance of zapcc against gcc-4.9.3 and clang-3.7&lt;/a&gt;. On debug builds, zapcc was about 2 times faster than gcc and 3 times faster than clang. In this post, I'm going to try some more recent compilers, namely gcc 5.4 and clang 3.9 on the same project. If you want more information on zapcc, read the previous posts, this post will concentrate on results.&lt;/p&gt;
&lt;p&gt;Again, I use my Expression Template Library
(&lt;a class="reference external" href="https://github.com/wichtounet/etl/"&gt;ETL&lt;/a&gt;). This is a purely header-only
library with lots of templates. I'm going to compile the full test cases.&lt;/p&gt;
&lt;p&gt;The results of the two articles are not directly comparable, since they were
obtained on two different computers. The one on which the present results are
done has a less powerful and only 16Go of RAM compared to the 32Go of RAM of my
build machine. Also take into account that that the present results were
obtained on a Desktop machine, there can be some perturbations from background
tasks.&lt;/p&gt;
&lt;p&gt;Just like on the previous results, it does not help using more threads than
physical cores, therefore, the results were only computed on up to 4 cores on
this machine.&lt;/p&gt;
&lt;p&gt;The link time is not taken into account on the results.&lt;/p&gt;
&lt;div class="section" id="debug-build"&gt;
&lt;h2&gt;Debug build&lt;/h2&gt;
&lt;p&gt;Let's start with the result of the debug build.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="55%"&gt;
&lt;col width="15%"&gt;
&lt;col width="15%"&gt;
&lt;col width="15%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;469s&lt;/td&gt;
&lt;td&gt;230s&lt;/td&gt;
&lt;td&gt;130s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9&lt;/td&gt;
&lt;td&gt;710s&lt;/td&gt;
&lt;td&gt;371s&lt;/td&gt;
&lt;td&gt;218s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;214s&lt;/td&gt;
&lt;td&gt;112s&lt;/td&gt;
&lt;td&gt;66s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;3.31&lt;/td&gt;
&lt;td&gt;3.31&lt;/td&gt;
&lt;td&gt;3.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC&lt;/td&gt;
&lt;td&gt;2.19&lt;/td&gt;
&lt;td&gt;2.05&lt;/td&gt;
&lt;td&gt;1.96&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The results are almost the same as the previous test. zapcc is 3.3 times faster
to compile than Clang and around 2 times faster than GCC. It seems that GCC 5.4
is a bit faster than GCC 4.9.3 while clang 3.9 is a bit slower than clang 3.7,
but nothing terribly significant.&lt;/p&gt;
&lt;p&gt;Overall, for debug builds, zapcc can bring a very significant improvement to
your compile times.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="release-build"&gt;
&lt;h2&gt;Release build&lt;/h2&gt;
&lt;p&gt;Let's see what is the status of Release builds. Since the results are comparable
between the numbers of threads, the results here are just for one thread.&lt;/p&gt;
&lt;p&gt;This is more time consuming since a lot of optimizations are enabled and more
features from ETL are enabled as well.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="79%"&gt;
&lt;col width="21%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-5.4.0&lt;/td&gt;
&lt;td&gt;782s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.9&lt;/td&gt;
&lt;td&gt;960s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;640s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;1.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC&lt;/td&gt;
&lt;td&gt;1.22&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;On a release build, the speedups are much less interesting. Nevertheless, they
are still significant. zapcc is still 1.2 times faster than gcc and 1.5 times
faster than clang. Then speedup against clang 3.9 is significantly higher than
it was on my experiment with clang 3.7, it's possible that clang 3.9 is slower
or simply has new optimization passes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The previous conclusion still holds with modern version of compilers: zapcc is
much faster than other compilers on Debug builds of template heavy code. More
than 3 times faster than clang-3.9 and about 2 times faster than gcc-5.4. Since
it's based on clang, there should not be any issue compiling projects that
already compile with a recent clang. Even though the speedups are less
interesting on a release build, it is still significantly, especially compared
against clang.&lt;/p&gt;
&lt;p&gt;I'm really interested in finding out what will be the pricing for zapcc once
out of the beta or if they will be able to get even faster!&lt;/p&gt;
&lt;p&gt;For the comparison with gcc 4.9.3 and clang 3.7, you can have a look at
&lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html"&gt;this article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want more information about zapcc, you can go to the
&lt;a class="reference external" href="https://www.zapcc.com/"&gt;official website of zapcc&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>compiler</category><category>etl</category><category>gcc</category><category>meta</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html</guid><pubDate>Mon, 05 Dec 2016 17:46:09 GMT</pubDate></item><item><title>New design: Faster and mobile compatible</title><link>http://baptiste-wicht.com/posts/2016/11/new-design-faster-and-mobile-compatible.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've finally taken the time to improve the design of the website!&lt;/p&gt;
&lt;p&gt;The site was becoming slower and slower, the design was not responsive at all
and was an horror on mobile.&lt;/p&gt;
&lt;p&gt;I've changed the design to focus more on content and removed superfluous things
such as the Google profile or slow things such as the 3D tag cloud. Moreover,
the design is now responsive again. It was a matter of removing a lot of bad
things I did in the CSS. Instead of having a vertical and an horizontal bars,
I now have only one vertical bar with both the navigation and a bit more
information. With these changes, the design is now also working on mobile phone!
It's about time.&lt;/p&gt;
&lt;p&gt;Moreover, I've also spent quite some time working on the speed of the website.
For this, I've bundled most of the JS and CSS files together and reduced them.
Moreover, the static files are now hosted and cached by CloudFlare. I've also
removed the 3D tag cloud which was quite slow. The Google API usage for the
Google profile badge were also quite slow. Overall, the index page is now really
fast. The article pages are also much faster but it's not perfect, especially
because of Disqus that does tons of requests and redirects everywhere. I've also
got rid of the Disqus ads which were really insignificant in the end. It may
take a while for the ads to disappear according to Disqus.&lt;/p&gt;
&lt;p&gt;I know that it's still not perfect, but I hope that user experience on the blog
is now improved for all readers and now article can be read on mobile normally.
I'll try to continue monitoring the speed and usability of the website to see if
I can improve it further in the coming days.&lt;/p&gt;
&lt;p&gt;If you have any issue on the updated website, don't hesitate to let me know
either by commenting on this post or sending me an email (check the Contact
page).&lt;/p&gt;&lt;/div&gt;</description><category>Performance</category><category>The site</category><category>Web</category><guid>http://baptiste-wicht.com/posts/2016/11/new-design-faster-and-mobile-compatible.html</guid><pubDate>Mon, 28 Nov 2016 06:55:48 GMT</pubDate></item><item><title>zapcc - a faster C++ compiler</title><link>http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Update: For a comparison against more modern compiler versions, you can read: &lt;a class="reference external" href="http://localhost:8000/posts/2016/12/zapcc-cpp-compilation-speed-against-gcc-54-and-clang-39.html"&gt;zapcc C++ compilation speed against gcc 5.4 and clang 3.9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I just joined the private beta program of zapcc. Zapcc is a c++ compiler, based
on Clang which aims at being much faster than other C++ compilers. How they are
doing this is using a caching server that saves some of the compiler structures,
which should speed up compilation a lot. The private beta is free, but once the
compiler is ready, it will be a commercial compiler.&lt;/p&gt;
&lt;p&gt;Every C++ developer knows that compilation time can quickly be an issue when
programs are getting very big and especially when working with template-heavy
code.&lt;/p&gt;
&lt;p&gt;To benchmark this new compiler, I use my Expression Template Library
(&lt;a class="reference external" href="https://github.com/wichtounet/etl/"&gt;ETL&lt;/a&gt;). This is a purely header-only
library with lots of templates. There are lots of test cases which is what I'm
going to compile. I'm going to compare against Clang-3.7 and gcc-4.9.3.&lt;/p&gt;
&lt;p&gt;I have configured zapcc to let is use 2Go RAM per caching server, which is the
maximum allowed. Moreover, I killed the servers before each tests.&lt;/p&gt;
&lt;div class="section" id="debug-build"&gt;
&lt;h2&gt;Debug build&lt;/h2&gt;
&lt;p&gt;Let's start with a debug build. In that configuration, there is no optimization
going on and several of the features of the library (GPU, BLAS, ...) are
disabled. This is the fastest way to compile ETL. I gathered this result on
a 4 core, 8 threads, Intel processor, with an SSD.&lt;/p&gt;
&lt;p&gt;The following table presents the results with different number of threads and
the difference of zapcc compared to the other compilers:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="42%"&gt;
&lt;col width="11%"&gt;
&lt;col width="13%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;th class="head"&gt;-j6&lt;/th&gt;
&lt;th class="head"&gt;-j8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.3&lt;/td&gt;
&lt;td&gt;350s&lt;/td&gt;
&lt;td&gt;185s&lt;/td&gt;
&lt;td&gt;104s&lt;/td&gt;
&lt;td&gt;94s&lt;/td&gt;
&lt;td&gt;91s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.7&lt;/td&gt;
&lt;td&gt;513s&lt;/td&gt;
&lt;td&gt;271s&lt;/td&gt;
&lt;td&gt;153s&lt;/td&gt;
&lt;td&gt;145s&lt;/td&gt;
&lt;td&gt;138s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;158s&lt;/td&gt;
&lt;td&gt;87s&lt;/td&gt;
&lt;td&gt;47s&lt;/td&gt;
&lt;td&gt;44s&lt;/td&gt;
&lt;td&gt;42s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;3.24&lt;/td&gt;
&lt;td&gt;3.103&lt;/td&gt;
&lt;td&gt;3.25&lt;/td&gt;
&lt;td&gt;3.29&lt;/td&gt;
&lt;td&gt;3.28&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC&lt;/td&gt;
&lt;td&gt;2.21&lt;/td&gt;
&lt;td&gt;2.12&lt;/td&gt;
&lt;td&gt;2.21&lt;/td&gt;
&lt;td&gt;2.13&lt;/td&gt;
&lt;td&gt;2.16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The result is pretty clear! zapcc is around &lt;strong&gt;three times faster than Clang&lt;/strong&gt; and around
&lt;strong&gt;two times faster than GCC&lt;/strong&gt;. This is pretty impressive!&lt;/p&gt;
&lt;p&gt;For those that think than Clang is always faster than GCC, keep in mind that
this is not the case for template-heavy code such as this library. In all my
tests, Clang has always been slower and much memory hungrier than GCC on
template-heavy C++ code. And sometimes the difference is very significant.&lt;/p&gt;
&lt;p&gt;Interestingly, we can also see that going past the physical cores is not really
interesting on this computer. On some computer, the speedups are interesting,
but not on this one. Always benchmark!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="release-build"&gt;
&lt;h2&gt;Release build&lt;/h2&gt;
&lt;p&gt;We have seen the results on a debug build, let's now compare on something a bit
more timely, a release build with all options of ETL enabled (GPU, BLAS, ...),
which should make it significantly longer to compile.&lt;/p&gt;
&lt;p&gt;Again, the table:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="40%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;col width="12%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Compiler&lt;/th&gt;
&lt;th class="head"&gt;-j1&lt;/th&gt;
&lt;th class="head"&gt;-j2&lt;/th&gt;
&lt;th class="head"&gt;-j4&lt;/th&gt;
&lt;th class="head"&gt;-j6&lt;/th&gt;
&lt;th class="head"&gt;-j8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;g++-4.9.3&lt;/td&gt;
&lt;td&gt;628s&lt;/td&gt;
&lt;td&gt;336s&lt;/td&gt;
&lt;td&gt;197s&lt;/td&gt;
&lt;td&gt;189s&lt;/td&gt;
&lt;td&gt;184s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;clang++-3.7&lt;/td&gt;
&lt;td&gt;663s&lt;/td&gt;
&lt;td&gt;388s&lt;/td&gt;
&lt;td&gt;215s&lt;/td&gt;
&lt;td&gt;212s&lt;/td&gt;
&lt;td&gt;205s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;zapcc++&lt;/td&gt;
&lt;td&gt;515s&lt;/td&gt;
&lt;td&gt;281s&lt;/td&gt;
&lt;td&gt;173s&lt;/td&gt;
&lt;td&gt;168s&lt;/td&gt;
&lt;td&gt;158s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS Clang&lt;/td&gt;
&lt;td&gt;1.28&lt;/td&gt;
&lt;td&gt;1.38&lt;/td&gt;
&lt;td&gt;1.24&lt;/td&gt;
&lt;td&gt;1.26&lt;/td&gt;
&lt;td&gt;1.29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Speedup VS GCC&lt;/td&gt;
&lt;td&gt;1.21&lt;/td&gt;
&lt;td&gt;1.30&lt;/td&gt;
&lt;td&gt;1.13&lt;/td&gt;
&lt;td&gt;1.12&lt;/td&gt;
&lt;td&gt;1.16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This time, we can see that the difference is much lower. Zapcc is &lt;strong&gt;between 1.2
and 1.4 times faster than Clang&lt;/strong&gt; and &lt;strong&gt;between 1.1 and 1.3 times faster than
GCC&lt;/strong&gt;. This shows that most of the speedups from zapcc are in the front end of
the compiler. This is not a lot but still significant over long builds,
especially if you have few threads where the absolute difference would be
higher.&lt;/p&gt;
&lt;p&gt;We can also observe that Clang is now almost on par with GCC which shows that
optimization is faster in Clang while front and backend is faster in gcc.&lt;/p&gt;
&lt;p&gt;You also have to keep in mind that zapcc memory usage is higher than Clang
because of all the caching. Moreover, the server are still up in between
compilations, so this memory usage stays between builds, which may not be what
you want.&lt;/p&gt;
&lt;p&gt;As for runtime, I have not seem any significant difference in performance
between the clang version and the zapcc. According to the official benchmarks
and documentation, there should not be any difference in that between zapcc and
the version of clang on which zapcc is based.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="incremental-build"&gt;
&lt;h2&gt;Incremental build&lt;/h2&gt;
&lt;p&gt;Normally, zapcc should shine at incremental building, but I was unable to show
any speedup when changing a single without killing the zapcc servers. Maybe
I did something wrong in my usage of zapcc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, we can see that zapcc is always faster than both GCC and Clang,
on my template-heavy library. Moreover, on debug builds, it is much faster than
any of the two compilers, being more than 2 times faster than GCC and more than
3 times faster than clang. This is really great. Moreover, I have not see any
issue with the tool so far, it can seamlessly replace Clang without problem.&lt;/p&gt;
&lt;p&gt;It's a bit weird that you cannot allocate more than 2Go to the zapcc servers.&lt;/p&gt;
&lt;p&gt;For a program, that's really impressive. I hope that they are continuing the
good work and especially that this motivates other compilers to improve the
speed of compilation (especially of templates).&lt;/p&gt;
&lt;p&gt;If you want more information, you can go to the
&lt;a class="reference external" href="https://www.zapcc.com/"&gt;official website of zapcc&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>clang</category><category>compiler</category><category>etl</category><category>gcc</category><category>projects</category><category>zapcc</category><guid>http://baptiste-wicht.com/posts/2016/11/zapcc-a-faster-cpp-compiler.html</guid><pubDate>Sat, 26 Nov 2016 12:17:50 GMT</pubDate></item><item><title>Blazing fast unit test compilation with doctest 1.1</title><link>http://baptiste-wicht.com/posts/2016/09/blazing-fast-unit-test-compilation-with-doctest-11.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;You may remember &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/06/reduce-compilation-time-by-another-16-with-catch.html"&gt;my quest for faster compilation times&lt;/a&gt;. I had made several changes to the Catch test framework macros in order to save some compilation at the expense of my test code looking a bit less nice:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_4a7f4861e17b493fa5a4c1ec565352a3-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;//Before&lt;/span&gt;
&lt;a name="rest_code_4a7f4861e17b493fa5a4c1ec565352a3-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE_EQUALS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;//After&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The first line is a little bit better, but using several optimizations, I was
able to dramatically change the compilation time of the test cases of ETL. In
the end, I don't think that the difference between the two lines justifies the
high overhead in compilation times.&lt;/p&gt;
&lt;div class="section" id="doctest"&gt;
&lt;h2&gt;doctest&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/onqtam/doctest"&gt;doctest&lt;/a&gt; is a framework quite similar to
Catch but that claims to be much lighter. I tested doctest 1.0 early on, but at
this point it was actually slower than Catch and especially slower than my
versions of the macro.&lt;/p&gt;
&lt;p&gt;Today, doctest 1.1 was released with promises of being even lighter than before
and providing several new ways of speeding up compilation. If you want the
results directly, you can take a look at the next section.&lt;/p&gt;
&lt;p&gt;First of all, this new version improved the basic macros to make expression
decomposition faster. When you use the standard REQUIRE macro, the expression is
composed by using several template techniques and operator overloading. This is
really slow to compile. By removing the need for this decomposition, the fast
Catch macros are much faster to compile.&lt;/p&gt;
&lt;p&gt;Moreover, doctest 1.1 also introduces CHECK_EQ that does not any expression
decomposition. This is close to what I did in my macros expect that it is
directly integrated into the framework and preserves all its features. It is
also possible to bypass the expression checking code by using FAST_CHECK_EQ
macro. In that case, the exceptions are not captured. Finally, a new
configuration option is introduced (DOCTEST_CONFIG_SUPER_FAST_ASSERTS) that
removes some features related to automatic debugger breaks. Since I don't use
the debugger features and I don't need to capture exception everywhere (it's
sufficient for me that the test fails completely if an exception is thrown), I'm
more than eager to use these new features.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="results"&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;For evaluation, I have compiled the complete test suite of ETL, with 1 thread,
using gcc 4.9.3 with various different options, starting from Catch to doctest
1.1 with all compilation time features. Here are the results, in seconds:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="29%"&gt;
&lt;col width="12%"&gt;
&lt;col width="14%"&gt;
&lt;col width="22%"&gt;
&lt;col width="23%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Version&lt;/th&gt;
&lt;th class="head"&gt;Time&lt;/th&gt;
&lt;th class="head"&gt;VS Catch&lt;/th&gt;
&lt;th class="head"&gt;VS Fast Catch&lt;/th&gt;
&lt;th class="head"&gt;VS doctest 1.0&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Catch&lt;/td&gt;
&lt;td&gt;724.22&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Fast Catch&lt;/td&gt;
&lt;td&gt;464.52&lt;/td&gt;
&lt;td&gt;-36%&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;doctest 1.0&lt;/td&gt;
&lt;td&gt;871.54&lt;/td&gt;
&lt;td&gt;+20%&lt;/td&gt;
&lt;td&gt;+87%&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;doctest 1.1&lt;/td&gt;
&lt;td&gt;614.67&lt;/td&gt;
&lt;td&gt;-16%&lt;/td&gt;
&lt;td&gt;+32%&lt;/td&gt;
&lt;td&gt;-30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;REQUIRE_EQ&lt;/td&gt;
&lt;td&gt;493.97&lt;/td&gt;
&lt;td&gt;-32%&lt;/td&gt;
&lt;td&gt;+6%&lt;/td&gt;
&lt;td&gt;-43%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;FAST_REQUIRE_EQ&lt;/td&gt;
&lt;td&gt;439.09&lt;/td&gt;
&lt;td&gt;-39%&lt;/td&gt;
&lt;td&gt;-6%&lt;/td&gt;
&lt;td&gt;-50%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;SUPER_FAST_ASSERTS&lt;/td&gt;
&lt;td&gt;411.11&lt;/td&gt;
&lt;td&gt;-43%&lt;/td&gt;
&lt;td&gt;-12%&lt;/td&gt;
&lt;td&gt;-53%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can see, doctest 1.1 is much faster to compile than doctest 1.0! This is
really great news. Moreover, it is already 16% faster than Catch. When all the
features are used, doctest is 12% faster than my stripped down versions of Catch
macros (and 43% faster than Catch standard macros). This is really cool! It
means that I don't have to do any change in the code (no need to strip macros
myself) and I can gain a lot of compilation time compared to the bare Catch
framework.&lt;/p&gt;
&lt;p&gt;I really think the author of doctest did a great job with the new version.
Although this was not of as much interest for me, there are also a lot of
other changes in the new version. You can consult the
&lt;a class="reference external" href="https://github.com/onqtam/doctest/blob/master/CHANGELOG.md"&gt;changelog&lt;/a&gt; if you want more information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Overall, doctest 1.1 is much faster to compile than doctest 1.0. Moreover, it
offers very fast macros for test assertions that are much faster to compile
than Catch versions and even faster than the versions I created myself to reduce
compilation time. I really thing this is a great advance for doctest. When
compiling with all the optimizations, doctest 1.1 saves me 50 seconds in
compilation time compared to the fast version of Catch macro and more than
5 minutes compared to the standard version of Catch macros.&lt;/p&gt;
&lt;p&gt;I'll probably start using doctest on my development machine. For now, I'll keep
Catch as well since I need it to generate the unit test reports in XML format
for Sonarqube. Once this feature appears in doctest, I'll probably drop Catch
from ETL and DLL&lt;/p&gt;
&lt;p&gt;If you need blazing fast compilation times for your unit tests, doctest 1.1 is
probably the way to go.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>Catch</category><category>Compilers</category><category>doctest</category><category>etl</category><category>gcc</category><category>Performances</category><category>Tests</category><category>time</category><guid>http://baptiste-wicht.com/posts/2016/09/blazing-fast-unit-test-compilation-with-doctest-11.html</guid><pubDate>Wed, 21 Sep 2016 19:45:13 GMT</pubDate></item><item><title>Short review of Bullseye Coverage</title><link>http://baptiste-wicht.com/posts/2016/09/short-review-of-bullseye-coverage.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;a class="reference external" href="http://www.bullseye.com/"&gt;Bullseye&lt;/a&gt; is a commercial Code Coverage analyzer.
It is fully-featured with an export to HTML, to XML and even a specific GUI to
see the application.It costs about 800$, with a renewal fee of about 200$ per
year.&lt;/p&gt;
&lt;p&gt;I'm currently using gcov and passing the results to Sonar. This works well, but
there are several problems. First, I need to use gcovr to generate the XML file,
that means two tools. Then, gcov has no way to merge coverage reports. In my
tests of ETL, I have seven different profiles being tested and I need the
overall coverage report. lcov has a merge feature but it is slow as hell (it
takes longer to merge the coverage files than to compile and run the complete
test suite seven times...). For now, I'm using a C++ program that I wrote to
combine the XML files or a Python script that does that, but neither are perfect
and it needs maintenance. Finally, it's impossible to exclude some code from the
coverage report (there is code that isn't meant to be executed (exceptional
code)). For now, I'm using yet another C++ program  that I wrote to do this from
comments in code.&lt;/p&gt;
&lt;p&gt;Bullseye does have all these feature, so I got an evaluation license online and
tried this tool and wrote a short review of it.&lt;/p&gt;
&lt;div class="section" id="usage"&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;The usage is pretty simple. You put the coverage executables in your PATH
variable and activate coverage globally. Then, we you compile, the compiler
calls will be intercepted and a coverage file will be generated. When the
compilation is done, run the program and the coverage measurements will be
filled.&lt;/p&gt;
&lt;p&gt;The coverage results can then be exported to HTML (or XML) or visualized using
the CoverageBrowser tool:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Screenshot of Bullseye main coverage view" src="http://baptiste-wicht.com/images/bullseye_view.png"&gt;
&lt;p class="caption"&gt;The main view of the Bullseye tool code coverage results&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It's a pretty good view of the coverage result. You have a breakdown by folders,
by file, by function and finally by condition. You can view directly the source
code:&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="Screenshot of Bullseye source code coverage view" src="http://baptiste-wicht.com/images/bullseye_source_view.png"&gt;
&lt;p class="caption"&gt;The source view of the Bullseye tool code coverage results&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you want to exclude some code from your coverage reports, you can use
a pragma:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_55976eb300d4430289a8eb936685493f-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;switch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_55976eb300d4430289a8eb936685493f-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;one&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_55976eb300d4430289a8eb936685493f-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;two&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_55976eb300d4430289a8eb936685493f-4"&gt;&lt;/a&gt;    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;three&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_55976eb300d4430289a8eb936685493f-5"&gt;&lt;/a&gt;    &lt;span class="cp"&gt;#pragma BullseyeCoverage off&lt;/span&gt;
&lt;a name="rest_code_55976eb300d4430289a8eb936685493f-6"&gt;&lt;/a&gt;    &lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;abort&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_55976eb300d4430289a8eb936685493f-7"&gt;&lt;/a&gt;    &lt;span class="cp"&gt;#pragma BullseyeCoverage on&lt;/span&gt;
&lt;a name="rest_code_55976eb300d4430289a8eb936685493f-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;So that the condition won't be set as uncovered.&lt;/p&gt;
&lt;p&gt;As for the coverage, it's pretty straightforward. For example:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_1428e56f8688498ea2c9dfed88ace2af-1"&gt;&lt;/a&gt;covmerge -c -ffinal.cov sse.cov avx.cov
&lt;/pre&gt;&lt;p&gt;and it's really fast. Unfortunately, the merging is only done at the function
level, not at the statement or at the condition level. This is a bit
disappointing, especially from a commercial tool. Nevertheless, it works well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;To conclude, Bullseye seems to be a pretty good tool. It has more features than
standard gcov coverage and all features are well integrated together. I have
only covered the features I was interested in, there are plenty of other things
you can look at on the &lt;a class="reference external" href="http://www.bullseye.com/"&gt;official website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, if you don't need the extra features such as the visualizer (or use
something like Sonar for this), or the merge or code excluding, it's probably
not worth paying the price for it. In my case, since the merge is not better
than my C++ tool (both do almost the same and my tool does some basic line
coverage merging as well) and I don't need the visualizer, I won't pay the price
for it. Moreover, they don't have student or open source licensing, therefore,
I'll continue with my complicated toolchain :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>coverage</category><category>test</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2016/09/short-review-of-bullseye-coverage.html</guid><pubDate>Fri, 16 Sep 2016 11:25:44 GMT</pubDate></item><item><title>Expression Templates Library (ETL) 1.0</title><link>http://baptiste-wicht.com/posts/2016/09/expression-templates-library-etl-10.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've just released the first official version of my Expression Templates Library
(ETL for short): The version 1.0.&lt;/p&gt;
&lt;p&gt;Until now, I was using a simple rolling release model, but I think it's now time
to switch to some basic versioning. The project is now at a stable state.&lt;/p&gt;
&lt;p&gt;ETL 1.0 has the following main features:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Smart Expression Templates&lt;/li&gt;
&lt;li&gt;Matrix and vector (runtime-sized and compile-time-sized)&lt;/li&gt;
&lt;li&gt;Simple element-wise operations&lt;/li&gt;
&lt;li&gt;Reductions (sum, mean, max, ...)&lt;/li&gt;
&lt;li&gt;Unary operations (sigmoid, log, exp, abs, ...)&lt;/li&gt;
&lt;li&gt;Matrix multiplication&lt;/li&gt;
&lt;li&gt;Convolution (1D and 2D and higher variations)&lt;/li&gt;
&lt;li&gt;Max Pooling&lt;/li&gt;
&lt;li&gt;Fast Fourrier Transform&lt;/li&gt;
&lt;li&gt;Use of SSE/AVX to speed up operations&lt;/li&gt;
&lt;li&gt;Use of BLAS/MKL/CUBLAS/CUFFT/CUDNN libraries to speed up operations&lt;/li&gt;
&lt;li&gt;Symmetric matrix adapter (experimental)&lt;/li&gt;
&lt;li&gt;Sparse matrix (experimental)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="examples"&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;Here is an example of expressions in ETL:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_54e7d893cf4e45ffa244d8da866d5d71-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;fast_matrix&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_54e7d893cf4e45ffa244d8da866d5d71-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;fast_matrix&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;4.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;4.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_54e7d893cf4e45ffa244d8da866d5d71-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;fast_matrix&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_54e7d893cf4e45ffa244d8da866d5d71-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_54e7d893cf4e45ffa244d8da866d5d71-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;fast_matrix&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;2.111&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Or another I'm using in my neural networks library:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_125eb398a99b46ddaf337a4bc452672d-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;In that case, the vector-matrix multiplication will be executed using a BLAS
kernel (if ETL is configured correclty) and the assignment, the sigmoid and the
addition will be automatically vectorized to use either AVX or SSE depending
on the machine.&lt;/p&gt;
&lt;p&gt;Or with a convolutional layer and a ReLU activation function:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_d8f17e0e51074db8adc58e08b0637c39-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NH1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NH2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;h_a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;conv_4d_valid_flipped&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NC&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NV1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NV2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v_a&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_d8f17e0e51074db8adc58e08b0637c39-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b_rep&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;h_a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This will automatically be computed either with NVIDIA CUDNN (if available) or
with optimized SSE/AVX kernels.&lt;/p&gt;
&lt;p&gt;For more information, you can take a look at the &lt;a class="reference external" href="https://github.com/wichtounet/etl/wiki"&gt;Reference&lt;/a&gt; on the wiki.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="next-version"&gt;
&lt;h2&gt;Next version&lt;/h2&gt;
&lt;p&gt;For the next version, I'll focus on several things:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Improve matrix-matrix multiplication kernels when BLAS is not available. There
is a lot of room for improvement here&lt;/li&gt;
&lt;li&gt;Complete support for symmetric matrices (currently experimental)&lt;/li&gt;
&lt;li&gt;Maybe some new adapters such as Hermitian matrices&lt;/li&gt;
&lt;li&gt;GPU improvements for some operations that can be done entirely on GPU&lt;/li&gt;
&lt;li&gt;New convolution performanceimprovements&lt;/li&gt;
&lt;li&gt;Perhaps more complete parallel support for some implementations&lt;/li&gt;
&lt;li&gt;Drop some compiler support to use full C++14 support&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="download-etl"&gt;
&lt;h2&gt;Download ETL&lt;/h2&gt;
&lt;p&gt;You can download ETL &lt;a class="reference external" href="https://github.com/wichtounet/etl"&gt;on Github&lt;/a&gt;. If you
only interested in the 1.0 version, you can look at the
&lt;a class="reference external" href="https://github.com/wichtounet/etl/releases"&gt;Releases pages&lt;/a&gt; or clone the tag
1.0. There are several branches:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;master&lt;/em&gt; Is the eternal development branch, may not always be stable&lt;/li&gt;
&lt;li&gt;&lt;em&gt;stable&lt;/em&gt; Is a branch always pointing to the last tag, no development here&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the future release, there always will tags pointing to the corresponding
commits. I'm not following the git flow way, I'd rather try to have a more
linear history with one eternal development branch, rather than an useless
develop branch or a load of other branches for releases.&lt;/p&gt;
&lt;p&gt;Don't hesitate to comment this post if you have any comment on this library or
any question. You can also open an Issue on Github if you have a problem using
this library or propose a Pull Request if you have any contribution you'd like
to make to the library.&lt;/p&gt;
&lt;p&gt;Hope this may be useful to some of you :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++14</category><category>Compilers</category><category>etl</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2016/09/expression-templates-library-etl-10.html</guid><pubDate>Fri, 02 Sep 2016 14:12:38 GMT</pubDate></item><item><title>Asgard: Home Automation project</title><link>http://baptiste-wicht.com/posts/2016/08/asgard-home-automation-project.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I have updated my asgard project to make it finally useful for me, so I figured
I'd present the project now.&lt;/p&gt;
&lt;p&gt;Asgard is my project of home automation based on a Raspberry Pi. I started this
project after Ninja Blocks kickstarter company went down and I was left with
useless sensors. So I figured why not have fun creating my own :P I know there
are some other projects out there that are pretty good, but I wanted to do some
more low level stuff for once, so what the hell.&lt;/p&gt;
&lt;p&gt;Of course, everything is written in C++, no surprise here. The project is built
upon a server / drivers architecture. The drivers and the server are talking via
network sockets, so they can be on different machines.  The server is displaying
the data it got on a web interface and also provide a way to trigger actions of
drivers either from the web interface or through the integrated rules engine.
The data are stored in a database, accessed with CPPSqlite3 (probably going to
be replaced by sqlcpp11) and the web server is handled with mongoose (with a c++
interface).&lt;/p&gt;
&lt;p&gt;I must mention that most of the web part of the project was made by a student of
mine, Stéphane Ly, who work on it as part of his study.&lt;/p&gt;
&lt;p&gt;Here is a picture of the Raspberry Pi system (not very pretty ;) ):&lt;/p&gt;
&lt;img alt="Asgard automation system hardware" class="align-center" src="http://baptiste-wicht.com/images/asgard_hardware.jpg"&gt;
&lt;p&gt;I plan to try to fit at least some of it on a nicer box with nicer cables and
such. Moreover, I also plan to add real antennas to the RF transmitter and
receiver, but I haven't received them so far.&lt;/p&gt;
&lt;div class="section" id="sensors"&gt;
&lt;h2&gt;Sensors&lt;/h2&gt;
&lt;p&gt;asgard support several sensors:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;DHT11 Temperature/Humdity Sensor&lt;/li&gt;
&lt;li&gt;WT450 Temperature/Humdity Sensor&lt;/li&gt;
&lt;li&gt;RF Button&lt;/li&gt;
&lt;li&gt;IR Remote&lt;/li&gt;
&lt;li&gt;CPU Temperature Sensor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can see the sensors data displayed on the web interface:&lt;/p&gt;
&lt;img alt="Asgard automation system home page" class="align-center" src="http://baptiste-wicht.com/images/asgard_home.png"&gt;
&lt;/div&gt;
&lt;div class="section" id="actions"&gt;
&lt;h2&gt;Actions&lt;/h2&gt;
&lt;p&gt;There are currently a few actions provided by the drivers:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Wake-On-Lan a computer by its MAC Address&lt;/li&gt;
&lt;li&gt;ITT-1500 smart plugs ON and OFF&lt;/li&gt;
&lt;li&gt;Kodi actions: Pause / Play / Next / Previous on Kodi&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are the rules engine:&lt;/p&gt;
&lt;img alt="Asgard automation system rules page" class="align-center" src="http://baptiste-wicht.com/images/asgard_rules.png"&gt;
&lt;/div&gt;
&lt;div class="section" id="my-home-automation"&gt;
&lt;h2&gt;My home automation&lt;/h2&gt;
&lt;p&gt;I'm currently using this system to monitor the temperature in my appartment.
Nothing great so far because I don't have enough sensors yet. And now, I'm also
using a wireless button to turn on my power socket, wait 2 seconds and then
power on my Kodi Home Theater with wake on lan.&lt;/p&gt;
&lt;p&gt;It's nothing fancy so far, but it's already better than what I had with Ninja
Blocks, except for the ugly hardware ;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="future"&gt;
&lt;h2&gt;Future&lt;/h2&gt;
&lt;p&gt;There are still tons of work on the project and on integration in my home.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;I'm really dissatisfied with the WT450 sensor, I've ordered new Oregon sensors to try to do better.&lt;/li&gt;
&lt;li&gt;I've ordered a few new sensors: Door intrusion detector and motion detector&lt;/li&gt;
&lt;li&gt;The rules system needs to be improve to support multiple conditions&lt;/li&gt;
&lt;li&gt;I plan to add a simple state system to the asgard server&lt;/li&gt;
&lt;li&gt;There are a lot of refactorings necessary in the code and&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, I don't know when I'll work on this again, my work on this project is
pretty episodic to say the least.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="code"&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The code is, as always, available on Github. There are multiple repositories:
&lt;a class="reference external" href="https://github.com/search?q=user%3Awichtounet+asgard"&gt;all asgard repositories&lt;/a&gt;.
It's not that much code for now, about 2000 lines of code, but some of it may be
useful. If you plan to use the system, keep in mind that it was never tested out
of my environment and that there is no documentation so far, but don't hesitate
to open Issues on Github if you have questions or post a comment here.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>asgard</category><category>Personal</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2016/08/asgard-home-automation-project.html</guid><pubDate>Sat, 27 Aug 2016 20:28:16 GMT</pubDate></item><item><title>Update: Thor, Thesis and Publications</title><link>http://baptiste-wicht.com/posts/2016/08/update-thor-thesis-and-publications.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Since it's been a real while since the last post I've written here, I wanted to
write a short status update.&lt;/p&gt;
&lt;p&gt;I had to serve one month in the army, which does not help at all for
productivity :P Since the update to Boost Spirit X3, I haven't worked on my
eddic compiler again, but I've switched back to my operating system project:
thor. I'm having a lot of fun with it again and it's in much better state than
before.&lt;/p&gt;
&lt;p&gt;We also have been very productive on the publication side, with four new
publications this year in various conferences. I'll update the blog when the
proceedings are published. I'll be going to ICANN 2016 and ANNPR 2016 next week
and probably to ICFHR in October. And of course, I'll go back to Meeting C++ in
November :) As for my thesis, it's finally going great, I've started writing
regularly and it's taking form!&lt;/p&gt;
&lt;div class="section" id="thor"&gt;
&lt;h2&gt;Thor&lt;/h2&gt;
&lt;p&gt;My project Thor Operating System now has much more features than before:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;64bit operating system&lt;/li&gt;
&lt;li&gt;Preemptive Multiprocessing&lt;/li&gt;
&lt;li&gt;Keyboard / Mouse driver&lt;/li&gt;
&lt;li&gt;Full ACPI support with ACPICA&lt;/li&gt;
&lt;li&gt;Read/Write ATA driver&lt;/li&gt;
&lt;li&gt;FAT32 file system support&lt;/li&gt;
&lt;li&gt;HPET/RTC/PIT drivers&lt;/li&gt;
&lt;li&gt;Basic PCI support&lt;/li&gt;
&lt;li&gt;Multi stage booting with FAT32&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since last time, I've fixed tons of bug in the system. Although there are still
some culprit, it's much more stable than before. They were a lot of bugs in the
scheduler with loads of race conditions. I hope I've working through most of
them now.&lt;/p&gt;
&lt;p&gt;I'm currently working on the network stack. I'm able to receive and send packets
using the Realtek 8139 card. I have working support for Ethernet, IP and ARP.
I'm currently working on adding ICMP support. I've come to realize that the
hardest part is not to develop the code here but to find a way to test it.
Network in Qemu is a huge pain in the ass to configure. And then, you need tools
to generate some packets or at least answer to packets send by the virtual
machine, and it's really bad... Nevertheless, it's pretty fun overall :)&lt;/p&gt;
&lt;p&gt;Aside from this, I'm also working on a window manager. I'll try to post an
update on this.&lt;/p&gt;
&lt;p&gt;You can take a look at the &lt;a class="reference external" href="https://github.com/wichtounet/thor-os"&gt;thor sources&lt;/a&gt; if you're interested.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="future"&gt;
&lt;h2&gt;Future&lt;/h2&gt;
&lt;p&gt;For the time being, I'll focus my effort on the thor project. I also have some
development to do on my home automation system: &lt;a class="reference external" href="https://github.com/wichtounet/asgard-server"&gt;asgard-server&lt;/a&gt; that I plan to finalize and deploy in a useful way this weekend in my apartment. You can also expect some updates on my deep learning library where I've started work to make it more user-friendly (kind of). I'm also still waiting on the first stable version of doctest for a new comparison with Catch.&lt;/p&gt;
&lt;p&gt;I really want to try to publish again some more posts on the blog. I'll
especially try to publish some more updates about Thor.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>deep learning</category><category>osdev</category><category>thor</category><guid>http://baptiste-wicht.com/posts/2016/08/update-thor-thesis-and-publications.html</guid><pubDate>Tue, 23 Aug 2016 05:40:13 GMT</pubDate></item><item><title>eddic 1.2.4: New Boost Spirit X3 parser and minor cleanups</title><link>http://baptiste-wicht.com/posts/2016/06/eddic-124-new-boost-spirit-x3-parser-and-minor-cleanups.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;After almost 2 years, the new version of eddic (the compiler of the EDDI
programming language) is out! eddic 1.2.4&lt;/p&gt;
&lt;p&gt;I haven't worked a lot on this project in the last years, I have been busy with
my Ph.D. related projects (ETL and DLL), my operating systems, cpm, ... I've
mostly worked on the parser to test the new version of Boost Spirit: X3. This
will be described on the next section, with the other changes in the later
section.&lt;/p&gt;
&lt;div class="section" id="new-boost-spirit-x3"&gt;
&lt;h2&gt;New Boost Spirit X3&lt;/h2&gt;
&lt;p&gt;Boost Spirit X3 is a completely revamped version of Boost Spirit X3. It's aimed
at performance, both at compile-time and at runtime and uses recent features of
modern C++. It's not compatible with Boost Spirit Qi, so you'll most likely
have to rewrite a lot of stuff, in the parser, in the Abstract Syntax Tree
(AST) and in the AST passes as well.&lt;/p&gt;
&lt;p&gt;For reference, I'm using the Boost 1.59 version.&lt;/p&gt;
&lt;div class="section" id="pros"&gt;
&lt;h3&gt;Pros&lt;/h3&gt;
&lt;p&gt;Let's start with the pros.&lt;/p&gt;
&lt;p&gt;First, the runtime performance is definitely better. Parsing all my eddi test
cases and samples, &lt;em&gt;takes 42% less time than with the previous parser&lt;/em&gt;. It is
important to know that the old parser was very optimized, with moves instead of
copies and with a static lexer. You can take a look at &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2013/06/improving-eddic-boost-spirit-parser-performances.html"&gt;this post&lt;/a&gt;
to see what was necessary to optimize the old Qi. I think it's a good result
since the new grammar does not use a lexer (x3 does not support it) and does
not need these optimizations. This improvement really was my objective. I'll
try to push it farther in the future.&lt;/p&gt;
&lt;p&gt;Compile-time performance is also much better. It takes 3 times less time to
compile the new parser (1 minute to around 20 seconds). Moreover, the new
parser is now in only one file, rather than being it necessary to split it all
over the place for compile-time performance. Even though it's not really
important for me, it's still good to have :)&lt;/p&gt;
&lt;p&gt;Especially due to the performance point, I've been able to remove some code,
the lexer, the generated static lexer and the special pointers optimizations of
the AST.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cons"&gt;
&lt;h3&gt;Cons&lt;/h3&gt;
&lt;p&gt;Unfortunately, there are some disadvantages of using the new Spirit X3.&lt;/p&gt;
&lt;p&gt;First, the AST needs to be changed. For good parsing performance, you need to
use x3::variant and x3::forward_ast. This is a major pain in the ass since
x3::variant is much less practical to use than boost::variant. Almost
everything is explicit, meaning uglier code than before, in my opinion.
Moreover, you need to work around x3::forward_ast for boost::get, whereas
boost::recursive_wrapper was working better in that matter. I've had to create
my own wrapper around boost::get in order to be able to use the new tree. In my
opinion, this is clearly a regression.&lt;/p&gt;
&lt;p&gt;Secondly, although X3 was also meant to remove the need to use some hacks in
the grammar, I ended up having more hacks than before. For instance, many AST
node have a fake field in order to make X3 happy. I've still had to use the
horrible eps hack at one place. I've had to create a few more rules in order to
fix type deduction that is working differently than before (worse for me). And
for some reasons, I had to replace some expectations from the grammar to make it
parse correctly. This is a really important regression in my opinion, since it
may make the parsing slower and will make the error message less nice.&lt;/p&gt;
&lt;p&gt;The previous error handling system allowed me to track the file from which an
AST node was parsed from. Although the new error handler is a lot nicer than
the old system, it does not have this feature, so I had to work around this by
using new annotation nodes and a new global handler. Overall, it's probably a
bit worse than before, but makes for lighter AST nodes.&lt;/p&gt;
&lt;p&gt;Finally, for some reason, I haven't been able to use the debug option of the
library (lots of compile time errors). That complicated a bit the debugging of
the parser.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="spirit-x3-or-spirit-qi"&gt;
&lt;h3&gt;Spirit X3 or Spirit Qi ?&lt;/h3&gt;
&lt;p&gt;Overall, I have to say I'm a bit disappointed by Spirit X3. Even though it's
faster at runtime and faster to compile, I was really expecting less issues
with it. What I really did not like was all the changes I had to make because
of x3::variant and x3::forward_ast. Overall, I really don't think it was worth
the trouble porting my parser to Spirit X3.&lt;/p&gt;
&lt;p&gt;If you have a new project, I would still consider using Boost Spirit X3.&lt;/p&gt;
&lt;p&gt;If you have an existing parser, I would probably not advice porting it to X3.
Unless you really have issues with parsing performances (and especially if you
have not already optimized QI parser), it's probably not worth the trouble and
all the time necessary for all the changes.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="other-changes"&gt;
&lt;h2&gt;Other changes&lt;/h2&gt;
&lt;p&gt;The other changes are much more minor. First of all, I've gotten rid of CMake.
This project has really made me hate CMake. I have actually gotten rid of it on
all my projects. I'm now using plain Makefiles and having a much better time
with them. I've also replaced boost Program Options with cxxopts. It's a much
more modern approach for program options parsing. Moreover, it's much more
lightweight and it's header only. Only advantages. There also have been lots of
changes to code (still not very good quality though).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="future"&gt;
&lt;h2&gt;Future&lt;/h2&gt;
&lt;p&gt;eddic was my first real project in C++ and this can be seen in the code and the
organization. The quality of the code is really bad now that I read it again.
Some things are actually terrible :P It's probably normal since I was a
beginner in C++ at the time.&lt;/p&gt;
&lt;p&gt;For the future version of the compiler, I want to clean the code a lot more and
focus on the EDDI language adding new features. Moreover, I'll also get rid of
Boost Test Framework by using Catch (or doctest if it is ready).&lt;/p&gt;
&lt;p&gt;As for now, I'm not sure on which project I'm going to focus. Either I'll
continue working on the compiler or I'll start working again on my operating
system (thor-os) in which I was working on process concurrency (without too
much success :P). I'll probably post next updates on this post in the coming
months.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="download"&gt;
&lt;h2&gt;Download&lt;/h2&gt;
&lt;p&gt;You can find the EDDI Compiler sources on the &lt;a class="reference external" href="https://github.com/wichtounet/eddic"&gt;Github repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The version is available in the &lt;em&gt;v1.2.4&lt;/em&gt; tag available in the GitHub
repository, in the releases pages or directly in the &amp;lt;em&amp;gt;master&amp;lt;/em&amp;gt; branch.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>Boost</category><category>C++</category><category>C++14</category><category>eddic</category><guid>http://baptiste-wicht.com/posts/2016/06/eddic-124-new-boost-spirit-x3-parser-and-minor-cleanups.html</guid><pubDate>Sun, 26 Jun 2016 20:35:04 GMT</pubDate></item><item><title>Reduce Catch tests compilation time by another 16%</title><link>http://baptiste-wicht.com/posts/2016/06/reduce-compilation-time-by-another-16-with-catch.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;No, it's not the same post as two days! I've been able to reduce the compilation
time of my test cases by another 16%!&lt;/p&gt;
&lt;p&gt;Two days ago, I posted an article about how &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2016/05/speedup-compilation-by-13-by-simplifying-unit-test-with-catch.html"&gt;I reduced the compilation time of my tests by 13%&lt;/a&gt;, by bypassing the expression deduction from Catch. I came up with the macro &lt;code class="cpp"&gt;&lt;span class="n"&gt;REQUIRE_EQUALS&lt;/span&gt;&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_4f22369f29c54fedb0fbe1aaea9b5bde-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4f22369f29c54fedb0fbe1aaea9b5bde-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;evaluate_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ResultBuilder&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt; &lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_4f22369f29c54fedb0fbe1aaea9b5bde-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setResultType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_4f22369f29c54fedb0fbe1aaea9b5bde-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setLhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_4f22369f29c54fedb0fbe1aaea9b5bde-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setRhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_4f22369f29c54fedb0fbe1aaea9b5bde-6"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setOp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"=="&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_4f22369f29c54fedb0fbe1aaea9b5bde-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;endExpression&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_4f22369f29c54fedb0fbe1aaea9b5bde-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;react&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_4f22369f29c54fedb0fbe1aaea9b5bde-9"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_4f22369f29c54fedb0fbe1aaea9b5bde-10"&gt;&lt;/a&gt;
&lt;a name="rest_code_4f22369f29c54fedb0fbe1aaea9b5bde-11"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define REQUIRE_EQUALS(lhs, rhs) \&lt;/span&gt;
&lt;a name="rest_code_4f22369f29c54fedb0fbe1aaea9b5bde-12"&gt;&lt;/a&gt;&lt;span class="cp"&gt;    evaluate_result(Catch::ResultBuilder( "REQUIRE", CATCH_INTERNAL_LINEINFO, #lhs " == " #rhs, Catch::ResultDisposition::Normal ), lhs, rhs);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This has the advantage that the left and right hand sides are directly set, not
deduced with templates and operator overloading. This still has exactly the same
features has the original macro, but it is a bit less nice in the test code.
I was quite happy with that optimization, but it turned out, I was not
aggressive enough in my optimizations.&lt;/p&gt;
&lt;p&gt;Even though it seems simple, the macro is still bloated. There are two
constructors calls: &lt;code class="cpp"&gt;&lt;span class="n"&gt;ResultBuilder&lt;/span&gt;&lt;/code&gt; and &lt;code class="cpp"&gt;&lt;span class="n"&gt;SourceLineInfo&lt;/span&gt;&lt;/code&gt; (hidden behind
&lt;code class="cpp"&gt;&lt;span class="n"&gt;CATCH_INTERNAL_LINEINFO&lt;/span&gt;&lt;/code&gt;). That means that if you test case has 100
assertions, 200 constructor calls will need to be processed by the compiler.
Considering that I have some test files with around 400 assertions, this is
a lot of overhead for nothing. Moreover, two parameters have always the same
value, no need to repeat them every time.&lt;/p&gt;
&lt;p&gt;Simplifying the macro to the minimum led me to this:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_5865e159678b45fb8efde9e2735102e0-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_5865e159678b45fb8efde9e2735102e0-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;evaluate_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;char&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;char&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt; &lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_5865e159678b45fb8efde9e2735102e0-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ResultBuilder&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"REQUIRE"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ResultDisposition&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Flags&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_5865e159678b45fb8efde9e2735102e0-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setResultType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_5865e159678b45fb8efde9e2735102e0-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setLhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_5865e159678b45fb8efde9e2735102e0-6"&gt;&lt;/a&gt;    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setRhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_5865e159678b45fb8efde9e2735102e0-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setOp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"=="&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_5865e159678b45fb8efde9e2735102e0-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;endExpression&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_5865e159678b45fb8efde9e2735102e0-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;react&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_5865e159678b45fb8efde9e2735102e0-10"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_5865e159678b45fb8efde9e2735102e0-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_5865e159678b45fb8efde9e2735102e0-12"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define REQUIRE_EQUALS(lhs, rhs) \&lt;/span&gt;
&lt;a name="rest_code_5865e159678b45fb8efde9e2735102e0-13"&gt;&lt;/a&gt;&lt;span class="cp"&gt;    evaluate_result(__FILE__, __LINE__, #lhs " == " #rhs, lhs, rhs);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The macro is now a simple function call. Even though the function is a template
function, it will only be compiled for a few types (&lt;code class="cpp"&gt;&lt;span class="kt"&gt;double&lt;/span&gt;&lt;/code&gt; and
&lt;code class="cpp"&gt;&lt;span class="kt"&gt;float&lt;/span&gt;&lt;/code&gt; in my case), whereas the code of the macro would be unconditionally
compiled for each invocation.&lt;/p&gt;
&lt;p&gt;With this new macro and function, the compilation time went down from 664
seconds to 554 seconds! This is &lt;strong&gt;more than 16% reduction in compilation
time&lt;/strong&gt;. When comparing against the original compilation time (without both
optimizations) of 764 seconds, this is a 27% reduction! And there are absolutely
no difference in features.&lt;/p&gt;
&lt;p&gt;This is a really great result, in my opinion. I don't think this can be cut down
more. However, there is still some room for optimization regarding the includes
that Catch need. Indeed, it is very bloated as well. A new test framework,
&lt;a class="reference external" href="https://github.com/onqtam/doctest"&gt;doctest&lt;/a&gt; follows the same philosophy, but
has much smaller include overhead. Once all the necessary features are in
doctest, I may consider adapting my macros for it and using it in place of Catch
is there is some substantial reduction in compilation time.&lt;/p&gt;
&lt;p&gt;If you want to take a look at the code, you can find the adapted code on &lt;a class="reference external" href="https://github.com/wichtounet/etl/blob/master/test/include/fast_catch.hpp"&gt;Github&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>Catch</category><category>Tests</category><guid>http://baptiste-wicht.com/posts/2016/06/reduce-compilation-time-by-another-16-with-catch.html</guid><pubDate>Wed, 01 Jun 2016 05:28:36 GMT</pubDate></item><item><title>Speed up compilation by 13% by simplifying Catch unit tests</title><link>http://baptiste-wicht.com/posts/2016/05/speedup-compilation-by-13-by-simplifying-unit-test-with-catch.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;In the previous two days, I've working on improving compilation time of my
project Expression Templates Library (ETL). I have been able to reduce the
compilation time of the complete test suite from 794 seconds to 764 seconds
(using only one thread). Trying to get further, I started checking what was
taking the most time in a test case when I saw that the REQUIRE calls of &lt;strong&gt;the
test library were taking a large portion of the compilation time!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I have been &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2014/07/catch-powerful-yet-simple-cpp-test-framework.html"&gt;using Catch as my test framework&lt;/a&gt;
for more than two years and it's really been great overall. It is a great tool,
header-only, fully-featured, XML reporting for Sonar, ... It really has
everything I need from a test framework.&lt;/p&gt;
&lt;p&gt;Contrary to some popular test frameworks that provides ASSERT_EQUALS,
ASSERT_GREATER and all fashion of assert macros, Catch only provides one
version: REQUIRE. For instance:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_90537b9e3d0946049182ce7036a7fa1f-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_90537b9e3d0946049182ce7036a7fa1f-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;5.5&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_90537b9e3d0946049182ce7036a7fa1f-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mf"&gt;22.01f&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The left and right part are detected with some smart template and operator
overloading techniques and this makes for very nice test output in case of
errors, for instance:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_b57f0726778c438293fcdeaf1dec1bf5-1"&gt;&lt;/a&gt;test/src/dyn_matrix.cpp:16: FAILED:
&lt;a name="rest_code_b57f0726778c438293fcdeaf1dec1bf5-2"&gt;&lt;/a&gt;  REQUIRE( test_matrix.rows() == 2UL )
&lt;a name="rest_code_b57f0726778c438293fcdeaf1dec1bf5-3"&gt;&lt;/a&gt;with expansion:
&lt;a name="rest_code_b57f0726778c438293fcdeaf1dec1bf5-4"&gt;&lt;/a&gt;  3 == 2
&lt;/pre&gt;&lt;p&gt;I think this is pretty nice and the tests are really clear. However, &lt;em&gt;it comes
with a cost&lt;/em&gt; and I underestimated this at first.&lt;/p&gt;
&lt;p&gt;To overcome this, I create two new macros (and few other variations)
REQUIRE_EQUALS and REQUIRE_DIRECT that simply bypass Catch deduction of the
expression:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-1"&gt;&lt;/a&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;evaluate_result_direct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ResultBuilder&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setResultType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setLhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="s"&gt;"true"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"false"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setOp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;""&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;endExpression&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-6"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-8"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-9"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;evaluate_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;ResultBuilder&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;L&lt;/span&gt; &lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setResultType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setLhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-12"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setRhs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Catch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rhs&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-13"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setOp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"=="&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-14"&gt;&lt;/a&gt;    &lt;span class="n"&gt;__result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;endExpression&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-15"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-17"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define REQUIRE_DIRECT(value) \&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-18"&gt;&lt;/a&gt;&lt;span class="cp"&gt;    evaluate_result_direct(Catch::ResultBuilder( "REQUIRE", CATCH_INTERNAL_LINEINFO, #value, Catch::ResultDisposition::Normal ), value);&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-19"&gt;&lt;/a&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-20"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define REQUIRE_EQUALS(lhs, rhs) \&lt;/span&gt;
&lt;a name="rest_code_8e6039c2388d4bcaa8cddcf4235b8e7c-21"&gt;&lt;/a&gt;&lt;span class="cp"&gt;    evaluate_result(Catch::ResultBuilder( "REQUIRE", CATCH_INTERNAL_LINEINFO, #lhs " == " #rhs, Catch::ResultDisposition::Normal ), lhs, rhs);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;There is really nothing too special about it, I simply followed the macros and
functions in Catch source code until I found out what to bypass.&lt;/p&gt;
&lt;p&gt;And now, we use them directly:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_5a3aa70991964041a11ae886d35c91a7-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE_DIRECT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;am_i_true&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_5a3aa70991964041a11ae886d35c91a7-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;REQUIRE_EQUALS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This is a bit less nice and it requires to know a few more macros, I admit, but
it turns out to be much faster (and who really cares about the beauty of test
code anyway...). Indeed, the total compilation time of the tests went from 764
seconds to 664 seconds!  This is &lt;strong&gt;a 13% reduction of the compilation time&lt;/strong&gt;!
I really am impressed of the overhead of this technique. I cannot justify this
slowdown just for a bit nicer test code. Finally, the output in case of error
remains exactly the same as before.&lt;/p&gt;
&lt;p&gt;This proves that sometimes the bottlenecks are not where we expect them :)&lt;/p&gt;
&lt;p&gt;If you are interested, you can find the adapted code on &lt;a class="reference external" href="https://github.com/wichtounet/etl/blob/master/test/include/fast_catch.hpp"&gt;Github&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>Tests</category><guid>http://baptiste-wicht.com/posts/2016/05/speedup-compilation-by-13-by-simplifying-unit-test-with-catch.html</guid><pubDate>Wed, 25 May 2016 10:35:16 GMT</pubDate></item><item><title>Simplify Deep Learning Library usage on Linux and Windows!</title><link>http://baptiste-wicht.com/posts/2016/04/simplify-deep-learning-library-usage-on-linux-and-windows.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;No, I'm not dead ;) I've been very busy with my Ph.D (and playing Path of Exile,
let's be honest...) and haven't had time to write something here in a long time.&lt;/p&gt;
&lt;p&gt;Until now, there was too way to use my
&lt;a class="reference external" href="https://github.com/wichtounet/dll/"&gt;Deep Learning Library (DLL)&lt;/a&gt; project:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Write a C++ program that uses the library&lt;/li&gt;
&lt;li&gt;Install DLL and write a configuration file to define your network and the problem to solve&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first version gives you all the features of the tool and allows you to build
exactly what you need. The second version is a bit more limited, but does not
require any C++ knowledge. However, it still does require a recent C++ compiler
and build system.&lt;/p&gt;
&lt;p&gt;Due to the high C++ requirements that are not met by Visual Studio and the fact
that I don't work on Windows, this platform is not supported by the tool. Until
now!&lt;/p&gt;
&lt;p&gt;I've added a third option to use DLL in the form of a Docker image to make the
second option even easier and allow the use of DLL on Windows. All you need is
Docker, which is available on Linux, Mac and Windows. This is still limited to
the second option in that you need to write a configuration describing the
network, but you need to build DLL and don't need to install all its
dependencies.&lt;/p&gt;
&lt;div class="section" id="usage"&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;To install the image, you can simply use &lt;cite&gt;docker pull&lt;/cite&gt;:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_d9d7f3cdd5214b5f81c1e59220bd416e-1"&gt;&lt;/a&gt;docker pull wichtounet/docker-dll
&lt;/pre&gt;&lt;p&gt;Then, to run it, you have to create a folder containing a &lt;cite&gt;dll.conf&lt;/cite&gt; file and
mount in the container at &lt;cite&gt;/dll/data/&lt;/cite&gt;. There are some examples in the
&lt;a class="reference external" href="https://github.com/wichtounet/docker-dll/"&gt;image repository&lt;/a&gt;.  For instance,
on Linux from the cloned repository:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_1c178f92050342ccbf847c0d7707cd4b-1"&gt;&lt;/a&gt;docker run -v &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;pwd&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/rbm_mnist/:/dll/data/ wichtounet/docker-dll
&lt;/pre&gt;&lt;p&gt;or on Windows:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_d280b305de874010a13c49e02cfe8365-1"&gt;&lt;/a&gt;docker run -v /c/Users/Baptiste/rbm_mnist/:/dll/data wichtounet/docker-dll
&lt;/pre&gt;&lt;p&gt;This will automatically run the actions specified in the configuration file and
train your network.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I would really have thought this would be harder, but it turned out that Docker
is a very good solution to deploy multiplatform demo tools :)&lt;/p&gt;
&lt;p&gt;As of now, there is only support for mnist data format in the tool in this
form, but I plan to add basic CSV support as well in the near future.&lt;/p&gt;
&lt;p&gt;I hope that this will help people willing to try the library with a simpler
usage.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>deep learning</category><category>dll</category><category>Linux</category><category>Machine Learning</category><category>projects</category><category>Windows</category><guid>http://baptiste-wicht.com/posts/2016/04/simplify-deep-learning-library-usage-on-linux-and-windows.html</guid><pubDate>Fri, 29 Apr 2016 10:48:18 GMT</pubDate></item><item><title>Use templight and Templar to debug C++ templates</title><link>http://baptiste-wicht.com/posts/2016/02/use-templight-and-templar-to-debug-cpp-templates.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;C++ has some very good tools to debug, profile and analyze source files and executables. This all works well for standard runtime program. But, when you are using templates, you sometimes want these tools to act at compile-time. And at this point the support is much more scarce.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/mikael-s-persson/templight"&gt;templight&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/schulmar/Templar"&gt;Templar&lt;/a&gt; and two tools that are trying to fix this issue.&lt;/p&gt;
&lt;p&gt;From the templight site:&lt;/p&gt;
&lt;blockquote&gt;
Templight is a Clang-based tool to profile the time and memory consumption of template instantiations and to perform interactive debugging sessions to gain introspection into the template instantiation process.&lt;/blockquote&gt;
&lt;p&gt;and Templar is a visualization tool for the traces generated by templight.&lt;/p&gt;
&lt;div class="section" id="installation"&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;Unfortunately, the templight installation is not user-friendly at all. You need to clone the complete LLVM/Clang tree and add templight inside it before compiling the complete clang toolchain. But that is the case for all clang-based tools... You also need to patch clang but that may not be necessary in the future. The complete instructions are available &lt;a class="reference external" href="https://github.com/mikael-s-persson/templight#getting-and-compiling-templight"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The installation of Templar is much more convenient:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_30749aaffda7450699e2850c65b8e384-1"&gt;&lt;/a&gt;git clone https://github.com/schulmar/Templar.git
&lt;a name="rest_code_30749aaffda7450699e2850c65b8e384-2"&gt;&lt;/a&gt;git checkout feature/templight2
&lt;a name="rest_code_30749aaffda7450699e2850c65b8e384-3"&gt;&lt;/a&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; Templar
&lt;a name="rest_code_30749aaffda7450699e2850c65b8e384-4"&gt;&lt;/a&gt;qmake .
&lt;a name="rest_code_30749aaffda7450699e2850c65b8e384-5"&gt;&lt;/a&gt;make
&lt;a name="rest_code_30749aaffda7450699e2850c65b8e384-6"&gt;&lt;/a&gt;sudo make install
&lt;/pre&gt;&lt;p&gt;The branch feature/templight2 has much more features than the master and should support both Qt4 and Qt5, but I have only tested it on Qt4.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example"&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Let's use the class Fibonacci function as an example:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-4"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-5"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-6"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-7"&gt;&lt;/a&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-8"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-9"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-10"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-11"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-13"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-14"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-15"&gt;&lt;/a&gt;    &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;constexpr&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-16"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-18"&gt;&lt;/a&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-19"&gt;&lt;/a&gt;    &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s"&gt;"Fibonacci&amp;lt;5&amp;gt;:"&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;Fibonacci&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_02130979603e4aa6ba2b20bc3064d0dc-20"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Nothing fancy here, we're simply printing the fifth Fibonacci number on the console.&lt;/p&gt;
&lt;p&gt;You can compile it with templight++:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_e4f2e8ff3cb44df58ba2c7c48dec19fc-1"&gt;&lt;/a&gt;templight++ -Xtemplight -profiler -Xtemplight -memory -Xtemplight -ignore-system -std&lt;span class="o"&gt;=&lt;/span&gt;c++14 main.cpp
&lt;/pre&gt;&lt;p&gt;All the templight options starts with -Xtemplight and then you can use any clang++ options. This will generate a &lt;em&gt;a.memory.trace.pbf&lt;/em&gt; file in the current directory. You can then run Templar. use File &amp;gt; Open Trace to open the trace file. This should open a window of this sort:&lt;/p&gt;
&lt;img alt="/images/templar.png" class="align-center" src="http://baptiste-wicht.com/images/templar.png"&gt;
&lt;p&gt;The top-left panel contains the source code of the application, automatically
refreshed whenever you move in the template tree. In the top right, there is
the template instantiation graph. In the bottom left, you'll see a list of list
of files to be able to filter them and in the bottom right, you'll see the list
of templates events. You can sort the list of template events by duration which
is really convenient. You can then select Fibonacci&amp;lt;5&amp;gt; by double clicking it in
the list (once sorted, it should be near the top). This should give you a tree
looking something like that:&lt;/p&gt;
&lt;img alt="/images/templar_tree.png" class="align-center" src="http://baptiste-wicht.com/images/templar_tree.png"&gt;
&lt;p&gt;The edgy nodes are template instantiations and the round nodes are template
memoization. We can directly see that each instantiation was only done once. I
think this graph view is really helpful if you need to debug computation done
at compile time. You can see that that not all nodes are displayed, this is
because there is a limit on the displayed depth. Simply click on Fibonacci&amp;lt;3&amp;gt;
and the remaining nodes will be shown.&lt;/p&gt;
&lt;p&gt;I have already used this tool to find the most time-consuming templates in ETL
an DLL. This is a great tool to indicate where you should focus on improving
the template compile-time. I have also been able to find some unnecessary
instantiations that could be avoided (either with SFINAE or with refactorings).&lt;/p&gt;
&lt;p&gt;templight also contains a fully-fledged debugger for template programs, but I haven't tested it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, I would say that templight and Templar are really helping with
template debugging and profiling. There is a real lack of tools in this domain
and I hope to see more tools of this kind in the future. I hope this will help
you develop template-heavy programs or template metaprograms.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>templates</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2016/02/use-templight-and-templar-to-debug-cpp-templates.html</guid><pubDate>Mon, 08 Feb 2016 07:11:18 GMT</pubDate></item><item><title>Improve DLL and ETL Compile Time further</title><link>http://baptiste-wicht.com/posts/2016/01/improve-dll-and-etl-compile-time-further.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;For a while, the compilation time of my matrix/vector computation library (ETL), based on Expression Templates has become more and more problematic. I've already worked on this problem &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2015/06/how-i-improved-a-bit-compile-time-of-etl.html"&gt;here&lt;/a&gt; and &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2015/06/improve-etl-compile-time-with-precompiled-headers.html"&gt;there&lt;/a&gt;, using some general techniques (pragmas, precompiled headers, header removals and so on). On this post, I'll talk about two major improvements I have been able to do directly in the code.&lt;/p&gt;
&lt;div class="section" id="use-of-static-if"&gt;
&lt;h2&gt;Use of static_if&lt;/h2&gt;
&lt;p&gt;Remember &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2015/07/simulate-static_if-with-c11c14.html"&gt;static_if&lt;/a&gt; ? I was able to use it to really reduce the compile time of DLL.&lt;/p&gt;
&lt;p&gt;I wrote a script to time each test case of the DLL project to find the test cases that took the longest to compile. Once I found the best candidate, I isolated the functions that took the longest to compile. It was quite tedious and I did it by hand, primarily by commenting parts of the code and going deeper and deeper in the code. I was quite suprised to find that a single function call (template function of course ;) ) was responsible for 60% of the compilation time of my candidate test case. The function was instantiating a whole bunch of expression templates (to compute the free energy of several models). The function itself was not really optimizable, but what was really interesting is that this function was only used in some very rare cases and that these cases were known at compile-time :) This was a perfect case to use a static_if. And once the call was inside the static_if, the test case was indeed about 60% faster. &lt;strong&gt;This reduced the overall compilation time of DLL by about 30%&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;This could also of course also have been achieved by using two functions, one with the call, one empty and selected by SFINAE (Substitution Failure Is Not An Error). I prefer the statif_if version since this really shows the intent and hides SFINAE behind nicer syntax.&lt;/p&gt;
&lt;p&gt;I was also able to use static_if at other places in the DLL code to avoid instantiating some templates, but the improvements were much less dramatic (about 1% of the total compilation time). I was very lucky to find a single function that accounted for so much compile time. After some more tests, I concluded that much of the compilation time of DLL was spent compiling the Expression Templates from my ETL library so I decided to delve into ETL code directly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="removal-of-std-async"&gt;
&lt;h2&gt;Removal of std::async&lt;/h2&gt;
&lt;p&gt;The second improvement was very surprising. I was working on improving the compilation of ETL and found out that the sum and average reductions of matrices were dramatically slow, about an order of magnitude slower than standard operations on matrices. In parallel (but the two facts are linked), I also found out another weird fact when splitting a file into 10 parts (the file was comprised of 10 test cases). Compiling the 10 parts separarely (and sequentially, not multiple threads) was about 40% faster than compiling the complete file. There was no swapping so it was not a memory issue. This is not expected. Generally, it is faster to compile a big file than to compile its parts separately. The advantage of smaller files is that you can compile them in parallel and that incremental builds are faster (only compile a small part).&lt;/p&gt;
&lt;p&gt;By elimination, I found out that most of the time was spent inside the function that was dispatching in parallel the work for accumulating the sum of a matrix. Here is the function:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;Functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;AccFunctor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-2"&gt;&lt;/a&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;dispatch_1d_acc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Functor&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AccFunctor&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-6"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-7"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-8"&gt;&lt;/a&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-9"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-10"&gt;&lt;/a&gt;            &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;async&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-11"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-13"&gt;&lt;/a&gt;        &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-14"&gt;&lt;/a&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-15"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="nl"&gt;fut&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-16"&gt;&lt;/a&gt;            &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fut&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-17"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-18"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-19"&gt;&lt;/a&gt;        &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-20"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_2fa441b6da1f4c8ca0ad2d78a54da4ac-21"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;There isn't anything really fancy about this function. This takes one functor that will be done in parallel and one function for accumulation.  It dispatches all the work in batch and then accumulates the results. I tried several things to optimize the compilation time of this function, but nothing worked. The line that was consuming all the time was the std::async line. This function was using std::async because the thread pool that I'm generally using does not support returning values from parallel functors. I decided to use a workaround and use my thread pool and I came out with this version:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;Functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;AccFunctor&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-2"&gt;&lt;/a&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;dispatch_1d_acc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Functor&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AccFunctor&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-5"&gt;&lt;/a&gt;        &lt;span class="n"&gt;cpp&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;default_thread_pool&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-7"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-8"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-10"&gt;&lt;/a&gt;        &lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;sub_functor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-11"&gt;&lt;/a&gt;            &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-12"&gt;&lt;/a&gt;        &lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-13"&gt;&lt;/a&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-14"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-15"&gt;&lt;/a&gt;            &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;do_task&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_functor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-16"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-17"&gt;&lt;/a&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-18"&gt;&lt;/a&gt;        &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-19"&gt;&lt;/a&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-20"&gt;&lt;/a&gt;        &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wait&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-21"&gt;&lt;/a&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-22"&gt;&lt;/a&gt;        &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="nl"&gt;fut&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-23"&gt;&lt;/a&gt;            &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fut&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-24"&gt;&lt;/a&gt;        &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-25"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-26"&gt;&lt;/a&gt;        &lt;span class="n"&gt;acc_functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;functor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-27"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_4a0576255c0b4ce79ab42fcfab770c9d-28"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I simply preallocate space for all the threads and create a new functor calling the input functor and saving its result inside the vector. It is less nice, but it works well. And it compiles MUCH faster. This &lt;strong&gt;reduced the compilation time&lt;/strong&gt; of my biggest test case &lt;strong&gt;by a factor of 8&lt;/strong&gt; (from 344 seconds to 44 seconds). This is really crazy. It also fixed the problem where splitting the test case was faster than big file (it is now twice faster to compile the big files than compiling all the small files separately). &lt;strong&gt;This reduced the total compilation time of dll by about 400%&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As of now, I still have no idea why this makes such a big difference. I have looked at the std::async code, but I haven't found a valid reason for this slowdown. If someone has any idea, I'd be very glad to discuss in the comments below.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="improving-the-template-instantiation-tree"&gt;
&lt;h2&gt;Improving the template instantiation tree&lt;/h2&gt;
&lt;p&gt;I recently discovered the templight tool that is a profiler for templates (pretty cool). After some time, I was able to build it and use it on ETL. For now, I haven't been able to reduce compile time a lot, but I have been able to reduce the template instantiation tree a lot seeing that some instantiations were completely useless and I optimized the code to remove them.&lt;/p&gt;
&lt;p&gt;I won't be go into much details here because I plan to write a post on this subject in the coming days.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, I would say that it is pretty hard to improve the compile time of complex C++ programs once you have gone through all the standard methods. However, I was very happy to found that &lt;strong&gt;two optimizations in the source code reduced the overall compilation of DLL by almost 500%&lt;/strong&gt;. I will continue working on this, but for now, the compilation time is much more reasonable.&lt;/p&gt;
&lt;p&gt;I hope the two main facts in this article were interesting. If you have similar experience, comments or ideas for further improvements, I'd be glad to discuss them with you in the comments :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>Compilers</category><category>dll</category><category>etl</category><category>gcc</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2016/01/improve-dll-and-etl-compile-time-further.html</guid><pubDate>Fri, 29 Jan 2016 16:02:34 GMT</pubDate></item><item><title>Detect overflows and more in Java with COJAC</title><link>http://baptiste-wicht.com/posts/2015/11/detect-overflows-and-more-in-java-with-cojac.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Back at school, I worked on the COJAC project to detect numeric overflow in
Java programs automatically. Since then, this project has evolved a lot and has
now more features:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;It can detect integer overflows&lt;/li&gt;
&lt;li&gt;Detect smearing and cancellation with float and double types&lt;/li&gt;
&lt;li&gt;Detect NaN and Infinite results from computations&lt;/li&gt;
&lt;li&gt;Detect offending type casting&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Moreover, all these features are available without any recompilation of your
program. You simply add an argument to the invocation of the Java virtual
machine and all these errors will be detected for you automatically!&lt;/p&gt;
&lt;p&gt;Frédéric Bapst, the person in charge of the project has recently published two
videos about the project, don't hesitate to check them out:&lt;/p&gt;
&lt;p&gt;The first video presents the automatic analysis features of the tool:&lt;/p&gt;
&lt;iframe width="425" height="344" src="https://www.youtube.com/embed/DqAFQfbWZOU?rel=0&amp;amp;hd=1&amp;amp;wmode=transparent"&gt;&lt;/iframe&gt;&lt;p&gt;And the second presents the numeric wrapper features of the tool for even more
features:&lt;/p&gt;
&lt;iframe width="425" height="344" src="https://www.youtube.com/embed/4x9mJEFjcGc?rel=0&amp;amp;hd=1&amp;amp;wmode=transparent"&gt;&lt;/iframe&gt;&lt;p&gt;If you have any question related to the project, you can add a comment to this
page or contact me directly be email.&lt;/p&gt;
&lt;p&gt;If you want more information on the project you can also check out its
repository on Github: &lt;a class="reference external" href="https://github.com/Cojac/Cojac"&gt;https://github.com/Cojac/Cojac&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</description><category>COJAC</category><category>Java</category><category>projects</category><guid>http://baptiste-wicht.com/posts/2015/11/detect-overflows-and-more-in-java-with-cojac.html</guid><pubDate>Fri, 27 Nov 2015 13:18:52 GMT</pubDate></item><item><title>Aggregator Plugin : Display global metrics in Sonarqube</title><link>http://baptiste-wicht.com/posts/2015/11/aggregator-plugin-display-global-metrics-in-sonarqube.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Recently, I wanted to know how many lines of code I had on my Sonar server with all my C++ projects. Sonarsource proposes a commercial plugins (Views) that allows to do that (and much more...), but I didn't wanted to pay thousands of dollars simply to get a total of my lines of code, therefore I wrote a very  simple Sonar plugin to compute some global metrics.&lt;/p&gt;
&lt;p&gt;This plugin is very simple, it only provides a global widgets that aggregates some stats over all your projects. For instance, here is the results on my Sonar server:&lt;/p&gt;
&lt;img alt="/images/aggregator_widget.png" class="align-center" src="http://baptiste-wicht.com/images/aggregator_widget.png"&gt;
&lt;p&gt;The plugin is freely available on Github: &lt;a class="reference external" href="https://github.com/wichtounet/aggregator-plugin"&gt;https://github.com/wichtounet/aggregator-plugin&lt;/a&gt; . However, it has only be tested on my Sonar server (4.5.2) and it is my first Sonar plugin, so it may not work everywhere. If you experience issues, don't hesitate to open an issue on Github or to propose a Pull Request.&lt;/p&gt;
&lt;p&gt;You can install the plugin by putting the .jar file (from the Github Releases page) into your sonar/extensions/plugins directory and restart Sonar. You should then have access to a new global widget that you can add to a dashboard.&lt;/p&gt;
&lt;p&gt;I hope this plugin helps some of you.&lt;/p&gt;&lt;/div&gt;</description><category>projects</category><category>Releases</category><category>Sonar</category><guid>http://baptiste-wicht.com/posts/2015/11/aggregator-plugin-display-global-metrics-in-sonarqube.html</guid><pubDate>Fri, 20 Nov 2015 10:37:55 GMT</pubDate></item><item><title>Upgrade to Nikola 7</title><link>http://baptiste-wicht.com/posts/2015/11/upgrade-to-nikola-7.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;I've finally taken the time to upgrade the website to Nikola 7 (it is about time, I know...).&lt;/p&gt;
&lt;p&gt;The migration worked flawlessly, I simply had to update configuration to migrate deprecated and renamed tags and it worked really well. I also had to add a comma to the COMPILERS list because of the use of Python 3.3 now.&lt;/p&gt;
&lt;p&gt;As you may have seen, I haven't posted in a while. I had quite some work for my thesis as well as for the courses I give at my school and I started playing Path Of Exile with took quite a bit of my free time :) I'll try to give some updates on the project I'm working on to make this blog live again.&lt;/p&gt;&lt;/div&gt;</description><category>Nikola</category><category>Personal</category><category>The site</category><guid>http://baptiste-wicht.com/posts/2015/11/upgrade-to-nikola-7.html</guid><pubDate>Fri, 20 Nov 2015 09:56:32 GMT</pubDate></item><item><title>Simulate static_if with C++11/C++14</title><link>http://baptiste-wicht.com/posts/2015/07/simulate-static_if-with-c11c14.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;If you are doing a lot of template metaprogramming and other template magic stuff, you are likely to miss a &lt;code&gt;static_if&lt;/code&gt; in the language. Unfortunately, it didn't make the cut for C++11 and it seems unlikely that it will make it in C++17.&lt;/p&gt;
&lt;div class="section" id="static-if"&gt;
&lt;h2&gt;static_if&lt;/h2&gt;
&lt;p&gt;As its name indicates, &lt;code&gt;static_if&lt;/code&gt; is an if statement but that is done at compile-time. At first, it could seem that the main point is performance, but that is not the case. With recent compilers, if you have an if statement with a compile-time constant, it will never be executed at runtime and only the correct branch will be included in the final executable code. However, even if the compiler knows that a branch will never be executed, it still has to ensure that this branch compiles. This is not the case with &lt;code&gt;static_if&lt;/code&gt;. With &lt;code&gt;static_if&lt;/code&gt;, only the valid branch is compiled, the other can contains invalid code. The most common reason to use a &lt;code&gt;static_if&lt;/code&gt; is inside a template where you perform a test on a template argument and execute code based on this test. &lt;code&gt;static_if&lt;/code&gt; has another advantage on standard if. Since only one branch is instantiated, it may save quite a lot of compile-time.&lt;/p&gt;
&lt;p&gt;Let's say we have to write a template function that, if the template argument is a string, removes the last character of the string argument, otherwise decrement the argument (I know, stupid example, but simple). With &lt;code&gt;static_if&lt;/code&gt;, you can write it like this:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_47ebd636e4a64937b446cfac3d221bdb-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_47ebd636e4a64937b446cfac3d221bdb-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_47ebd636e4a64937b446cfac3d221bdb-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;static_if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_47ebd636e4a64937b446cfac3d221bdb-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop_back&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_47ebd636e4a64937b446cfac3d221bdb-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_47ebd636e4a64937b446cfac3d221bdb-6"&gt;&lt;/a&gt;        &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_47ebd636e4a64937b446cfac3d221bdb-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_47ebd636e4a64937b446cfac3d221bdb-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I think it is quite elegant.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-problem"&gt;
&lt;h2&gt;The problem&lt;/h2&gt;
&lt;p&gt;Some may think, that we could do the same with C++ standard if statement:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_b23585ebdb4444588972779451678a9f-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_b23585ebdb4444588972779451678a9f-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b23585ebdb4444588972779451678a9f-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b23585ebdb4444588972779451678a9f-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop_back&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_b23585ebdb4444588972779451678a9f-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b23585ebdb4444588972779451678a9f-6"&gt;&lt;/a&gt;        &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b23585ebdb4444588972779451678a9f-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b23585ebdb4444588972779451678a9f-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;However, this won't work. This template cannot be instantiated for &lt;code&gt;std::string&lt;/code&gt; since it doesn't have an operator -- and it cannot be instantiated for int since it doesn't have a &lt;code&gt;pop_back()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;There are two solutions in plain C++: specialization and SFINAE. Let's start with specialization:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_84ae10ee9abe4489a57e450b40a99886-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_84ae10ee9abe4489a57e450b40a99886-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_84ae10ee9abe4489a57e450b40a99886-3"&gt;&lt;/a&gt;    &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_84ae10ee9abe4489a57e450b40a99886-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_84ae10ee9abe4489a57e450b40a99886-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_84ae10ee9abe4489a57e450b40a99886-6"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_84ae10ee9abe4489a57e450b40a99886-7"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_84ae10ee9abe4489a57e450b40a99886-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop_back&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_84ae10ee9abe4489a57e450b40a99886-9"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;We do a specialization for &lt;code&gt;std::string&lt;/code&gt; case so that in the general case it uses -- and in the &lt;code&gt;std::string&lt;/code&gt; case, it uses &lt;code&gt;pop_back()&lt;/code&gt;. And the SFINAE version:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_10f434a264ad4c61a4840b316de2d396-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;enable_if_t&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;!&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_10f434a264ad4c61a4840b316de2d396-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_10f434a264ad4c61a4840b316de2d396-3"&gt;&lt;/a&gt;    &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_10f434a264ad4c61a4840b316de2d396-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_10f434a264ad4c61a4840b316de2d396-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_10f434a264ad4c61a4840b316de2d396-6"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;enable_if_t&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_10f434a264ad4c61a4840b316de2d396-7"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_10f434a264ad4c61a4840b316de2d396-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop_back&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_10f434a264ad4c61a4840b316de2d396-9"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The first function is enabled when the type is not a &lt;code&gt;std::string&lt;/code&gt; and the second function is enabled when the type is a &lt;code&gt;std::string&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Both solutions needs two functions to make it work. In this particular case, specialization is easier since the condition states exactly one type. If the condition was more complex for instance testing that a constant inside the type is equals to some value, we could only do it with SFINAE.&lt;/p&gt;
&lt;p&gt;Even if both solutions work, both solutions are more complicated than the static_if version and both solutions are creating more functions than what should be necessary.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="one-solution"&gt;
&lt;h2&gt;One solution&lt;/h2&gt;
&lt;p&gt;There is one way to emulate a kind of &lt;code&gt;static_if&lt;/code&gt; with C++14 generic lambdas. It is kind of using anonymous template function to emulate what we did with the previous solutions but does it behind the scene. Here the code I'm using for this emulation:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;namespace&lt;/span&gt; &lt;span class="n"&gt;static_if_detail&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-3"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;identity&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-4"&gt;&lt;/a&gt;    &lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="k"&gt;operator&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-6"&gt;&lt;/a&gt;        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;forward&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-9"&gt;&lt;/a&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-10"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;Cond&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-11"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;statement&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-12"&gt;&lt;/a&gt;    &lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-13"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-14"&gt;&lt;/a&gt;        &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;identity&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-15"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-16"&gt;&lt;/a&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-17"&gt;&lt;/a&gt;    &lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-18"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;else_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="p"&gt;){}&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-19"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-20"&gt;&lt;/a&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-21"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-22"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;statement&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-23"&gt;&lt;/a&gt;    &lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-24"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="p"&gt;){}&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-25"&gt;&lt;/a&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-26"&gt;&lt;/a&gt;    &lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-27"&gt;&lt;/a&gt;    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;else_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-28"&gt;&lt;/a&gt;        &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;identity&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-29"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-30"&gt;&lt;/a&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-31"&gt;&lt;/a&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-32"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="c1"&gt;//end of namespace static_if_detail&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-33"&gt;&lt;/a&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-34"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;Cond&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-35"&gt;&lt;/a&gt;&lt;span class="n"&gt;static_if_detail&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;statement&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Cond&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;static_if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-36"&gt;&lt;/a&gt;    &lt;span class="n"&gt;static_if_detail&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;statement&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Cond&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;if_&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-37"&gt;&lt;/a&gt;    &lt;span class="n"&gt;if_&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;then&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-38"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;if_&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_b3c351fd6d504b5f8e3a5c0c97e73727-39"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Note: I got the idea (and most of the code) from the &lt;a class="reference external" href="http://lists.boost.org/Archives/boost/2014/08/216607.php"&gt;Boost Mailing List&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The condition is passed a non-type template parameter and the code for the branch is a passed a generic lambda functor. The &lt;code&gt;static_if&lt;/code&gt; function returns a statement structure. We could avoid returning a struct and directly execute, or not, the functor based on the condition, but using a structure allows for the &lt;code&gt;else_&lt;/code&gt; part which may be practical. The structure &lt;code&gt;statement&lt;/code&gt; is specialized on the condition. If the condition is true, the right part will execute the functor while the false part will not execute anything. The specialization when the condition is false willl do the contrary. A special point here is the use of the identity function. The function is passed to the lambda. The user can then use this function to make non-dependent type dependent. This is necessary if we want to call functions on non-dependent types and these functions may not exist. For instance, you may want to call a function on &lt;code&gt;this&lt;/code&gt;, which is not a dependent type.&lt;/p&gt;
&lt;p&gt;Here is how the code will look using this solution:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_ba8c7284a1b144b3a2b2b44591ebb7ad-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_ba8c7284a1b144b3a2b2b44591ebb7ad-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;decrement_kindof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_ba8c7284a1b144b3a2b2b44591ebb7ad-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;static_if&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;is_same&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_ba8c7284a1b144b3a2b2b44591ebb7ad-4"&gt;&lt;/a&gt;        &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;pop_back&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;a name="rest_code_ba8c7284a1b144b3a2b2b44591ebb7ad-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;}).&lt;/span&gt;&lt;span class="n"&gt;else_&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_ba8c7284a1b144b3a2b2b44591ebb7ad-6"&gt;&lt;/a&gt;        &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_ba8c7284a1b144b3a2b2b44591ebb7ad-7"&gt;&lt;/a&gt;    &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_ba8c7284a1b144b3a2b2b44591ebb7ad-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;It is not as elegant as the "real" &lt;code&gt;static_if&lt;/code&gt; version, but it is closer than the other solutions.&lt;/p&gt;
&lt;p&gt;If you don't use the lazy identity function (f), it still works on g++, but not on clang for some reasons.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We saw that there are some solutions to emulate &lt;code&gt;static_if&lt;/code&gt; in C++ that you may use to make the code easier to read. I'm personally using this trick on branches with few lines of code and when I don't have to use the identity function too much, otherwise it is cleaner to use standard SFINAE functions to do the job. When you only have a if and no else, this trick is even better because that is where it saves the more code.&lt;/p&gt;
&lt;p&gt;I hope this can be useful to some of you ;)&lt;/p&gt;
&lt;p&gt;You can find my implementation &lt;a class="reference external" href="https://github.com/wichtounet/cpp_utils/blob/master/static_if.hpp"&gt;on Github&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++11</category><category>C++14</category><guid>http://baptiste-wicht.com/posts/2015/07/simulate-static_if-with-c11c14.html</guid><pubDate>Sun, 12 Jul 2015 13:23:34 GMT</pubDate></item><item><title>Improve ETL compile-time with Precompiled Headers</title><link>http://baptiste-wicht.com/posts/2015/06/improve-etl-compile-time-with-precompiled-headers.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Very recently, I started trying to improve the compile-time of the ETL test suite. While not critical, it is always better to have tests that compile as fast as possible. In a &lt;a class="reference external" href="http://baptiste-wicht.com/posts/2015/06/how-i-improved-a-bit-compile-time-of-etl.html"&gt;previous post&lt;/a&gt;, I was able to improve the time a bit by improve the makefile, using pragra once and avoiding &lt;cite&gt;&amp;lt;iostream&amp;gt;&lt;/cite&gt; headers. With these techniques, I reduced the compile-time from 87.5 to 84.1, which is not bad, but not as good as I would have expected.&lt;/p&gt;
&lt;p&gt;In the previous, I had not tried to use Precompiled Headers (PCH) to improve the compile time, so I thought it would be a good time to do it.&lt;/p&gt;
&lt;div class="section" id="precompiled-headers"&gt;
&lt;h2&gt;Precompiled Headers&lt;/h2&gt;
&lt;p&gt;Precompiled Headers are an option of the compiler, where one header gets compiled. Normally, you only compile source files into object files, but you can also compile headers, although it is not the same thing. When a compiler compiles a header, it can do a lot of preprocessing (macros, includes, AST, symbols) and then store all the results into a precompiled header file. Once you compile the source files, the compiler will try to use the precompiled header file instead of the real header file. Of course, this can breaks the C++ standard since with that a header can not have different behaviour based on macros for instance. For these reasons (and probably implementation reasons as well), precompiled headers are really limited.&lt;/p&gt;
&lt;p&gt;If we take the case of G++, G++ will consider the precompiled header file instead of the standard header only if (for a complete list, take a look at the GCC docs):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The same compilation flags are the same between the two compilations&lt;/li&gt;
&lt;li&gt;The same compiler binary is used for the compilations&lt;/li&gt;
&lt;li&gt;Only one precompiled header can be used in each compilation&lt;/li&gt;
&lt;li&gt;The same macros must be defined&lt;/li&gt;
&lt;li&gt;The include of the header must be before every possible C/C++ token&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If all these conditions are met and you try to &lt;cite&gt;#include "header.hpp&lt;/cite&gt; and there is a header.hpp.gch (the precompiled file) available in the search path, then the precompiled header will be taken instead of the standard one.&lt;/p&gt;
&lt;p&gt;With clang, it is a bit different because the precompiled header cannot be included automatically, but has to be included explicitely in the source code, meaning you have to modify your code for this technique to work. This is a bad thing in my opinion, you never should have to modify your code to profit from a compiler feature. This is why I haven't used and don't plan to use precompiled headers with clang.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-to"&gt;
&lt;h2&gt;How-to&lt;/h2&gt;
&lt;p&gt;Once you know all the conditions for a precompiled header to be automatically included, it is quite straightforward to use them.&lt;/p&gt;
&lt;p&gt;To generate a PCH file is easy:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
g++ options header.hpp
&lt;/pre&gt;
&lt;p&gt;This will generate header.hpp.gch. When you compile your source file using header.hpp, you don't have anything to do, you just have to compile it as usually and if all the conditions are met, the PCH file will be used instead of the other header.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="results-and-conclusion"&gt;
&lt;h2&gt;Results and conclusion&lt;/h2&gt;
&lt;p&gt;I added precompiled header support into my &lt;a class="reference external" href="https://github.com/wichtounet/make-utils"&gt;make-utils&lt;/a&gt; collection of Makefile utilities and tested it on ETL. I have precompiled a header that itself included Catch and ETL. Almost all test files are including this header. With this change, I went from 84 seconds to 78seconds. Headers are taking 1.5seconds to be precompiled. This is a nice result I think. If your application is not as template-heavy as mine or if you have more source files, you should expect better improvements.&lt;/p&gt;
&lt;p&gt;To conclude, even if precompiled headers are a sound way to reduce compile-time, they are really limited to some cases. I'm not a fan of the feature overally. It is not portable between compilers and not standard. Anyway, if you are really in need of saving some time, you should not hesitate too much ;)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>Compilers</category><category>etl</category><category>gcc</category><category>Performances</category><guid>http://baptiste-wicht.com/posts/2015/06/improve-etl-compile-time-with-precompiled-headers.html</guid><pubDate>Sat, 20 Jun 2015 13:08:31 GMT</pubDate></item><item><title>How I improved (a bit) compile time of ETL ?</title><link>http://baptiste-wicht.com/posts/2015/06/how-i-improved-a-bit-compile-time-of-etl.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Recently I read several articles about C++ and compile time and I wondered if I could improve the compile time of my Expression Template Library (ETL) project. ETL is a header-only and template-heavy library. I'm not going to the change the design completely or to use type erasure techniques to reduce the compile time, ETL is all about performance.&lt;/p&gt;
&lt;p&gt;As a disclaimer, don't expect fancy results from this post, I haven't been able to reduce compile time a lot, but I still wanted to share my experience.&lt;/p&gt;
&lt;p&gt;I've used g++-4.9.2 to perform these tests.&lt;/p&gt;
&lt;p&gt;I'm compiling the complete test suite (around 6900 source lines of codes in 36 files) in release mode. Each test file includes the ETL (around 10K SLOC). Each test is run with 8 threads (make -j8). For each result, I have run a complete build 5 times and taken the best result as the final result. Everything is run on a SSD and I have more than enough RAM to handle all the compilation in parallel.&lt;/p&gt;
&lt;p&gt;The reference build time was 87.5 seconds.&lt;/p&gt;
&lt;div class="section" id="compile-and-generate-dependency-files-at-the-same-time"&gt;
&lt;h2&gt;Compile and generate dependency files at the same time&lt;/h2&gt;
&lt;p&gt;To help write my makefiles, I'm using a set of functions that I have written. This includes automatic dependency generation using -MM -MT options of the compiler. Until now, I had two targets, one to compile the cpp file into the object file and another one to generate the dependency file. I recently saw that compilers were able to do both at the same time! Clang, G++ and the Intel compiler all have a -MD -MF options that lets you generate the dependency file at the same time you compile your file, saving you at least one read of the file.&lt;/p&gt;
&lt;p&gt;My compilation rule in my makefile has now become:&lt;/p&gt;
&lt;pre class="code makefile"&gt;&lt;a name="rest_code_9a510b6f16d643d4a88cdef39fe8dc2d-1"&gt;&lt;/a&gt;&lt;span class="nf"&gt;release/$(1)/%.cpp.o&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;1&lt;span class="k"&gt;)&lt;/span&gt;/%.&lt;span class="n"&gt;cpp&lt;/span&gt;
&lt;a name="rest_code_9a510b6f16d643d4a88cdef39fe8dc2d-2"&gt;&lt;/a&gt;    @ mkdir -p release/&lt;span class="k"&gt;$(&lt;/span&gt;1&lt;span class="k"&gt;)&lt;/span&gt;/
&lt;a name="rest_code_9a510b6f16d643d4a88cdef39fe8dc2d-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;$(&lt;/span&gt;CXX&lt;span class="k"&gt;)&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;CXX_FLAGS&lt;span class="k"&gt;)&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;RELEASE_FLAGS&lt;span class="k"&gt;)&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;2&lt;span class="k"&gt;)&lt;/span&gt; -MD -MF release/&lt;span class="k"&gt;$(&lt;/span&gt;1&lt;span class="k"&gt;)&lt;/span&gt;/&lt;span class="nv"&gt;$$&lt;/span&gt;*.cpp.d -o release/&lt;span class="k"&gt;$(&lt;/span&gt;1&lt;span class="k"&gt;)&lt;/span&gt;/&lt;span class="nv"&gt;$$&lt;/span&gt;*.cpp.o -c &lt;span class="k"&gt;$(&lt;/span&gt;1&lt;span class="k"&gt;)&lt;/span&gt;/&lt;span class="nv"&gt;$$&lt;/span&gt;*.cpp
&lt;a name="rest_code_9a510b6f16d643d4a88cdef39fe8dc2d-4"&gt;&lt;/a&gt;    @ sed -i -e &lt;span class="s1"&gt;'s@^\(.*\)\.o:@\1.d \1.o:@'&lt;/span&gt; release/&lt;span class="k"&gt;$(&lt;/span&gt;1&lt;span class="k"&gt;)&lt;/span&gt;/&lt;span class="nv"&gt;$$&lt;/span&gt;*.cpp.d
&lt;/pre&gt;&lt;p&gt;This reduced the compilation time to 86.8 seconds. Not that much reduction, but it still is quite nice to know that. I would have expected this to reduce more the compile time.&lt;/p&gt;
&lt;div class="section" id="use-pragma-once"&gt;
&lt;h3&gt;Use #pragma once&lt;/h3&gt;
&lt;p&gt;Normally, I'm not a fan of #pragma since it is not standard, but for now ETL only supports three compilers and only very recent of them, so I have the guarantee that #pragma once is available, so what the hell!&lt;/p&gt;
&lt;p&gt;I've replaced all the include guards by single #pragma once directives.&lt;/p&gt;
&lt;p&gt;Again, the results are not impressive, this reduced the compile time to 86.2 seconds. I would only advise to use this if you are sure of the compilers you want to support and you need the extra time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="avoid-iostream"&gt;
&lt;h3&gt;Avoid &amp;lt;iostream&amp;gt;&lt;/h3&gt;
&lt;p&gt;I've read that the &amp;lt;iostream&amp;gt; header was one of the slowest to compile of the STL. It is only one that is included several times in my headers only for stream operators and it turns out that there is a &amp;lt;iosfwd&amp;gt; header that forward declares a lot of things from the &amp;lt;iostream&amp;gt; and other I/O headers.&lt;/p&gt;
&lt;p&gt;By replacing all &amp;lt;iostream&amp;gt; include by &amp;lt;iosfwd&amp;gt;, compile time has gone down to 84.1 seconds.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;By using the three techniques, I've reduced the compile time from 87.5 to 84.1 seconds. I would have honestly hoped for more improvements, but this is a already a good start.&lt;/p&gt;
&lt;p&gt;As a side note, clang compile time is 45.2 seconds under the same conditions (was 46.2 seconds before the optimizations). It is really much faster :) I'm still using GCC a lot since in several cases, it does generate much better code and in average, the generated code if faster (on my benchmarks at least). I don't have the numbers for icc, but icc is definitely the slowest of the three. When I have it available (at work), I use for release build before running something. The generated executables are generally faster (I only use Intel processors) and sometimes the difference can be quite important.&lt;/p&gt;
&lt;p&gt;If you have ideas to reduce further the compile time on this test case, I'd be glad to hear them and put them to the test.&lt;/p&gt;
&lt;p&gt;I hope that this small experience would be helpful to some of you :)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="other-techniques"&gt;
&lt;h3&gt;Other techniques&lt;/h3&gt;
&lt;p&gt;There are several other techniques that you can use to reduce compile time:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Precompiled Headers are supported by both Clang and GCC, altough not in a compatible. I haven't tested this in a while, but it is quite effective and a very interesting technique. The main problem with this is that is not standard and not compatible between compilers. But it probably is the most efficient techniques when you have lots of headers and lots of templates as in my case.&lt;/li&gt;
&lt;li&gt;Unity builds can make full rebuild much faster. I personally don't like unity builds especially because it is only really good for full builds and you generally don't do full rebuilds that much (I know, I know, this is also the test done in this article :) ). Moreover, it also sucks at doing parallel builds.&lt;/li&gt;
&lt;li&gt;Pimpl idioms and other type erasure techniques can reduce compile time a lot. If it is well done, it can be implemented without so much overhead.&lt;/li&gt;
&lt;li&gt;Explicit instantiation of templates can also help, but only in the case of a user program. In the case of a library itself, you cannot do anything.&lt;/li&gt;
&lt;li&gt;Reduce inclusions and use forward declarations, obviously...&lt;/li&gt;
&lt;li&gt;Use tools like distcc (I very rarely use it) and ccache (I generally use it).&lt;/li&gt;
&lt;li&gt;Update your compiler&lt;/li&gt;
&lt;li&gt;Upgrade your computer ;)&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>Compilers</category><category>gcc</category><category>Performance</category><guid>http://baptiste-wicht.com/posts/2015/06/how-i-improved-a-bit-compile-time-of-etl.html</guid><pubDate>Tue, 16 Jun 2015 20:00:21 GMT</pubDate></item><item><title>Continuous Performance Management with CPM for C++</title><link>http://baptiste-wicht.com/posts/2015/06/continuous-performance-management-with-cpm-for-cpp.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;For some time, I have wanted some tool to monitor the performance of some of my projects. There are plenty of tools for Continuous Integration and Sonar is really great for continuous monitoring of code quality, but I haven't found anything satisfying for monitoring performance of C++ code. So I decided to write my own. &lt;a class="reference external" href="https://github.com/wichtounet/cpm"&gt;Continous Performance Monitor (CPM)&lt;/a&gt; is a simple C++ tool that helps you running benchmarks for your C++ programs and generate web reports based on the results. In this article, I will present this tool. CPM is especially made to benchmark several sub parts of libraries, but it perfectly be used to benchmark a whole program as well.&lt;/p&gt;
&lt;p&gt;The idea is to couple it with a Continuous Integration tool (I use Jenkins for instance) and run the benchmarks for every new push in a repository. With that, you can check if you have performance regression for instance or simply see if your changes were really improving the performance as much as you thought.&lt;/p&gt;
&lt;p&gt;It is made of two separate parts:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;A header-only library that you can use to benchmark your program and that will give you the performance results. It will also generate a JSON report of the collected data.&lt;/li&gt;
&lt;li&gt;A program that will generate a web version of the reports with analysis over time, over different compilers or over different configurations.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;CPM is especially made to benchmark functions that takes input data and which runtime depends on the dimensions of the input data. For each benchmark, CPM will execute it with several different input sizes. There are different ways to define a benchmark:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;two_pass&lt;/em&gt;: The benchmark is made of two part, the initialization part is called once for each input size and then the benchmark part is repeated several times for the measure. This is the most general version.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;global&lt;/em&gt;: The benchmark will be run with different input sizes but uses global data that will be randomized before each measure&lt;/li&gt;
&lt;li&gt;&lt;em&gt;simple&lt;/em&gt;: The benchmark will be run with different input sizes, data will not be randomized&lt;/li&gt;
&lt;li&gt;&lt;em&gt;once&lt;/em&gt;: The benchmark will be run with no input size.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: The randomization of the data can be disabled.&lt;/p&gt;
&lt;p&gt;You can run independent benchmarks or you can run sections of benchmarks. A section is used to compared different implementations of the same thing. For instance, I use them to compare different implementation of convolution or to see how ETL compete with other Expression Templates library.&lt;/p&gt;
&lt;img alt="/images/cpm_large.png" src="http://baptiste-wicht.com/images/cpm_large.png"&gt;
&lt;div class="section" id="examples"&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;I've uploaded three generated reports so that you can have look at some results:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Results of the ETL benchmark (one page per benchmark): &lt;a class="reference external" href="http://baptiste-wicht.com/cpm/etl/"&gt;Site 1&lt;/a&gt; &lt;a class="reference external" href="https://github.com/wichtounet/etl/blob/master/workbench/benchmark.cpp"&gt;Source 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Results of simple ETL / Blaze / Eigen3 comparison: &lt;a class="reference external" href="http://baptiste-wicht.com/cpm/etl_blaze_eigen/"&gt;Site 2&lt;/a&gt; &lt;a class="reference external" href="https://github.com/wichtounet/etl_vs_blaze/blob/master/src/simple.cpp"&gt;Source 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Results of the CPM dummy examples: &lt;a class="reference external" href="http://baptiste-wicht.com/cpm/examples/"&gt;Site 3&lt;/a&gt; &lt;a class="reference external" href="https://github.com/wichtounet/cpm/blob/master/examples/simple.cpp"&gt;Source 3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="run-benchmarks"&gt;
&lt;h2&gt;Run benchmarks&lt;/h2&gt;
&lt;p&gt;There are two ways of running CPM. You can directly use the library to run the benchmarks or you can use the macro facilities to make it easier. I recommend to use the second way since it is easier and I'm gonna try to keep it stable while the library can change. If you want an example of using the library directly, you can take a look at &lt;a class="reference external" href="https://github.com/wichtounet/cpm/blob/master/examples/simple.cpp"&gt;this example&lt;/a&gt;. In this chapter, I'm gonna focus on the macro-way.&lt;/p&gt;
&lt;p&gt;The library is available &lt;a class="reference external" href="https://github.com/wichtounet/cpm"&gt;here&lt;/a&gt;, you can either include as a submodule of your projects or install it globally to have access to its headers.&lt;/p&gt;
&lt;p&gt;The first thing to do is to include the CPM header:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_307196d783624d1fa21edd64c08e6bb8-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define CPM_BENCHMARK "Example Benchmarks"&lt;/span&gt;
&lt;a name="rest_code_307196d783624d1fa21edd64c08e6bb8-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#include&lt;/span&gt; &lt;span class="cpf"&gt;"cpm/cpm.hpp"&lt;/span&gt;&lt;span class="cp"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;You have to name your benchmark. This will automatically creates a main and will run all the declared benchmark.&lt;/p&gt;
&lt;div class="section" id="define-benchmarks"&gt;
&lt;h3&gt;Define benchmarks&lt;/h3&gt;
&lt;p&gt;Benchmarks can be defined either in a CPM_BENCH functor or in the global scope with &lt;code&gt;CPM_DIRECT_BENCH&lt;/code&gt;.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;simple&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_c7080e359c8a44f18636d3af526b6fc4-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_DIRECT_BENCH_SIMPLE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"bench_name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The first argument is the name of the benchmark and the second argument is the function that will be benchmarked by the system, this function takes the input size as input.&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;global&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_df63c9835721478cbf5be95a03657039-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_BENCH&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_df63c9835721478cbf5be95a03657039-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_df63c9835721478cbf5be95a03657039-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_GLOBAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"bench_name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_df63c9835721478cbf5be95a03657039-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;The first argument is the name of the benchmark, the second is the function being benchmarked and the following arguments must be references to global data that will be randomized by CPM.&lt;/p&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;two_pass&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_bf3acb8c7a6d4862be454215cf4f9b01-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_DIRECT_BENCH_TWO_PASS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"bench_name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_bf3acb8c7a6d4862be454215cf4f9b01-2"&gt;&lt;/a&gt;    &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;});&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
&lt;a name="rest_code_bf3acb8c7a6d4862be454215cf4f9b01-3"&gt;&lt;/a&gt;    &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_bf3acb8c7a6d4862be454215cf4f9b01-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Again, the first argument is the name. The second argument is the initialization functor. This functor must returns a tuple with all the information that will be passed (unpacked) to the third argument (the benchmark functor). Everything that is being returned by the initialization functor will be randomized.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="select-the-input-sizes"&gt;
&lt;h3&gt;Select the input sizes&lt;/h3&gt;
&lt;p&gt;By default, CPM will invoke your benchmarks with values from 10 to 1000000, multiplying it by 10 each step. This can be tuned for each benchmark and section independently. Each benchmark macro has a _P suffix that allows you to set the size policy:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_e29a2b120685426d8db4a45b3b259aaf-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_SIMPLE_P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_e29a2b120685426d8db4a45b3b259aaf-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;VALUES_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_e29a2b120685426d8db4a45b3b259aaf-3"&gt;&lt;/a&gt;    &lt;span class="s"&gt;"simple_a_n"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_e29a2b120685426d8db4a45b3b259aaf-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;});&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;You can also have several sizes (for multidimensional data structures or algorithms):&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_ba56af30c99b407b800d292b4b957c47-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_DIRECT_BENCH_TWO_PASS_P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_ba56af30c99b407b800d292b4b957c47-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;NARY_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VALUES_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;VALUES_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
&lt;a name="rest_code_ba56af30c99b407b800d292b4b957c47-3"&gt;&lt;/a&gt;    &lt;span class="s"&gt;"convmtx2"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_ba56af30c99b407b800d292b4b957c47-4"&gt;&lt;/a&gt;    &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dmat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dmat&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;d1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
&lt;a name="rest_code_ba56af30c99b407b800d292b4b957c47-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="cm"&gt;/*d1*/&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dmat&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dmat&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;etl&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;convmtx2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_ba56af30c99b407b800d292b4b957c47-6"&gt;&lt;/a&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="configure-benchmarks"&gt;
&lt;h3&gt;Configure benchmarks&lt;/h3&gt;
&lt;p&gt;By default, each benchmark is run 10 times for warmup and then repeated 50 times, but you can define your own values:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_c090af1a19a24362af2e5403bdc2fa20-1"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define CPM_WARMUP 3&lt;/span&gt;
&lt;a name="rest_code_c090af1a19a24362af2e5403bdc2fa20-2"&gt;&lt;/a&gt;&lt;span class="cp"&gt;#define CPM_REPEAT 10&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This must be done before the inclusion of the header.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="define-sections"&gt;
&lt;h3&gt;Define sections&lt;/h3&gt;
&lt;p&gt;Sections are simply a group of benchmarks, so instead of putting several benchmarks inside a &lt;code&gt;CPM_BENCH&lt;/code&gt;, you can put them inside a &lt;code&gt;CPM_SECTION&lt;/code&gt;. For instance:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_SECTION&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"mmul"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_SIMPLE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"std"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_SIMPLE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"fast"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_SIMPLE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"common"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;});&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-5"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_SECTION&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"conv"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_TWO_PASS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"std"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-8"&gt;&lt;/a&gt;        &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;});&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-9"&gt;&lt;/a&gt;        &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-10"&gt;&lt;/a&gt;        &lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-11"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_TWO_PASS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"fast"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-12"&gt;&lt;/a&gt;        &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;make_tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;});&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-13"&gt;&lt;/a&gt;        &lt;span class="p"&gt;[](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-14"&gt;&lt;/a&gt;        &lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_7f7a7577de3d4ff08f6f11b3d5021487-15"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;You can also set different warmup and repeat values for each section by using &lt;code&gt;CPM_SECTION_O&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_d5507109eb1642559391cd8dc1d501f2-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_SECTION_O&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"fft"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;51&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_d5507109eb1642559391cd8dc1d501f2-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_d5507109eb1642559391cd8dc1d501f2-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_d5507109eb1642559391cd8dc1d501f2-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_GLOBAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"std"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_d5507109eb1642559391cd8dc1d501f2-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_GLOBAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"mkl"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;this_thread&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sleep_for&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_ns&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_d5507109eb1642559391cd8dc1d501f2-6"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;will be warmup 11 times and run 51 times.&lt;/p&gt;
&lt;p&gt;The size policy can also be changed for the complete section (cannot be changed independently for benchmarks inside the section):&lt;/p&gt;
&lt;pre class="code cpp"&gt;&lt;a name="rest_code_68d365d58fb3424eb91f465a121f9755-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;CPM_SECTION_P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"mmul"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_68d365d58fb3424eb91f465a121f9755-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;NARY_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;STD_STOP_POLICY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;VALUES_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;VALUES_POLICY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;a name="rest_code_68d365d58fb3424eb91f465a121f9755-3"&gt;&lt;/a&gt;    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_68d365d58fb3424eb91f465a121f9755-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;
&lt;a name="rest_code_68d365d58fb3424eb91f465a121f9755-5"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_GLOBAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"std"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d3&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="cm"&gt;/* Something */&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_68d365d58fb3424eb91f465a121f9755-6"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_GLOBAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"mkl"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d3&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="cm"&gt;/* Something */&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_68d365d58fb3424eb91f465a121f9755-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;CPM_GLOBAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"bla"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;d3&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="cm"&gt;/* Something */&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_68d365d58fb3424eb91f465a121f9755-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="run"&gt;
&lt;h3&gt;Run&lt;/h3&gt;
&lt;p&gt;Once your benchmarks and sections are defined, you can build you program as a normal C++ main and run it. You can pass several options:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_21f68c27c58d466cb6492fd1fc50ba4b-1"&gt;&lt;/a&gt;./debug/bin/full -h
&lt;a name="rest_code_21f68c27c58d466cb6492fd1fc50ba4b-2"&gt;&lt;/a&gt;Usage:
&lt;a name="rest_code_21f68c27c58d466cb6492fd1fc50ba4b-3"&gt;&lt;/a&gt;  ./debug/bin/full [OPTION...]
&lt;a name="rest_code_21f68c27c58d466cb6492fd1fc50ba4b-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_21f68c27c58d466cb6492fd1fc50ba4b-5"&gt;&lt;/a&gt;  -n, --name arg           Benchmark name
&lt;a name="rest_code_21f68c27c58d466cb6492fd1fc50ba4b-6"&gt;&lt;/a&gt;  -t, --tag arg            Tag name
&lt;a name="rest_code_21f68c27c58d466cb6492fd1fc50ba4b-7"&gt;&lt;/a&gt;  -c, --configuration arg  Configuration
&lt;a name="rest_code_21f68c27c58d466cb6492fd1fc50ba4b-8"&gt;&lt;/a&gt;  -o, --output arg         Output folder
&lt;a name="rest_code_21f68c27c58d466cb6492fd1fc50ba4b-9"&gt;&lt;/a&gt;  -h, --help               Print help
&lt;/pre&gt;&lt;p&gt;The tag is used to distinguish between runs, I recommend that you use a SCM identifier for the tag. If you want to run your program with different configurations (compiler options for instance), you'll have to set the configuration with the --configuration option.&lt;/p&gt;
&lt;p&gt;Here is a possible output:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-1"&gt;&lt;/a&gt; Start CPM benchmarks
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-2"&gt;&lt;/a&gt;    Results will be automatically saved in /home/wichtounet/dev/cpm/results/10.cpm
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-3"&gt;&lt;/a&gt;    Each test is warmed-up 10 times
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-4"&gt;&lt;/a&gt;    Each test is repeated 50 times
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-5"&gt;&lt;/a&gt;    Time Sun Jun 14 15:33:51 2015
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-6"&gt;&lt;/a&gt;
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-7"&gt;&lt;/a&gt;    Tag: 10
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-8"&gt;&lt;/a&gt;    Configuration:
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-9"&gt;&lt;/a&gt;    Compiler: clang-3.5.0
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-10"&gt;&lt;/a&gt;    Operating System: Linux x86_64 3.16.5-gentoo
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-11"&gt;&lt;/a&gt;
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-12"&gt;&lt;/a&gt;
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-13"&gt;&lt;/a&gt; simple_a(10) : mean: 52.5us (52.3us,52.7us) stddev: 675ns min: 48.5us max: 53.3us througput: 190KEs
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-14"&gt;&lt;/a&gt; simple_a(100) : mean: 50.1us (48us,52.2us) stddev: 7.53us min: 7.61us max: 52.3us througput: 2MEs
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-15"&gt;&lt;/a&gt; simple_a(1000) : mean: 52.7us (52.7us,52.7us) stddev: 48.7ns min: 52.7us max: 53us througput: 19MEs
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-16"&gt;&lt;/a&gt; simple_a(10000) : mean: 62.6us (62.6us,62.7us) stddev: 124ns min: 62.6us max: 63.5us througput: 160MEs
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-17"&gt;&lt;/a&gt; simple_a(100000) : mean: 161us (159us,162us) stddev: 5.41us min: 132us max: 163us througput: 622MEs
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-18"&gt;&lt;/a&gt; simple_a(1000000) : mean: 1.16ms (1.16ms,1.17ms) stddev: 7.66us min: 1.15ms max: 1.18ms througput: 859MEs
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-19"&gt;&lt;/a&gt;
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-20"&gt;&lt;/a&gt;-----------------------------------------
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-21"&gt;&lt;/a&gt;|            gemm |       std |     mkl |
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-22"&gt;&lt;/a&gt;-----------------------------------------
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-23"&gt;&lt;/a&gt;|           10x10 | 51.7189us | 64.64ns |
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-24"&gt;&lt;/a&gt;|         100x100 | 52.4336us | 63.42ns |
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-25"&gt;&lt;/a&gt;|       1000x1000 | 56.0097us |  63.2ns |
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-26"&gt;&lt;/a&gt;|     10000x10000 | 95.6123us | 63.52ns |
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-27"&gt;&lt;/a&gt;|   100000x100000 | 493.795us | 63.48ns |
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-28"&gt;&lt;/a&gt;| 1000000x1000000 | 4.46646ms |  63.8ns |
&lt;a name="rest_code_2336f5b1f820482db48c8891da8b7df7-29"&gt;&lt;/a&gt;-----------------------------------------
&lt;/pre&gt;&lt;p&gt;The program will give you for each benchmark, the mean duration (with confidence interval), the standard deviation of the samples, the min and max duration and an estimated throughput. The throughput is simply using the size and the mean duration. Each section is directly compared with an array-like output. Once the benchmark is run, a JSON report will be generated inside the output folder.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="continuous-monitoring"&gt;
&lt;h2&gt;Continuous Monitoring&lt;/h2&gt;
&lt;p&gt;Once you have run the benchmark, you can use the CPM program to generate the web reports. It will generate:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;1 performance graph for each benchmark and section&lt;/li&gt;
&lt;li&gt;1 graph comparing the performances over time of your benchmark sections if you have run the benchmark several time&lt;/li&gt;
&lt;li&gt;1 graph comparing different compiler if you have compiled your program with different compiler&lt;/li&gt;
&lt;li&gt;1 graph comparing different configuration if you have run the benchmark with different configuration&lt;/li&gt;
&lt;li&gt;1 table summary for each benchmark / section&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First you have to build and install the CPM program (you can have a look at the &lt;a class="reference external" href="https://github.com/wichtounet/cpm/blob/master/README.rst"&gt;Readme&lt;/a&gt; for more informations.&lt;/p&gt;
&lt;p&gt;Several options are available:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-1"&gt;&lt;/a&gt;Usage:
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-2"&gt;&lt;/a&gt;  cpm [OPTION...]  results_folder
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-4"&gt;&lt;/a&gt;      --time-sizes             Display multiple sizes in the time graphs
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-5"&gt;&lt;/a&gt;  -t, --theme arg              Theme name [raw,bootstrap,boostrap-tabs] (default:bootstrap)
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-6"&gt;&lt;/a&gt;  -c, --hctheme theme_name     Highcharts Theme name [std,dark_unica] (default:dark_unica)
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-7"&gt;&lt;/a&gt;  -o, --output output_folder   Output folder (default:reports)
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-8"&gt;&lt;/a&gt;      --input arg              Input results
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-9"&gt;&lt;/a&gt;  -s, --sort-by-tag            Sort by tag instaed of time
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-10"&gt;&lt;/a&gt;  -p, --pages                  General several HTML pages (one per bench/section)
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-11"&gt;&lt;/a&gt;  -d, --disable-time           Disable time graphs
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-12"&gt;&lt;/a&gt;      --disable-compiler       Disable compiler graphs
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-13"&gt;&lt;/a&gt;      --disable-configuration  Disable configuration graphs
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-14"&gt;&lt;/a&gt;      --disable-summary        Disable summary table
&lt;a name="rest_code_7feabbf1ea9649c39667b776a1450ee2-15"&gt;&lt;/a&gt;  -h, --help                   Print help
&lt;/pre&gt;&lt;p&gt;There are 3 themes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;bootstrap&lt;/em&gt;: The default theme, using Bootstrap to make a responsive interface.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;bootstrap-tabs&lt;/em&gt;: Similar to the &lt;em&gt;bootstrap&lt;/em&gt; theme except that only is displayed at the same time for each benchmark, with tabs.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;raw&lt;/em&gt; : A very basic theme, only using Highcharts library for graphs. It is very minimalistic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, here are how the reports are generated for the ETL benchmark:&lt;/p&gt;
&lt;pre class="code text"&gt;&lt;a name="rest_code_eb5d173efee3410e9bf1462dddc70ebf-1"&gt;&lt;/a&gt;cpm -p -s -t bootstrap -c dark_unica -o reports results
&lt;/pre&gt;&lt;p&gt;Here is the graph generated for the "R = A + B + C" benchmark and different compilers:&lt;/p&gt;
&lt;img alt="/images/cpm_etl_compiler.png" src="http://baptiste-wicht.com/images/cpm_etl_compiler.png"&gt;
&lt;p&gt;and its summary:&lt;/p&gt;
&lt;img alt="/images/cpm_etl_summary.png" src="http://baptiste-wicht.com/images/cpm_etl_summary.png"&gt;
&lt;p&gt;Here is the graph for a 2D convolution with ETL:&lt;/p&gt;
&lt;img alt="/images/cpm_etl_section.png" src="http://baptiste-wicht.com/images/cpm_etl_section.png"&gt;
&lt;p&gt;And the graph for different configurations of ETL and the dense matrix matrix multiplication:&lt;/p&gt;
&lt;img alt="/images/cpm_etl_configuration.png" src="http://baptiste-wicht.com/images/cpm_etl_configuration.png"&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion-and-future-work"&gt;
&lt;h2&gt;Conclusion and Future Work&lt;/h2&gt;
&lt;p&gt;Although CPM is already working, there are several things that could be done to improve it further:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The generated web report could benefit from a global summary.&lt;/li&gt;
&lt;li&gt;The throughput evaluation should be evaluated more carefully.&lt;/li&gt;
&lt;li&gt;The tool should automatically evaluate the number of times that each tests should be run to have a good result instead of global warmup and repeat constants.&lt;/li&gt;
&lt;li&gt;A better bootstrapping procedure should be used to determine the quality of the results and compute the confidence intervals.&lt;/li&gt;
&lt;li&gt;The performances of the website with lots of graphs should be improved.&lt;/li&gt;
&lt;li&gt;Make CPM more general-purpose to support larger needs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here it is, I have summed most of the features of the CPM Continuous Performance Analysis tool. I hope that it will be helpful to some of you as well.&lt;/p&gt;
&lt;p&gt;If you have other ideas or want to contribute something to the project, you can directly open an issue or a pull request on &lt;a class="reference external" href="https://github.com/wichtounet/cpm"&gt;Github&lt;/a&gt;. Or contact me via this site or Github.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>Benchmarks</category><category>C++</category><category>Performances</category><category>Sonar</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2015/06/continuous-performance-management-with-cpm-for-cpp.html</guid><pubDate>Sun, 14 Jun 2015 12:02:57 GMT</pubDate></item><item><title>C++17 Fold Expressions</title><link>http://baptiste-wicht.com/posts/2015/05/cpp17-fold-expressions.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;div class="section" id="variadic-templates"&gt;
&lt;h2&gt;Variadic Templates&lt;/h2&gt;
&lt;p&gt;C++11 introduced variadic template to the languages. This new feature allows to write template functions and classes taking an arbitrary number of template parameters. This a feature I really like and I already used it quite a lot in my different libraries. Here is a very simple example computing the sum of the parameters:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_685f6cd8f84446cbbb07baa6c651b1c2-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="nf"&gt;old_sum&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
&lt;a name="rest_code_685f6cd8f84446cbbb07baa6c651b1c2-2"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_685f6cd8f84446cbbb07baa6c651b1c2-3"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;a name="rest_code_685f6cd8f84446cbbb07baa6c651b1c2-4"&gt;&lt;/a&gt;
&lt;a name="rest_code_685f6cd8f84446cbbb07baa6c651b1c2-5"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;typename&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_685f6cd8f84446cbbb07baa6c651b1c2-6"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;old_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T1&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;ts&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_685f6cd8f84446cbbb07baa6c651b1c2-7"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;old_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ts&lt;/span&gt;&lt;span class="p"&gt;...);;&lt;/span&gt;
&lt;a name="rest_code_685f6cd8f84446cbbb07baa6c651b1c2-8"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;What can be seen here is a typical use of variadic templates. Almost all the time, is is necessary to use recursion and several functions to unpack the parameters and process them. There is only one way to unpack the arguments, by using the ... operator that simply put comma between arguments. Even if it works well, it is a bit heavy on the code. This will likely be completely optimized to a series of addition by the compiler, but it may still happen in more complicated functions that this is not done. Moreover, the intent is not always clear with that.&lt;/p&gt;
&lt;p&gt;That is why C++17 introduced an extension for the variadic template, fold expressions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fold-expressions"&gt;
&lt;h2&gt;Fold expressions&lt;/h2&gt;
&lt;p&gt;Fold expressions are a new way to unpack variadic parameters with operators. For now, only Clang 3.6 supports C++17 fold expression, with the -std=c++1z flag. That is the compiler I used to validate the examples of this post.&lt;/p&gt;
&lt;p&gt;The syntax is bit disturbing at first but quite logical:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_986c9c63bd4e42e3baee5aee8d0d1066-1"&gt;&lt;/a&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;pack&lt;/span&gt; &lt;span class="n"&gt;op&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;             &lt;span class="c1"&gt;//(1)&lt;/span&gt;
&lt;a name="rest_code_986c9c63bd4e42e3baee5aee8d0d1066-2"&gt;&lt;/a&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;op&lt;/span&gt; &lt;span class="n"&gt;pack&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;             &lt;span class="c1"&gt;//(2)&lt;/span&gt;
&lt;a name="rest_code_986c9c63bd4e42e3baee5aee8d0d1066-3"&gt;&lt;/a&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;pack&lt;/span&gt; &lt;span class="n"&gt;op&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;op&lt;/span&gt; &lt;span class="n"&gt;init&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;     &lt;span class="c1"&gt;//(3)&lt;/span&gt;
&lt;a name="rest_code_986c9c63bd4e42e3baee5aee8d0d1066-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;init&lt;/span&gt; &lt;span class="n"&gt;op&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;op&lt;/span&gt; &lt;span class="n"&gt;pack&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;     &lt;span class="c1"&gt;//(4)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Where &lt;em&gt;pack&lt;/em&gt; is an unexpanded parameter pack, &lt;em&gt;op&lt;/em&gt; an operator and &lt;em&gt;init&lt;/em&gt; a value. The version (1) is a right fold that is expanded like (P1 op (P2 op (P3 ... (PN-1 op PN)))). The version (2) is a left fold where the expansion is taken from the left. The (3) and (4) versions are almost the value except for an init value. Only some operators (+,*,&amp;amp;,|,&amp;amp;&amp;amp;,||, ,) have defined init values and can be used with the versions (1) and (2). The other operators can only be used with an init value.&lt;/p&gt;
&lt;p&gt;For instance, here is how we could write the sum functions with fold expressions:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_7c23df9b1e8444bd981bff9b56f3a6d9-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_7c23df9b1e8444bd981bff9b56f3a6d9-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;fold_sum_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_7c23df9b1e8444bd981bff9b56f3a6d9-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(...&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_7c23df9b1e8444bd981bff9b56f3a6d9-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;I personally think it is much better, it clearly states our intent and does not need recursion. By default, the init value used for addition is 0, but you can change it:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_66dafa676bcc48e6bf98091bdf6154c9-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_66dafa676bcc48e6bf98091bdf6154c9-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;fold_sum_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;a name="rest_code_66dafa676bcc48e6bf98091bdf6154c9-3"&gt;&lt;/a&gt;    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;a name="rest_code_66dafa676bcc48e6bf98091bdf6154c9-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;This will yield the sum of the elements plus one.&lt;/p&gt;
&lt;p&gt;This can be also very practical to print some elements for instance:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_c5c34668147e49b1981b98628d75183c-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;typename&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt;&lt;span class="n"&gt;Args&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_c5c34668147e49b1981b98628d75183c-2"&gt;&lt;/a&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;print_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Args&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;a name="rest_code_c5c34668147e49b1981b98628d75183c-3"&gt;&lt;/a&gt;    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cout&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="sc"&gt;'\n'&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;a name="rest_code_c5c34668147e49b1981b98628d75183c-4"&gt;&lt;/a&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;And this can even be used when doing Template Metaprogramming, for instance here is a TMP version of the and operator:&lt;/p&gt;
&lt;pre class="code c++"&gt;&lt;a name="rest_code_8025fb80014d46c68a098b685fa592fc-1"&gt;&lt;/a&gt;&lt;span class="k"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;a name="rest_code_8025fb80014d46c68a098b685fa592fc-2"&gt;&lt;/a&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="nl"&gt;fold_and&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;integral_constant&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;...)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{};&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;C++17 fold expressions are a really nice additions to the language that makes working with variadic templates much easier. This already makes me wish for C++17 release :)&lt;/p&gt;
&lt;p&gt;The source code for the examples are available on Github: &lt;a class="reference external" href="https://github.com/wichtounet/articles/blob/master/src/fold_expressions.cpp"&gt;https://github.com/wichtounet/articles/blob/master/src/fold_expressions.cpp&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>C++17</category><guid>http://baptiste-wicht.com/posts/2015/05/cpp17-fold-expressions.html</guid><pubDate>Sat, 23 May 2015 17:08:20 GMT</pubDate></item><item><title>Sonar C++ Community Plugin Review</title><link>http://baptiste-wicht.com/posts/2015/05/sonar-cpp-community-plugin-review.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;It's been a long time since I have written on this blog. I have had quite a lot of work between my Ph.D and my teaching. I have several projects going on, I'll try to write updates on them later on.&lt;/p&gt;
&lt;p&gt;Some time ago, I wrote an article &lt;cite&gt;about the official C++ plugin for Sonar &amp;lt;http://baptiste-wicht.com/posts/2014/10/sonarqube-inspections-for-cpp-projects.html&amp;gt;&lt;/cite&gt;. I was quite disappointed by the quality of a plugin. I was expecting more from an expensive official plugin.&lt;/p&gt;
&lt;p&gt;There is an open-source alternative to the commercial plugin: &lt;cite&gt;sonar-cxx-plugin &amp;lt;https://github.com/wenns/sonar-cxx&amp;gt;&lt;/cite&gt;. I already tested it quite some time ago (a development version of the 0.9.1 version) and the results were quite bad. I'm using C++11 and C++14 in almost all of my projects and the support was quite bad at that time. Happily, this support has now gotten much better :) In this article, I'll talk about the version 0.9.2.&lt;/p&gt;
&lt;div class="section" id="usage"&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;The usage of this plugin is very easy, you don't need any complicated build wrapping techniques for it. You simply need to complete a &lt;em&gt;sonar-project.properties&lt;/em&gt; file:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sonar.projectKey=etl
sonar.projectName=etl
sonar.projectVersion=1.0

sonar.sourceEncoding=UTF-8
sonar.sources=include,test,workbench
sonar.language=c++
&lt;/pre&gt;
&lt;p&gt;After that, you simply have to use &lt;em&gt;sonar-runner&lt;/em&gt; as for any other Sonar project:&lt;/p&gt;
&lt;blockquote&gt;
sonar-runner&lt;/blockquote&gt;
&lt;p&gt;And the analysis will be run.&lt;/p&gt;
&lt;p&gt;I haven't had any issues with the analysis. However, the plugin is not yet completely C++11/C++14 compatible, therefore I'm encountering a lot of parser errors during the analysis. When an error is encountered by the parser, the line is skipped and the parser goes to the next line. This means that the analysis of the line is incomplete, which may lead to false positives or to missing issues. This comes from that sonar-cxx uses its own parser, which is to on par with clang-compatible parsers for instance.&lt;/p&gt;
&lt;p&gt;Here is the Sonar summary of my ETL project:&lt;/p&gt;
&lt;img alt="/images/sonar_summary.png" src="http://baptiste-wicht.com/images/sonar_summary.png"&gt;
&lt;/div&gt;
&lt;div class="section" id="inspections"&gt;
&lt;h2&gt;Inspections&lt;/h2&gt;
&lt;p&gt;This plugin supports some inspections on itself. Nevertheless, you have to enable since it seems that most of them are disable by default. Code duplication is also automatically generated during the analysis:&lt;/p&gt;
&lt;img alt="/images/sonar_duplicated.png" src="http://baptiste-wicht.com/images/sonar_duplicated.png"&gt;
&lt;p&gt;The philosophy of this project is not to develop all inspections, but to integrate with other tools. For instance, cppcheck is already supported and the integration works perfectly. Here are the tools that sonar-cxx supports for quality analysis:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;cppcheck&lt;/li&gt;
&lt;li&gt;valgrind&lt;/li&gt;
&lt;li&gt;Vera++&lt;/li&gt;
&lt;li&gt;RATS&lt;/li&gt;
&lt;li&gt;PC-Lint&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I have only tested cppcheck for now. I plan to use valgrind running on my tests in the future. I don't plan to use the others.&lt;/p&gt;
&lt;p&gt;It should also be noted that the plugin supports compiler warnings coming from G++ and Visual Studio. I don't use this since I compile all my projects with -Werror.&lt;/p&gt;
&lt;p&gt;The biggest absent here is Clang, there is no support for its warnings, its static-analyzer or its advanced clang-tidy tool. If clang-tidy support does not come in the near future, I'm planning to try to add it myself, provided I find some time.&lt;/p&gt;
&lt;p&gt;You can have to some inspections on one of my project:&lt;/p&gt;
&lt;img alt="/images/sonar_inspections.png" src="http://baptiste-wicht.com/images/sonar_inspections.png"&gt;
&lt;p&gt;As with any Sonar projects, you have access to the Hotsposts view:&lt;/p&gt;
&lt;img alt="/images/sonar_hotspots.png" src="http://baptiste-wicht.com/images/sonar_hotspots.png"&gt;
&lt;/div&gt;
&lt;div class="section" id="unit-tests-integration"&gt;
&lt;h2&gt;Unit Tests Integration&lt;/h2&gt;
&lt;p&gt;I have been able to integrate my unit tests results inside Sonar. The plugin expects JUnit compatible format. Several of C++ unit test libraries already generates compatible format. In my case, I used Catch and it worked very well.&lt;/p&gt;
&lt;p&gt;What is even more interesting is the support for code coverage. You have to run your coverage-enabled executable and then use gcovr to generate an XML file that the plugin can read.&lt;/p&gt;
&lt;p&gt;This support works quite well. The only thing I haven't been able to make work is the execution time computation of the unit tests, but that is not something I really care about.&lt;/p&gt;
&lt;p&gt;Here are the coverage results for one of my files:&lt;/p&gt;
&lt;img alt="/images/sonar_coverage.png" src="http://baptiste-wicht.com/images/sonar_coverage.png"&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Pros&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Support of a lot of external tools&lt;/li&gt;
&lt;li&gt;Very easy to use&lt;/li&gt;
&lt;li&gt;Duplicated code analysis&lt;/li&gt;
&lt;li&gt;Very good code coverage analysis integration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Cons&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Too few integrated inspections&lt;/li&gt;
&lt;li&gt;Limited parsing of C++&lt;/li&gt;
&lt;li&gt;Not fully compatible with C++11/C++14&lt;/li&gt;
&lt;li&gt;False positives&lt;/li&gt;
&lt;li&gt;Not enough love for clang (compiler warnings, clang-tidy, tooling, static-analyzer, ...)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The provided metrics are really good, the usage is quite simple and this plugin supports some external tools adding interesting inspections. Even if this plugin is not perfect, it is a very good way to do Continuous Quality Analysis of your C++ projects. I personally find it superior to the official plugin. The usage is more simple (no build-wrapper that does not work), it supports more external tools and supports JUnit reports. On the other hand, it has much less integrated inspections and rely more on external tools. Both have problems with modern C++ features.&lt;/p&gt;
&lt;p&gt;What I would really like in this plugin is the support of the clang-tidy analyzer (and other Clang analysis tools) and also complete C++11/C++14 support. I honestly think that the only way to fix the latter is to switch to Clang parsing with libtooling rather than developing an in-house parser, but that is not up to me.&lt;/p&gt;
&lt;p&gt;I will definitely continue to use this plugin to generate metrics for my C++ projects. I use it with Jenkins which launch the analysis every time I push to one my git repositories. This plugin definitely shows promises.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>C++</category><category>Review</category><category>Sonar</category><category>Tools</category><guid>http://baptiste-wicht.com/posts/2015/05/sonar-cpp-community-plugin-review.html</guid><pubDate>Thu, 14 May 2015 15:09:59 GMT</pubDate></item><item><title>How to speed up RAID (5-6) growing with mdadm ?</title><link>http://baptiste-wicht.com/posts/2015/03/how-to-speed-up-raid-5-6-growing-with-mdadm.html</link><dc:creator>Baptiste Wicht</dc:creator><description>&lt;div&gt;&lt;p&gt;Yesterday, I added my 11th disk to my RAID 6 array. As the last time it took my more than 20 hours, I spent some time investigating how to speed things up and this post contains some tips on how to achieve good grow performances. With these tips, I have been able to reach a speed of about 55K in average during reshape. It did finish in about 13 hours.&lt;/p&gt;
&lt;p&gt;First, take into account that some of these tips may depend on your configuration. In my case, this server is only used for this RAID, so I don't care if the CPU is used a lot during rebuild or if other processes are suffering from the load. This may not be the case with your configuration. Moreover, I speak only of hard disks, if you use SSD RAID, there are probably better way of tuning the rebuild (or perhaps it is fast enough). Finally, you have know that a RAID reshape is going to be slow, there is no way you'll grow a 10+ RAID array in one hour. G&lt;/p&gt;
&lt;p&gt;In the examples, I use /dev/md0 as the raid array, you'll have to change this to your array name.&lt;/p&gt;
&lt;p&gt;The first 3 tips can be used even after the rebuild has started and you should the differences in real-time. But, these 3 tips will also be erased after each reboot.&lt;/p&gt;
&lt;div class="section" id="increase-speed-limits"&gt;
&lt;h2&gt;Increase speed limits&lt;/h2&gt;
&lt;p&gt;The easiest thing to do is to increase the system speed limits on raid. You can see the current limits on your system by using these commands:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_47ad45551f464c2d8972aac2918e479b-1"&gt;&lt;/a&gt;sysctl dev.raid.speed_limit_min
&lt;a name="rest_code_47ad45551f464c2d8972aac2918e479b-2"&gt;&lt;/a&gt;sysctl dev.raid.speed_limit_max
&lt;/pre&gt;&lt;p&gt;These values are set in Kibibytes per second (KiB/s).&lt;/p&gt;
&lt;p&gt;You can put them to high values:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_aaa014e1373a43c48eda411b2cba6a44-1"&gt;&lt;/a&gt;sysctl -w dev.raid.speed_limit_min&lt;span class="o"&gt;=&lt;/span&gt;100000
&lt;a name="rest_code_aaa014e1373a43c48eda411b2cba6a44-2"&gt;&lt;/a&gt;sysctl -w dev.raid.speed_limit_max&lt;span class="o"&gt;=&lt;/span&gt;500000
&lt;/pre&gt;&lt;p&gt;At least with these values, you won't be limited by the system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="increase-stripe-cache-size"&gt;
&lt;h2&gt;Increase stripe cache size&lt;/h2&gt;
&lt;p&gt;By allowing the array to use more memory for its stripe cache, you may improve the performances. In some cases, it can improve performances by up to 6 times. By default, the size of the stripe cache is 256, in pages. By default, Linux uses 4096B pages. If you use 256 pages for the stripe cache and you have 10 disks, the cache would use 10*256*4096=10MiB of RAM. In my case, I have increased it to 4096:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_877e143ecc514c89802ab986fb03338f-1"&gt;&lt;/a&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="m"&gt;4096&lt;/span&gt; &amp;gt; /sys/block/md0/md/stripe_cache_size
&lt;/pre&gt;&lt;p&gt;The maximum value is 32768. If you have many disks, this may well take all your available memory. I don't think values higher than 4096 will improve performance, but feel free to try it ;)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="increase-read-ahead"&gt;
&lt;h2&gt;Increase read-ahead&lt;/h2&gt;
&lt;p&gt;If configured too low, the read-ahead of your array may make things slower.&lt;/p&gt;
&lt;p&gt;You can see get the current read-ahead value with this command:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_a2b26f53d123499f982f9b49f18ea79f-1"&gt;&lt;/a&gt;blockdev --getra /dev/md0
&lt;/pre&gt;&lt;p&gt;These values are in 512B sector. You can set it to 32MB to be sure:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_ae60e975ca4b44249b4d2820e462d296-1"&gt;&lt;/a&gt;blockdev --setra &lt;span class="m"&gt;65536&lt;/span&gt; /dev/md0
&lt;/pre&gt;&lt;p&gt;This can improve the performances, but don't expect this to be a game-changer unless it was configured really low at the first place.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="bonus-speed-up-standard-resync-with-a-write-intent-bitmap"&gt;
&lt;h2&gt;Bonus: Speed up standard resync with a write-intent bitmap&lt;/h2&gt;
&lt;p&gt;Although it won't speed up the growing of your array, this is something that you should do after the rebuild has finished. Write-intent bitmaps is a kind of map of what needs to be resynced. This is of great help in several cases:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;When the computer crash (power shutdown for instance)&lt;/li&gt;
&lt;li&gt;If a disk is disconnected, then reconnected.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In these case, it may totally avoid the need of a rebuild which is great in my opinion. Moreover, it does not take any space on the array since it uses space that is not usable by the array.&lt;/p&gt;
&lt;p&gt;Here is how to enable it:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_8446ae3468e54d78944843f24118e610-1"&gt;&lt;/a&gt;mdadm --grow --bitmap&lt;span class="o"&gt;=&lt;/span&gt;internal /dev/md0
&lt;/pre&gt;&lt;p&gt;However, it may cause some write performance degradation. In my case, I haven't seen any noticeable degradation, but if it is the case, you may want to disable it:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_f495312fdff64912a5734fcf49229e47-1"&gt;&lt;/a&gt;mdadm --grow --bitmap&lt;span class="o"&gt;=&lt;/span&gt;none /dev/md0
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="bonus-monitor-rebuild-process"&gt;
&lt;h2&gt;Bonus: Monitor rebuild process&lt;/h2&gt;
&lt;p&gt;If you want to monitor the build process, you can use the watch command:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_ea0354d3de194c07ad083c5ddc8f7e95-1"&gt;&lt;/a&gt;watch cat /proc/mdstat
&lt;/pre&gt;&lt;p&gt;With that you'll see the rebuild going in real-time.&lt;/p&gt;
&lt;p&gt;You can also monitor the I/O statistics:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_e36f59d41d8949ce9426647ce0262c6d-1"&gt;&lt;/a&gt;watch iostat -k &lt;span class="m"&gt;1&lt;/span&gt; 2
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="bonus-how-to-grow-a-raid-5-6-array"&gt;
&lt;h2&gt;Bonus: How to grow a RAID 5-6 array&lt;/h2&gt;
&lt;p&gt;As a sidenote, this section indicates how to grow an array. If you  want to add the disk /dev/sdl to the array /dev/md0, you'll first have to add it:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_3e7a35ecca214197baa183eb7f848904-1"&gt;&lt;/a&gt;mdadm --add /dev/md0 /dev/sdl
&lt;/pre&gt;&lt;p&gt;This will add the disk as a spare disk. If you had 5 disks before, you'll want to grow it to 6:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_1faf843266aa459396c50c22560d3fbe-1"&gt;&lt;/a&gt;mdadm --grow --backup-file&lt;span class="o"&gt;=&lt;/span&gt;/root/grow_md0_backup_file --raid-devices&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt; /dev/md0
&lt;/pre&gt;&lt;p&gt;The backup file must be on another disk of course. The backup file is optional but improves the chance of success if you have a power shutdown or another form of unexpected shutdown. If you know what you're doing, you can grow it without backup-file:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_59af157c263748e7bbcc89acbf910ded-1"&gt;&lt;/a&gt;mdadm --grow --raid-devices&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt; /dev/md0
&lt;/pre&gt;&lt;p&gt;This command will return almost instantly, but the actual reshape won't likely be finished for hours (maybe days). kkkkkkkkkkkkk&lt;/p&gt;
&lt;p&gt;Once the rebuild is finished, you'll still have to extend the partitions with resize2fs. If you use LVM on top of the array, you'll have to resize the Physical Volume (PV) first:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_9bfb235d0178457dba854b58d76f0367-1"&gt;&lt;/a&gt;pvresize /dev/md0
&lt;/pre&gt;&lt;p&gt;and then extend the Logical Volume (s) (LV). For instance, if you want to add 1T to a LV named /dev/vgraid/work:&lt;/p&gt;
&lt;pre class="code bash"&gt;&lt;a name="rest_code_adbf49e182a44376b54960f1bca8cc56-1"&gt;&lt;/a&gt;vgextend -r -L+1T /dev/vgraid/work
&lt;/pre&gt;&lt;p&gt;The -r option will automatically resize the underlying filesystem. Otherwise, you'd still have to resize it with resize2fs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;These are the changes I have found that speed up the reshape process. There are others that you may test in your case. For instance, in some systems disabling NCQ on each disk may help.&lt;/p&gt;
&lt;p&gt;I hope that these tips will help you doing fast rebuilds in your RAID array :)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>Gentoo</category><category>Hardware</category><category>Home Server</category><category>Linux</category><category>LVM</category><guid>http://baptiste-wicht.com/posts/2015/03/how-to-speed-up-raid-5-6-growing-with-mdadm.html</guid><pubDate>Sat, 07 Mar 2015 14:01:56 GMT</pubDate></item></channel></rss>