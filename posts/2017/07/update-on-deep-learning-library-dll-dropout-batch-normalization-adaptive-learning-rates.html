<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Update on Deep Learning Library (DLL): Dropout, Batch Normalization, Adaptive Learning Rates, ... | Blog blog("Baptiste Wicht");</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.xml">
<link rel="canonical" href="http://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html">
<!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Baptiste Wicht">
<link rel="prev" href="../06/jenkins-tip-send-notifications-fixed-builds-declarative-pipeline.html" title="Jenkins Tip: Send notifications on fixed builds in declarative pipeline" type="text/html">
<link rel="next" href="../08/expression-templates-library-etl-11.html" title="Expression Templates Library (ETL) 1.1" type="text/html">
<meta property="og:site_name" content='Blog blog("Baptiste Wicht");'>
<meta property="og:title" content="Update on Deep Learning Library (DLL): Dropout, Batch Normalization, A">
<meta property="og:url" content="http://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html">
<meta property="og:description" content="It's been a while since I've posted something on this, especially since I had
one month vacation. This year I've been able to integrate a great number of
changes into my Deep Learning Library (DLL) pr">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-07-16T15:41:51+02:00">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="dll">
<meta property="article:tag" content="etl">
<meta property="article:tag" content="publications">
<meta property="article:tag" content="thesis">
<link href="../../../favicon.ico" rel="icon" type="image/x-icon">
<link rel="publisher" href="https://plus.google.com/+BaptisteWicht">
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-2175227-7', 'auto');
  var metas = document.getElementsByTagName('meta'), tagsList = [];
  for (var i=0; i<metas.length; i++) {
    if (metas[i].getAttribute('property') == 'article:tag') {
      tagsList.push( metas[i].getAttribute('content'));
    }
  }
  ga('set', 'dimension1', tagsList.join('|'));
  ga('send', 'pageview');
</script>
</head>
<body>

<!-- Menubar -->

<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<div class="container-fluid">
<!-- This keeps the margins nice -->
    <div class="row">
        <div class="col-sm-3 col-lg-2">
            <nav class="navbar navbar-inverse navbar-fixed-side"><div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="http://baptiste-wicht.com/">
                        <span id="blog-title">Blog blog("Baptiste Wicht");</span>
                    </a>
                </div>
<!-- /.navbar-header -->

                <div class="collapse navbar-collapse navbar-ex1-collapse">
                    <ul class="nav navbar-nav">
<li>
<a href="../../../stories/about.html">About</a>
                </li>
<li>
<a href="../../../stories/publications.html">Publications</a>
                </li>
<li>
<a href="../../../stories/projects.html">Projects</a>
                </li>
<li>
<a href="../../../categories/index.html">Tags</a>
                </li>
<li>
<a href="../../../archive.html">Archives</a>
                </li>
<li>
<a href="http://feeds.feedburner.com/BaptisteWicht">RSS</a>



                            </li>
<li class="navbar-content">
                                <h3>Related posts</h3>
                            </li>
                            

                            <li><a href="../08/dll-blazing-fast-neural-network-library.html">DLL: Blazing Fast Neural Network Library</a></li>
<li><a href="../../2016/04/simplify-deep-learning-library-usage-on-linux-and-windows.html">Simplify Deep Learning Library usage on Linux and Windows!</a></li>
<li><a href="../02/publication-cpu-performance-optimizations-rbm-crbm.html">Publication: CPU Performance Optimizations for RBM and CRBM</a></li>
<li><a href="../../2014/09/short-introduction-to-deep-learning.html">Short introduction to deep learning</a></li>
<li><a href="../../2016/01/improve-dll-and-etl-compile-time-further.html">Improve DLL and ETL Compile Time further</a></li>
<li><a href="../03/partial-type-erasing-deep-learning-library-dll-improve-compilation-time.html">Partial type erasing in Deep Learning Library (DLL) to improve compilation time</a></li>


                            <li class="navbar-block">

                        <li class="wicht-navbar-right">
                            <a target="_blank" title="Follow @wichtounet on Twitter" href="https://twitter.com/wichtounet">
                                <img src="../../../assets/img/twitter.png" alt="Follow @wichtounet on Twitter"></a>
                        </li>

                        <li class="wicht-navbar-right">
                            <a target="_blank" title="Follow +BaptisteWicht on Google+" href="https://plus.google.com/+BaptisteWicht">
                                <img src="../../../assets/img/google_plus.png" alt="Follow +BaptisteWicht on Google+"></a>
                        </li>


                    </ul>
</div>
<!-- /.navbar-collapse -->
            </nav>
</div> <!-- col -->
        <div class="col-sm-9 col-lg-10">
            <div id="content"></div>
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="#" class="u-url">Update on Deep Learning Library (DLL): Dropout, Batch Normalization, Adaptive Learning Rates, ...</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Baptiste Wicht
            </span></p>
            <p class="dateline"><a href="#" rel="bookmark"><time class="published dt-published" datetime="2017-07-16T15:41:51+02:00" itemprop="datePublished" title="2017-07-16 15:41">2017-07-16 15:41</time></a></p>
                <p class="commentline">
        
    <a href="update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html#disqus_thread" data-disqus-identifier="cache/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html">Comments</a>


            
        </p>
<p class="sourceline"><a href="update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>It's been a while since I've posted something on this, especially since I had
one month vacation. This year I've been able to integrate a great number of
changes into my Deep Learning Library (DLL) project. It has seen a lot of
refactorings and a lot of new features making it look like a real neural network
library now. In this post, I'll try to outline the last new features and changes
of the library.</p>
<p>For those that don't know, DLL is a library for neural network training, written
in C++ and for C++. You can train Fully-Connected Neural Networks and
Convolutional Neural Networks. The focus of the framework is on speed and easy
use in C++.</p>
<p>As for my ETL project and again thanks to my thesis supervisor, the project now
has a logo:</p>
<img alt="DLL Logo" class="align-center" src="../../../images/dll_logo.png"><div class="section" id="adaptive-learning-rates">
<h2>Adaptive Learning Rates</h2>
<p>Before, the framework only supported simple SGD and Momentum updates for the
different parameters of the network. Moreover, it was not very well extendable.
Therefore, I reviewed the system to be able to configure an optimizer for each
network to train. Once that was done, the first thing I did was to add support
for Nesterov Accelerated Gradients (NAG) as a third optimizer. After this,
I realized it was then easy to integrate support for more advanced optimizers
including support for adaptive learning rates. This means that the learning rate
will be adapted for each parameter depending on what the network is learning.
Some of the optimizers even don't need any learning rate. So far, I've
implemented support for the following optimizers: Adagrad, RMSProp, Adam (with
and without bias correction), Adamax (Adam with infinite norm), Nadam (Adam with
Nesterov momentum) and Adadelta (no more learning rate). The user can now choose
the optimizer of its choice, for instance NADAM, as a parameter of the network:</p>
<pre class="code c++"><a name="rest_code_e2ef621fb991425d9fd1531a11be5d8f-1"></a><span class="c1">// Use a Nadam optimizer</span>
<a name="rest_code_e2ef621fb991425d9fd1531a11be5d8f-2"></a><span class="n">dll</span><span class="o">::</span><span class="n">updater</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">updater_type</span><span class="o">::</span><span class="n">NADAM</span><span class="o">&gt;</span>
</pre>
<p>Another improvement in the same domain is that the learning rate can also be
decayed over time automatically by the optimizer.</p>
<p>If you want more information on the different optimizers, you can have a look at
this very good article:
<a class="reference external" href="http://ruder.io/optimizing-gradient-descent/">An overview of gradient descent optimization algorithms</a>
from Sebastian Ruder.</p>
</div>
<div class="section" id="better-loss-support">
<h2>Better loss support</h2>
<p>Before, DLL was automatically using Categorical Cross Entropy Loss, but it was
not possible to change it and it was not even possible to see the loss over
time. Now, the current value of the loss is displayed after each epoch of
training and the loss used for training is now configurable. So far, only three
different losses are supported, but it it not difficult to add new loss to the
system. The three losses supported are: Categorical Cross Entropy Loss, Binary
Cross Entropy Loss and Mean Squared Error Loss.</p>
<p>Again, each network can specify the loss to use:</p>
<pre class="code C++"><a name="rest_code_6be3701cabbd4bf584c6631644b9530f-1"></a><span class="c1">// Use a Binary Cross Entropy Loss</span>
<a name="rest_code_6be3701cabbd4bf584c6631644b9530f-2"></a><span class="n">dll</span><span class="o">::</span><span class="n">loss</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">loss_function</span><span class="o">::</span><span class="n">BINARY_CROSS_ENTROPY</span><span class="o">&gt;</span>
</pre>
</div>
<div class="section" id="dropout">
<h2>Dropout</h2>
<p>Dropout is a relatively new technique for neural network training. This is
especially made to reduce overfitting since a large number of sub networks will
be trained and it should prevent co-adaptation between different neurons. This
technique is relatively simple. Indeed, it simply randomly sets to zero some of
the input neurons of layers. At each batch, a new mask will be used and this
should lead to a large number of sub networks being trained.</p>
<p>Here is example of a MLP with Dropout (p=0.5):</p>
<pre class="code C++"><a name="rest_code_932d4caff37a4100b1679ee4d265dea1-1"></a><span class="k">using</span> <span class="n">network_t</span> <span class="o">=</span> <span class="n">dll</span><span class="o">::</span><span class="n">dyn_dbn_desc</span><span class="o">&lt;</span>
<a name="rest_code_932d4caff37a4100b1679ee4d265dea1-2"></a>    <span class="n">dll</span><span class="o">::</span><span class="n">dbn_layers</span><span class="o">&lt;</span>
<a name="rest_code_932d4caff37a4100b1679ee4d265dea1-3"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dense_desc</span><span class="o">&lt;</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">500</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_932d4caff37a4100b1679ee4d265dea1-4"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dropout_layer_desc</span><span class="o">&lt;</span><span class="mi">50</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_932d4caff37a4100b1679ee4d265dea1-5"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dense_desc</span><span class="o">&lt;</span><span class="mi">500</span><span class="p">,</span> <span class="mi">250</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_932d4caff37a4100b1679ee4d265dea1-6"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dropout_layer_desc</span><span class="o">&lt;</span><span class="mi">50</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_932d4caff37a4100b1679ee4d265dea1-7"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dense_desc</span><span class="o">&lt;</span><span class="mi">250</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">activation</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">function</span><span class="o">::</span><span class="n">SOFTMAX</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="o">&gt;</span>
<a name="rest_code_932d4caff37a4100b1679ee4d265dea1-8"></a>    <span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">updater</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">updater_type</span><span class="o">::</span><span class="n">MOMENTUM</span><span class="o">&gt;</span>     <span class="c1">// Momentum</span>
<a name="rest_code_932d4caff37a4100b1679ee4d265dea1-9"></a>    <span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">100</span><span class="o">&gt;</span>                          <span class="c1">// The mini-batch size</span>
<a name="rest_code_932d4caff37a4100b1679ee4d265dea1-10"></a>    <span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">shuffle</span>                                  <span class="c1">// Shuffle before each epoch</span>
<a name="rest_code_932d4caff37a4100b1679ee4d265dea1-11"></a><span class="o">&gt;::</span><span class="n">dbn_t</span><span class="p">;</span>
</pre>
</div>
<div class="section" id="batch-normalization">
<h2>Batch Normalization</h2>
<p>Batch Normalization is another new technique for training neural networks. This
technique will ensure that each of the layer will receive inputs that look
kind of similar. This is a very large advantage since then you reduce the
different in impact of hyper parameters on different layers. Google reported
much faster training with this technique by getting rid of Dropout and by
increasing the learning rate of training.</p>
<p>Here is an example of using Batch Normalization in a CNN:</p>
<pre class="code C++"><a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-1"></a><span class="k">using</span> <span class="n">network_t</span> <span class="o">=</span> <span class="n">dll</span><span class="o">::</span><span class="n">dyn_dbn_desc</span><span class="o">&lt;</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-2"></a>    <span class="n">dll</span><span class="o">::</span><span class="n">dbn_layers</span><span class="o">&lt;</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-3"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">conv_desc</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-4"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">batch_normalization_layer_4d_desc</span><span class="o">&lt;</span><span class="mi">8</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-5"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">mp_layer_2d_desc</span><span class="o">&lt;</span><span class="mi">8</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-6"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">conv_desc</span><span class="o">&lt;</span><span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-7"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">batch_normalization_layer_4d_desc</span><span class="o">&lt;</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-8"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">mp_layer_2d_desc</span><span class="o">&lt;</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-9"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dense_desc</span><span class="o">&lt;</span><span class="mi">8</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">150</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-10"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">batch_normalization_layer_2d_desc</span><span class="o">&lt;</span><span class="mi">150</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-11"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dense_desc</span><span class="o">&lt;</span><span class="mi">150</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">activation</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">function</span><span class="o">::</span><span class="n">SOFTMAX</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="o">&gt;</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-12"></a>    <span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">updater</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">updater_type</span><span class="o">::</span><span class="n">ADADELTA</span><span class="o">&gt;</span>     <span class="c1">// Adadelta</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-13"></a>    <span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">100</span><span class="o">&gt;</span>                          <span class="c1">// The mini-batch size</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-14"></a>    <span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">shuffle</span>                                  <span class="c1">// Shuffle the dataset before each epoch</span>
<a name="rest_code_d53252ce2a0344b1bda177b00e4ec293-15"></a><span class="o">&gt;::</span><span class="n">dbn_t</span><span class="p">;</span>
</pre>
<p>You may notice that the layer is set as 4D so should only be used after
convolutional layer (or after the input). If you want to use it after
fully-connected layers, you can use the 2D version that works the same way.</p>
</div>
<div class="section" id="better-dataset-support">
<h2>Better dataset support</h2>
<p>At the beginning, I designed DLL so that the user could directly pass data for
training in the form of STL Containers such as the std::vector. This is good in
some cases, but in some cases, the user does not know how to read the data , or
does not want to be bothered with it. Therefore, several data sets reader are
now available. Moreover, the entire system has been reworked to use generators
for data. A generator is simply a concept that has some data to produce. The
advantage of this new system is data augmentation is now supported every where
and much more efficiently than before. It is now possible to perform random
cropping and mirroring of images for instance. Moreover, the data augmentation
can be done in a secondary thread so as to be sure that there is always enough
data available for the training.</p>
<p>The library now has a powerful dataset reader for both MNIST and CIFAR-10 and
the reader for ImageNet is almost ready. The project has already been used and
tested with these three datasets now. Moreover, the support for directly passing
STL containers has been maintained. In this case, a generator is simply created
around the data provided in the container and the generator is then passed to
the system for training.</p>
<p>Here for instance is how to read MNIST data and scale (divide) all pixel values
by 255:</p>
<pre class="code c++"><a name="rest_code_35293b9fa5f64ed3a4fee954bc74b4a9-1"></a><span class="c1">// Load the dataset</span>
<a name="rest_code_35293b9fa5f64ed3a4fee954bc74b4a9-2"></a><span class="k">auto</span> <span class="n">dataset</span> <span class="o">=</span> <span class="n">dll</span><span class="o">::</span><span class="n">make_mnist_dataset</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">100</span><span class="o">&gt;</span><span class="p">{},</span> <span class="n">dll</span><span class="o">::</span><span class="n">scale_pre</span><span class="o">&lt;</span><span class="mi">255</span><span class="o">&gt;</span><span class="p">{});</span>
<a name="rest_code_35293b9fa5f64ed3a4fee954bc74b4a9-3"></a><span class="n">dataset</span><span class="p">.</span><span class="n">display</span><span class="p">();</span>
<a name="rest_code_35293b9fa5f64ed3a4fee954bc74b4a9-4"></a>
<a name="rest_code_35293b9fa5f64ed3a4fee954bc74b4a9-5"></a><span class="c1">// Train the network</span>
<a name="rest_code_35293b9fa5f64ed3a4fee954bc74b4a9-6"></a><span class="n">net</span><span class="o">-&gt;</span><span class="n">fine_tune</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">train</span><span class="p">(),</span> <span class="mi">25</span><span class="p">);</span>
<a name="rest_code_35293b9fa5f64ed3a4fee954bc74b4a9-7"></a>
<a name="rest_code_35293b9fa5f64ed3a4fee954bc74b4a9-8"></a><span class="c1">// Test the network</span>
<a name="rest_code_35293b9fa5f64ed3a4fee954bc74b4a9-9"></a><span class="n">net</span><span class="o">-&gt;</span><span class="n">evaluate</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">test</span><span class="p">());</span>
</pre>
</div>
<div class="section" id="much-faster-performance">
<h2>Much faster performance</h2>
<p>I've spent quite a lot of time improving the performance of the framework. I've
focused on every part of training in order to make training of neural networks
as fast as possible. I've also made a comparison of the framework against
several popular machine learning framework (Caffe, TensorFlow, Keras, Torch and
DeepLearning4J). For instance, here are the results on a small CNN experiment on
MNIST with all the different frameworks in CPU mode and in GPU mode:</p>
<img alt="DLL Comparison Against other frameworks" class="align-center" src="../../../images/dll_comparison.png"><p>As you can see, DLL is by far the fastest framework on CPU. On GPU, there is
still some work to be done, but this is already ongoing (although a lot of work
remains). This is confirmed on each of the four experiments performed on MNIST,
CIFAR-10 and ImageNet, although the margin is smaller for larger networks (still
about 40% faster than TensorFlow and Keras which are the fastest framework after
DLL on CPU on my tests).</p>
<p>Overall, DLL is between 2 and 4 times faster than before and is always the
fastest framework for neural network training when training is performed on CPU.</p>
<p>I proposed a talk about these optimizations and performance for Meeting C++ this
year, but it has unfortunately not been accepted. We also have submitted
a publication about the framework to a conference later this year.</p>
</div>
<div class="section" id="examples">
<h2>Examples</h2>
<p>The project now has a few examples (available <a class="reference external" href="https://github.com/wichtounet/dll/tree/master/examples/src">here</a>), well-designed and I try to update them with the latest updates of the framework.</p>
<p>For instance, here is the CNN example for MNIST (without includes):</p>
<pre class="code c++"><a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-1"></a><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="cm">/*argc*/</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="cm">/*argv*/</span> <span class="p">[])</span> <span class="p">{</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-2"></a>    <span class="c1">// Load the dataset</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-3"></a>    <span class="k">auto</span> <span class="n">dataset</span> <span class="o">=</span> <span class="n">dll</span><span class="o">::</span><span class="n">make_mnist_dataset</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">100</span><span class="o">&gt;</span><span class="p">{},</span> <span class="n">dll</span><span class="o">::</span><span class="n">scale_pre</span><span class="o">&lt;</span><span class="mi">255</span><span class="o">&gt;</span><span class="p">{});</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-4"></a>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-5"></a>    <span class="c1">// Build the network</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-6"></a>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-7"></a>    <span class="k">using</span> <span class="n">network_t</span> <span class="o">=</span> <span class="n">dll</span><span class="o">::</span><span class="n">dyn_dbn_desc</span><span class="o">&lt;</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-8"></a>        <span class="n">dll</span><span class="o">::</span><span class="n">dbn_layers</span><span class="o">&lt;</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-9"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">conv_desc</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-10"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">mp_layer_2d_desc</span><span class="o">&lt;</span><span class="mi">8</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-11"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">conv_desc</span><span class="o">&lt;</span><span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-12"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">mp_layer_2d_desc</span><span class="o">&lt;</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-13"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">dense_desc</span><span class="o">&lt;</span><span class="mi">8</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">150</span><span class="o">&gt;::</span><span class="n">layer_t</span><span class="p">,</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-14"></a>            <span class="n">dll</span><span class="o">::</span><span class="n">dense_desc</span><span class="o">&lt;</span><span class="mi">150</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">activation</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">function</span><span class="o">::</span><span class="n">SOFTMAX</span><span class="o">&gt;&gt;::</span><span class="n">layer_t</span><span class="o">&gt;</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-15"></a>        <span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">updater</span><span class="o">&lt;</span><span class="n">dll</span><span class="o">::</span><span class="n">updater_type</span><span class="o">::</span><span class="n">MOMENTUM</span><span class="o">&gt;</span>     <span class="c1">// Momentum</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-16"></a>        <span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">batch_size</span><span class="o">&lt;</span><span class="mi">100</span><span class="o">&gt;</span>                          <span class="c1">// The mini-batch size</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-17"></a>        <span class="p">,</span> <span class="n">dll</span><span class="o">::</span><span class="n">shuffle</span>                                  <span class="c1">// Shuffle the dataset before each epoch</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-18"></a>    <span class="o">&gt;::</span><span class="n">dbn_t</span><span class="p">;</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-19"></a>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-20"></a>    <span class="k">auto</span> <span class="n">net</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">network_t</span><span class="o">&gt;</span><span class="p">();</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-21"></a>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-22"></a>    <span class="n">net</span><span class="o">-&gt;</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">;</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-23"></a>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-24"></a>    <span class="c1">// Display the network and dataset</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-25"></a>    <span class="n">net</span><span class="o">-&gt;</span><span class="n">display</span><span class="p">();</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-26"></a>    <span class="n">dataset</span><span class="p">.</span><span class="n">display</span><span class="p">();</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-27"></a>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-28"></a>    <span class="c1">// Train the network</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-29"></a>    <span class="n">net</span><span class="o">-&gt;</span><span class="n">fine_tune</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">train</span><span class="p">(),</span> <span class="mi">25</span><span class="p">);</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-30"></a>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-31"></a>    <span class="c1">// Test the network on test set</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-32"></a>    <span class="n">net</span><span class="o">-&gt;</span><span class="n">evaluate</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">test</span><span class="p">());</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-33"></a>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-34"></a>    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<a name="rest_code_6fa608bf27654d77a5cb8fa898bf3476-35"></a><span class="p">}</span>
</pre>
</div>
<div class="section" id="reproducible-results">
<h2>Reproducible results</h2>
<p>And last, but maybe not least, I've finally united all the random number
generation code. This means that DLL can now set a global seed and that two
training of the same network and data with the same seed will now produce
exactly the same result.</p>
<p>The usage is extremely simple:</p>
<pre class="code c++"><a name="rest_code_46b31164a5e14a1a996ad7008a7f76b5-1"></a><span class="n">dll</span><span class="o">::</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">);</span>
</pre>
</div>
<div class="section" id="conclusion">
<h2>Conclusion</h2>
<p>After all these changes, I truly feel that the library is now in a much better
state and could be useful in several projects. I hope that this will be useful
to some more people. Moreover, as you can see by the performance results, the
framework is now extremely efficient at training neural networks on CPU.</p>
<p>If you want more information, you can consult the
<a class="reference external" href="https://github.com/wichtounet/dll">dll Github Repository</a>. You can also add
a comment to this post. If you find any problem on the project or have specific
question or request, don't hesitate to open an issue on Github.</p>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../../categories/deep-learning.html" rel="tag">deep learning</a></li>
            <li><a class="tag p-category" href="../../../categories/dll.html" rel="tag">dll</a></li>
            <li><a class="tag p-category" href="../../../categories/etl.html" rel="tag">etl</a></li>
            <li><a class="tag p-category" href="../../../categories/publications.html" rel="tag">publications</a></li>
            <li><a class="tag p-category" href="../../../categories/thesis.html" rel="tag">thesis</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../06/jenkins-tip-send-notifications-fixed-builds-declarative-pipeline.html" rel="prev" title="Jenkins Tip: Send notifications on fixed builds in declarative pipeline">Previous post</a>
            </li>
            <li class="next">
                <a href="../08/expression-templates-library-etl-11.html" rel="next" title="Expression Templates Library (ETL) 1.1">Next post</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
        
        
        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="blogwichtounet",
            disqus_url="http://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html",
        disqus_title="Update on Deep Learning Library (DLL): Dropout, Batch Normalization, Adaptive Learning Rates, ...",
        disqus_identifier="cache/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="blogwichtounet";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div> <!-- col -->
    </div>
<!-- row  -->
</div>
<!-- container-fluid -->

<!-- End of Menubar -->

<!-- Footer -->

<footer>
    Contents © 2017         <a href="mailto:baptistewicht@gmail.com">Baptiste Wicht</a> - Powered by         <a href="http://getnikola.com" rel="nofollow">Nikola</a>         - License: 
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="padding-left:5px;border-width:0" src="../../../assets/img/cc.png"></a>
        <ul class="footer_inline_ul">
<li>
    <a href="update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.rst" id="sourcelink">Source</a>
    </li>

        </ul></footer><!-- Late loading stuff  --><script src="../../../assets/js/all-nocdn.js"></script><!-- Google platform JS -->
</body>
</html>
