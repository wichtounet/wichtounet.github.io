<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Comparison of DLL against popular machine learning frameworks, showing its speed advantage.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>DLL: Blazing Fast Neural Network Library | Blog blog("Baptiste Wicht");</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.xml">
<link rel="canonical" href="http://baptiste-wicht.com/posts/2017/08/dll-blazing-fast-neural-network-library.html">
<!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Baptiste Wicht">
<link rel="prev" href="compiler-benchmark-gcc-clang-cpp-library-etl.html" title="Compiler benchmark GCC and Clang on C++ library (ETL)" type="text/html">
<link rel="next" href="how-to-fix-mdadm-raid5-raid6-growing-stuck-at-0ks.html" title="How to fix mdadm RAID5 / RAID6 growing stuck at 0K/s ?" type="text/html">
<meta property="og:site_name" content='Blog blog("Baptiste Wicht");'>
<meta property="og:title" content="DLL: Blazing Fast Neural Network Library">
<meta property="og:url" content="http://baptiste-wicht.com/posts/2017/08/dll-blazing-fast-neural-network-library.html">
<meta property="og:description" content="Comparison of DLL against popular machine learning frameworks, showing its speed advantage.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-08-11T11:09:14+02:00">
<meta property="article:tag" content="C++">
<meta property="article:tag" content="dll">
<meta property="article:tag" content="etl">
<meta property="article:tag" content="GPU">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="projects">
<link href="../../../favicon.ico" rel="icon" type="image/x-icon">
<link rel="publisher" href="https://plus.google.com/+BaptisteWicht">
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-2175227-7', 'auto');
  var metas = document.getElementsByTagName('meta'), tagsList = [];
  for (var i=0; i<metas.length; i++) {
    if (metas[i].getAttribute('property') == 'article:tag') {
      tagsList.push( metas[i].getAttribute('content'));
    }
  }
  ga('set', 'dimension1', tagsList.join('|'));
  ga('send', 'pageview');
</script>
</head>
<body>

<!-- Menubar -->

<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<div class="container-fluid">
<!-- This keeps the margins nice -->
    <div class="row">
        <div class="col-sm-3 col-lg-2">
            <nav class="navbar navbar-inverse navbar-fixed-side"><div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="http://baptiste-wicht.com/">
                        <span id="blog-title">Blog blog("Baptiste Wicht");</span>
                    </a>
                </div>
<!-- /.navbar-header -->

                <div class="collapse navbar-collapse navbar-ex1-collapse">
                    <ul class="nav navbar-nav">
<li>
<a href="../../../stories/about.html">About</a>
                </li>
<li>
<a href="../../../stories/publications.html">Publications</a>
                </li>
<li>
<a href="../../../stories/projects.html">Projects</a>
                </li>
<li>
<a href="../../../categories/index.html">Tags</a>
                </li>
<li>
<a href="../../../archive.html">Archives</a>
                </li>
<li>
<a href="http://feeds.feedburner.com/BaptisteWicht">RSS</a>



                            </li>
<li class="navbar-content">
                                <h3>Related posts</h3>
                            </li>
                            

                            <li><a href="../07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html">Update on Deep Learning Library (DLL): Dropout, Batch Normalization, Adaptive Learning Rates, ...</a></li>
<li><a href="../10/dll-new-features-embeddings-and-merge-layers.html">DLL New Features: Embeddings and Merge layers</a></li>
<li><a href="../10/expression-templates-library-etl-1-2-complete-gpu-support.html">Expression Templates Library (ETL) 1.2 - Complete GPU support</a></li>
<li><a href="../02/publication-cpu-performance-optimizations-rbm-crbm.html">Publication: CPU Performance Optimizations for RBM and CRBM</a></li>
<li><a href="../10/deep-learning-library-10-fast-neural-network-library.html">Deep Learning Library 1.0 - Fast Neural Network Library</a></li>
<li><a href="../01/publications-sudoku-recognition-with-deep-belief-network.html">Publications - Sudoku Recognition with Deep Belief Network</a></li>


                            <li class="navbar-block">

                        <li class="wicht-navbar-right">
                            <a target="_blank" title="Follow @wichtounet on Twitter" href="https://twitter.com/wichtounet">
                                <img src="../../../assets/img/twitter.png" alt="Follow @wichtounet on Twitter"></a>
                        </li>

                        <li class="wicht-navbar-right">
                            <a target="_blank" title="Follow +BaptisteWicht on Google+" href="https://plus.google.com/+BaptisteWicht">
                                <img src="../../../assets/img/google_plus.png" alt="Follow +BaptisteWicht on Google+"></a>
                        </li>


                    </ul>
</div>
<!-- /.navbar-collapse -->
            </nav>
</div> <!-- col -->
        <div class="col-sm-9 col-lg-10">
            <div id="content"></div>
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="#" class="u-url">DLL: Blazing Fast Neural Network Library</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Baptiste Wicht
            </span></p>
            <p class="dateline"><a href="#" rel="bookmark"><time class="published dt-published" datetime="2017-08-11T11:09:14+02:00" itemprop="datePublished" title="2017-08-11 11:09">2017-08-11 11:09</time></a></p>
                <p class="commentline">
        
    <a href="dll-blazing-fast-neural-network-library.html#disqus_thread" data-disqus-identifier="cache/posts/2017/08/dll-blazing-fast-neural-network-library.html">Comments</a>


            
        </p>
<p class="sourceline"><a href="dll-blazing-fast-neural-network-library.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>A few weeks ago, I talked about all
<a class="reference external" href="https://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html">the new features of my Deep Learning Library (DLL)</a>
project. I've mentioned that, on several experiments, DLL was always
significantly faster than some popular deep learning frameworks such as
TensorFlow. I'll now go into more details into this comparison and provide all
the results. So far, the paper we wrote about these results has not been
published, so I'll not provide the paper directly yet.</p>
<p>For those that may not know, DLL is the project I've been developing to support
my Ph.D. thesis. This is a neural network framework  that supports
Fully-Connected Neural Network (FCNN), Convolutional Neural Network (CNN),
Restricted Boltzmann Machine (RBM), Deep Belief Network (DBN), Convolutional RBM
(CRBM) and Convolutional DBN (CDBN). It also supports a large variety of options
such as Dropout, Batch Normalization and Adaptive Learning Rates. You can read
read the
<a class="reference external" href="https://baptiste-wicht.com/posts/2017/07/update-on-deep-learning-library-dll-dropout-batch-normalization-adaptive-learning-rates.html">previous post</a>
if you want more information about the new features of the framework. And, as those of
you that read my blog frequently may know, I'm a bit obsessed with performance
optimization, so I've spent a considerable amount of time optimizing
the performance of neural network training, on CPU. Since, at the beginning of my
thesis, I had no access to GPU for training, I've focused on CPU. Although there
is now support for GPU, the gains are not yet important enough.</p>
<div class="section" id="evaluation">
<h2>Evaluation</h2>
<p>To see how fast, or not, the library was, it was compared against five popular
machine learning libraries:</p>
<ol class="arabic simple">
<li>Caffe, installed from sources</li>
<li>TensorFlow 1.0, from pip</li>
<li>Keras 2.0, from pip</li>
<li>Torch, installed from sources</li>
<li>DeepLearning4J 0.7, from Maven</li>
</ol>
<p>I've run four different experiments with all these frameworks and compared the
efficiency of each of them for training the same neural networks with the same
options. In each case, the training or testing error have also been compared to
ensure that each framework is doing roughly the same. I wont present here the
details, but in each experiment DLL showed around the same accuracies as the
other frameworks. I will only focus on the speed results in this article.</p>
<p>Each experiment is done once with only CPU and once with a GPU. For DLL, I only
report the CPU time in both modes, since it's more stable and more optimized.</p>
<p>The code for the evaluation is available online on the
<a class="reference external" href="https://github.com/wichtounet/frameworks">Github repository of the frameworks project</a>.</p>
</div>
<div class="section" id="mnist-fully-connected-neural-network">
<h2>MNIST: Fully Connected Neural Network</h2>
<p>The first experiment is performed on The MNIST data set. It consists of 60'000
grayscale images of size 28x28. The goal is to classify each image of a digit
from 0 to 9. To solve this task, I trained a very small fully-connected neural
network with 500 hidden units in the first layer, 250 in the second and 10 final
hidden units (or output units) for classification. The first two layers are
using the logistic sigmoid activation function and the last layer is using the
softmax activation function. The network is trained for 50 epochs with a
categorical cross entropy loss, with mini-batches of 100 images. Here are
results of this experiment:</p>
<div class="figure align-center">
<img alt="Training time performance for the different frameworks on the Fully-Connected Neural Network experiment, on MNIST." src="../../../images/dll_fcnn.png"><p class="caption">Training time performance for the different frameworks on the Fully-Connected
Neural Network experiment, on MNIST. All the times are in seconds.</p>
</div>
<p>In DLL mode, the DLL framework is the clear winner here! It's about 35% faster
than TensorFlow and Keras which are coming at the second place. DLL is more than
four times slower than DLL and the last two frameworks (Caffe and
DeepLearning4J) are five times slower than DLL! Once we add a GPU to the system,
the results are very different. Caffe is now the fastest framework, three times
faster than DLL. DLL is less than two times slower than Keras and TensorFlow.
Interestingly, DLL is still faster than Torch and DeepLearning4J.</p>
</div>
<div class="section" id="mnist-convolutional-neural-network">
<h2>MNIST: Convolutional Neural Network</h2>
<p>Although a Fully-Connected Neural Network is an interesting tool, the trend now
is to use Convolutional Neural Network which have proved very efficient at
solving a lot of problems. The second experiment is also using the same data
set. Again, it's a rather small network. The first layer is a convolutional
layer with 8 5x5 kernels, followed by max pooling layer with 2x2 kernel. They
are followed by one more convolutional layers with 8 5x5 kernels and a 2x2 max
pooling layer. These first four layers are followed by two fully-connected
layers, the first with 150 hidden units and the last one with 10 output units.
The activation functions are the same as for the first network, as is the
training procedure. This takes significantly longer to train than the first
network because of the higher complexity of the convolutional layers compared to
the fully-connected layers even though they have much less weights. The results
are present in the next figure:</p>
<div class="figure align-center">
<img alt="Training time performance for the different frameworks on the Convolutional Neural Network experiment, on MNIST." src="../../../images/dll_cnn.png"><p class="caption">Training time performance for the different frameworks on the Convolutional
Neural Network experiment, on MNIST. All the times are in seconds.</p>
</div>
<p>Again, on CPU, DLL is the clear winner, by a lot! It's already 3.6 times faster
than the second frameworks Keras and TensorFlow, more than four times faster
than Caffe and Torch and 8 times faster than DeepLearning4J that is proving very
slow on this experiment. Once a GPU is added, Keras and TensorFlow are about
twice faster than DLL. However, DLL is still faster than the other frameworks
even though they are taking advantage of the GPU.</p>
</div>
<div class="section" id="cifar-10">
<h2>CIFAR-10</h2>
<p>The second data set that is tested is the CIFAR-10 data set. It's an object
recognition with 10 classes for classification. The training set is composed of
50'000 colour images for 32x32 pixels. The network that is used for this data
set is similar in architecture than the first network, but has more parameters.
The first convolutional layer now has 12 5x5 kernels and the second
convolutional layer has 24 3x3 kernels. The pooling layers are the same. The
first fully-connected has 64 hidden units and the last one has 10 output units.
The last layer again use a softmax activation function while the other layers
are using Rectifier Linear Units (ReLU). The training is done in the same manner
as for the two first networks. Unfortunately, it was not possible to train
DeepLearning4J on this data set, even though there is official support for this
data set. Since I've had no answer to my question regarding this issue, the
results are simply removed from this experiment. It may not seem so but it's
considerably longer to train this network because of the larger number of input
channels and larger number of convolutional kernels in each layer. Let's get to
the results now:</p>
<div class="figure align-center">
<img alt="Training time performance for the different frameworks on the Convolutional Neural Network experiment, on CIFAR-10." src="../../../images/dll_cifar10.png"><p class="caption">Training time performance for the different frameworks on the Convolutional
Neural Network experiment, on CIFAR-10. All the times are in seconds.</p>
</div>
<p>DLL is still the fastest on CPU, but the margin is less than before. It's about
40% faster than TensorFlow and Keras, twice faster than Torch and 2.6 times
faster than Caffe. Once a GPU is added, DLL is about as fast as Torch but slower
than the other three frameworks. TensorFlow and Keras are about four times
faster than DLL while Caffe is about twice faster than DLL. We can see that
with this larger network, the GPU becomes more interesting and that there is
a smaller margin for improvements compared to the other frameworks.</p>
</div>
<div class="section" id="imagenet">
<h2>ImageNet</h2>
<p>The last experiment is made on the ImageNet data set. I used the ILSVRC 2012
subset, that consists "only" of about 1.2 million images for training. I've
resized all the images to 256x256 pixels, this makes for 250 times more colour
values than a MNIST image. This dimension and the number of images makes it
impractical to keep the dataset in memory. The images must be loaded in batch
from the disk. No random cropping or mirroring was performed. The network is
much larger to solve this task. The network starts with 5 pairs of convolutional
layers and max pooling layers. The convolutional layers have 3x3 kernels, 16 for
the first two layers and 32 for the three following one. The five max pooling
layers use 2x2 kernels. Each convolutional layer uses zero-padding so that their
output features are the same dimensions as the input. They are followed by two
fully-connected layer. The first one with 2048 hidden units and the last one
with 1000 output units (one for each class). Except for the last layer, using
softmax, the layers all uses ReLU. The network is trained with mini-batches of
128 images (except for DeepLearning4J and Torch, which can only use 64 images on
the amount of RAM available on my machine). To ease the comparison, I report the
time necessary to train one batch of data (or two for DeepLearning4J and Torch).
The results, presented in logarithmic scale because of DeepLearning4J disastrous
results, are as follows:</p>
<div class="figure align-center">
<img alt="Training time performance for the different frameworks on the Convolutional Neural Network experiment, on ImageNet." src="../../../images/dll_imagenet.png"><p class="caption">Training time performance for the different frameworks on the Convolutional
Neural Network experiment, on ImageNet. The times are the time necessary to
train a batch of 128 images. All the times are in milliseconds.</p>
</div>
<p>For this final experiment, DLL is again significantly faster than all the other
frameworks. It's about 40% faster than Keras, twice faster than TensorFlow and
Caffe and more than three times faster than Torch. Although 40% may seem not
that much, don't forget that this kind of training may take days, so it can save
you a lot of time. All the frameworks are much faster than DeepLearning4J. Based
on several posts on the internet, I suspect that this comes from the model of
GPU I have been used (GTX 960), but all the other frameworks seem to handle this
card pretty well.</p>
</div>
<div class="section" id="conclusion">
<h2>Conclusion</h2>
<p>I hope this is not too much of a bragging post :P We can see that my efforts to
make the code as fast as possible have paid :) As was shown in the experiments,
my DLL framework is always the fastest framework when the neural network is
trained on CPU. I'm quite pleased with the results since I've done a lot of work
to optimize the speed as much as possible and since I'm competing with
well-known libraries that have been developed by several persons.  Moreover, the
accuracies of the trained networks is similar to that of the networks trained
with the other frameworks. Even when the other frameworks are using GPU, the
library still remains competitive, although never the fastest.</p>
<p>In the next step (I've no idea when I'll have the time though), I will want to
focus on GPU speed. This will mostly come from a better support of the GPU in
the ETL library on which DLL is based. I have many ideas to improve it a lot,
but it will take me a lot of time.</p>
<p>If you want more information on the DLL library, you can have a look at
<a class="reference external" href="https://github.com/wichtounet/dll">its Github repository</a> and especially at
<a class="reference external" href="https://github.com/wichtounet/dll/tree/master/examples/src">the few examples</a>.
You can also have a look at <a class="reference external" href="https://baptiste-wicht.com/categories/dll.html">my posts about DLL</a>.
Finally, don't hesitate to comment or contact me through Github issues if you
have comments or problems with this post, the library or anything ;)</p>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../../categories/c%2B%2B.html" rel="tag">C++</a></li>
            <li><a class="tag p-category" href="../../../categories/dll.html" rel="tag">dll</a></li>
            <li><a class="tag p-category" href="../../../categories/etl.html" rel="tag">etl</a></li>
            <li><a class="tag p-category" href="../../../categories/gpu.html" rel="tag">GPU</a></li>
            <li><a class="tag p-category" href="../../../categories/machine-learning.html" rel="tag">Machine Learning</a></li>
            <li><a class="tag p-category" href="../../../categories/projects.html" rel="tag">projects</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="compiler-benchmark-gcc-clang-cpp-library-etl.html" rel="prev" title="Compiler benchmark GCC and Clang on C++ library (ETL)">Previous post</a>
            </li>
            <li class="next">
                <a href="how-to-fix-mdadm-raid5-raid6-growing-stuck-at-0ks.html" rel="next" title="How to fix mdadm RAID5 / RAID6 growing stuck at 0K/s ?">Next post</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
        
        
        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="blogwichtounet",
            disqus_url="http://baptiste-wicht.com/posts/2017/08/dll-blazing-fast-neural-network-library.html",
        disqus_title="DLL: Blazing Fast Neural Network Library",
        disqus_identifier="cache/posts/2017/08/dll-blazing-fast-neural-network-library.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="blogwichtounet";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div> <!-- col -->
    </div>
<!-- row  -->
</div>
<!-- container-fluid -->

<!-- End of Menubar -->

<!-- Footer -->

<footer>
    Contents © 2017         <a href="mailto:baptistewicht@gmail.com">Baptiste Wicht</a> - Powered by         <a href="http://getnikola.com" rel="nofollow">Nikola</a>         - License: 
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="padding-left:5px;border-width:0" src="../../../assets/img/cc.png"></a>
        <ul class="footer_inline_ul">
<li>
    <a href="dll-blazing-fast-neural-network-library.rst" id="sourcelink">Source</a>
    </li>

        </ul></footer><!-- Late loading stuff  --><script src="../../../assets/js/all-nocdn.js"></script><!-- Google platform JS -->
</body>
</html>
